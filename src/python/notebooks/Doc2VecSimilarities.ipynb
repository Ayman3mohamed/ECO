{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../v4/misc/data_access.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "import textacy\n",
    "from tqdm import tqdm, trange\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using spacy to kick out stop words, get the word lemmas and detect entities. Entity recognition is a bit funky.\n",
    "We are using textacy for lazyness. Otherwise we might speed up things with a custom spacy pipeline.\n",
    "\n",
    "Since building a doc2vec model requires to run over the text twice (once for building the dictionary and once for the model building) , we store the TaggedDocuments in a file (temporary).\n",
    "- StreamTaggedDocumentobject: parses and stores the TaggedDocuments in a json file\n",
    "- StreamTaggedDocumentFile: rereads the TaggedDocuments for the model building\n",
    "\n",
    "**StreamTaggedDocumentobject** allows to set named_entity_tags, which would keep named entities that Spacy finds to the tags of an document. However when getting the results of similarity search, it would sometimes give one of these tags instead of the index, which fucks everything up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_folder = get_data_folder() + 'NAIL_DATAFIELD_txt/parsed_v3/merged/'\n",
    "\n",
    "def name_to_file_name(author_name, file_format = 'txt'):\n",
    "    return (author_name + '.' + file_format).replace(\",\",\"_\")\n",
    "\n",
    "def get_author_file(author_name):\n",
    "    return base_folder + name_to_file_name(author_name)\n",
    "\n",
    "def sentence_to_clean_word_list(text,get_named_entities = False):\n",
    "    s_doc = textacy.Doc(text,lang='en').spacy_doc\n",
    "    for t in s_doc:\n",
    "        clean_words = [t.lemma_ for t in s_doc if not (t.is_punct or t.is_stop or t.lemma_ == '' or t.is_space)]\n",
    "    if not get_named_entities:\n",
    "        return clean_words, _\n",
    "    else:\n",
    "        return clean_words, [entity.text for entity in s_doc.ents]\n",
    "        \n",
    "def get_similar(model, words, top_n):\n",
    "    inferred_vector = model.infer_vector(words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=top_n)\n",
    "    return sims\n",
    "\n",
    "def author_model_path(author_name):\n",
    "    return get_model_folder() + \"doc2vec/\"+ name_to_file_name(author_name,'doc2vec')\n",
    "\n",
    "def save_model(model, mode_file_path):\n",
    "    model.clear_sims()\n",
    "    model.delete_temporary_training_data()\n",
    "    model.save(mode_file_path)\n",
    "    \n",
    "def load_author_model(author_name):\n",
    "    return Doc2Vec.load(author_model_path(author_name))\n",
    "        \n",
    "        \n",
    "class StreamTaggedDocumentobject:\n",
    "    \n",
    "    def __init__(self, file_path, store_while_streaming = True, named_entity_tags = False):\n",
    "        self.file_path = file_path\n",
    "        self.store_while_streaming = store_while_streaming\n",
    "        self.temp_tagged_docs_file_path = self.file_path + '.taggeddocs.json'\n",
    "        self.named_entity_tags = named_entity_tags\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.store_while_streaming:\n",
    "            temp_file = open(self.temp_tagged_docs_file_path,'w')\n",
    "        for uid, line in enumerate(open(self.file_path)):\n",
    "            words, entities = sentence_to_clean_word_list(line, self.named_entity_tags)\n",
    "            tags = [uid]\n",
    "            if self.named_entity_tags:\n",
    "                tags += entities\n",
    "            tagged_doc = TaggedDocument(words=words, tags=tags)\n",
    "            if self.store_while_streaming:                    \n",
    "                temp_file.write(json.dumps({\"words\":tagged_doc.words, \"tags\":tagged_doc.tags}) + '\\n')\n",
    "            yield tagged_doc\n",
    "\n",
    "\n",
    "class StreamTaggedDocumentFile:\n",
    "\n",
    "    def __init__(self,tagged_doc_file_path):\n",
    "        self.file_path = tagged_doc_file_path\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with open(self.file_path) as fin:\n",
    "            for line in fin:\n",
    "                td = json.loads(line)\n",
    "                yield TaggedDocument(words = td[\"words\"], tags = td[\"tags\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a  model from an arbitrary text file or an ECO v4 Author based model with one call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(corpus_file_path, model_file_path):\n",
    "    model = Doc2Vec(size=100,  min_count=2, iter=55)\n",
    "    tagged_docs_stream = StreamTaggedDocumentobject(corpus_file_path)\n",
    "    print(\"building model from\",corpus_file_path)\n",
    "    print(\"building vocab...\")\n",
    "    model.build_vocab(tagged_docs_stream)\n",
    "    tagged_docs_stream = StreamTaggedDocumentFile(tagged_docs_stream.temp_tagged_docs_file_path)\n",
    "    print(\"training model...\")\n",
    "    model.train(tagged_docs_stream, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    print(model.estimate_memory())\n",
    "    print(\"saving model to\", model_file_path)\n",
    "    save_model(model, model_file_path)\n",
    "    print('done')\n",
    "    \n",
    "    \n",
    "def create_author_model(author_name):\n",
    "    create_model(get_author_file(author_name),author_model_path(author_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_author_model(\"Chomsky,Noam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create 2 classes to work with models, so to load them and to grab the associated text document.\n",
    "- Doc2VecSimilarities: is for arbitrary doc2vec models \n",
    "- AuthorDoc2VecSimilarities for Authors (simpler call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecSimilarities:\n",
    "    \n",
    "    def __init__(self,model_file_path, text_file_path):\n",
    "        print(\"Loading model\")\n",
    "        self.model = Doc2Vec.load(model_file_path)\n",
    "        print(\"Reading corpus\")\n",
    "        self.text = list(textacy.fileio.read.read_file_lines(text_file_path))\n",
    "        \n",
    "    def get_top_n(self, \n",
    "                  sentence, \n",
    "                  top_n = 10, \n",
    "                  trash_scores = True,\n",
    "                  only_indices = False,\n",
    "                  min_score = 0):\n",
    "        words = sentence_to_clean_word_list(sentence)[0]\n",
    "        results = get_similar(self.model, words, top_n)\n",
    "        return_list = []\n",
    "        for result in results:\n",
    "            if result[1] > min_score:\n",
    "                sentence = int(result[0]) if only_indices else self.text[result[0]].strip()\n",
    "                res = sentence if trash_scores else (sentence,result[1])\n",
    "                return_list.append(res)\n",
    "        return return_list\n",
    "\n",
    "    \n",
    "    def get_all_more_similar_then(self, \n",
    "                                  sentence, \n",
    "                                  min_score = 0.8, \n",
    "                                  max_results = 10, \n",
    "                                  trash_scores = True, \n",
    "                                  only_indices = False):\n",
    "        return self.get_top_n(sentence,max_results, trash_scores,only_indices,min_score)\n",
    "        \n",
    "    def get_wmdistance(self):\n",
    "        return self.model.wv.wmdistance('germany, europe, politics','england war, terror, drons')\n",
    "        \n",
    "class AuthorDoc2VecSimilarities(Doc2VecSimilarities):\n",
    "\n",
    "    def __init__(self,author_name):\n",
    "        Doc2VecSimilarities.__init__(self, author_model_path(author_name),get_author_file(author_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = AuthorDoc2VecSimilarities(\"Chomsky,Noam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** HOLY CRAP, THE RESULTS ARE NON-DETERMINSTIC??? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(json.dumps(sim.get_top_n(\"West germany is the shit. literally it is shit\", 10),indent=2))\n",
    "print(json.dumps(sim.get_all_more_similar_then(\"fact is freedom in Nicaragua is a fraud .\", 0.55, trash_scores= False),indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway. Let's create a second Author model and check for all chomsky sentences if there is a similar Goldman sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_author_model(\"Goldman,Emma\")\n",
    "sim2 = AuthorDoc2VecSimilarities(\"Goldman,Emma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim2sim2_top_5 = []\n",
    "for sentence in tqdm(sim.text):\n",
    "    simi = sim2.get_top_n(sentence,5, trash_scores=False, only_indices = True)\n",
    "    sim2sim2_top_5.append(simi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_max = (0,0,0)\n",
    "\n",
    "thresh = 0.8\n",
    "all_goods = []\n",
    "\n",
    "for index,res in enumerate(sim2sim2_top_5):\n",
    "    best = res[0]\n",
    "    if best[1] > thresh:\n",
    "        all_goods.append((index,best[0],best[1]))\n",
    "    if best[1] > total_max[2]:\n",
    "        total_max = (index,best[0],best[1])\n",
    "        \n",
    "print(total_max)\n",
    "print(len(all_goods))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
