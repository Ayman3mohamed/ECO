{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_project_folder(): /Users/raminsoleymani/git/ECO/\n",
      "get_data_folder(): /Users/raminsoleymani/git/ECO/data/\n",
      "get_model_folder(): /Users/raminsoleymani/git/ECO/models/\n"
     ]
    }
   ],
   "source": [
    "%run ../../v4/misc/data_access.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documentes\n",
      " 4741\n",
      "\n",
      "Number of authors\n",
      " 2554\n",
      "\n",
      "Authors with more then 10 documents ( 39 )\n",
      "Counter({'No Author': 214, 'Chomsky,Noam': 73, 'October': 67, 'Bogost,Ian': 39, 'Hayles,Katherine': 36, 'Nader,Laura': 35, 'Bakunin,Mikhail': 35, 'Goldman,Emma': 34, 'Black,Bob': 31, 'Zerzan,John': 31, 'Malatesta,Errico': 30, 'Kropotkin,Peter': 30, 'Garland,David': 24, 'Thacker,Eugene': 23, 'Foster,Roger': 19, 'Bookchin,Murray': 19, 'Agrawal,Nelson': 19, 'Galloway,Alexander': 18, 'Harvey,Adrian': 17, 'Krauss,Rosalind': 17, 'Churchill,Ward': 17, 'Foster,Hal': 15, 'Bonanno,Alfredo': 15, 'Mag√≥n,Flores': 14, 'Avrich,Paul': 14, 'De Cleyre': 13, 'Franks,Benjamin': 13, 'Lovink,Geert': 13, 'Smil,Vaclav': 13, 'Home,Stewart': 13, 'Berkman,Alexander': 12, 'Danto,Arthur': 12, 'Valverde,Mariana': 12, 'Newman,James': 11, 'Cae': 11, 'Smith,Stephen': 11, 'Mitchell,William': 11, 'Weiss,Allen': 11, 'Bell,David': 11})\n",
      "\n",
      "Top 50 Authors with accumulated file sizes:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>docs</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>October</td>\n",
       "      <td>67</td>\n",
       "      <td>6399579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Goldman,Emma</td>\n",
       "      <td>34</td>\n",
       "      <td>3969364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Featherstone,Mike</td>\n",
       "      <td>6</td>\n",
       "      <td>3404807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fieser</td>\n",
       "      <td>1</td>\n",
       "      <td>3363083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chomsky,Noam</td>\n",
       "      <td>73</td>\n",
       "      <td>3349165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sullivan</td>\n",
       "      <td>5</td>\n",
       "      <td>2882483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lovink,Geert</td>\n",
       "      <td>13</td>\n",
       "      <td>2819149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Anttiroiko,Ari Veikko</td>\n",
       "      <td>1</td>\n",
       "      <td>2799658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Smith,Stephen</td>\n",
       "      <td>11</td>\n",
       "      <td>2693772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Charles Sanders Pierce</td>\n",
       "      <td>1</td>\n",
       "      <td>2556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lessig,Lawrence</td>\n",
       "      <td>6</td>\n",
       "      <td>2490135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chemerinsky,Erwin</td>\n",
       "      <td>2</td>\n",
       "      <td>2440437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Cutler</td>\n",
       "      <td>1</td>\n",
       "      <td>2161596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Philander</td>\n",
       "      <td>1</td>\n",
       "      <td>2115106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Smil,Vaclav</td>\n",
       "      <td>13</td>\n",
       "      <td>2071748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Elkins,James</td>\n",
       "      <td>10</td>\n",
       "      <td>2014350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lawson,Bryan</td>\n",
       "      <td>6</td>\n",
       "      <td>2009663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Daly,Herman</td>\n",
       "      <td>4</td>\n",
       "      <td>1962590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Marshall,Peter</td>\n",
       "      <td>2</td>\n",
       "      <td>1920920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Krech Iii,Shepard</td>\n",
       "      <td>2</td>\n",
       "      <td>1820464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Shaw,Malcolm</td>\n",
       "      <td>5</td>\n",
       "      <td>1820195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Demeny,Mcnicoll</td>\n",
       "      <td>1</td>\n",
       "      <td>1806941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Johnson,Cb</td>\n",
       "      <td>6</td>\n",
       "      <td>1725486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Castells,Haraway</td>\n",
       "      <td>9</td>\n",
       "      <td>1673842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Berkman,Alexander</td>\n",
       "      <td>12</td>\n",
       "      <td>1644056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Diamond,Jared</td>\n",
       "      <td>5</td>\n",
       "      <td>1643844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Jensen,Derrick</td>\n",
       "      <td>8</td>\n",
       "      <td>1605418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Drake,William</td>\n",
       "      <td>2</td>\n",
       "      <td>1583715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Lewis,Troy</td>\n",
       "      <td>4</td>\n",
       "      <td>1560278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Kropotkin,Peter</td>\n",
       "      <td>30</td>\n",
       "      <td>1540878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Rawls</td>\n",
       "      <td>5</td>\n",
       "      <td>1527332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Costanza,Robert</td>\n",
       "      <td>3</td>\n",
       "      <td>1471278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Drahos,Peter</td>\n",
       "      <td>4</td>\n",
       "      <td>1389463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Ehrlich,Paul</td>\n",
       "      <td>1</td>\n",
       "      <td>1381921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Biggs,Michael</td>\n",
       "      <td>2</td>\n",
       "      <td>1375866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Thacker,Eugene</td>\n",
       "      <td>23</td>\n",
       "      <td>1374614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Jennings,Marianne</td>\n",
       "      <td>1</td>\n",
       "      <td>1322779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Allaby,Michael</td>\n",
       "      <td>1</td>\n",
       "      <td>1299210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Henderson,Ann</td>\n",
       "      <td>2</td>\n",
       "      <td>1276188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Hayles,Katherine</td>\n",
       "      <td>36</td>\n",
       "      <td>1229486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Nienhuis</td>\n",
       "      <td>2</td>\n",
       "      <td>1220412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Schubert,Frank</td>\n",
       "      <td>1</td>\n",
       "      <td>1210776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Reinhardt</td>\n",
       "      <td>1</td>\n",
       "      <td>1207187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Feather,Sturges</td>\n",
       "      <td>1</td>\n",
       "      <td>1203609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Pierrehumbert,Raymond</td>\n",
       "      <td>1</td>\n",
       "      <td>1190366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Wilson,Stephen</td>\n",
       "      <td>3</td>\n",
       "      <td>1172394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Sifakis</td>\n",
       "      <td>1</td>\n",
       "      <td>1168676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Turkle,Sherry</td>\n",
       "      <td>6</td>\n",
       "      <td>1142686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Foster,Roger</td>\n",
       "      <td>19</td>\n",
       "      <td>1123747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Feibel,Werner</td>\n",
       "      <td>1</td>\n",
       "      <td>1105828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name  docs     size\n",
       "0                  October    67  6399579\n",
       "1             Goldman,Emma    34  3969364\n",
       "2        Featherstone,Mike     6  3404807\n",
       "3                   Fieser     1  3363083\n",
       "4             Chomsky,Noam    73  3349165\n",
       "5                 Sullivan     5  2882483\n",
       "6             Lovink,Geert    13  2819149\n",
       "7    Anttiroiko,Ari Veikko     1  2799658\n",
       "8            Smith,Stephen    11  2693772\n",
       "9   Charles Sanders Pierce     1  2556000\n",
       "10         Lessig,Lawrence     6  2490135\n",
       "11       Chemerinsky,Erwin     2  2440437\n",
       "12                  Cutler     1  2161596\n",
       "13               Philander     1  2115106\n",
       "14             Smil,Vaclav    13  2071748\n",
       "15            Elkins,James    10  2014350\n",
       "16            Lawson,Bryan     6  2009663\n",
       "17             Daly,Herman     4  1962590\n",
       "18          Marshall,Peter     2  1920920\n",
       "19       Krech Iii,Shepard     2  1820464\n",
       "20            Shaw,Malcolm     5  1820195\n",
       "21         Demeny,Mcnicoll     1  1806941\n",
       "22              Johnson,Cb     6  1725486\n",
       "23        Castells,Haraway     9  1673842\n",
       "24       Berkman,Alexander    12  1644056\n",
       "25           Diamond,Jared     5  1643844\n",
       "26          Jensen,Derrick     8  1605418\n",
       "27           Drake,William     2  1583715\n",
       "28              Lewis,Troy     4  1560278\n",
       "29         Kropotkin,Peter    30  1540878\n",
       "30                   Rawls     5  1527332\n",
       "31         Costanza,Robert     3  1471278\n",
       "32            Drahos,Peter     4  1389463\n",
       "33            Ehrlich,Paul     1  1381921\n",
       "34           Biggs,Michael     2  1375866\n",
       "35          Thacker,Eugene    23  1374614\n",
       "36       Jennings,Marianne     1  1322779\n",
       "37          Allaby,Michael     1  1299210\n",
       "38           Henderson,Ann     2  1276188\n",
       "39        Hayles,Katherine    36  1229486\n",
       "40                Nienhuis     2  1220412\n",
       "41          Schubert,Frank     1  1210776\n",
       "42               Reinhardt     1  1207187\n",
       "43         Feather,Sturges     1  1203609\n",
       "44   Pierrehumbert,Raymond     1  1190366\n",
       "45          Wilson,Stephen     3  1172394\n",
       "46                 Sifakis     1  1168676\n",
       "47           Turkle,Sherry     6  1142686\n",
       "48            Foster,Roger    19  1123747\n",
       "49           Feibel,Werner     1  1105828"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import codecs\n",
    "import json\n",
    "from collections import Counter\n",
    "import operator\n",
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def read_json_file(file_name):\n",
    "    with codecs.open(file_name,encoding='utf-8') as fin:\n",
    "        return json.loads(fin.read())\n",
    "    \n",
    "# def write_dict_to_file(file_name,dict_):\n",
    "#     with codecs.open(file_name,'w', encoding='utf-8') as fout:\n",
    "#         fout.write(json.dumps(dict_, indent=2, ensure_ascii=False))\n",
    "\n",
    "\n",
    "def get_accum_file_sizes(doc_list, get_sorted = True):\n",
    "    authors_accum_file_size = {}\n",
    "    author_set = get_author_set(doc_list)\n",
    "    for author in author_set:\n",
    "        authors_accum_file_size[author] = sum([doc[\"file_size\"] for doc in doc_list if doc[\"author_name\"] == author])\n",
    "    if not get_sorted:\n",
    "        return authors_accum_file_size\n",
    "    sorted_sizes = sorted(authors_accum_file_size.items(), key=operator.itemgetter(1),reverse=True) \n",
    "    return sorted_sizes\n",
    "    \n",
    "def get_author_counter(doc_list):\n",
    "    return Counter([text[\"author_name\"] for text in doc_list])\n",
    "\n",
    "def get_author_set(doc_list):\n",
    "    author_counter = get_author_counter(doc_list)\n",
    "    del author_counter[\"No Author\"]\n",
    "    return list(set(dict(author_counter)))\n",
    "    \n",
    "def log_JSON_Analysis(log_json, n = 30):\n",
    "    col = read_json_file(log_json)['file_descriptors']\n",
    "    doc_list = list(col.values())\n",
    "    print(\"Number of Documentes\\n\",len(col))\n",
    "    print()\n",
    "\n",
    "    author_counter = get_author_counter(doc_list)\n",
    "    author_set = get_author_set(doc_list)\n",
    "    \n",
    "    print(\"Number of authors\\n\",len(author_set))\n",
    "    print()\n",
    "        \n",
    "    most_common = {ac:author_counter[ac] for ac in author_counter if author_counter[ac] > 10}\n",
    "    print(\"Authors with more then 10 documents (\", len(most_common),')')\n",
    "\n",
    "    print(Counter(most_common))\n",
    "    print()   \n",
    "    \n",
    "    print(\"Top %s Authors with accumulated file sizes:\" % n)\n",
    "    print()\n",
    "    \n",
    "    sorted_sizes = get_accum_file_sizes(doc_list)\n",
    "    top_n = sorted_sizes[:n]\n",
    "    row_list = []\n",
    "    for a in top_n:\n",
    "        new_row = {}\n",
    "        new_row['name'] = a[0]\n",
    "        new_row['size'] = a[1]\n",
    "        new_row['docs'] = author_counter[a[0]]\n",
    "        row_list.append(new_row)\n",
    "        \n",
    "    top_n_df = pd.DataFrame(row_list,columns=['name','docs','size'])  \n",
    "    display(top_n_df)\n",
    "    \n",
    "\n",
    "log_JSON_Analysis('log-final.json', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/raminsoleymani/git/ECO/data/NAIL_DATAFIELD_txt/parsed_v3/\n",
      "Creating: /Users/raminsoleymani/git/ECO/data/NAIL_DATAFIELD_txt/parsed_v3/merged/Chomsky_Noam.txt\n",
      "Merging 73 files\n",
      "DONE!\n",
      "Resulting file is a size of 3349165 bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/raminsoleymani/git/ECO/data/NAIL_DATAFIELD_txt/parsed_v3/merged/Chomsky_Noam.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codecs\n",
    "import json\n",
    "import os\n",
    "\n",
    "def read_json_file(file_name):\n",
    "    with codecs.open(file_name,encoding='utf-8') as fin:\n",
    "        return json.loads(fin.read())\n",
    "\n",
    "def merge_docs(author_name,log_file, dest_folder = None,overwrite= False):\n",
    "    log = read_json_file(log_file)\n",
    "    author_docs = [doc for doc in list(log['file_descriptors'].values()) if doc['author_name'] == author_name]\n",
    "\n",
    "    base_folder = get_data_folder() + log[\"folder_path\"] \n",
    "    print(base_folder)\n",
    "    if not os.path.isdir(base_folder):\n",
    "        print('''Value: \"%s\" is not set correctly. Cannot find folder:\\n%s\n",
    "        The folder is relative to the project data folder\\nBYE''' %(\"folder_path\", base_folder))\n",
    "        return\n",
    "    if not dest_folder:\n",
    "        dest_folder = base_folder + \"merged/\"\n",
    "    dest_file = (dest_folder + author_name + '.txt').replace(\",\",\"_\")\n",
    "    \n",
    "    print(\"Creating:\",dest_file)\n",
    "    \n",
    "    if not os.path.exists(dest_folder): os.makedirs(dest_folder)\n",
    "        \n",
    "    if os.path.isfile(dest_file) and not overwrite:\n",
    "        print('File %s exists already & overwrite is not set. ending here...' %(dest_file))\n",
    "        return dest_file\n",
    "    \n",
    "    def get_docs_abs_path(doc_dict):\n",
    "        return base_folder + doc_dict[\"rel_path\"] + doc_dict[\"file_name\"]\n",
    "    \n",
    "    file_names = [get_docs_abs_path(doc) for doc in author_docs]\n",
    "    print('Merging %s files' % len(file_names))\n",
    "    \n",
    "\n",
    "    \n",
    "    with codecs.open(dest_file, 'w') as outf:\n",
    "        for fname in file_names:\n",
    "            if not os.path.exists(fname):\n",
    "                print(\"%s\\ndoes not exist. skipping\")\n",
    "                continue\n",
    "            with open(fname) as inf:\n",
    "                for line in inf:\n",
    "                    outf.write(line)\n",
    "                    \n",
    "    print(\"DONE!\\nResulting file is a size of %s bytes\" % os.stat(dest_file).st_size)\n",
    "    return dest_file\n",
    "\n",
    "merge_docs('Chomsky,Noam','log-final.json',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting /Users/raminsoleymani/git/ECO/data/NAIL_DATAFIELD_txt/parsed_v3//merged/Chomsky_Noam.txt\n",
      "Destination folder: /Users/raminsoleymani/git/ECO/data/NAIL_DATAFIELD_txt/parsed_v3//merged/Chomsky_Noam_SPLIT/\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from random import random\n",
    "import os\n",
    "\n",
    "def split_for_train_test(file_path, destination_folder = None, test_ratio = 0.05, overwrite=False):\n",
    "        if not destination_folder:\n",
    "            destination_folder = file_path[:file_path.rindex('.')]+'_SPLIT/'\n",
    "        print(\"Splitting %s\" % file_path)\n",
    "        print(\"Destination folder:\",destination_folder)\n",
    "        \n",
    "        if not os.path.exists(destination_folder): os.makedirs(destination_folder)\n",
    "            \n",
    "        fout_train = destination_folder + 'train.txt'\n",
    "        fout_test = destination_folder + 'test.txt'\n",
    "        \n",
    "        if os.path.exists(fout_train) and not overwrite:\n",
    "            print(\"%s exists and overwrite is not set.\\nBye\" % fout_train)\n",
    "            return\n",
    "\n",
    "        if os.path.exists(fout_test) and not overwrite:\n",
    "            print(\"%s exists and overwrite is not set. Bye\" % fout_test)\n",
    "            return\n",
    "            \n",
    "            \n",
    "        f_in = codecs.open(file_path, 'r', 'UTF-8')\n",
    "        f_out_train = codecs.open(fout_train, 'w', 'UTF-8')\n",
    "        f_out_test = codecs.open(fout_test, 'w', 'UTF-8')\n",
    "\n",
    "        for line in f_in:\n",
    "            if random() < test_ratio:\n",
    "                f_out_test.write(line)\n",
    "            else:\n",
    "                f_out_train.write(line)\n",
    "        print(\"DONE\")\n",
    "\n",
    "log_file = 'log-final.json'\n",
    "file_path = get_data_folder() + read_json_file(log_file)[\"folder_path\"] + '/merged/Chomsky_Noam.txt'\n",
    "split_for_train_test(file_path ,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "October\n",
      "Goldman,Emma\n",
      "Featherstone,Mike\n",
      "Fieser\n",
      "Chomsky,Noam\n",
      "Sullivan\n",
      "Lovink,Geert\n",
      "Anttiroiko,Ari Veikko\n",
      "Smith,Stephen\n",
      "Charles Sanders Pierce\n",
      "Lessig,Lawrence\n",
      "Chemerinsky,Erwin\n",
      "Cutler\n",
      "Philander\n",
      "Smil,Vaclav\n",
      "Elkins,James\n",
      "Lawson,Bryan\n",
      "Daly,Herman\n",
      "Marshall,Peter\n",
      "Krech Iii,Shepard\n",
      "Shaw,Malcolm\n",
      "Demeny,Mcnicoll\n",
      "Johnson,Cb\n",
      "Castells,Haraway\n",
      "Berkman,Alexander\n",
      "Diamond,Jared\n",
      "Jensen,Derrick\n",
      "Drake,William\n",
      "Lewis,Troy\n",
      "Kropotkin,Peter\n",
      "ALL DONE\n"
     ]
    }
   ],
   "source": [
    "### Get the top 30 create their merged texts and split them in train/test\n",
    "import codecs \n",
    "import json\n",
    "\n",
    "TOP_N = 30\n",
    "def read_json_file(file_name):\n",
    "    with codecs.open(file_name,encoding='utf-8') as fin:\n",
    "        return json.loads(fin.read())\n",
    "\n",
    "log_file = 'log-final.json'\n",
    "col = read_json_file(log_file)['file_descriptors']\n",
    "doc_list = list(col.values())\n",
    "    \n",
    "sorted_sizes = get_accum_file_sizes(doc_list)\n",
    "top_n_names = [author_size_tuple[0] for author_size_tuple in sorted_sizes[:TOP_N]]\n",
    "for author in top_n_names:\n",
    "    print(author)\n",
    "#     merge_docs(author,log_file)\n",
    "print(\"ALL DONE\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %run -n ../../v4/pytorch_RVAE/train_word_embeddings.py\n",
    "# %run -n ../../v4/pytorch_RVAE/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_RVAE import train_word_embeddings \n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def author_folder_name(author_name):\n",
    "    return author_name.replace(\",\",\"_\")\n",
    "\n",
    "def prepare_rvae_train(author_name,log_file,rvae_data_folder):\n",
    "    merged_file = merge_docs(author_name,log_file)\n",
    "    split_for_train_test(merged_file,rvae_data_folder,overwrite=True)\n",
    "\n",
    "def run_rvae():\n",
    "    train_word_embeddings.run()\n",
    "    train.run()\n",
    "    pass\n",
    "\n",
    "def move_rvae_model_files(rvae_data_folder, model_folder):\n",
    "    model_files = os.listdir(rvae_data_folder)\n",
    "    for file in model_files:\n",
    "        shutil.move(rvae_data_folder + file,model_folder + file)\n",
    "    \n",
    "log_file = 'log-final.json'\n",
    "rvae_data_folder = get_project_folder() + 'src/v4/pytorch_RVAE/data/'\n",
    "author_model_folder = get_data_folder() + author_folder_name('Chomsky,Noam') + '/'\n",
    "        \n",
    "# prepare_rvae_train('Goldman,Emma',log_file,rvae_data_folder)\n",
    "# run_rvae()\n",
    "# move_rvae_model_files(rvae_data_folder,author_model_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
