{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../v4/misc/data_access.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "from collections import Counter\n",
    "import operator\n",
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def read_json_file(file_name):\n",
    "    with codecs.open(file_name,encoding='utf-8') as fin:\n",
    "        return json.loads(fin.read())\n",
    "    \n",
    "# def write_dict_to_file(file_name,dict_):\n",
    "#     with codecs.open(file_name,'w', encoding='utf-8') as fout:\n",
    "#         fout.write(json.dumps(dict_, indent=2, ensure_ascii=False))\n",
    "\n",
    "\n",
    "def get_accum_file_sizes(doc_list, get_sorted = True):\n",
    "    authors_accum_file_size = {}\n",
    "    author_set = get_author_set(doc_list)\n",
    "    for author in author_set:\n",
    "        authors_accum_file_size[author] = sum([doc[\"file_size\"] for doc in doc_list if doc[\"author_name\"] == author])\n",
    "    if not get_sorted:\n",
    "        return authors_accum_file_size\n",
    "    sorted_sizes = sorted(authors_accum_file_size.items(), key=operator.itemgetter(1),reverse=True) \n",
    "    return sorted_sizes\n",
    "    \n",
    "def get_author_counter(doc_list):\n",
    "    return Counter([text[\"author_name\"] for text in doc_list])\n",
    "\n",
    "def get_author_set(doc_list):\n",
    "    author_counter = get_author_counter(doc_list)\n",
    "    del author_counter[\"No Author\"]\n",
    "    return list(set(dict(author_counter)))\n",
    "    \n",
    "def log_JSON_Analysis(log_json, n = 30):\n",
    "    col = read_json_file(log_json)['file_descriptors']\n",
    "    doc_list = list(col.values())\n",
    "    print(\"Number of Documentes\\n\",len(col))\n",
    "    print()\n",
    "\n",
    "    author_counter = get_author_counter(doc_list)\n",
    "    author_set = get_author_set(doc_list)\n",
    "    \n",
    "    print(\"Number of authors\\n\",len(author_set))\n",
    "    print()\n",
    "        \n",
    "    most_common = {ac:author_counter[ac] for ac in author_counter if author_counter[ac] > 10}\n",
    "    print(\"Authors with more then 10 documents (\", len(most_common),')')\n",
    "\n",
    "    print(Counter(most_common))\n",
    "    print()   \n",
    "    \n",
    "    print(\"Top %s Authors with accumulated file sizes:\" % n)\n",
    "    print()\n",
    "    \n",
    "    sorted_sizes = get_accum_file_sizes(doc_list)\n",
    "    top_n = sorted_sizes[:n]\n",
    "    row_list = []\n",
    "    for a in top_n:\n",
    "        new_row = {}\n",
    "        new_row['name'] = a[0]\n",
    "        new_row['size'] = a[1]\n",
    "        new_row['docs'] = author_counter[a[0]]\n",
    "        row_list.append(new_row)\n",
    "        \n",
    "    top_n_df = pd.DataFrame(row_list,columns=['name','docs','size'])  \n",
    "    display(top_n_df)\n",
    "    \n",
    "\n",
    "log_JSON_Analysis('log-final.json', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import os\n",
    "\n",
    "def read_json_file(file_name):\n",
    "    with codecs.open(file_name,encoding='utf-8') as fin:\n",
    "        return json.loads(fin.read())\n",
    "\n",
    "def merge_docs(author_name,log_file, dest_folder = None,overwrite= False):\n",
    "    log = read_json_file(log_file)\n",
    "    author_docs = [doc for doc in list(log['file_descriptors'].values()) if doc['author_name'] == author_name]\n",
    "\n",
    "    base_folder = get_data_folder() + log[\"folder_path\"] \n",
    "    print(base_folder)\n",
    "    if not os.path.isdir(base_folder):\n",
    "        print('''Value: \"%s\" is not set correctly. Cannot find folder:\\n%s\n",
    "        The folder is relative to the project data folder\\nBYE''' %(\"folder_path\", base_folder))\n",
    "        return\n",
    "    if not dest_folder:\n",
    "        dest_folder = base_folder + \"merged/\"\n",
    "    dest_file = (dest_folder + author_name + '.txt').replace(\",\",\"_\")\n",
    "    \n",
    "    print(\"Creating:\",dest_file)\n",
    "    \n",
    "    if not os.path.exists(dest_folder): os.makedirs(dest_folder)\n",
    "        \n",
    "    if os.path.isfile(dest_file) and not overwrite:\n",
    "        print('File %s exists already & overwrite is not set. ending here...' %(dest_file))\n",
    "        return dest_file\n",
    "    \n",
    "    def get_docs_abs_path(doc_dict):\n",
    "        return base_folder + doc_dict[\"rel_path\"] + doc_dict[\"file_name\"]\n",
    "    \n",
    "    file_names = [get_docs_abs_path(doc) for doc in author_docs]\n",
    "    print('Merging %s files' % len(file_names))\n",
    "    \n",
    "\n",
    "    \n",
    "    with codecs.open(dest_file, 'w') as outf:\n",
    "        for fname in file_names:\n",
    "            if not os.path.exists(fname):\n",
    "                print(\"%s\\ndoes not exist. skipping\")\n",
    "                continue\n",
    "            with open(fname) as inf:\n",
    "                for line in inf:\n",
    "                    outf.write(line)\n",
    "                    \n",
    "    print(\"DONE!\\nResulting file is a size of %s bytes\" % os.stat(dest_file).st_size)\n",
    "    return dest_file\n",
    "\n",
    "merge_docs('Chomsky,Noam','log-final.json',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from random import random\n",
    "import os\n",
    "\n",
    "def split_for_train_test(file_path, destination_folder = None, test_ratio = 0.05, overwrite=False):\n",
    "        if not destination_folder:\n",
    "            destination_folder = file_path[:file_path.rindex('.')]+'_SPLIT/'\n",
    "        print(\"Splitting %s\" % file_path)\n",
    "        print(\"Destination folder:\",destination_folder)\n",
    "        \n",
    "        if not os.path.exists(destination_folder): os.makedirs(destination_folder)\n",
    "            \n",
    "        fout_train = destination_folder + 'train.txt'\n",
    "        fout_test = destination_folder + 'test.txt'\n",
    "        \n",
    "        if os.path.exists(fout_train) and not overwrite:\n",
    "            print(\"%s exists and overwrite is not set.\\nBye\" % fout_train)\n",
    "            return\n",
    "\n",
    "        if os.path.exists(fout_test) and not overwrite:\n",
    "            print(\"%s exists and overwrite is not set. Bye\" % fout_test)\n",
    "            return\n",
    "            \n",
    "            \n",
    "        f_in = codecs.open(file_path, 'r', 'UTF-8')\n",
    "        f_out_train = codecs.open(fout_train, 'w', 'UTF-8')\n",
    "        f_out_test = codecs.open(fout_test, 'w', 'UTF-8')\n",
    "\n",
    "        for line in f_in:\n",
    "            if random() < test_ratio:\n",
    "                f_out_test.write(line)\n",
    "            else:\n",
    "                f_out_train.write(line)\n",
    "        print(\"DONE\")\n",
    "\n",
    "log_file = 'log-final.json'\n",
    "file_path = get_data_folder() + read_json_file(log_file)[\"folder_path\"] + '/merged/Chomsky_Noam.txt'\n",
    "split_for_train_test(file_path ,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Get the top 30 create their merged texts and split them in train/test\n",
    "import codecs \n",
    "import json\n",
    "\n",
    "TOP_N = 30\n",
    "def read_json_file(file_name):\n",
    "    with codecs.open(file_name,encoding='utf-8') as fin:\n",
    "        return json.loads(fin.read())\n",
    "\n",
    "log_file = 'log-final.json'\n",
    "col = read_json_file(log_file)['file_descriptors']\n",
    "doc_list = list(col.values())\n",
    "    \n",
    "sorted_sizes = get_accum_file_sizes(doc_list)\n",
    "top_n_names = [author_size_tuple[0] for author_size_tuple in sorted_sizes[:TOP_N]]\n",
    "for author in top_n_names:\n",
    "    print(author)\n",
    "#     merge_docs(author,log_file)\n",
    "print(\"ALL DONE\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -n ../../v4/pytorch_RVAE/train_word_embeddings.py\n",
    "%run -n ../../v4/pytorch_RVAE/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pytorch_RVAE import train_word_embeddings \n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def author_folder_name(author_name):\n",
    "    return author_name.replace(\",\",\"_\")\n",
    "\n",
    "def prepare_rvae_train(author_name,log_file,rvae_data_folder):\n",
    "    merged_file = merge_docs(author_name,log_file)\n",
    "    split_for_train_test(merged_file,rvae_data_folder,overwrite=True)\n",
    "\n",
    "def run_rvae():\n",
    "    train_word_embeddings.run()\n",
    "    train.run()\n",
    "    pass\n",
    "\n",
    "def move_rvae_model_files(rvae_data_folder, model_folder):\n",
    "    model_files = os.listdir(rvae_data_folder)\n",
    "    for file in model_files:\n",
    "        shutil.move(rvae_data_folder + file,model_folder + file)\n",
    "    \n",
    "log_file = 'log-final.json'\n",
    "rvae_data_folder = get_project_folder() + 'src/v4/pytorch_RVAE/data/'\n",
    "author_model_folder = get_data_folder() + author_folder_name('Chomsky,Noam') + '/'\n",
    "        \n",
    "prepare_rvae_train('Chomsky,Noam',log_file,rvae_data_folder)\n",
    "run_rvae()\n",
    "move_rvae_model_files(rvae_data_folder,author_model_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
