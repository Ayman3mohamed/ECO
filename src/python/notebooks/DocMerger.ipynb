{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_path = \"../../../data/NAIL_DATAFIELD_txt/parsed_v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "def read_json_file(file_name):\n",
    "    with codecs.open(file_name,encoding='utf-8') as fin:\n",
    "        return json.loads(fin.read())\n",
    "    \n",
    "# def write_dict_to_file(file_name,dict_):\n",
    "#     with codecs.open(file_name,'w', encoding='utf-8') as fout:\n",
    "#         fout.write(json.dumps(dict_, indent=2, ensure_ascii=False))\n",
    "\n",
    "def log_JSON_Analysis(log_json):\n",
    "    col = read_json_file(log_json)['file_descriptors']\n",
    "    doc_list = list(col.values())\n",
    "    print(\"Number of Documentes\\n\",len(col))\n",
    "    print()\n",
    "\n",
    "    author_counter = Counter([text[\"author_name\"] for text in doc_list])\n",
    "    del author_counter[\"No Author\"]\n",
    "    \n",
    "    author_set = list(set(dict(author_counter)))\n",
    "    print(\"Number of authors\\n\",len(author_set))\n",
    "    print()\n",
    "        \n",
    "    most_common = {ac:author_counter[ac] for ac in author_counter if author_counter[ac] > 10}\n",
    "    print(\"Authors with more then 10 documents (\", len(most_common),')')\n",
    "\n",
    "    print(Counter(most_common))\n",
    "    print()\n",
    "    \n",
    "    authors_accum_file_size = {}\n",
    "    for author in author_set:\n",
    "        authors_accum_file_size[author] = sum([doc[\"file_size\"] for doc in doc_list if doc[\"author_name\"] == author])\n",
    "\n",
    "    sorted_size = sorted(authors_accum_file_size.items(), key=operator.itemgetter(1),reverse=True)      \n",
    "    \n",
    "    print(\"Top 20 Authors with accumulated file sizes\")\n",
    "    print(sorted_size[:20])\n",
    "        \n",
    "\n",
    "log_JSON_Analysis('log-final.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import os\n",
    "\n",
    "def read_json_file(file_name):\n",
    "    with codecs.open(file_name,encoding='utf-8') as fin:\n",
    "        return json.loads(fin.read())\n",
    "\n",
    "def merge_docs(author_name,log_file, overwrite= False):\n",
    "    log = read_json_file(log_file)\n",
    "    author_docs = [doc for doc in list(log['file_descriptors'].values()) if doc['author_name'] == author_name]\n",
    "\n",
    "    base_folder = log[\"folder_path\"] \n",
    "    if not os.path.isdir(base_folder):\n",
    "        print('Value: \"%s\" is not set correctly. Cannot find folder:\\n%s\\nBYE' %(\"folder_path\", base_folder))\n",
    "        return\n",
    "    dest_folder = base_folder + \"merged/\"\n",
    "    dest_file = (dest_folder + author_name + '.txt').replace(\",\",\"_\")\n",
    "    \n",
    "    print(\"Creating:\",dest_file)\n",
    "    \n",
    "    if not os.path.exists(dest_folder): os.makedirs(dest_folder)\n",
    "        \n",
    "    if os.path.isfile(dest_file) and not overwrite:\n",
    "        print('File %s exists already & overwrite is not set. ending here...' %(dest_file))\n",
    "        return\n",
    "    \n",
    "    def get_docs_abs_path(doc_dict):\n",
    "        return base_folder + doc_dict[\"rel_path\"] + doc_dict[\"file_name\"]\n",
    "    \n",
    "    file_names = [get_docs_abs_path(doc) for doc in author_docs]\n",
    "    print('Merging %s files' % len(file_names))\n",
    "\n",
    "    \n",
    "    with codecs.open(dest_file, 'w') as outf:\n",
    "        for fname in file_names:\n",
    "            if not os.path.exists(fname):\n",
    "                print(\"%s\\ndoes not exist. skipping\")\n",
    "                continue\n",
    "            with open(fname) as inf:\n",
    "                for line in inf:\n",
    "                    outf.write(line)\n",
    "                    \n",
    "    print(\"DONE!\\nResulting file is a size of %s bytes\" % os.stat(dest_file).st_size)\n",
    "\n",
    "\n",
    "merge_docs('Chomsky,Noam','log-final.json',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from random import random\n",
    "import os\n",
    "\n",
    "def split_for_train_test(file_path, destination_folder = None, test_ratio = 0.05, overwrite=False):\n",
    "        if not destination_folder:\n",
    "            destination_folder = file_path[:file_path.rindex('.')]+'_SPLIT/'\n",
    "        print(\"Splitting %s\" % file_path)\n",
    "        print(\"Destination folder:\",destination_folder)\n",
    "        \n",
    "        if not os.path.exists(destination_folder): os.makedirs(destination_folder)\n",
    "            \n",
    "        fout_train = destination_folder + 'train.txt'\n",
    "        fout_test = destination_folder + 'test.txt'\n",
    "        \n",
    "        if os.path.exists(fout_train) and not overwrite:\n",
    "            print(\"%s exists and overwrite is not set.\\nBye\" % fout_train)\n",
    "            return\n",
    "\n",
    "        if os.path.exists(fout_test) and not overwrite:\n",
    "            print(\"%s exists and overwrite is not set. Bye\" % fout_test)\n",
    "            return\n",
    "            \n",
    "            \n",
    "        f_in = codecs.open(file_path, 'r', 'UTF-8')\n",
    "        f_out_train = codecs.open(fout_train, 'w', 'UTF-8')\n",
    "        f_out_test = codecs.open(fout_test, 'w', 'UTF-8')\n",
    "\n",
    "        for line in f_in:\n",
    "            if random() < test_ratio:\n",
    "                f_out_test.write(line)\n",
    "            else:\n",
    "                f_out_train.write(line)\n",
    "        print(\"DONE\")\n",
    "\n",
    "split_for_train_test(main_path + '/merged/Chomsky_Noam.txt',overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
