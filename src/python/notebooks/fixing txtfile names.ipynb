{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing the filenames\n",
    "Goal: don't actually move or rename the file but create a json file with structural information \n",
    "\n",
    "1. Preparing to fix some file names, by checking how many specific punctuation characters they have\n",
    "2. structure them: each file has a description object `FileDescriptor` . a folder of files are in a `DescriptorCollection` object. the final abstract structure is a dictionary of these collections. This is called a log, which can be stored on the drive\n",
    "3. the auto naming function can be improved\n",
    "4. a runthough of all files can if not auto-named assign persons to them\n",
    "5. a manual naming function, gives options and an input field.\n",
    "6. log files can be (simply) mergerd\n",
    "\n",
    "## DO IT!\n",
    "\n",
    "In order to skip the examples and go straight to the process just execute the 6 required cells, which have class and function definition in them. they are marked like this:\n",
    "\n",
    "```# <<< 1/6```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check where the files are first\n",
    "#! cd ../../../data/NAIL_DATAFIELD_txt/parsed_v3 && pwd && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <<< 1/ (1/6)\n",
    "import os\n",
    "import json\n",
    "import codecs\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from random import choice\n",
    "from copy import copy\n",
    "from collections import Counter\n",
    "import textacy\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "main_path = '../../../data/NAIL_DATAFIELD_txt/parsed_v3/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a json object valid_files with the category folders as keys (e.g. law_legal_theory_prison_ip) and another dict as values:\n",
    "```\n",
    "{\n",
    "    path: <folder_path>,  \n",
    "    valid_files: [\n",
    "        {\n",
    "            file_name: <file_name>,\n",
    "            automatic_name: True|False,\n",
    "            auto_group_assigned: <some_auto_name_group_name> \n",
    "            assigned_to: \"M\"|\"R\",\n",
    "            manually_set: True|False,\n",
    "            author_name: <first_name(,last_name)?>,\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A maybe getting better and better auto naming algo is applied. For non auto detected filenames a person is assigned to set the name manually.\n",
    "For each auto naming of a file the descicion tree is run through:\n",
    "\n",
    "### Descicion tree:\n",
    "\n",
    "- auto_name_found:\n",
    "    - yes: auto_named already?\n",
    "        - yes: same name?\n",
    "            - yes: all cool\n",
    "            - no : warning: dont set new name, only when flag set (override_old_auto)\n",
    "        - no : manually_named already?\n",
    "            - yes: same name?\n",
    "                - yes: all cool\n",
    "                - no : critical warning. don't set\n",
    "            - no : cool. set name\n",
    "    - no : auto_named already?\n",
    "        - yes: critical warning. change of algo messed auto nameing up\n",
    "        - no : assign to a person if not assigned already\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <<< 2/ (2/6)\n",
    "\n",
    "auto_name_groups = ['1comma_1dash','1comma_>1dash','1dash']\n",
    "\n",
    "two_words = re.compile(\"([a-zA-Z]+[,-_]){2}\")\n",
    "\n",
    "class FileDescriptor:\n",
    "    \n",
    "    def __init__(self, file_name, dict_ = None):\n",
    "        self.file_name = file_name\n",
    "        self.auto_named = False # flag indicating that auto name applied\n",
    "        self.auto_group_name = None # auto name rule name for debugging... \n",
    "        self.assigned_to = None # assigned to person (M|R for splitting :D )\n",
    "        self.manually_named = False # flag indicating that name was set manually\n",
    "        self.author_name = \"\" # final author, JUST ONE\n",
    "        self.rel_path = \"\"\n",
    "        \n",
    "        if dict_:\n",
    "            self.from_dict(dict_)\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return self.__dict__.copy()\n",
    "    \n",
    "    def from_dict(self, dict_):\n",
    "        self.__dict__ = dict_.copy()\n",
    "    \n",
    "    def set_auto_name(self, author_name, auto_group_name, override_old_auto = False, override_manual = False, debug = False):\n",
    "        \"\"\"\n",
    "        contains the desciocion tree. read above\n",
    "        \"\"\"\n",
    "        if debug: print('setting name to',author_name)\n",
    "        # auto_named_already?\n",
    "        if self.auto_named:\n",
    "            if debug: print('auto_named already')\n",
    "            # yes: same name?\n",
    "            if self.author_name == author_name:\n",
    "                # yes: all cool. set the new group anyway\n",
    "                self.auto_group_name = auto_group_name\n",
    "            # no : warning: dont set new name, only when flag set\n",
    "            else:\n",
    "                # check flag set (override_old_auto)\n",
    "                if override_old_auto:\n",
    "                    self.author_name = author_name\n",
    "                    self.auto_group_name = auto_group_name\n",
    "                else:\n",
    "                    print('Warning: New Auto name does not match old one.')\n",
    "                    print(self.file_name,'old',self.author_name,'new auto',author_name)\n",
    "                    print('Not gonna take it. check your algo')\n",
    "        # no (not auto_named_already)\n",
    "        else: \n",
    "            if debug: print('not auto_named yet')\n",
    "            # manually named already?\n",
    "            if self.manually_named:\n",
    "                # yes: same name?\n",
    "                if self.author_name == author_name:\n",
    "                    # yes: all cool\n",
    "                    self.auto_group_name = auto_group_name\n",
    "                # no : only set when flag set (override_manual). Otherwise warning\n",
    "                else:\n",
    "                    if override_manual:\n",
    "                        self.author_name = author_name\n",
    "                        self.auto_group_name = auto_group_name\n",
    "                        self.auto_named = True\n",
    "                        self.manually_named = False\n",
    "                    else:\n",
    "                        print(\"Warning. Name has been set manually already.\")\n",
    "                        print(self.file_name)\n",
    "                        print('old',self.author_name,'new auto',author_name)\n",
    "                        print('Not gonna take it. check your algo')\n",
    "            else:\n",
    "                # new find: name that shit!\n",
    "                self.author_name = author_name\n",
    "                self.auto_group_name = auto_group_name  \n",
    "                self.auto_named = True\n",
    "                        \n",
    "    def __repr__(self):\n",
    "        return json.dumps(self.to_dict())\n",
    "    \n",
    "    def auto_name_check(self, debug = False):\n",
    "        \"\"\"\n",
    "        THIS IS THE SMART FUNCTION. IF YOU KNOW SOME GOOD RULES TO FIND THE NAME FROM A TXT FILE PUT IT HERE\n",
    "        \"\"\"\n",
    "        f = self.file_name\n",
    "        auto_name = None\n",
    "        if debug:\n",
    "            print('checking',self.file_name)\n",
    "            print(', :',f.count(','),'   - :',f.count('-'))\n",
    "        if f.count(',') == 1:\n",
    "            if debug: print('found  1 comma')\n",
    "            # FINAL this is a great set. lastname, firstname - title something like ALBERRO, NORVELL-recording_conceptual_art\n",
    "            if f.count('-') == 1:\n",
    "                auto_name = (f[:f.find('-')].strip(), '1comma_1dash')\n",
    "            # FINAL? here we have the words in the title separated with - or a minus in a a word \n",
    "            elif f.count('-') > 1:\n",
    "                auto_name = (f[:f.find('-')].strip(), '1comma_>1dash')\n",
    "            else:\n",
    "                pass\n",
    "        elif f.count(',') > 1:\n",
    "            pass\n",
    "        # FINAL. basically lastname - something to separate words in the title\n",
    "        elif f.count('-') == 1:\n",
    "            if debug: print('found 1dash')\n",
    "            auto_name = (f[:f.find('-')].strip(), '1dash')\n",
    "        elif f.count('-') > 1:\n",
    "            if debug: print('found  >1 -')\n",
    "            pass\n",
    "        # not so many anymore. do them manually. often no author\n",
    "        else :\n",
    "            pass\n",
    "        return auto_name    \n",
    "  \n",
    "    def auto_name(self, override_old_auto = False, override_manual = False, debug= False):\n",
    "        auto_name = self.auto_name_check(debug)\n",
    "        if auto_name:\n",
    "            self.set_auto_name(*auto_name, override_old_auto, override_manual, debug)\n",
    "        # TODO add a warning, when we had a auto_name befor and don't get it anymore\n",
    "\n",
    "    def manual_name_options(self):\n",
    "        file_name_alt = self.file_name.replace(' ','')\n",
    "        options = {'no':'n: no author'}\n",
    "        find_name_match = two_words.match(file_name_alt)\n",
    "        if find_name_match:\n",
    "            potential_name =file_name_alt[:find_name_match.span()[1]-1]\n",
    "            potential_name = potential_name.replace('-',',')\n",
    "            potential_name = potential_name.replace('_',',')\n",
    "            splitguess = 'g: ' + potential_name\n",
    "            options['guess'] = splitguess\n",
    "            if ',' in potential_name:\n",
    "                options['guess2'] = 'd: ' + potential_name.split(',')[0]\n",
    "        return options\n",
    "\n",
    "    def manual_naming(self):\n",
    "        text = ['set last_name(,first_name)? for',self.file_name,'']\n",
    "        options = self.manual_name_options()\n",
    "        text.extend(list(options.values()))\n",
    "        text = '\\n'.join(text)\n",
    "        name = input(text + '\\n\\n')\n",
    "        if name == '':\n",
    "            return False\n",
    "        else:\n",
    "            if name == 'n':\n",
    "                self.author_name = options['no']\n",
    "            elif name == 'g' and 'guess' in options:\n",
    "#                 print('selected option:',options[3])\n",
    "                self.author_name = options['guess'][3:]\n",
    "            elif name == 'd' and 'guess2' in options:\n",
    "                self.author_name = options['guess2'][3:]\n",
    "            else:\n",
    "                self.author_name = name\n",
    "            self.auto_named = False\n",
    "            # keep auto_name_group so we see, if there was something before\n",
    "            self.manually_named = True\n",
    "            return True\n",
    "\n",
    "    def simple_manual_merge(self, other_desrc):\n",
    "        new_file_descr = copy(self)   \n",
    "        if other_desrc.manually_named and not new_file_descr.manually_named:\n",
    "            new_file_descr.manually_named = True\n",
    "            new_file_descr.author_name = other_desrc.author_name\n",
    "        return new_file_descr\n",
    "    \n",
    "class DescriptorCollection:\n",
    "    \"\"\"\n",
    "    File descriptors for a folder of files\n",
    "    \"\"\"\n",
    "    def __init__(self,folder_name,folder_path, dict_ = None):\n",
    "        self.folder_name = folder_name\n",
    "        self.folder_path = folder_path\n",
    "        self.file_descriptors = {}\n",
    "        ## Edit. Flat Collections basically only exist cuz of their convinient functions...\n",
    "        ## the folder_name and path have no use anymore( or just function as basepath). all file_descriptors keep their own path \n",
    "        self._is_flat = False \n",
    "               \n",
    "        if dict_:\n",
    "            self.from_dict(dict_)\n",
    "\n",
    "    def to_dict(self):\n",
    "        dict_ = self.__dict__.copy()\n",
    "        dict_['file_descriptors'] = {file_descr: self.file_descriptors[file_descr].to_dict() for file_descr in self.file_descriptors}\n",
    "        return dict_\n",
    "            \n",
    "    def from_dict(self, dict_):\n",
    "        self.__dict__ = dict_\n",
    "        self.file_descriptors = {file_descr_name : FileDescriptor('',self.file_descriptors[file_descr_name]) \n",
    "                                 for file_descr_name in self.file_descriptors}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return json.dumps(self.to_dict())\n",
    "    \n",
    "    def short_description(self):\n",
    "        dict_ = self.to_dict().copy()\n",
    "        dict_['file_descriptors'] = len(dict_['file_descriptors'])\n",
    "        return json.dumps(dict_)\n",
    "    \n",
    "    def build(self, file_filter = None, file_name_processor= None):\n",
    "        \"\"\"\n",
    "        for initialisation:\n",
    "        builds a log dict for a list of file in a folder. \n",
    "        the actual selection of files can be filtered with a passed function\n",
    "        for each selected file the empty description is created\n",
    "        \"\"\"\n",
    "        for file_ in os.listdir(self.folder_path):\n",
    "            if not file_filter or file_filter(file_):\n",
    "                file_key = file_ \n",
    "                if file_name_processor:\n",
    "                    file_key = file_name_processor(file_)\n",
    "                self.file_descriptors[file_key] = FileDescriptor(file_)\n",
    "#         return log_files\n",
    "    \n",
    "    def get_descriptor(self, file_name_or_index):\n",
    "        if type(file_name_or_index) == str:\n",
    "            return self.file_descriptors[file_name_or_index]\n",
    "        elif type(file_name_or_index) == int:\n",
    "            return list(self.file_descriptors.values())[file_name_or_index]\n",
    "        \n",
    "    def auto_name_all(self, override_old_auto = False, override_manual = False, debug= False):\n",
    "        for file_descr in list(self.file_descriptors.values()):\n",
    "            file_descr.auto_name(override_old_auto, override_manual, debug)\n",
    "            \n",
    "    def info(self, printIt=True):\n",
    "        num_docs = len(self.file_descriptors)\n",
    "        num_auto_named = sum([1 for text_descr in list(self.file_descriptors.values()) if text_descr.auto_named])\n",
    "        num_man_named = 0 # sum([1 for text_descr in list(self.file_descriptors.values()) if text_descr.manually_named])\n",
    "        assignment_names_counter = Counter()\n",
    "        man_named_names_counter = Counter()\n",
    "        num_assigned = 0 \n",
    "        for text_descr in list(self.file_descriptors.values()):\n",
    "            if text_descr.assigned_to:\n",
    "                assignment_names_counter[text_descr.assigned_to] += 1\n",
    "                num_assigned += 1\n",
    "                if text_descr.manually_named:\n",
    "                    man_named_names_counter[text_descr.assigned_to] += 1\n",
    "                    num_man_named += 1\n",
    "        if printIt: \n",
    "            print('Collection:',self.folder_name)\n",
    "            print(num_docs, 'docs')\n",
    "            print(num_auto_named, 'auto named')\n",
    "            print(num_man_named,'/',num_assigned, 'manually named/assigned', assignment_names_counter, man_named_names_counter)\n",
    "        return {\n",
    "            \"folder_name\": self.folder_name,\n",
    "            \"num_docs\": num_docs,\n",
    "            \"num_auto_named\": num_auto_named,\n",
    "            \"num_man_named\": num_man_named,\n",
    "            \"num_assigned\": num_assigned,\n",
    "            \"assignment_names_counter\" : assignment_names_counter,\n",
    "            \"man_named_names_counter\": man_named_names_counter\n",
    "        }\n",
    "              \n",
    "    def file_descr_list(self):\n",
    "        return list(self.file_descriptors.values())\n",
    "    \n",
    "    def get_all_missing(self):    \n",
    "        return [text_descr for text_descr in self.file_descr_list() if not text_descr.author_name]\n",
    "        \n",
    "    def get_all_assigned_to(self, name = None, only_missing = False):\n",
    "        return [text_descr for text_descr in list(self.file_descriptors.values()) \n",
    "                if text_descr.assigned_to == name  and not(only_missing and text_descr.manually_named)]\n",
    " \n",
    "    def merge(self, other_collection):\n",
    "        \"\"\"\n",
    "        merge in another collection. should be the same basis, just with different file_descriptior values:\n",
    "        manually assigned author names... basically\n",
    "        creates a NEW COLLECTION\n",
    "        Preference goes to auto_naming, if it doens't exist in the 2nd collection. that means the other has a smarter algo\n",
    "        \n",
    "        \"\"\"\n",
    "        # TODO\n",
    "#         for file_descr_name in self.file_descriptors:\n",
    "#             # file descriptor not in other collection? weird... ignore\n",
    "#             if file_descr_name not in other_collection:\n",
    "#                 print(file_descr, 'is not in the 2nd collection. not gonna take it')\n",
    "#             file_descr1 = self.file_descriptors[file_descr_name]\n",
    "#             file_descr2 = other_collection.file_descriptors[file_descr_name]\n",
    "#             new_file_descr = FileDescriptor(file_descr1.file_name)\n",
    "#             # we need to cover 4 cases. auto-auto, auto-man, man-auto, man-man, \n",
    "#             # if file_descr1.auto_named and not file_descr2.auto_named\n",
    "        \n",
    "            \n",
    "    def simple_manual_merge(self, other_collection):\n",
    "        \"\"\"\n",
    "        just slam the file_descr together to complete the authors.\n",
    "        assumption is that auto_naming is the same and just different assigned descriptors are merges\n",
    "        Returns a new log\n",
    "        \"\"\"\n",
    "        new_col_desrc = DescriptorCollection(self.folder_name, self.folder_path)\n",
    "        for file_descr_name in self.file_descriptors:\n",
    "            file_descr1 = self.file_descriptors[file_descr_name]\n",
    "            file_descr2 = other_collection.file_descriptors[file_descr_name]\n",
    "            new_file_descr = file_descr1.simple_manual_merge(file_descr2)\n",
    "            new_col_desrc.file_descriptors[file_descr_name] = new_file_descr \n",
    "        return new_col_desrc\n",
    "    \n",
    "    def get_all_auto_named(self):\n",
    "        return list(filter(lambda fd: fd.auto_named, self.file_descr_list()))\n",
    "#         return [file_descr for file_descr in self.file_descr_list() if file_descr.auto_named]\n",
    "    \n",
    "    def get_all_manualy_named(self):\n",
    "        return list(filter(lambda fd: fd.manually_named and fd.author_name, self.file_descr_list()))\n",
    "#         return [file_descr for file_descr in self.file_descr_list() if file_descr.manually_named and file_descr.author_name]\n",
    "    \n",
    "        \n",
    "    def get_filter(self, filter_fct):\n",
    "        return list(filter(filter_fct, self.file_descr_list()))\n",
    "    \n",
    "    def get_names_set(self):\n",
    "        return set([file_descr.author_name for file_descr in self.file_descr_list() if file_descr.author_name])\n",
    "    \n",
    "    def get_names_Counter(self):\n",
    "        counter = Counter ()\n",
    "        for file_descr in self.file_descr_list():\n",
    "            if file_descr.author_name:\n",
    "                counter[file_descr.author_name] += 1\n",
    "        return counter\n",
    "    \n",
    "    def set_file_descr_paths(self, path):\n",
    "        for file_descr in self.file_descr_list():\n",
    "            if not path:\n",
    "                path = self.folder_path\n",
    "            file_descr.rel_path = path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next part is simple.\n",
    "Initiate the description files for our corpus.\n",
    "\n",
    "This is only for initialisation. If it's done you can just **load a log file**...\n",
    "At the end we get a dict, where the keys are foldernames and the values are DescriptorCollections\n",
    "```\n",
    "{\n",
    "    folder_name: {\n",
    "        <DescriptorCollections>: as_json:\n",
    "        folder_name: <folder_name>\n",
    "        folder_path: <folder_path>,\n",
    "        log_files: <list of FileDescriptors>\n",
    "    }\n",
    "},\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define some methods to initiate a set of collections, read and dump them to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <<< 3/ (3/6)\n",
    "\n",
    "def build_descr_folder(base_path, folder_names, file_filter, file_name_processor):\n",
    "    \"\"\"\n",
    "    build multiple folder in one base folder to a dict\n",
    "    {key: folder_name ; value: {path: folder_path, list of fileDescriptions}}\n",
    "    \"\"\"\n",
    "    descr_collections = []\n",
    "    for folder_name in folder_names:\n",
    "        descr_folder = DescriptorCollection(folder_name, base_path + folder_name)\n",
    "        descr_folder.build(file_filter, file_name_processor)\n",
    "        descr_collections.append(descr_folder)    \n",
    "    return {collection.folder_name : collection for collection in descr_collections} \n",
    "\n",
    "def load_descr_folder_from_dict(dict_):\n",
    "    log = {}\n",
    "    for col_folder in dict_:\n",
    "        log[col_folder] = DescriptorCollection(None, None, dict_[col_folder])\n",
    "    return log\n",
    "\n",
    "# <<< (4/6)\n",
    "\n",
    "def init_log(base_path, file_filter = None, file_name_processor = None):\n",
    "    \"\"\"\n",
    "    initialises\n",
    "    \"\"\"\n",
    "    folder_names = [obj for obj in os.listdir(base_path) if os.path.isdir(base_path + obj)]\n",
    "    log = build_descr_folder(main_path, folder_names, file_filter, file_name_processor)\n",
    "  \n",
    "    # print(log_files)\n",
    "    total_size = 0\n",
    "    for folder in log:\n",
    "        print(folder)\n",
    "        print(len(log[folder].file_descriptors),'log files')\n",
    "        total_size += len(log[folder].file_descriptors)\n",
    "    print('TOTAL SIZE',total_size)\n",
    "    return log\n",
    "\n",
    "        \n",
    "def read_log(log_file_name):\n",
    "    try:\n",
    "        with codecs.open(log_file_name,encoding='utf-8') as fin:\n",
    "            log_dict = json.loads(fin.read())\n",
    "    except FileNotFoundError:\n",
    "        write_log_file()\n",
    "    return load_descr_folder_from_dict(log_dict)\n",
    "\n",
    "\n",
    "def write_log_file(log, log_file_name):\n",
    "    \"\"\"\n",
    "    log is a dict of collection. dump it to drive...\n",
    "    \"\"\"\n",
    "    dict_ = {col: log[col].to_dict() for col in log} \n",
    "    with codecs.open(log_file_name,'w', encoding='utf-8') as fout:\n",
    "        fout.write(json.dumps(dict_, indent=2, ensure_ascii=False))\n",
    "        \n",
    "def sum_info(log, printIt=True):\n",
    "    sum_num_docs = 0\n",
    "    sum_num_auto_named = 0\n",
    "    sum_num_man_named = 0\n",
    "    sum_num_assigned = 0\n",
    "    sum_assignment_names_counter = Counter()\n",
    "    sum_man_named_names_counter = Counter()\n",
    "    \n",
    "    for collection in list(log.values()):\n",
    "        info = collection.info(False)\n",
    "        sum_num_docs += info['num_docs']\n",
    "        sum_num_auto_named += info['num_auto_named']\n",
    "        sum_num_man_named += info['num_man_named']\n",
    "        sum_num_assigned += info['num_assigned']\n",
    "        sum_assignment_names_counter += info['assignment_names_counter']\n",
    "        sum_man_named_names_counter += info['man_named_names_counter']\n",
    "        \n",
    "    if printIt:\n",
    "        print('TOTAL NUM OF DOCS:',sum_num_docs)\n",
    "        print(\"Auto named:\",sum_num_auto_named)\n",
    "        print(\"Manually named\",sum_num_man_named)\n",
    "        print(\"Done/Assigned:\",sum_man_named_names_counter,sum_assignment_names_counter)\n",
    "    return {\n",
    "        \"sum_num_docs\": sum_num_docs,\n",
    "        \"sum_num_auto_named\": sum_num_auto_named,\n",
    "        \"sum_num_man_named\": sum_num_man_named,\n",
    "        \"sum_num_assigned\": sum_num_assigned,\n",
    "        \"sum_assignment_names_counter\": sum_assignment_names_counter,\n",
    "        \"sum_man_named_names_counter\": sum_man_named_names_counter\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_file_filter(file_name):\n",
    "    return '_valid' in file_name\n",
    "\n",
    "def valid_file_name_processor(file_name):\n",
    "    return file_name[:-len('_valid.txt')]\n",
    "\n",
    "log = init_log(main_path, valid_file_filter, valid_file_name_processor)\n",
    "write_log_file(log,'log.json')\n",
    "\n",
    "# now we can grab a file descriptor either by some index or by it's file name:\n",
    "file_descr = log['arts_arthistory_aesthetics'].get_descriptor(0)\n",
    "print(json.dumps(file_descr.to_dict(), indent = 2))\n",
    "file_descr = log['arts_arthistory_aesthetics'].get_descriptor('Batsford - Gwen.White-Perspective.A.Guide.For.Artists,.Architects.and.Designers')\n",
    "print(json.dumps(file_descr.to_dict(), indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking a single file if it would find a name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_descr.auto_name_check(debug = True)\n",
    "# that looks good..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the auto naming function, which will if it finds a name and the deciscion tree rules are cool set the \"file description\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test auto_name\n",
    "file_descr.auto_name(debug= True)\n",
    "print(json.dumps(file_descr.to_dict(), indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manual naming could work like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#file_descr.manual_naming()\n",
    "#print(json.dumps(file_descr.to_dict(), indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we go through all files in bulk. Instead of `auto_name_check` we use `auto_name`,\n",
    "which will call `set_auto_name` in case we found something.\n",
    "The complete check also allowes us to assign a random 'person name' to each text document which has not been auto_named. At the end, we get an overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <<< (5/6)\n",
    "\n",
    "def complete_check(dict_of_collections, assign_equally_to = None, \n",
    "                   override_old_auto = False, override_manual = False, debug= False):\n",
    "    \"\"\"\n",
    "    when assign_equally_to contains a list, it will randomly choose one for each file that is not\n",
    "    auto-named\n",
    "    \"\"\"\n",
    "    for collection in list(dict_of_collections.values()):\n",
    "        collection.auto_name_all()\n",
    "        for file_descr in list(collection.file_descriptors.values()):\n",
    "            if not file_descr.auto_named and assign_equally_to:\n",
    "                file_descr.assigned_to = choice(assign_equally_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_check(log,['M','R'])\n",
    "print()\n",
    "for col in log:\n",
    "    log[col].info()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at all assigned descriptors of own_mixed_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[file_descr for file_descr in list(log['own_mixed_collection'].file_descriptors.values()) if file_descr.assigned_to]\n",
    "# we could also call `get_all_missing` which returns all file_descr. which don't have an author yet\n",
    "# log['own_mixed_collection'].get_all_missing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nice... now lets get all of them assigned to 'R'. I added a function for that, to have it handy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assigned_to_R = log['own_mixed_collection'].get_all_assigned_to('R')\n",
    "assigned_to_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's manually name them and get the info of that collection again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file_descr in assigned_to_R:\n",
    "    file_descr.manual_naming()\n",
    "    clear_output()\n",
    "print('cool all done')\n",
    "log['own_mixed_collection'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you tired you can write the log now. for later you can also just grab those, which are not set yet. \n",
    "The second parameter 'only_missing' default False does just that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assigned_to_R = log['own_mixed_collection'].get_all_assigned_to('R', True)\n",
    "assigned_to_R\n",
    "# EMPTY SINCE R did all ot his files..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to merge 2 logs. Let's not rely on git with that... :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <<< (6/6)\n",
    "\n",
    "def simple_manual_merge_logs(log1, log2):\n",
    "    final_log = {}\n",
    "    for col_name in log1:\n",
    "        if col_name in log2:\n",
    "            final_log[col_name] = log1[col_name].simple_manual_merge(log2[col_name]) \n",
    "        else:\n",
    "            print('collection', col_name,'is missing')\n",
    "    return final_log\n",
    "# simple_manual_merge_logs(log,log)\n",
    "\n",
    "def run_fct_on_col_dict(log,fct):\n",
    "    \"\"\"\n",
    "    run an arbitrary function on a collection dict.\n",
    "    the return value is a dict with the collectionnames as keys and the function results as values\n",
    "    \"\"\"\n",
    "    #     pass\n",
    "    ret_dict = {} \n",
    "    for collection in log:\n",
    "        ret_dict[collection] = map()\n",
    "        ret_dict[collection]= list() \n",
    "    # probably just a map fct\n",
    "    pass\n",
    "\n",
    "def run_filter_on_col_dict(filter_fct, col_dict):\n",
    "    return {col_name : col_dict[col_name].get_filter(filter_fct) for col_name in col_dict}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing the merge... \n",
    "a bit hacky...\n",
    "- initiate a second log `log2`, \n",
    "- do auto naming without assignments\n",
    "- copy the assignments of `own_mixed_collection` from `log` to `log2` \n",
    "- manualy name log2 `own_mixed_collection` for M\n",
    "- now `log` has all R manualy named and `log2` all Ms\n",
    "- merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# still a bit strange...\n",
    "\n",
    "# initiate a second log\n",
    "log2 = init_log(main_path, valid_file_filter, valid_file_name_processor)\n",
    "# do auto naming without assignments\n",
    "complete_check(log2)\n",
    "\n",
    "# copy the assignments of own_mixed_collection from log to log2\n",
    "for file_descr in log['own_mixed_collection'].file_descriptors:\n",
    "   assigned_to = log['own_mixed_collection'].file_descriptors[file_descr].assigned_to\n",
    "   log2['own_mixed_collection'].file_descriptors[file_descr].assigned_to = assigned_to\n",
    "\n",
    "# manualy name log2 own_mixed_collection for M\n",
    "assigned_to_M = log2['own_mixed_collection'].get_all_assigned_to('M')\n",
    "print(assigned_to_M)\n",
    "for file_descr in assigned_to_M:\n",
    "    file_descr.manual_naming()\n",
    "    clear_output()\n",
    "\n",
    "log2['own_mixed_collection'].info()\n",
    "\n",
    "newLog = simple_manual_merge_logs(log,log2)\n",
    "newLog['own_mixed_collection'].info() \n",
    "newLog['own_mixed_collection'].get_all_manualy_named()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last final part!**\n",
    "*Run all the required 6 cells before*\n",
    "\n",
    "1. initiate or load\n",
    "\n",
    "2. load a second and merge it in\n",
    "\n",
    "3. run through all collections a manualy fill in missing file descriptions\n",
    "\n",
    "4. save it\n",
    "\n",
    "**WHEN THE OUTPUT STAYS BLANK BUT THE CELL IS STILL RUNNING: PRESS STOP AND RUN THE FOLLOWING CELL. THEN RUN THE MAIN CELL AGAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE MAIN PROCESS!\n",
    "\n",
    "I_AM = 'R' # alternative 'M'\n",
    "\n",
    "log_file = 'log-'+I_AM+'.json'\n",
    "alt_log_file = 'log-'+('M' if I_AM == 'R' else 'R')+'.json'\n",
    "\n",
    "if I_AM == 'R':\n",
    "    main_path = '../../../data/NAIL_DATAFIELD_txt/parsed_v3/'\n",
    "else:\n",
    "    # naaa?\n",
    "    main_path = '../../../data/NAIL_DATAFIELD_txt/parsed_v3/'\n",
    "    \n",
    "def valid_file_filter(file_name):\n",
    "    return '_valid' in file_name\n",
    "\n",
    "def valid_file_name_processor(file_name):\n",
    "    return file_name[:-len('_valid.txt')]\n",
    "\n",
    "# 1. initiate or load\n",
    "# init\n",
    "# log = init_log(main_path, valid_file_filter, valid_file_name_processor)\n",
    "# complete_check(log,['M','R'])\n",
    "\n",
    "# load\n",
    "log = read_log(log_file)\n",
    "\n",
    "for col in list(log.values()):\n",
    "    col.info()\n",
    "\n",
    "  \n",
    "# 2. load a second and merge it in (if exists)\n",
    "if os.path.isfile(alt_log_file):\n",
    "    print('MERGING')\n",
    "    alt_log = read_log(alt_log_file)\n",
    "    log = simple_manual_merge_logs(log,alt_log)\n",
    "\n",
    "print()\n",
    "sum_info(log)    \n",
    "print()\n",
    "    \n",
    "# 3. run through all collections a manualy fill in missing file descriptions\n",
    "quit = False\n",
    "for collection in list(log.values()):\n",
    "    assigned_to_me = collection.get_all_assigned_to(I_AM, True)\n",
    "    for file_descr in assigned_to_me:\n",
    "        if not file_descr.manual_naming():\n",
    "            quit = True\n",
    "            break\n",
    "        clear_output()\n",
    "    if quit:\n",
    "        break\n",
    "        \n",
    "clear_output()\n",
    "\n",
    "sum_info(log)    \n",
    "\n",
    "# 4. save it\n",
    "write_log_file(log,log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_log_file(log,log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Sorting and merging authot names\n",
    "\n",
    "1. get overview of how ofter an author appears. Set a filter for minium appearace\n",
    "2. put text files for each author together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work with a new log/ json. throw all file_descriptors into one collections. nice to handler.\n",
    "helper functions to dump/read one collection to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# >>> \n",
    "\n",
    "p2_log_file = 'log-p2.json'\n",
    "# log = read_log(p2_log_file)\n",
    "\n",
    "redundant = []\n",
    "\n",
    "def read_col_from_file(col_file_name):\n",
    "    with codecs.open(col_file_name,encoding='utf-8') as fin:\n",
    "        return DescriptorCollection(None, None, json.loads(fin.read()))\n",
    "    \n",
    "def write_col_to_file(col, file_name):\n",
    "    with codecs.open(file_name,'w', encoding='utf-8') as fout:\n",
    "        fout.write(json.dumps(col.to_dict(), indent=2, ensure_ascii=False))\n",
    "        \n",
    "def read_json_file(file_name):\n",
    "    with codecs.open(file_name,encoding='utf-8') as fin:\n",
    "        return json.loads(fin.read())\n",
    "    \n",
    "def write_dict_to_file(file_name,dict_):\n",
    "    with codecs.open(file_name,'w', encoding='utf-8') as fout:\n",
    "        fout.write(json.dumps(dict_, indent=2, ensure_ascii=False))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the collection_dict to one Collection flattening. collection contains base dir. file_descriptor have a path variable. **DONE**\n",
    "\n",
    "Also creates a list of redundant documents **redundant.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creates one collection, with which we gonna work from now on.\n",
    "# also finds duplicates. texts that are found in more then 1 folder and stores them into a redundant.json\n",
    "def flattern_col_dict(col_dict, path):\n",
    "    flat_collection = DescriptorCollection('nails',path)\n",
    "    for collection in list(col_dict.values()):\n",
    "        collection.set_file_descr_paths(collection.folder_name + '/')\n",
    "        #\n",
    "        for file_descr_name in collection.file_descriptors:\n",
    "            if file_descr_name in flat_collection.file_descriptors:\n",
    "                redundant.append(collection.file_descriptors[file_descr_name].to_dict())\n",
    "                if collection.file_descriptors[file_descr_name] != collection.file_descriptors[file_descr_name]:\n",
    "                    print('warning. different author names')\n",
    "                pass\n",
    "            else:\n",
    "                flat_collection.file_descriptors[file_descr_name] = collection.file_descriptors[file_descr_name]\n",
    "    flat_collection._is_flat = True\n",
    "    with codecs.open('redundant.json',\"w\",encoding='utf-8') as fout: fout.write(json.dumps(redundant))\n",
    "    print(len(redundant),'redundant files')\n",
    "    return flat_collection\n",
    "\n",
    "# PREPARATION. COMMENT OUT AFTER USAGE\n",
    "# IF IOPUB ERROR OCCURS:\n",
    "# run `jupter notebook --generate-config` in terminal\n",
    "# and set ... c.NotebookApp.iopub_data_rate_limit = 50000000 or something\n",
    "\"\"\"\n",
    "I_AM = 'R'\n",
    "log_file = 'log-'+I_AM+'.json'\n",
    "log = read_log(log_file)\n",
    "col = flattern_col_dict(log, main_path)\n",
    "print(len(col.file_descriptors),'actual docs')\n",
    "write_col_to_file(col,p2_log_file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some introspection. Crucial datastructures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = read_col_from_file(p2_log_file)\n",
    "print(len(col.file_descr_list()), 'documents')\n",
    "authors_Counter = col.get_names_Counter()\n",
    "author_set = set(list(authors_Counter))\n",
    "\n",
    "print(len(authors_Counter),\"authors\")\n",
    "\n",
    "counts = Counter()\n",
    "dict_by_counts = {}\n",
    "\n",
    "\n",
    "for author in authors_Counter:\n",
    "    count = authors_Counter[author]\n",
    "    counts[count] +=1\n",
    "    if not count in dict_by_counts:\n",
    "        dict_by_counts[count] = []\n",
    "    dict_by_counts[count].append(author)\n",
    "#     if authors_Counter[author] > 3:\n",
    "#         print(author,authors_Counter[author])\n",
    "        \n",
    "# print(counts.most_common())\n",
    "# print(authors_Counter)\n",
    "print('authors with more then 5 texts:')\n",
    "for ac in dict_by_counts:\n",
    "#     print(ac)\n",
    "    if ac > 10:\n",
    "        print(ac,dict_by_counts[ac])\n",
    "        \n",
    "### Write author set once to disc. for name replacements: DONE/ or when fixes fuck the names up...\n",
    "author_set_dict = {author: author for author in list(author_set)}\n",
    "write_dict_to_file('author-fix.json',author_set_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author:\n",
    "    \n",
    "    def __init__(self, file_descr_list):\n",
    "        self.file_descr_list = file_descr_list\n",
    "     \n",
    "    \n",
    "def get_all_docs_from_author(col, author):\n",
    "    return col.get_filter(lambda fd :fd.author_name == author)\n",
    "\n",
    "\n",
    "# some_author = get_all_docs_from_author(col,\"OCTOBER\")\n",
    "# print(some_author)\n",
    "# for text in some_author:\n",
    "#     print(json.dumps(text.to_dict(),indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually fix some broken names. **DONE! SAVED IN `name-fixes.json`** updates saved in **p2_log_file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authors = read_json_file('author-fix.json')\n",
    "\n",
    "# def remove_irregular_chars(name):\n",
    "#     return re.search('[^a-zA-Z0-9_,\\s]', name,re.UNICODE)\n",
    "\n",
    "### FIX BROKEN AUTHOR NAMES AND DUMP TO A FILE\n",
    "# fixed_author_names = {}\n",
    "# with codecs.open('name-fixes.json') as fin:\n",
    "#     fixed_author_names = json.loads(fin.read())\n",
    "    \n",
    "    \n",
    "\n",
    "# print('Sherow, James_' in fixed_author_names)\n",
    "\n",
    "# for author in author_set:\n",
    "#     find = remove_irregular_chars(author)\n",
    "\n",
    "#     if find and not author in fixed_author_names:\n",
    "#         print(author, find)\n",
    "#         new_name = input(author)\n",
    "#         fixed_author_names[author] = new_name\n",
    "#         clear_output()\n",
    "    \n",
    "# with codecs.open('name-fixes.json','w') as fout:\n",
    "#     fout.write(json.dumps(fixed_author_names,indent=2,ensure_ascii=False))\n",
    "\n",
    "#####\n",
    "\n",
    "### LOAD THAT FIX FILE AND UPDATE AUTHOR_DICT\n",
    "# fixed_author_names = {}\n",
    "# with codecs.open('name-fixes.json') as fin:\n",
    "#     fixed_author_names = json.loads(fin.read())\n",
    "\n",
    "### fill in into author-fix.json\n",
    "# for broken_author in fixed_author_names:\n",
    "#     authors[broken_author] = fixed_author_names[broken_author]\n",
    "# write_dict_to_file('author-fix.json',authors)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some fix functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_capital_fix(name):\n",
    "    # need this comprehension around, since there are some empty strings in the result\n",
    "    words = [s for s in re.findall('\\w*',name) if s]\n",
    "    \n",
    "    commaIndex = name.find(',')\n",
    "    res =  ' '.join([word[0].upper() + word[1:].lower() for word in words])\n",
    "    if commaIndex > -1:\n",
    "        res = res[:commaIndex] + ',' + res[commaIndex + 1:]\n",
    "    return res\n",
    "\n",
    "def one_letter_drop(name):\n",
    "    words = [s for s in re.findall('\\w*',name) if s]\n",
    "    words = [w for w in words if len(w) > 1]\n",
    "    res = ' '.join(words)\n",
    "    commaIndex = name.find(',')\n",
    "    if commaIndex > -1:\n",
    "        res = res[:commaIndex] + ',' + res[commaIndex + 1:]\n",
    "    return res\n",
    "    \n",
    "        \n",
    "# def missing_first_name_merge(name_set):\n",
    "#     for name in name_set:\n",
    "#         print(name)\n",
    "  \n",
    "def kill_underscore(name):\n",
    "    return name.replace('_',' ')\n",
    "\n",
    "# manually fixes\n",
    "fixed_author_names = {}\n",
    "with codecs.open('name-fixes.json') as fin:\n",
    "    fixed_author_names = json.loads(fin.read())\n",
    "    \n",
    "def manual_correction(name):\n",
    "    return fixed_author_names.get(name,name)\n",
    "\n",
    "def end_comma(name):\n",
    "    return name[:-1] if name[-1] == ',' else name\n",
    "\n",
    "personal_name_fixes = {}\n",
    "with codecs.open('personal-fixes.json') as fin:\n",
    "    personal_name_fixes = json.loads(fin.read())\n",
    "\n",
    "def manual_correction2(name):\n",
    "#     print('c',name)\n",
    "#     print(personal_name_fixes.get(name,'-'))\n",
    "    return personal_name_fixes.get(name,name)\n",
    "\n",
    "# for author in author_set:\n",
    "#     new_name = one_letter_drop(author)\n",
    "#     if new_name != author:\n",
    "#         print(author,\"->\",new_name)\n",
    "\n",
    "        \n",
    "# missing_first_name_merge(author_set)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_fixes  = read_json_file('author-fix.json')\n",
    "\n",
    "def apply_fix(fix_method,author_dict, print_fixes = False, use_orig_name = False):\n",
    "    fix_count = 0\n",
    "    for author_orig in author_dict:\n",
    "        if use_orig_name:\n",
    "            act_name = author_orig\n",
    "        else:\n",
    "            act_name = author_dict[author_orig]\n",
    "        if not act_name:\n",
    "            print(author_orig,'broke')\n",
    "        fix_name = fix_method(act_name)\n",
    "        if fix_name != act_name:\n",
    "            if len(fix_name) <= 1:\n",
    "                print('no cool fix', act_name, '>> ',fix_name)\n",
    "                continue\n",
    "            if print_fixes : print('fix', act_name, '>> ',fix_name)\n",
    "            author_dict[author_orig] = fix_name\n",
    "            fix_count += 1\n",
    "    print(fix_method.__name__,'fixed',fix_count,'names')\n",
    "    return fix_count\n",
    "\n",
    "# this must be first, since it's a key > value fix\n",
    "# seems that this is already in the log... ?\n",
    "# # # apply_fix(manual_correction,authors,print_fixes=True)\n",
    "\n",
    "print_fixes = False\n",
    "# this must be first, since it's a key > value fix\n",
    "\n",
    "\n",
    "# run once\n",
    "print(\"------\",'run once')\n",
    "apply_fix(manual_correction2,author_fixes,print_fixes=print_fixes, use_orig_name=True)\n",
    "change = True\n",
    "fix_fct_list = [name_capital_fix,one_letter_drop,kill_underscore,end_comma]\n",
    "iteration = 1\n",
    "while change:\n",
    "    print(\"------\",iteration)\n",
    "    fix_count = 0\n",
    "    for fct in fix_fct_list:\n",
    "        fix_count += apply_fix(fct,author_fixes,print_fixes=print_fixes)\n",
    "    change = fix_count > 0  \n",
    "    if fix_count < 20:\n",
    "        print_fixes = True\n",
    "    iteration += 1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "write_dict_to_file('author-fix.json',author_fixes)\n",
    "\n",
    "# print('fixed',counter,'documents')\n",
    "# write_col_to_file(col,p2_log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_first_name_completion(author_dict):\n",
    "    only_last_name = []\n",
    "    last_and_first_name = []\n",
    "    last_and_first_cut_first = []\n",
    "    for a in list(author_dict.values()):\n",
    "        if ',' in a:\n",
    "            last_and_first_name.append(a)\n",
    "            last_and_first_cut_first.append(a[:a.index(',')])\n",
    "        else:\n",
    "            only_last_name.append(a)\n",
    "#     print('last&first:',len(last_and_first_name),'/ last:',len(only_last_name))\n",
    "\n",
    "\n",
    "    for only_last in only_last_name:\n",
    "        if only_last in last_and_first_cut_first:\n",
    "            index = last_and_first_cut_first.index(only_last)\n",
    "            for sa in author_dict:\n",
    "                if author_dict[sa] == only_last:\n",
    "                    author_dict[sa] = last_and_first_name[index]\n",
    "#             print(a,'>>',last_and_first_name[index])\n",
    "\n",
    "last_first_name_completion(author_fixes)\n",
    "write_dict_to_file('author-fix.json',author_fixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_counter = dict(col.get_names_Counter())\n",
    "fixed_counter = Counter()\n",
    "for author in authors_counter:\n",
    "    author_name = author\n",
    "    if author_name in author_fixes:\n",
    "#         print('replace',author_name,'by',author_fixes[author_name])\n",
    "        author_name = author_fixes[author_name]\n",
    "        \n",
    "    fixed_counter[author_name] += authors_counter[author]\n",
    "    \n",
    "# print([f for f in fixed_counter if fix_counter[f] > 10])\n",
    "# fixed_counter\n",
    "# {ac:authors_counter[ac] for ac in authors_counter if authors_counter[ac] > 10}\n",
    "{ac:fixed_counter[ac] for ac in fixed_counter if fixed_counter[ac] > 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run fix functions over author_set and save the author_set with replacements in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_final_file = 'log-final.json'\n",
    "for file_descr in col.file_descr_list():\n",
    "#     print(file_descr.author_name)\n",
    "  file_descr.author_name  = author_fixes[file_descr.author_name]\n",
    "    \n",
    "write_col_to_file(col,log_final_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some Textacy stuff. experiments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = col.folder_path + col.file_descr_list()[0].rel_path + col.file_descr_list()[0].file_name\n",
    "print(path)\n",
    "\n",
    "os.path.isfile(path)\n",
    "text = textacy.fileio.read.read_file(path,encoding='utf-8')\n",
    "te_doc = textacy.Doc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod_text = textacy.preprocess.preprocess_text(te_doc.text,fix_unicode=True,lowercase=False,transliterate=True,no_contractions=True,no_accents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod_text == te_doc.text\n",
    "lines_orig = te_doc.text.split('\\n')\n",
    "lines_mod = mod_text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(lines_orig)-1):\n",
    "#     print(index)\n",
    "    if lines_orig[index] != lines_mod[index]:\n",
    "        print(lines_orig[index])\n",
    "        print(lines_mod[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_doc = te_doc.spacy_doc\n",
    "for t in sp_doc:\n",
    "    print(t.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_doc = nlp(u'(),Hello, world. Here are two sentences. F5fygi. He saw the movie twice.')\n",
    "for t in en_doc:\n",
    "    print(t,t.tag_,t.tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
