{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import codecs\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "\n",
    "filename = '/mnt/drive1/data/eco/NAIL_DATAFIELD_txt/parsed_v3/parsed_v3_valid.txt'\n",
    "# starting from the expoeriments folder\n",
    "filename = '../../data/NAIL_DATAFIELD_txt/parsed_v3/arts_arthistory_aesthetics_valid.txt'\n",
    "if not os.path.isfile(filename):\n",
    "    print \"file\",filename,'is not there...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def read_file(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        return file.read().decode('utf-8')\n",
    "    \n",
    "def stream_file(file_name):\n",
    "    with codecs.open(file_name, 'r', 'utf-8') as file:\n",
    "        for line in file:\n",
    "            yield line\n",
    "\n",
    "def write_lists_to_file(lists):\n",
    "    with codecs.open('character_file.txt','w','utf-8') as out:\n",
    "        for al in lists:\n",
    "            try:\n",
    "                out.write(al[0]+','+str(al[1])+'\\n')\n",
    "            except:\n",
    "                print 'error with',al\n",
    "    # safety dump\n",
    "    with open('character.dump','w') as dump_file:\n",
    "        pickle.dump(characters,dump_file)\n",
    "    \n",
    "def find_character_occurences(processed_txt):\n",
    "    \"\"\"\n",
    "    Return a list of actors from `doc` with corresponding occurences.\n",
    "    \"\"\"\n",
    "    total_len = len(processed_txt)\n",
    "    characters = Counter()\n",
    "    \n",
    "    index = 0\n",
    "    for ent in processed_txt.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            characters[ent.lemma_] += 1\n",
    "        if index % (total_len/10) == 0:\n",
    "            print '*',\n",
    "        index += 1\n",
    "            \n",
    "    return characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'The first three volumes provide a broad historic and philosophical framework for the understanding of the arts in education .\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tests\n",
    "file = stream_file(filename)\n",
    "file.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# higher helper functions\n",
    "\n",
    "class spacy_helper:\n",
    "    \n",
    "    def __init__(self, lang='en'):\n",
    "        \n",
    "        self.nlp = spacy.load(lang) \n",
    "        self.in_generator = None\n",
    "        \n",
    "    def spacy_process_file(self, file_name):\n",
    "        self.in_generator = stream_file(file_name)\n",
    "        for line in self.in_generator:\n",
    "            yield self.nlp(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The first three volumes provide a broad historic and philosophical framework for the understanding of the arts in education ."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_spacy = spacy_helper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first three volumes provide a broad historic and philosophical framework for the understanding of the arts in education .\n",
      "\n",
      "first ORDINAL\n",
      "three CARDINAL\n",
      "The subsequent volumes elaborate the implications of this comprehensive aesthetic for each of the six major art disciplines and for the teaching of the arts in the primary school .\n",
      "\n",
      "six CARDINAL\n",
      "Library of Congress Cataloging-in-Publication Data The Symbolic order : a contemporary reader on the arts debate edited by Peter Abbs. p. cm .\n",
      "\n",
      "Peter Abbs PERSON\n",
      "Peter Abbs $$$\n"
     ]
    }
   ],
   "source": [
    "space_gen = sup_spacy.spacy_process_file(filename)\n",
    "person = None\n",
    "while not person:\n",
    "    sentence = space_gen.next()\n",
    "    print sentence\n",
    "\n",
    "    for ent in sentence.ents:\n",
    "        print ent, ent.label_\n",
    "        if ent.label_ == 'PERSON':\n",
    "            print ent, '$$$'\n",
    "            person = ent\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open the file\n",
    "a = spacy_helper()\n",
    "# text = read_file(filename)\n",
    "# print 'text read'\n",
    "# # Process `text` with Spacy NLP Parser\n",
    "# processed_text = nlp(text)\n",
    "# print 'text processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He ’ d approve the inverted commas , believing that you can ’ t teach poetry , only ‘ the craft of verse… ’ .\n",
      "\n",
      "<type 'spacy.tokens.doc.Doc'>\n",
      "#########\n",
      "He PRON PRP nsubj 0.0\n",
      "’ PUNCT . punct 0.0\n",
      "d X XX aux 0.0\n",
      "approve VERB VB ROOT 0.0\n",
      "the DET DT det 0.0\n",
      "inverted ADJ JJ amod 0.0\n",
      "commas NOUN NN dobj 0.0\n",
      ", PUNCT , punct 0.0\n",
      "believing VERB VBG advcl 0.0\n",
      "that ADP IN mark 0.0\n",
      "you PRON PRP nsubj 0.0\n",
      "can VERB MD aux 0.0\n",
      "’ VERB VB ccomp 0.0\n",
      "t PRON PRP dobj 0.0\n",
      "teach VERB VB conj 0.0\n",
      "poetry NOUN NN dobj 0.0\n",
      ", PUNCT , punct 0.0\n",
      "only ADV RB advmod 0.0\n",
      "‘ VERB MD ccomp 0.0\n",
      "the DET DT det 0.0\n",
      "craft NOUN NN dobj 0.0\n",
      "of ADP IN prep 0.0\n",
      "verse NOUN NN pobj 0.0\n",
      "… PUNCT NFP punct 0.0\n",
      "’ PUNCT NFP punct 0.0\n",
      ". PUNCT . punct 0.0\n",
      "\n",
      " SPACE SP  0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1):\n",
    "    sentence = space_gen.next()\n",
    "    a.nlp.parser(sentence)\n",
    "#     parsed = \n",
    "    print sentence\n",
    "    print type(sentence)\n",
    "    print '#########'\n",
    "    for t in sentence:\n",
    "        print t,t.pos_, t.tag_, t.dep_, t.sentiment\n",
    "#         print 'ent_type_',t.ent_type_\n",
    "#         print 'lemma_',t.lemma_\n",
    "#         print 'shape', t.shape_\n",
    "#         ent = token.ent_\n",
    "#         print type(ent)\n",
    "#         print '--'\n",
    "#         print ent, ent.label_\n",
    "#         print 'lemma', ent.lemma_\n",
    "#         print 'root.dep', ent.root.dep_\n",
    "#         for child in ent.root.head.children:\n",
    "#             print '-',child\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "characters = find_character_occurences(processed_text)\n",
    "print 'characters found'\n",
    "write_lists_to_file(characters)\n",
    "print 'characters written. done for today'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(u'Hello, Rubert. Natural Language Processing in 10 lines of code.')\n",
    "characters = find_character_occurences(doc)\n",
    "print characters\n",
    "write_lists_to_file(characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets have some recursive folder parsing\n",
    "# in the doc2vec_helper\n",
    "base_dir = '../../data/parsed_v3/'\n",
    "base_dir += 'arts_arthistory_aesthetics'\n",
    "\n",
    "def path(base_path,file_name):\n",
    "    if not base_path.endswith('/'):\n",
    "        base_path += '/'\n",
    "    return base_path + file_name\n",
    "\n",
    "def get_dirs(base):\n",
    "    return [f for f in os.listdir(base) if os.path.isdir(path(base,f))]\n",
    "\n",
    "def get_files(base):\n",
    "    if not base.endswith('/'):\n",
    "        base += '/'\n",
    "    return [f for f in os.listdir(base) if os.path.isfile(path(base,f))]    \n",
    "\n",
    "def my_filter(base_path,file_list):\n",
    "    def cool_file(path):\n",
    "        return path.endswith('.txt') and os.stat(path).st_size > 0        \n",
    "    return [f for f in file_list if cool_file(path(base_path,f))]  \n",
    "\n",
    "folder_list = get_dirs(base_dir)\n",
    "print folder_list\n",
    "files =  my_filter(base_dir,get_files(base_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### feeding depency parse into a rnn in order to detect broken sentences\n",
    "text = read_file(folder_list)\n",
    "processed_text = nlp(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
