{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os, codecs, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "act_dir = '../../data/NAIL_DATAFIELD_txt/parsed_v3/library_and_archive_theory/'\n",
    "le = os.listdir(act_dir)\n",
    "print len(le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    \n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.folder):\n",
    "            if fname[-4:] == '.txt':\n",
    "                with codecs.open(self.folder + fname, 'r', 'UTF-8') as out:\n",
    "                    yield len(out.read())\n",
    "                    \n",
    "lem = MyCorpus(act_dir)\n",
    "# for i in lem:\n",
    "#     print i\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doc_clean_tokenz_():\n",
    "    global sected_global_counter\n",
    "    for f in iter_file_filter(act_dir, formats=['_valid.txt']):\n",
    "        print sected_global_counter,f\n",
    "        sected_global_counter += 1\n",
    "        with codecs.open(act_dir + f, 'r', 'UTF-8') as file_int:\n",
    "            text = file_int.read()\n",
    "        doc = nlp(text)\n",
    "        tokenz_clean = [token.text for token in doc if not token.is_stop]\n",
    "        yield tokenz_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting doc_helper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile doc_helper.py\n",
    "import codecs\n",
    "\n",
    "def make_doc_one_liner(doc_file):\n",
    "    \"\"\"\n",
    "    turns a document with one sentence per line into a document with all sentences in one line\n",
    "    returns the path of the new file\n",
    "    \"\"\"\n",
    "    with codecs.open(doc_file, 'r', 'UTF-8') as file_int:\n",
    "        new_file_path = add_file_descriptor(doc_file,'1Line')\n",
    "        with codecs.open(new_file_path, 'w', 'UTF-8') as file_out:\n",
    "            for l in file_int:\n",
    "                file_out.write(l.strip() +' ')\n",
    "    return new_file_path\n",
    "\n",
    "def add_file_descriptor(file_name, descriptor):\n",
    "    \"\"\"\n",
    "    add a file descriptor which attached to the end of the file name with _<descriptor>\n",
    "    use for something like _valid, _fault, _1Line\n",
    "    \"\"\"\n",
    "    ending = file_name[file_name.rfind('.'):]\n",
    "    return file_name[:file_name.rfind('.')] + '_' + descriptor + ending\n",
    "\n",
    "\n",
    "def read_text(doc_file):\n",
    "    \"\"\"\n",
    "    utf-8 read whole document\n",
    "    \"\"\"\n",
    "    with codecs.open(doc_file, 'r', 'UTF-8') as file_int:\n",
    "        return file_int.read()\n",
    "\n",
    "## REPAIR!!! select descriptor. e.g. _valid _1Line\n",
    "def iter_file_filter(folder, format='.txt', descriptors=['']):\n",
    "    for file_ in os.listdir(folder):\n",
    "        path = folder + file_\n",
    "        size = os.stat(path).st_size\n",
    "        if  size == 0:\n",
    "            continue\n",
    "            \n",
    "        for form in formats:\n",
    "            if file_[-len(form):] == form: # use . check \n",
    "                yield file_            \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spacy_helper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spacy_helper.py\n",
    "\n",
    "import spacy\n",
    "import doc_helper\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "    \n",
    "class Spacy_helper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def doc_clean_tokenz(self, doc_file):\n",
    "        text = doc_helper.read_text(doc_file)\n",
    "        doc = nlp(text)\n",
    "        yield doc\n",
    "        tokenz_clean = [token for token in doc if not token.is_stop]\n",
    "        yield tokenz_clean\n",
    "        \n",
    "    def doc_yield_lemmas(self, tokenz):\n",
    "        return (token.lemma_ for token in tokenz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_file_descriptor(ne_file,'1Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "ne_file = '../../data/NAIL_DATAFIELD_txt/adams, thomas-r-a-new-model-for-the-study-of-the-book_valid.txt'\n",
    "make_doc_one_liner(ne_file)\n",
    "with codecs.open(ne_file+'.1L.txt', 'r', 'UTF-8') as file_in:\n",
    "    for in_,l in enumerate(file_in):\n",
    "        print in_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2602\n"
     ]
    }
   ],
   "source": [
    "print len(token_list)\n",
    "make_doc_one_liner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def filter_single_words(tokenz):\n",
    "        texts = [[token for token in text if frequency[token] > 1] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenz_clean': [u'Bibliography',\n",
       "  u'meant',\n",
       "  u'writing',\n",
       "  u',',\n",
       "  u'listing',\n",
       "  u'books',\n",
       "  u'.']}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "0 adams, thomas-r-a-new-model-for-the-study-of-the-book_valid.txt\n",
      "1 Allen_W._Wood_Unsettling_Obligations_Essays_on_Reason,_Reality_and_the_Ethics_of_Belief_Center_for_the_Study_of_Language_and_Information_-_Lecture_Notes__2002_valid.txt\n",
      "2 Allison, Henry -Kants-Groundwork-Metaphysics-Morals-Commentary-better-scan_valid.txt\n",
      "3 BARRY.E-beckett_and_authority_the use of cliche_valid.txt\n",
      "4 Bishop-editor-Ann-Peterson-Digital-Library-Use-Social-Practice-Design-and-Evaluation_valid.txt\n",
      "5 Bloom-Jonathan-Paper-Print-History-and-Impact-Paper-Islamic-World_valid.txt\n",
      "6 Bordiga-Amadeo-Amadeo-Bordiga-Archive_valid.txt\n",
      "7 BROTHMAN-afterglow_conceptions_of_record and Evidence in Archive_valid.txt\n",
      "8 Casson,Lionel_-_Libraries_in_the_Ancient_World__2002_valid.txt\n",
      "9 Chartier-Roger-Order-Books-Readers-Authors-and-Libraries-Europe-Between-14th-and-18th-Centuries_valid.txt\n",
      "10 Colburn, Pamela_-_Harland_The_Learning_Commons_Seven_Simple_Steps_to_Transform_Your_Library____2011_valid.txt\n",
      "11 COOK.T-the_concept_of_the archival fonds_valid.txt\n",
      "12 COOK.T-what_is_past_is prologue- A History of Archives from 1898_valid.txt\n",
      "13 Cubitt, Sean-Archive_Ethics_2015_valid.txt\n",
      "14 DRAKE-Encyclopedia of Library & Information Science_valid.txt\n",
      "15 DURANTI-diplomatics_new_uses_for an old science_valid.txt\n",
      "16 DURANTI-the_archival_bond_valid.txt\n",
      "17 FEATHER-STURGES-International Encyclopedia of Information and Library Science_valid.txt\n",
      "18 FEATHERSTONE [ed]-Problematizing Global Knowledge_valid.txt\n",
      "19 Foucault-Michel-Fantasia-Library_valid.txt\n",
      "20 GILLIAND-afterword_in_and_out of the archives_valid.txt\n",
      "21 Gilliland-Swetland-enduring_paradigm_new_opportunities The Value of the Archival Perspective in the Digital_valid.txt\n",
      "22 HANSEN-building-a-virtual-library_valid.txt\n",
      "23 HINE,Christine-.New.Infrastructures.for.Knowledge.Production.Understanding.E-Science.(2006).DDU.LotB_valid.txt\n",
      "24 Hooper-Greenhill-Eileen-Museums-and-Shaping-Knowledge_valid.txt\n",
      "25 Horn - A Natural History of Negation_valid.txt\n",
      "26 Howlett-Peter-How-well-do-facts-travel_valid.txt\n",
      "27 KANT-religion_within_the_bounds of reason_valid.txt\n",
      "28 Kelty, Chrisotpher - The disappearing virtual library (2012)_valid.txt\n",
      "29 kingsley, charles - Alexandria and her Schools_valid.txt\n",
      "30 KNUTH-burning_books_and_leveling libraries_valid.txt\n",
      "31 KNUTH-libricide_the_regime_sponsored_valid.txt\n",
      "32 Konig, Jason & Katerina Oikonom-Ancient Libraries - _947_valid.txt\n",
      "33 Landau, Herbert B. - The Small and Public Library Survival Guide  2009_valid.txt\n",
      "34 Leckie-editor-Gloria-J-Critical-theory-library-and-information-science-exploring-social-across-disci_valid.txt\n",
      "35 LECKIE-etal-Critical Theory for Library and Information Science_valid.txt\n",
      "36 Levine-Norman-Marxs-Discourse-Hegel_valid.txt\n",
      "37 LUBIANO-acts_of_courage (poetry)_valid.txt\n",
      "38 LUPO, SPYER-Homer Document digitiser Manual Sept2011_valid.txt\n",
      "39 MACNEIL-contemporary_archival_diplomatics_as_valid.txt\n",
      "40 MANGUEL-the_library_at_night_valid.txt\n",
      "41 MILLAR-the_death_of_the Fonds_valid.txt\n",
      "42 MORTIMER-Descriptive_Cataloging1_valid.txt\n",
      "43 Nodier, Charles-bibliomaniac_valid.txt\n",
      "44 Pettegree, ADM-French-Book-and-European-Book-World-Library-Written-Word-1_valid.txt\n",
      "45 Raven, James-Lost-Libraries-Destruction-Great-Book-Collections-Antiquity_valid.txt\n",
      "46 Spivak - The Rani of Sirmur - An Essay in Reading the Archives_valid.txt\n",
      "47 Springer, Anna-Sophie - Turpin_Etienne_eds_Fantasies_of_the_Library_valid.txt\n",
      "48 Striphas-Ted-Late-Age-Print-Everyday-Book-Culture-Consumerism-Control_valid.txt\n",
      "49 Svenonius-Elaine-Intellectual-Foundation-Information-Organization_valid.txt\n",
      "50 SWEDENBORG-Books—Their Sphere and Influence_valid.txt\n",
      "51 WIGLEY-unleashing_the_archive_valid.txt\n",
      "Dictionary(52688 unique tokens: [u'demystifies', u'Lactans', u'Negroponte', u'Ninety-\\ufb01ve', u'Poetry']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "print len(list(iter_file_filter(act_dir, formats=['_valid.txt'])))\n",
    "sected_global_counter = 0\n",
    "    \n",
    "di = Dictionary(doc_clean_tokenz())\n",
    "di.save('library_and_archive_theory_raw.dict')\n",
    "print di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(52688 unique tokens: [u'demystifies', u'Lactans', u'Negroponte', u'Ninety-\\ufb01ve', u'Poetry']...)\n"
     ]
    }
   ],
   "source": [
    "print di\n",
    "di.save_as_text('library_and_archive_theory_raw.dict.txt')\n",
    "di.filter_extremes()\n",
    "# print di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sublime library_and_archive_theory_raw.dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inwardness\n",
      "Marianne\n",
      "Cyberinfrastructure\n",
      "Barribeau\n",
      "eing\n",
      "indictors\n",
      "Informal\n",
      "Charming\n",
      "patentability\n",
      "Emperor\n",
      "Burton\n",
      "downloading\n",
      "separ\n",
      "incontinence\n",
      "mapped\n",
      "ewer\n",
      "wayward\n",
      "territorial\n",
      "diarrhoea\n",
      "averse\n",
      "experiences\n",
      "recorder\n",
      "METHODS\n",
      "Lennon\n",
      "lefts\n",
      "cerning\n",
      "coedited\n",
      "engrossed\n",
      "performatives\n",
      "Recruitment\n",
      "shocked\n",
      "jolts\n",
      "Arch\n",
      "Duchamp\n",
      "SisterNet\n",
      "officer\n",
      "Punishment\n",
      "chronological\n",
      "Universités\n",
      "intransigently\n",
      "scanners\n",
      "underscored\n",
      "enclosing\n",
      "microvariations\n",
      "interests\n",
      "con\u0003icts\n",
      "automatism\n",
      "contentious\n",
      "vividly\n",
      "favourite\n",
      "distancing\n",
      "aspired\n",
      "seize\n",
      "veil\n",
      "Proust\n",
      "an~hing\n",
      "errorproneness\n",
      "Relief\n",
      "Seek\n",
      "emotionally\n",
      "counterparts\n",
      "Dalton\n",
      "associology\n",
      "coercing\n",
      "Matthews\n",
      "shoot\n",
      "menacing\n",
      "omnipresent—\n",
      "sticks\n",
      "attained\n",
      "centrally\n",
      "Carpenter\n",
      "Bauer2\n",
      "pitched\n",
      "paperlllakers\n",
      "battered\n",
      "chartes\n",
      "Government\n",
      "Khaldun\n",
      "Duguid\n",
      "AUPs\n",
      "Godhead\n",
      "www.bnf.fr\n",
      "exodus\n",
      "Justiﬁed\n",
      "bibliographictool\n",
      "destabilizing\n",
      "alliance\n",
      "irreconcilable\n",
      "reliability\n",
      "Epistle\n",
      "Barneses\n",
      "kitchens\n",
      "Baltimore\n",
      "philosopher.17\n",
      "SCIENTOMETRICS\n",
      "aids\n",
      "G.\n",
      "Grmek\n",
      "interand\n"
     ]
    }
   ],
   "source": [
    "for a in range(100):\n",
    "    print random.choice(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
