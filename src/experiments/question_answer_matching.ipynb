{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7841\n"
     ]
    }
   ],
   "source": [
    "questions = open('questions.txt', 'r').readlines()\n",
    "answers = open('digital_11000_temp0.5_parsed.txt', 'r').readlines()\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mar/.virtualenvs/eco3_experiments/lib/python3.4/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "google_path = '/home/mar/code/marcel/ECO/data/GoogleNews-vectors-negative300.bin'\n",
    "model = model = gensim.models.Word2Vec.load_word2vec_format(google_path, binary=True)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = []\n",
    "for index, item in enumerate(questions):\n",
    "    if len(item) > 5:\n",
    "        q.append(item.rstrip())\n",
    "questions = q\n",
    "\n",
    "a = []\n",
    "\n",
    "for index, item in enumerate(answers):\n",
    "    if len(item) > 5:\n",
    "        a.append(item.rstrip())\n",
    "answers = a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import textblob\n",
    "import random\n",
    "sentiments = []\n",
    "for text in answers:\n",
    "    text = textblob.TextBlob(text)\n",
    "    sentiments.append((text.string, text.sentiment))\n",
    "sentiments.sort(key=lambda sentiments: sentiments[1], reverse=False)\n",
    "\n",
    "# prints post positive answers\n",
    "for i in range(10):\n",
    "    pass\n",
    "    #print(sentiments[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance\n",
    "\n",
    "def avg_feature_vector(words, model, num_features):\n",
    "    # function to average all words vectors in a given paragraph\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float64\")\n",
    "    nwords = 0\n",
    "\n",
    "    # list containing names of words in the vocabulary\n",
    "    # index2word_set = set(model.index2word) this is moved as input param for performance reasons\n",
    "    for word in words:\n",
    "        if word in model.vocab:\n",
    "            nwords = nwords+1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "        #else:\n",
    "        #    print('not in vocabulary: ' + word)\n",
    "\n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.chat import eliza, iesha, rude,suntsu\n",
    "import copy\n",
    "\n",
    "def tag(string):\n",
    "    \"\"\"\n",
    "    returns tagging for different types of inputs.\n",
    "    sentence as a string > list of tags\n",
    "    sentence as a list > list of tags\n",
    "    word as string > tag\n",
    "    \"\"\"\n",
    "    s_type = type(string)\n",
    "    #print string, \n",
    "    if s_type == list:\n",
    "        return pos_tag(string)\n",
    "    elif s_type == str or s_type == unicode:\n",
    "        sent = word_tokenize(string)\n",
    "        #print sent,len(sent)\n",
    "        if len(sent) > 1:\n",
    "            return pos_tag(sent)\n",
    "        else:\n",
    "            #print pos_tag(sent)\n",
    "            return pos_tag(sent)[0]\n",
    "\n",
    "def remove_unknown_chars(text, model):\n",
    "    return_text = ''\n",
    "    for word in text.split():\n",
    "        if word in model.vocab:\n",
    "            return_text += word\n",
    "            return_text += ' '\n",
    "    return return_text\n",
    "\n",
    "def filter_tags(text):\n",
    "    tags = ['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBZ', 'VBD', 'VBP', 'VBN', 'VBG', 'JJ', 'JJR', 'JJS'] #nouns, verbs and adjectives\n",
    "    good_words = []\n",
    "    for word in text.split():\n",
    "        word_tag = tag(word)\n",
    "        if word_tag[1] in tags:\n",
    "            good_words.append(word)\n",
    "    return ' '.join(good_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7308\n"
     ]
    }
   ],
   "source": [
    "from gensim import matutils\n",
    "from numpy import array\n",
    "from numpy import dot\n",
    "\n",
    "answer_vectors = {}\n",
    "for answer in answers:\n",
    "    a = remove_unknown_chars(answer, model)\n",
    "    a_original = copy.copy(a)\n",
    "    a = filter_tags(a).split()\n",
    "    if not a or len(a_original.split()) < 8 or len(a_original.split()) > 30:\n",
    "        continue\n",
    "    vector = [model[word] for word in a]\n",
    "    v = matutils.unitvec(array(vector).mean(axis=0))\n",
    "    answer_vectors[a_original] = v\n",
    "print(len(answer_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import language_check\n",
    "tool = language_check.LanguageTool('en-US')\n",
    "\n",
    "question = random.choice(questions)\n",
    "question = 'i like computers'\n",
    "matches = tool.check(question)\n",
    "question = language_check.correct(question, matches)\n",
    "print('---- Random question:')\n",
    "print(question)\n",
    "\n",
    "similarities = []\n",
    "q = remove_unknown_chars(question, model)\n",
    "for word in q.split():\n",
    "    word_tag = tag(word)\n",
    "    print(word + ' ' + word_tag[1])\n",
    "q = filter_tags(q)\n",
    "q_vector = [model[word] for word in q.split()]\n",
    "q_v = matutils.unitvec(array(q_vector).mean(axis=0))\n",
    "for key, value in answer_vectors.items():\n",
    "    dist = dot(q_v, value)\n",
    "    similarities.append((key, dist))\n",
    "print(len(similarities))\n",
    "similarities.sort(key=lambda similarities: similarities[1], reverse=True)\n",
    "print('---- Most similar match by n_similarity:')\n",
    "for i in range(5):\n",
    "    text = similarities[i]\n",
    "    print(text)\n",
    "    matches = tool.check(similarities[i][0])\n",
    "    corrected = language_check.correct(similarities[i][0], matches)\n",
    "    print(corrected)\n",
    "print('---- Least similar match by n_similarity:')\n",
    "for i in range(5):\n",
    "    print(similarities[(-i) - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = model.most_similar(positive=['I', 'shaking'], negative=['do', 'sleeping'], topn=10)\n",
    "print(res)\n",
    "res2 = model.similarity('shaking','do')\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
