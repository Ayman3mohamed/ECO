{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from IPython.core.display import display, HTML\n",
    "import locale, os, socket\n",
    "from numpy import int64\n",
    "locale.setlocale(locale.LC_ALL, 'de_DE.utf-8')\n",
    "\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "\n",
    "host = socket.gethostname() \n",
    "if host == 'lyrik':\n",
    "    model_file = '/home/ramin/projects/ECO/src/python/modelbuilder/parsed_v3_valid.doc2vec'\n",
    "    print \"U ARE ON LYRIK\"\n",
    "    # ... move data and model into some convinient folder. so that model/parsed_v3_valid is there and\n",
    "    # NAIL_DATAFIELD_txt/parsed_v3/parsed_v3_valid.txt is there\n",
    "else:\n",
    "    # local\n",
    "    model_file = '../../models/parsed_v3_valid.doc2vec'    \n",
    "\n",
    "if not os.path.isfile(model_file):\n",
    "    print \"MODEL FILE IS NOT THERE. GO AND FIND IT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2 Build sentence list (each sentence needs at least 1 tag)\n",
    "import socket, os\n",
    "import gensim\n",
    "\n",
    "host = socket.gethostname() \n",
    "print(host)\n",
    "model_file = '/home/marcel/drive/data/eco/NAIL_DATAFIELD_txt/parsed_v4/parsed_v4_valid.doc2vec'\n",
    "\n",
    "if host == 'lyrik':\n",
    "    filename = '/home/marcel/drive/data/eco/NAIL_DATAFIELD_txt/parsed_v4/parsed_v4_valid.txt'\n",
    "else:\n",
    "    # local\n",
    "    filename = '../../data/parsed_v3/parsed_v3_valid.txt' # parsed_v3_all.txt\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    print \"TEXTFILE FILE IS NOT THERE\"    \n",
    "\n",
    "sentences = []\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "for uid, line in enumerate(open(filename)):\n",
    "    if uid % 1000 == 0:\n",
    "        print(str(uid))\n",
    "    csv_split = line.split(';')\n",
    "    ls = gensim.models.doc2vec.LabeledSentence(words=csv_split[0], tags=['SENT_%s' % uid, csv_split[1]])\n",
    "    sentences.append(ls)\n",
    "print len(sentences),'sentences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3 TRAINING OR LOADING the doc2vec model and save it\n",
    "# ALTERNATIVE: LOAD THE MODEL IN THE NEXT CELL\n",
    "\n",
    "# tutorial https://rare-technologies.com/doc2vec-tutorial/\n",
    "# proposes shuffling or learning reate adjustment. we gonna do both\n",
    "# in total 20 epochs\n",
    "# took ca. 6.30 hours\n",
    "\n",
    "# FOR SAFETY REASON, BUILD ONLY WHEN FLAG IS SET\n",
    "\n",
    "train_model = True\n",
    "\n",
    "if train_model:\n",
    "    model = gensim.models.Doc2Vec(alpha=0.025, min_alpha=0.025)  # use fixed learning rate\n",
    "    print('building vocab') \n",
    "    model.build_vocab(sentences)\n",
    "\n",
    "    base_alpha = model.alpha\n",
    "    base_min_alpha = model.min_alpha\n",
    "\n",
    "    for mepoch in range(2):\n",
    "        model.alpha = base_alpha \n",
    "        model.min_alpha = base_min_alpha\n",
    "        for epoch in range(10):\n",
    "            print('epoch',mepoch * 10 + epoch)\n",
    "            model.train(sentences)\n",
    "            model.alpha -= 0.002  # decrease the learning rate\n",
    "            model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "        shuffle(sentences)\n",
    "\n",
    "    # saving the model    \n",
    "    model.save(model_file)\n",
    "    print 'model trained and saved'\n",
    "else:\n",
    "    model = gensim.models.Doc2Vec.load(model_file)\n",
    "    print 'model loaded.',len(model.docvecs), 'vectors'\n",
    "    if len(sentences) != len(model.docvecs):\n",
    "        print 'something is fishy, unequal length: ',len(sentences),'sentences and',len(model.docvecs), 'vectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4 Tiny helper functions\n",
    "\n",
    "def print_word_list(wl):\n",
    "    print wl\n",
    "    str =  ' '.join(wl)\n",
    "    pattern = re.compile('\\s\\W\\s')\n",
    "    shift = 0\n",
    "    for ma in pattern.finditer(str):\n",
    "        str = str[:ma.start(0)-shift]+ma.group(0)[1:]+ str[ma.end(0)-shift:]\n",
    "        shift +=1\n",
    "    if str[-2] == ' ':\n",
    "        str = str[:-2] + str[-1:]\n",
    "    return str\n",
    "\n",
    "def get_print(sentence_or_similar):\n",
    "    if type(sentence_or_similar) is gensim.models.doc2vec.LabeledSentence:\n",
    "        word_list = sentence_or_similar.words\n",
    "    elif type(sentence_or_similar) is int64 or type(sentence_or_similar) is int: # just an index\n",
    "        word_list = sentences[sentence_or_similar].words\n",
    "    else: # TaggedDocument class\n",
    "        word_list = sentences[int(sentence_or_similar[0][5:])][0]\n",
    "    return print_word_list(word_list)\n",
    "    \n",
    "def get_index_tag(sentence):\n",
    "    return sentence.tags[0]\n",
    "\n",
    "def get_index(sentence_or_similar):\n",
    "    if type(sentence_or_similar) is gensim.models.doc2vec.LabeledSentence:\n",
    "        return int64(get_index_tag(sentence_or_similar)[5:])\n",
    "    else:\n",
    "        return int64(sentence_or_similar[0][5:])\n",
    "    \n",
    "def equal_word_lists(index1, index2):\n",
    "    wl1 = sentences[index1].words\n",
    "    wl2 = sentences[index2].words\n",
    "    if len(wl1) != len(wl2):\n",
    "        return False\n",
    "    else:\n",
    "        for i in range(len(wl1)):\n",
    "            if wl1[i] != wl2[i]:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def get_lab_sent_by_similar(similar):\n",
    "    print get_index(similar)\n",
    "    return sentences[get_index(similar)]\n",
    "\n",
    "def get_similarity_by_index(index1, index2):\n",
    "    return model.docvecs.similarity(index1,index2)\n",
    "\n",
    "# HTML Helper\n",
    "def pack_into_elem(tag, clazz, content):\n",
    "    return '<' + tag + ' class=\"' + clazz + '\"> ' + content+ ' </' + tag +'>'\n",
    "\n",
    "pre = '''<style>\n",
    "          .act {font-weight: bold}\n",
    "          .i {color: grey}\n",
    "          .sim {color: orange}\n",
    "          .n {color: blue}\n",
    "          .p {color: red}\n",
    "          .r {color: green}          \n",
    "     </style>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_print(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5 Test: printing sentence 9 and getting the most similar ones.\n",
    "\n",
    "do_tests = False\n",
    "\n",
    "if do_tests:\n",
    "    test_sentence_index = 2639533\n",
    "    print get_print(test_sentence_index)\n",
    "    sims = model.docvecs.most_similar('SENT_'+str(test_sentence_index),topn = 30)\n",
    "    print 'similar sentence',len(sims)\n",
    "    print '\\nSIMILAR SENTENCES\\n'\n",
    "    for sim in sims:\n",
    "        print get_print(sim),sim\n",
    "        \n",
    "# 6 Test: iterate over similar sentences\n",
    "# needs the sentences loaded (cell 2)\n",
    "if do_tests:\n",
    "    index = 1983\n",
    "    # len(sentences)\n",
    "    # print sentences[index]\n",
    "    sentence = get_print(index)\n",
    "    print sentence\n",
    "    selected_indices = [index]\n",
    "\n",
    "    for sentence in range(10):\n",
    "        sims = model.docvecs.most_similar('SENT_'+str(index))\n",
    "        while True:\n",
    "            selected = random.choice(sims)\n",
    "            check_index = int(selected[0][5:])\n",
    "            if check_index not in selected_indices:\n",
    "                break\n",
    "        index = check_index\n",
    "        selected_indices.append(index)\n",
    "        print get_print(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 7 Story Treenode class\n",
    "\n",
    "PARENT = -1\n",
    "QUIT = -2\n",
    "NEXT = -3\n",
    "\n",
    "num_similars = 20\n",
    "num_random = 20\n",
    "\n",
    "class LabSentTreeNode:\n",
    "    \n",
    "    def __init__(self, labeledSentence, parent = None):\n",
    "        self.sentence = labeledSentence\n",
    "        self.sentence_index = get_index(self.sentence)\n",
    "        self.similars = self.get_similars()  \n",
    "        self.randoms = self.get_randoms()\n",
    "        self.children = {} # index: SentenceTreeNode\n",
    "        self.selected_child = '' # None\n",
    "        self.parent = parent\n",
    "             \n",
    "    def get_similars(self):\n",
    "        return model.docvecs.most_similar(get_index_tag(self.sentence),topn = num_similars)\n",
    "    \n",
    "    def get_randoms(self):\n",
    "        randoms = []\n",
    "        for index in range(num_random):\n",
    "            rnd_sen = sentences[random.randint(0,len(sentences))]\n",
    "            randoms.append(rnd_sen)     \n",
    "        return randoms\n",
    "                \n",
    "    def print_options(self):\n",
    "        for index, sentence in enumerate(self.similars):\n",
    "            add = '(*)' if get_similar_index(sentence) in self.children else ''\n",
    "            print index, add, get_print(sentence), \"%.3f\" % sentence[1]\n",
    "        if self.parent:\n",
    "            print 'p: ', get_print(self.parent.sentence)\n",
    "        if self.sentence_index < len(sentences) - 2:\n",
    "            print 'n: ', get_print(sentences[self.sentence_index + 1])           \n",
    "        for index,sentence in enumerate(self.randoms):\n",
    "            print 'r'+str(index) +\": \",  get_print(sentence)\n",
    "            \n",
    "    def get_options_html(self):\n",
    "        html = ''\n",
    "        for index, sentence in enumerate(self.similars):\n",
    "            content = pack_into_elem('span','',get_print(sentence))\n",
    "            index = pack_into_elem('span','',str(index)+': ')\n",
    "            sentence_index = get_index(sentence)\n",
    "            index += '❗️' if sentence_index in added_sentences else ''\n",
    "            index_distance =  locale.format('%d', abs(self.sentence_index - sentence_index), 1)\n",
    "            similar = pack_into_elem('span','sim',(\"%.3f\" % sentence[1]) + ' / ' + str(index_distance) + ' / ' + str(self.sentence_index))\n",
    "            html += pack_into_elem('div', '', index + content + similar)\n",
    "        if self.parent:\n",
    "            content = pack_into_elem('span','',get_print(self.parent.sentence))\n",
    "            index = pack_into_elem('span','','P: ')\n",
    "            sentence_index = self.parent.sentence_index\n",
    "            index_distance =  locale.format('%d', abs(self.sentence_index - sentence_index), 1)\n",
    "            similarity = get_similarity_by_index(self.sentence_index, self.parent.sentence_index)\n",
    "            similar = pack_into_elem('span','sim',(\"%.3f\" % similarity) + ' / ' + str(index_distance) + ' / ' + str(self.sentence_index))\n",
    "            html += pack_into_elem('div', 'p', index + content + similar)\n",
    "        if self.sentence_index < len(sentences) - 2:\n",
    "            content = pack_into_elem('span','n',get_print(sentences[self.sentence_index + 1]))\n",
    "            index = pack_into_elem('span','','N: ')\n",
    "            sentence_index = self.sentence_index + 1\n",
    "            index += '❗️' if sentence_index in added_sentences else ''\n",
    "            index_distance =  locale.format('%d', abs(self.sentence_index - sentence_index), 1)\n",
    "            similarity = get_similarity_by_index(self.sentence_index, self.sentence_index + 1)\n",
    "            similar = pack_into_elem('span','sim',(\"%.3f\" % similarity) + ' / ' + str(index_distance) + ' / ' + str(self.sentence_index))\n",
    "            html += pack_into_elem('div', 'n', index + content + similar)\n",
    "        for index,sentence in enumerate(self.randoms):\n",
    "            content = pack_into_elem('span','',get_print(sentence))\n",
    "            index = pack_into_elem('span','','R'+str(index)+': ')\n",
    "            sentence_index = get_index(sentence)\n",
    "            index += '❗️' if sentence_index in added_sentences else ''\n",
    "            index_distance =  locale.format('%d', abs(self.sentence_index - sentence_index), 1)\n",
    "            similarity = get_similarity_by_index(self.sentence_index, get_index(sentence))\n",
    "            similar = pack_into_elem('span','sim',(\"%.3f\" % similarity) + ' / ' + str(index_distance) + ' / ' + str(self.sentence_index))\n",
    "            html += pack_into_elem('div', 'r', index + content + similar)\n",
    "        html += pack_into_elem('div', '', 'Q: Quit 💣')\n",
    "        return html\n",
    "    \n",
    "    def get_sentence_html(self):\n",
    "        return pack_into_elem('p', 'act', get_print(self.sentence))\n",
    "    \n",
    "    def select_child(self):\n",
    "        u_input = raw_input('Next child: ')\n",
    "        if u_input == 'p':\n",
    "            selected_index = PARENT\n",
    "        elif u_input == 'q':\n",
    "            return None\n",
    "        elif u_input == 'n':\n",
    "            selected_index = NEXT\n",
    "        elif u_input.startswith('r'):\n",
    "            selected_index = 100 + int(u_input[1:])\n",
    "        else:\n",
    "            try:\n",
    "                selected_index = int(u_input)\n",
    "            except ValueError:\n",
    "                return self\n",
    "        if selected_index >= 0 and selected_index < len(self.similars):\n",
    "            lab_sent = get_lab_sent_by_similar(self.similars[selected_index])\n",
    "            child =  LabSentTreeNode(lab_sent, self)\n",
    "            self.children[u_input] = child\n",
    "            self.selected_child = u_input\n",
    "            return child\n",
    "        elif selected_index >= 100 and selected_index < len(self.randoms) + 100:\n",
    "            #print 'random sen'\n",
    "            child =  LabSentTreeNode(self.randoms[selected_index - 100], self)\n",
    "            self.children[u_input] = child\n",
    "            self.selected_child = u_input   \n",
    "            return child\n",
    "        elif selected_index == PARENT and self.parent:\n",
    "            return self.parent\n",
    "        elif selected_index == NEXT:\n",
    "            child =  LabSentTreeNode(sentences[self.sentence_index + 1], self)\n",
    "            self.children[u_input] = child\n",
    "            self.selected_child = u_input   \n",
    "            return child\n",
    "        # a weird number\n",
    "        return self\n",
    "        \n",
    "    def toJSON(self):\n",
    "        children_toJSON = {}\n",
    "        for child_index in self.children:\n",
    "            children_toJSON[child_index] = self.children[child_index].toJSON()\n",
    "            \n",
    "        return {'sentence':get_print(self.sentence),\n",
    "                'index':get_index(self.sentence),\n",
    "               'children':children_toJSON,\n",
    "                'selected_child':self.selected_child\n",
    "               }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 8 Story creator/log helper functions\n",
    "\n",
    "def get_story(root_node):\n",
    "    act_sentence = root_node\n",
    "    story = []\n",
    "    while act_sentence:\n",
    "        story.append(get_print(act_sentence.sentence))\n",
    "        if act_sentence.selected_child  != '':\n",
    "            act_sentence = act_sentence.children[act_sentence.selected_child]\n",
    "        else:\n",
    "            break\n",
    "    return story\n",
    "\n",
    "def log_json(root_node):\n",
    "    with open('logs/log-'+str(number_logs)+'.json','w') as output:\n",
    "        output.write(json.dumps(root_node.toJSON(),indent=2))\n",
    "    \n",
    "def log_story(root_node):\n",
    "    story = get_story(root_node)\n",
    "    with open('logs/story-'+str(number_logs)+'.txt','w') as output:\n",
    "        for l in story:\n",
    "            output.write(l+'\\n')   \n",
    "    \n",
    "def print_story(root_node):\n",
    "    story = get_story(root_node)\n",
    "    for l in story:\n",
    "        print(l)   \n",
    "\n",
    "def dump_story(root_node):\n",
    "    with open('logs/story-'+str(number_logs)+'.dump','w') as dump_file:\n",
    "        pickle.dump(root_node,dump_file)\n",
    "        \n",
    "# one list comprehension!!!\n",
    "def concordance(search_word,sen_range = 0):\n",
    "    sentence_indices = []\n",
    "    for index,sen in enumerate(sentences):\n",
    "        wl = sen[0]\n",
    "        for word in wl:\n",
    "            if word == search_word:\n",
    "                sentence_indices.append(index)\n",
    "#                 print 'o',get_print(index)\n",
    "                for r in range(sen_range):\n",
    "                    # prevent missing element\n",
    "                    sentence_indices.append(index-r)\n",
    "                    sentence_indices.append(index+r)\n",
    "    return sentence_indices\n",
    "\n",
    "def concordance_result(sentences,sen_range = 0):\n",
    "    if len(sentences) < 10:\n",
    "        for sentence in sentences:\n",
    "            print sentence, get_print(sentence)\n",
    "    else:\n",
    "        print len(sentences),'sentences. 5 random ones:'\n",
    "        for i in range(5):\n",
    "            print sentence, get_print(random.choice(sentences))\n",
    "\n",
    "def ask_word_input():\n",
    "    indices = []\n",
    "    while len(indices) == 0:\n",
    "        first_input = raw_input('First input: ')\n",
    "        indices = concordance(first_input)\n",
    "    print len(indices), 'Sentences with',first_input\n",
    "    sentence = sentences[random.choice(indices)]\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test... doesn't work\n",
    "# print model.most_similar([sentence1,sentence2],[sentence3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# An Alternative method of creating stories.\n",
    "# Start with a Start term and end End term.\n",
    "# up to 'inbetweens' sentences inbetween will be added\n",
    "\n",
    "import numpy as np\n",
    "from gensim import matutils\n",
    "\n",
    "inbetweens = 20\n",
    "\n",
    "sentence1 = ask_word_input()\n",
    "print get_print(sentence1)\n",
    "print '-----'\n",
    "sentence2 = ask_word_input()\n",
    "print get_print(sentence2)\n",
    "\n",
    "vec1 = model.docvecs[get_index_tag(sentence1)]\n",
    "vec2 = model.docvecs[get_index_tag(sentence2)]\n",
    "\n",
    "\n",
    "ps = ([np.linspace(vec1[index],vec2[index],inbetweens) for index,v in enumerate(vec1)])\n",
    "ar = np.ndarray(shape=(inbetweens,100), dtype=float)\n",
    "for ind,v in enumerate(ps):\n",
    "    for i in range(inbetweens):\n",
    "        ar[i][ind] = v[i]\n",
    "\n",
    "story = []\n",
    "topn = 3\n",
    "model.docvecs.init_sims()\n",
    "for index, v in enumerate(ar):\n",
    "    print (inbetweens - index - 1),\n",
    "    dists = np.dot(model.docvecs.doctag_syn0norm, v)\n",
    "    best = matutils.argsort(dists, topn, reverse=True)\n",
    "    ind = 0\n",
    "    while best[ind] in story:\n",
    "        ind +=1\n",
    "        if ind == topn:\n",
    "            break\n",
    "\n",
    "    if ind < topn:\n",
    "        # Hard sentence equality comparison\n",
    "        contains = [sen for sen in story if equal_word_lists(best[ind],sen)]\n",
    "        if not contains:\n",
    "            story.append(best[ind])\n",
    "            print '*',\n",
    "\n",
    "clear_output()        \n",
    "for ind in story:\n",
    "    print get_print(ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### 9 Story creator\n",
    "\n",
    "load_from_log = True\n",
    "\n",
    "added_sentences = set()  \n",
    "\n",
    "number_logs = max(0,len(filter(lambda file_ : file_.endswith('dump'),\n",
    "                          os.listdir(\"logs\"))) - 1)\n",
    "\n",
    "if load_from_log:\n",
    "    with open('logs/story-'+str(number_logs)+'.dump','r') as in_file:   \n",
    "        root_node = pickle.load(in_file)\n",
    "        actual_node = root_node\n",
    "        while actual_node.selected_child != '':\n",
    "            added_sentences.add(actual_node.sentence_index)\n",
    "            actual_node = actual_node.children[actual_node.selected_child]            \n",
    "else:  \n",
    "    sentence = ask_word_input()\n",
    "    root_node = LabSentTreeNode(sentence)\n",
    "    actual_node = root_node\n",
    "  \n",
    "    print get_print(sentence)\n",
    "while actual_node:\n",
    "    clear_output()\n",
    "    log_json(root_node)\n",
    "    log_story(root_node)\n",
    "    dump_story(root_node)\n",
    "    added_sentences.add(actual_node.sentence_index)\n",
    "    display(HTML(pre + actual_node.get_sentence_html() + actual_node.get_options_html()))\n",
    "    time.sleep(0.4)\n",
    "    actual_node = actual_node.select_child()\n",
    "clear_output()\n",
    "print '📖 ⭐ 📖'\n",
    "print_story(root_node)\n",
    "print '👋🏽'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model.docvecs.doctag_syn0norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRYING TO TRAIN MORE\n",
    "# RUNS BUT DOESNT EXTEND THE MODEL FILE\n",
    "# print len(model.docvecs)\n",
    "# line = 'Therefore, if you tell me the truth, they are not going to reject what you say.'\n",
    "# ls = gensim.models.doc2vec.LabeledSentence(words=line.split(), tags=['SENT_%s' % len(sentences)])\n",
    "# sentences.append(ls)\n",
    "# model.train([ls])\n",
    "# print len(model.docvecs)\n",
    "# print sentences[len(sentences)-1]\n",
    "# get_similarity_by_index(len(sentences)-1,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next part is for speech Synthesis in the browser get your story spoken\n",
    "\n",
    "1. get the english voices out of the browser\n",
    "2. let it speak. tweak some parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1 get the voices out of the browser. js > python :)\n",
    "display(HTML('''\n",
    "<script>\n",
    "voices = window.speechSynthesis.getVoices();\n",
    "eng_voices = Array()\n",
    "for(var i=0; i < voices.length; i++){\n",
    "    if(voices[i].lang == \"en-US\") {\n",
    "        eng_voices.push(voices[i].name);\n",
    "    }\n",
    "}\n",
    "console.log(eng_voices);\n",
    "IPython.notebook.kernel.execute('voices='+eng_voices);'''))\n",
    "print voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pitch = 1.0\n",
    "rate = 1.0\n",
    "voice = 'Junior'\n",
    "\n",
    "text = get_story(root_node)\n",
    "var = 'var text= \"'+' '.join(text) + '\";'\n",
    "\n",
    "html = '''\n",
    "<script>\n",
    "var synth = window.speechSynthesis;\n",
    "var   voices = synth.getVoices();\n",
    "\n",
    "'''+ var +'''\n",
    "  var utterThis = new SpeechSynthesisUtterance(text);\n",
    "  var selectedOption = \"'''+voice+'''\"\n",
    "  for(i = 0; i < voices.length ; i++) {\n",
    "    if(voices[i].name === selectedOption) {\n",
    "      utterThis.voice = voices[i];\n",
    "    }\n",
    "  }\n",
    "  utterThis.pitch = '''+str(pitch)+''';\n",
    "  utterThis.rate = '''+str(rate)+''';\n",
    "  synth.speak(utterThis);\n",
    "\n",
    "</script>\n",
    "'''\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for all sentences with a specific word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_word = 'motherfucker'\n",
    "sentence_indices = []\n",
    "le = len(sentences)\n",
    "\n",
    "for index,sen in enumerate(sentences):\n",
    "    wl = sen[0]\n",
    "    for word in wl:\n",
    "        if word == search_word:\n",
    "            sentence_indices.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(sentence_indices) \n",
    "print(sentences[sentence_indices[2]])\n",
    "print get_print(sentences[sentence_indices[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model accuracy function outputs bullsh..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# not sure what crap output that is...\n",
    "model.accuracy('questions-words.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
