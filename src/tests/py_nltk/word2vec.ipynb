{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec is a technique for encoding words (or other tokens in a sequence) into high dimensional vectors. These vectors can be used for similarity lookups and arithmetic operations. The word2vec algorithm is implemented by [gensim](https://radimrehurek.com/gensim/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of spending days training our own model on lots of text, we can load a pre-trained model. This one was [trained by Google](https://code.google.com/p/word2vec/) on three million unique words and phrases in news articles. Each word is embedded in a 300-dimensional space. It's a 3.6GB file (compressed to 1.6GB) and can take almost two minutes to load this model from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load_word2vec_format('models/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look up the vector for a single word like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector = model['Germany']\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=[30, 10])\n",
    "plt.plot(vector)\n",
    "plt.plot(model['China'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def word_similar_cosmul_p(word,topn=10, do_print = True,words_only = False):\n",
    "    if do_print: print \"word:\",word\n",
    "    similarities = model.most_similar_cosmul([word],[],topn)\n",
    "    if do_print: print_similarities(similarities)    \n",
    "    if words_only:\n",
    "        similarities = map(lambda sim : sim[0],similarities)\n",
    "    return similarities  \n",
    "\n",
    "def print_similarities(similarity_list):\n",
    "    for similarity in similarity_list:\n",
    "        print similarity[0].ljust(18),similarity[1]\n",
    "        \n",
    "def tag(word):\n",
    "    return TextBlob(word).tags[0]\n",
    "#print tag('win')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we sort the dimensions by one vector, we can see the similar words follow it, and the dissimilar one does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[30, 3])\n",
    "plt.plot(sorted(zip(model['Tuesday'], model['Wednesday'], model['Thursday'], model['teapot'])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these vectors we can make metaphors, such as \"What is the Berlin of Japan (instead of Germany)?\" also written:\n",
    "`Berlin + Japan - Germany`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['Berlin', 'Japan'], negative=['Germany'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, \"What is a queen as a man (instead of a woman)?\" Notice that the words are case sensitive.\n",
    "`queen + man - woman`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['queen', 'man'], negative=['woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look for the most similar words in the 1000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "limit = 1000\n",
    "limited = model.index2word[:limit]\n",
    "matches = [(model.most_similar(positive=[word], topn=1, restrict_vocab=limit)[0], word) for word in limited]\n",
    "sorted([(x[1],x[0],y) for x,y in matches], reverse=True)[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim also provides interfaces for how similar two words are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.similarity('computer', 'calculator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.similarity('computer', 'rain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or which words don't belong, like \"[Which one of these things is not like the other](https://www.youtube.com/watch?v=gCxrkl2igGY)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[30, 8])\n",
    "words = ['hammer','shoe','handsaw','pliers','king','space','astronaut']\n",
    "for word in words:        \n",
    "    plt.plot(model[word])\n",
    "plt.show()\n",
    "not_watch = model.doesnt_match(words) # try \"saw\" instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence generation method:\n",
    "from the non-stop words replace the last matching word with same tag &: similar , different sentinement, translation to another area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(\"sweet sour salty wet\".split()) # add umami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you work with word2vec you will notice that opposites are more similar to each other than they are to very different things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.similarity('hot', 'cold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.similarity('hot', 'laptop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a set of words we can plot their similarities to each other, and see that each group of three is similar to each other but not to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# words = [str(i) for i in range(9)]\n",
    "# words = ['first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth', 'tenth', 'eleventh', 'twelfth']\n",
    "# words = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "words = ['Monday', 'Britian', 'Cool', 'Barcelona', 'Tango', 'Art', 'Friday']\n",
    "# words = ['breakfast', 'lunch', 'dinner','computer', 'desktop', 'laptop','sun', 'moon', 'stars']\n",
    "vectors = [model[word] for word in words]\n",
    "similar = [np.dot(vectors, vector) for vector in vectors]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pcolor(np.array(similar), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.yticks(0.5 + np.arange(len(words)), words)\n",
    "plt.xticks(0.5 + np.arange(len(words)), '' * len(words))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another idea is to find a path from one word to another. Instead of doing a linear interpolation from one vector to another, we need to do a high-dimensional spherical interpolation. This example uses code from [Transorthogonal Linguistics](https://github.com/thoppe/transorthogonal-linguistics/blob/master/transorthogonal_linguistics/slerp_word_path.py). They do some extra filtering to get better results, and you can test it [here](http://transorthogonal-linguistics.herokuapp.com/TOL/boy/man)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "def slerp_points(x0,x1,slerp_n):\n",
    "    theta = np.arccos(x0.dot(x1))\n",
    "    st = np.sin(theta)\n",
    "    T  = np.linspace(0,1,slerp_n)\n",
    "    L1 = np.sin((1-T)*theta)/st\n",
    "    L2 = np.sin(T*theta)/st\n",
    "    SL = np.outer(L1,x0) + np.outer(L2,x1)\n",
    "    return (SL.T / np.linalg.norm(SL,axis=1)).T\n",
    "\n",
    "def print_path(start, end, steps=10, limit=10000, topn=10):\n",
    "    x0 = model[start]\n",
    "    x1 = model[end]\n",
    "    all_words = [start, end]\n",
    "    between = slerp_points(x0, x1, steps)\n",
    "    limited = model.syn0norm if limit is None else model.syn0norm[:limit]\n",
    "    for x in between:\n",
    "        sims = np.dot(limited, matutils.unitvec(x))\n",
    "        best = matutils.argsort(sims, topn=topn, reverse=True)\n",
    "        print ' '.join([model.index2word[i] for i in best if not model.index2word[i] in all_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_path('man', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a set of words you can sort them so that the path from one word to the next is always similar. This can be done with a traveling salesperson solver, like [this one in Python](https://github.com/dmishin/tsp-solver) or [this one from Google](https://developers.google.com/optimization/routing/tsp?hl=en). Here's a [list of moods](https://gist.github.com/kylemcdonald/e425243c41b2c41de5ba) sorted by a traveling salesperson solver their word2vec vector distance: `...fearless courageous brave daring bold framed blank fake phony inflated manipulated...`\n",
    "\n",
    "We can also use PCA or t-SNE to plot a collection of word2vec vectors in 2d. Let's see if there is a principle axis that the \"capital city\" relationship exists along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(model['China']-model['Beijing'])\n",
    "plt.plot(model['Japan']-model['Tokyo'])\n",
    "# plt.plot(model['man']-model['woman'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "pairs = [\n",
    "    'China', 'Beijing',\n",
    "    'Japan', 'Tokyo',\n",
    "    'Russia', 'Moscow',\n",
    "    'South_Korea', 'Seoul',\n",
    "    'Indonesia', 'Jakarta',\n",
    "    'United_Kingdom', 'London',\n",
    "    'Peru', 'Lima',\n",
    "    'Thailand', 'Bangkok',\n",
    "    'Iran', 'Tehran',\n",
    "    'Egypt', 'Cairo',\n",
    "    'Germany','Berlin',\n",
    "    'Barcelona', 'Berlin']\n",
    "'''\n",
    "pairs = [\n",
    "   'good','better',\n",
    "    'fast','faster',\n",
    "    'big','bigger',\n",
    "    'small',model.most_similar(positive=['better', 'small'], negative=['good'])[0][0]]\n",
    "# pairs = [\n",
    "#     'eyes', 'glasses',\n",
    "#     'hand', 'pencil',\n",
    "#     'head', 'hat',\n",
    "#     'feet', 'shoes',\n",
    "#     'legs', 'pants']\n",
    "pairs_vectors = [model[pair] for pair in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pairs_pca = pca.fit_transform(pairs_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(pairs_pca[:,0], pairs_pca[:,1], '.')\n",
    "pair_pts = zip(pairs_pca[::2], pairs_pca[1::2])\n",
    "pair_names = zip(pairs[::2], pairs[1::2])\n",
    "for pt_pair, name_pair in zip(pair_pts, pair_names):\n",
    "    pt0, pt1 = pt_pair\n",
    "    plt.arrow(pt0[0], pt0[1], pt1[0]-pt0[0], pt1[1]-pt0[1])\n",
    "    name0, name1 = name_pair\n",
    "    plt.annotate(name0, pt0, va='top')\n",
    "    plt.annotate(name1, pt1, va='top')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the relationship between a bunch of countries, you'll see the most related countries ending up next to each other. _Note: Some Countries are missing because they're not in word2vec._ For more words check out [corpora](https://github.com/dariusk/corpora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "with open('data/animals.txt') as f:\n",
    "    words = [line.strip() for line in f]\n",
    "print(len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "\n",
    "words_vectors = []\n",
    "print len(words)\n",
    "print \">>>\"\n",
    "#add_similars(words,n)\n",
    "max_words = len(words) * 2\n",
    "for word in words:\n",
    "    try:\n",
    "        print len(words),max_words\n",
    "\n",
    "        word_tag = tag(word)\n",
    "        token = word.replace(' ', '_')\n",
    "        words_vectors.append(model[token])\n",
    "        #similar = model.most_similar(positive=['queen', 'man'], negative=['woman'])\n",
    "        similar_words = word_similar_cosmul_p(word,do_print = False)\n",
    "        for sim in similar_words:\n",
    "            # print sim\n",
    "            single = Word(sim[0]).singularize()\n",
    "            \n",
    "            if tag(single)[1] == word_tag[1] and single != word and sim[1] > 0.7:       \n",
    "                words_vectors.append(model[single])\n",
    "                words.append(single)\n",
    "                #print \">>\",single\n",
    "                if max_words <= len(words):\n",
    "                    break\n",
    "        if max_words <= len(words):\n",
    "            break\n",
    "    except KeyError:\n",
    "        print 'Ignoring \"' + word + '\"'\n",
    "        \n",
    "print len(words),words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(words),words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "max = len(model.vocab) -1\n",
    "print max\n",
    "wordVocab = [k for (k, v) in model.vocab.iteritems()]\n",
    "model.similarity(wordVocab[randint(0,max)],wordVocab[randint(0,max)])\n",
    "\n",
    "words_vectors2 = []\n",
    "for (i,k, v) in model.vocab.iteritems():\n",
    "    if(i < 10):\n",
    "        print k\n",
    "    words_vectors2.append(v)\n",
    "    \n",
    "print words_vectors2[3]\n",
    "\n",
    "\n",
    "def export_vocab(model,filename):\n",
    "    word_dict = {}\n",
    "    for (k, v) in model.vocab.iteritems():\n",
    "        word_dict[k] = v\n",
    "\n",
    "    return word_dict\n",
    "\n",
    "#word_dict = export_vocab(model,'model')\n",
    "#len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "tsne = manifold.TSNE(n_components=2, perplexity=10, learning_rate=100, verbose=2)\n",
    "#%time countries_tsne = tsne.fit_transform(countries_vectors)\n",
    "\n",
    "%time countries_tsne = tsne.fit_transform(words_vectors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "plt.plot(countries_tsne[:,0], countries_tsne[:,1], '.')\n",
    "for pt, name in zip(countries_tsne, words):\n",
    "    plt.annotate(name, pt)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"test.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import rasterfairy\n",
    "#print countries_tsne\n",
    "\n",
    "num = len(countries_tsne)\n",
    "arrangements = rasterfairy.getArrangements(num)\n",
    "masks = rasterfairy.arrangementListToRasterMasks(arrangements)\n",
    "\n",
    "c = 0\n",
    "for mask in masks:    \n",
    "    print c, \"Type:\",mask['type'],\"\\tProportion:\",mask['width'],\"x\",mask['height'],\"\\tHexagonal:\",mask['hex']\n",
    "    c += 1\n",
    "    \n",
    "rasterMask = masks[0]\n",
    "grid_xy, (width,height) = rasterfairy.transformPointCloud2D(countries_tsne,target=rasterMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(80,30))\n",
    "plt.plot(grid_xy[:,0], grid_xy[:,1], '.')\n",
    "for pt, name in zip(grid_xy, words):\n",
    "    plt.annotate(name, pt)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"test.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "wörter verbinden. die k nächsten\n",
    "sätze anzeigen. constellations\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
