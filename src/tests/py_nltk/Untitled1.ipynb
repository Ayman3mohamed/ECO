{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model memory size, gb: 8.10250639915\n"
     ]
    }
   ],
   "source": [
    "import lib\n",
    "model = lib.loadModel('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in lib... remove from here\n",
    "from nltk.chat import eliza, iesha, rude,suntsu\n",
    "class NltkChatbots:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.bots = {'eliza':eliza.eliza_chatbot,'rude':rude.rude_chatbot,\n",
    "            'iesha':iesha.iesha_chatbot,'suntsu':suntsu.suntsu_chatbot}\n",
    "\n",
    "    def Bot(self, botname):\n",
    "        return self.bots.get(botname,eliza.eliza_chatbot)\n",
    "\n",
    "    def response(self, botname, input):\n",
    "        return self.bots.get(botname,eliza.eliza_chatbot).respond(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resp: What the ancients called a clever fighter is one who not only wins, but excels in winning with ease.\n",
      "pos: ['What'] neg: []\n",
      "Which              0.743672430515\n",
      "How                0.705591380596\n",
      "So                 0.688559770584\n",
      "That               0.686138629913\n",
      "Why                0.669762074947\n",
      "pos: ['the'] neg: []\n",
      "this               0.593737840652\n",
      "in                 0.542929649353\n",
      "that               0.526256978512\n",
      "ofthe              0.515028178692\n",
      "another            0.474835246801\n",
      "pos: ['ancients'] neg: []\n",
      "ancient_Greeks     0.772796213627\n",
      "Ancient_Greeks     0.698848485947\n",
      "ancient_Egyptians  0.691039204597\n",
      "ancient_Romans     0.685644447803\n",
      "Pythagoreans       0.656545102596\n",
      "pos: ['called'] neg: []\n",
      "dubbed             0.643797516823\n",
      "termed             0.604648113251\n",
      "calling            0.593817651272\n",
      "Called             0.570537447929\n",
      "referred           0.546565294266\n",
      "pos: ['a'] neg: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'a' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4e73eb4391a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresponse_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msimilars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarities_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# here we must identify the first real differen word. not different cases, not/normal form of word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-c2efc5ddce52>\u001b[0m in \u001b[0;36msimilarities_p\u001b[0;34m(pos, neg, topn, doPrint)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdoPrint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"pos:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"neg:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdoPrint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/raminsoleymani/anaconda2/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1255\u001b[0m                 \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute similarity with no input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'a' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "bots = NltkChatbots()\n",
    "response = bots.response('suntsu',\"This is the greatest\")\n",
    "print 'resp:',response\n",
    "from nltk.tokenize import word_tokenize\n",
    "response_words = word_tokenize(response)\n",
    "for word in response_words:\n",
    "    similars = similarities_p([word],topn=5)\n",
    "    # here we must identify the first real differen word. not different cases, not/normal form of word\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: ['Germany'] neg: ['Spain']\n",
      "Volker             0.458871692419\n",
      "Bernd              0.453555166721\n",
      "Wolfgang           0.444049060345\n",
      "JÃ¼rgen             0.441551566124\n",
      "Christoph          0.440208911896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'Volker', 0.4588716924190521),\n",
       " (u'Bernd', 0.453555166721344),\n",
       " (u'Wolfgang', 0.44404906034469604),\n",
       " (u'J\\xfcrgen', 0.4415515661239624),\n",
       " (u'Christoph', 0.44020891189575195)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_similarities(similarity_list):\n",
    "    for similarity in similarity_list:\n",
    "        print similarity[0].ljust(18),similarity[1]\n",
    "\n",
    "def similarities_p(pos=[],neg=[],topn=10,doPrint = True):\n",
    "    if doPrint: \n",
    "        print \"pos:\",pos,\"neg:\",neg\n",
    "    similarities = model.most_similar(pos,neg,topn)\n",
    "    if doPrint: print_similarities(similarities)\n",
    "    return similarities\n",
    "\n",
    "similarities_p(['Germany'],['Spain'],topn=5)\n",
    "\n",
    "print model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
