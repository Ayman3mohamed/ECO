Hundreds of people stuck in a giant swimming pool passively floating to the rhythm of artificial waves. The poor resolution of the found footage muddles them into a contextless and faceless crowd. Nobody tries to escape the crowd, or go against the current. They are trapped but happy enough. It’s like Dante’s Inferno but without the drama. Just the people floating in the mud.

The final scene of Mainsqueeze captures “a contemporary atmosphere or mood” which sets the present as a time out of joint, encapsulated by the washing machine that tears itself apart over the course of the film. Rafman poses the present escape from the real towards the simulated as the result of a general feeling of turmoil that leads to flight rather than revolt. In the video, the first readable line of text is written on the forehead of a sleeping drunk man at the beginning of the film: “LOSER”. He smiles, and we are led to wonder who the loser really is.

Yet Rafman is not making a particular ethical statement: “Mainsqueeze expresses a moral condition or atmosphere without making a moral judgment. I gravitate towards communities like 4chan because I see in them a compelling mix of attraction and repulsion. This ambivalence is reflected in the current cultural moment.”

Surfing the deep web, Rafman collects, orders, observes, and makes his source material visible to us: “Mainsqueeze is entirely composed of footage found through my online explorations. The voice over text is a combination of modified quotes from literature, Tumblr, and comments on various message boards. I feel less of a need to create original material from scratch due to the sheer abundance of material out in the world to work with. The craft is found in the searching, selecting or curating, and editing together of the materials pulled from far-flung corners of the web.” Yet, he insists “it is not about fetish tourism or shocking people about what exists in the dark corners of the net, rather, I am giving the sourced material a poetic treatment.”

Rafman assumes, and in turn invites us to assume, a difficult position: he is simultaneously the drunk man, the one who paints his face and the one passerby who thinks they are both stoned. Sometimes he himself indulges in degradation, and we as viewers are equally implicated.

An original use of voice-over contributes to this sense of viewer involvement. It is used neither to generate empathy nor to signify a complete alterity (as when synthesized voices are used). Rafman explains that “This particular tone came about through experimenting with a montage of a wide range of material; moments of philosophical epiphany, pseudo-intellectual quotes from tumblr, banal confessional message boards, comments from reddit threads, etc.” InMainsqueeze, this montage is turned into a profound yet familiar voice that often addresses the viewer directly, dragging you into the world depicted on screen. Let yourself in.

Our story begins between the end of the 1950s and the beginning of the 1960s, when technological progress on one hand and developments in art on the other created the conditions for art, science and technology to intertwine once more. Such an encounter was anything but new in the history of art, having been vigorously embraced by the avant-garde movements: see Lazlo Moholy-Nagy, often invoked as one of the founding fathers of New Media Art, above all for his Licht-raum-modulator (1930), a kinetic sculpture that produces fascinating light effects. And it was the historic avant-garde movements that informed the new artistic experiences that sought to go beyond what then looked like the dead end of Abstract Expressionism: New Dada, Nouveau Réalisme, Gutai, Happening, Fluxus, Kinetic Art, Arte Programmata, Optical Art, Pop Art and Video Art. Reality, in the shape of real or represented objects, entered artworks; the pop culture conveyed by the media began to capture the attention of artists; art appropriated all media, from the human body to consumer products, from advertising to television sets to cars, and theoretical developments like cybernetics and information theory informed the lexicon of art. This is, for example, what John Brockman says about John Cage.


For the first and only time in the history of art, the implicit perspective in the most generic interpretation of the expression New Media Art became a mass strategy, common to all the avantgarde art of the period. This situation was short-lived: while a few “new media” and artistic strategies, from assemblage to photography, performance and conceptual interventions on mechanically reproduced images rapidly became the stuff of the establishment, the more radically technophile or science-based expressions, like Kinetic and Optical art, were put out of action, and video entered a splendid isolation of its own that was to last until the early 1990s. At the same time, in the States, the spectre of permanent war gave an incredible boost to scientific and technological research. In 1946 the University of Pennsylvania presented the first digital calculator, ENIAC (Electronic Numerical Integrator and Computer); 1951 saw the launch of UNIVAC, the first computer to hit the market, capable of processing both numerical data and text. These were huge machines without any kind of user interface, that accepted programs in the shape of perforated cards and could only be operated by highly skilled users. Accessibility was also very restricted: developed for military applications, they resided mostly in research centers and universities. It was in Bell Laboratories in Murray Hill (New Jersey) in particular that the first studies on the algorithmic production of text, music and images were carried out, and not by artists, but engineers and researchers who saw these experiments as more or less necessary diversions to their research work. The electronic engineer A. Michael Noll, for example, was taken on by Bell Labs in 1961 and worked there for 15 years. In the summer of 1962 he created his first works of “Computer Art”, abstract images generated by algorithms and mathematical functions that were an evident tribute to Piet Mondrian and Cubism. Around 1963 many pioneers began working in this direction, including Lillian Schwartz, Herbert Franke, Manfred Mohr, Jean-Pierre Hébert and Roman Verotsko. In April 1965 the Howard Wise Gallery in New York, the same venue that brought Gruppo Zero and Kinetic Art to America, staged the exhibition Computer-Generated Pictures by Bela Julesz and Michael Noll. Computer Art appeared in a number of group shows, including Cybernetic Serendipity (ICA, London 1968), Tendencija 4 (Zagreb 1969) and Computerkunst (Hannover 1969). [2] At the same time, potential uses of computers in literature and music were also being studied: on one hand there was the combinatory literature developed by Alison Knowles at Bell Labs and the members of the European group OuLiPo (Ouvroir de Littérature Potentielle), founded in 1960 by Raymond Queneau and François Le Lionnais; and on the other the work of the composer James Tenney at Bell Labs. [3] This initial foray into Computer Art therefore came about in an extremely restricted context, in both sociological and technological terms. From an aesthetic point of view the massive mainframes of the sixties placed great limitations on artists and were extremely difficult to use, and the result was that in this niche engineers vastly outnumbered genuine artists. In view of this, much Computer Art of the sixties is exceptionally ingenuous aesthetically speaking – in the words of Jim Pomeroy, it rolled out «flashy geometric logos tunneling through twirling wire-frames,’ graphic nudes, adolescent sci-fi fantasies, and endless variations on the Mona Lisa». [4] A. Michael Noll candidly confesses.

Yet dismissing Computer Art as merely ingenuous would be a simplistic way of looking at things. Even supposing that the only achievement of Noll and the first computer artists was to show it was possible to make art with a computer, their contribution to the evolution of the medium was crucial. For Computer Art not only paved the way for New Media Art, but the whole of computer graphics, which over the years has progressed to photorealistic videogames and 3D animation. Even considering merely this dual legacy we can appreciate the scope of its contribution to the culture of the twentieth century. And the success, however fleeting, of Computer Art also points up something else: the openness of the art world of the sixties to the most advanced, precarious fringes of cultural experimentation, its acceptance of ideas that would be hard pressed to find a welcome elsewhere. The best demonstration of this was probably the 1968 exhibition Cybernetic Serendipity curated by Jasia Reichardt at the Institute of Contemporary Art in London. This show was part of the work of the Independent Group and resulted from the 1965 encounter between Reichardt and Max Bense, the German philosopher, a key figure of the Stuttgart school, who studied the relationships between maths, language and art, and coined the term “information aesthetics”. According to Brent MacGregor, it was Bense who told Reichardt to “look into computers”. [6] In 1966 the exhibition was announced at a public conference, and fundraising began. Despite initial expectations, the only private company to invest significantly in it was IBM; the rest was covered by the Arts Council. Cybernetic Serendipity was not an exhibition of Computer Art, but a multidisciplinary event that explored the impact of information technology and cybernetic theory on life and contemporary creativity. It was divided into three sections: the first featured works – images, but also music, animations and texts – generated by computers, the second contained cybernetic robots and “painting machines”, and the third explored the social uses of computers and the history of cybernetics. Alongside the pioneers of Computer Art and cybernetic art, from Charles Csuri to Michael Noll, John Whitney to Edward Ihnatowicz to the Computer Technique Group of Tokyo, were artists who shared aesthetic, thematic or formal characteristics with the latter (Nam June Paik, Jean Tinguely and his machines, James Seawright, the Optical painter Bridget Riley, and avant-garde musicians like John Cage and Jannis Xenakis). But there were also explanatory elements and even a computer, provided by IBM, that offered a service for booking flights. According to the curator.

Cybernetic Serendipity came about in a context, the British context, which was of great interest. Catherine Mason’s research [8] has in fact shown that Britain’s distinctive education system facilitated the development of relationships between art, science and technology between the sixties and the eighties. A legacy of the Victorian education system, Britain’s design schools provided both artistic education and training in the applied arts. In the 1950s, the Independent Group addressed, among other things, the implications of science, technology and the mass media on art and society, culminating in the exhibition This Is Tomorrow (Whitechapel Art Gallery, 1956). In 1953 Richard Hamilton went to teach at King’s College in Newcastle, where, together with Victor Pasmore, he held a Basic Design course. Among their students was Roy Ascott, who was encouraged to cultivate his interest in communication, interactivity and cybernetics. In 1961, Ascott was asked by the Ealing Art School to create a two-year course based on the principles of cybernetics: his Ground Course, along with his subsequent appointments, was to play a crucial role in the education of a new generation of artists and designers. In 1967 the first polytechnics appeared, thanks to sizeable government investments in technology in the post war period, which also led to the creation of a Ministry of Technology. In the polytechnics, as Catherine Mason notes, an art student could also learn programming. In the seventies this led to a wide network of schools engaged in Computer Art, yielding interesting results above all in computer graphics for television and advertising. At the same time, these academic roots enabled students and lecturers to develop their own creative work, despite the relative lack of interest in digital art from the art world. And while British Computer Art survived in the world of academe, it soon developed systems of support and critical debate. In 1968, in connection with the British Computer Society, the Computer Arts Society (CAS) was founded. In 1969, CAS launched its own publication, Page, as a platform for debate and critical engagement. Equally early on, CAS began to look beyond the United Kingdom, setting up chapters in various European countries and coming to the States in 1971. In 1970 the association had 377 members, including libraries and institutions, in 17 countries. In this period it put together a collection that included works by pioneers like Manuel Barbadillo, Charles Csuri, Herbert W. Franke, Edward Ihnatowicz, Ken Knowlton, Manfred Mohr, Georg Nees, Frieder Nake, Lillian Schwartz and Alan Sutcliffe, and in 2007, with Mason’s involvement, this collection was bought by the Victoria and Albert Museum in London. Just how receptive the art world of the sixties was to the “art and technology” pairing is also proved by the milieu that sprung up around the distinctive figure of Billy Klüver (1927 – 2004). An electronic engineer of Swedish origin, in 1958 Klüver was hired by Bell Labs in Murray Hill. With a life-long interest in art, in the early seventies he began to work with artists. In 1960 he provided technical support to the Swiss artist Jean Tinguely (after being introduced to him by Pontus Hultén) for his spectacular Homage to New York (1960), a kinetic machine that self-destructed in the Sculpture Garden of the MoMA in New York. Robert Rauschenberg was also involved in this project. Following that, Klüver provided technical support to various artists: he worked with Rauschenberg on the installation Oracle (1962 – 1965) supplying the artist with remote controlled radios, and he helped Jasper Johns and Andy Warhol, providing the material for the latter’s famous Silver Clouds, the helium-filled pillows that accompanied his temporary break from painting, presented in a solo show at the Leo Castelli gallery in 1966. 1966 also saw Klüver’s first major production, the outcome of a collaboration with Rauschenberg. From 14 to 23 October 1966, at the 69th Regiment Armory in New York, he presented the event 9 Evenings: Theatre and Engineering, a series of multimedia performances featuring ten artists working with thirty engineers and scientists from Bell Labs. Participants included Robert Rauschenberg, John Cage, David Tudor, Yvonne Rainer, Robert Whitman and Öyvind Fahlström. During the event Klüver discussed the idea of giving this collaboration between artists and engineers more stable foundations, and this was what led to the establishment of Experiments in Arts and Technology (E.A.T.), a no-profit association launched at the start of the following year that promoted collaborations between artists and engineers with both technical and financial input, thanks to ongoing links with the technology industry. By 1969 E.A.T boasted 4,000 members and various chapters throughout the United States. [9] Klüver’s collaborative model was in fact a two-way process: while on one hand he was convinced that technicians could help artists achieve their objectives, on the other he believed that artists, as visionaries and active agents of social change, could influence the development of technology. This is Barbara Rose’s take on the matter.

If some kind of follow-up had materialized, these early experiments, and the model pursued by E.A.T. – to get acknowledged exponents of the artistic avant-garde working in close contact with engineers, while keeping their respective roles distinct – could feasibly be attributed a key role in the history of contemporary art. So how did it come to pass that the great emphasis placed in the sixties on the “art and technology” pairing by key figures like Jasia Reichardt, Roy Ascott, Billy Klüver, Robert Rauschenberg and Pontus Hultén, as well as Jack Burnham, gradually waned in subsequent years, leaving only the faintest of traces in the official historiography of art? How was it that one of the most significant components of the neo avant-garde ended up as an underground phenomenon, carving out a niche that enabled it to go unnoticed for the next thirty years? There is no single answer to this question: we must rather look to a series of circumstances that emerged during the seventies. In the first place, in this period the “art and technology” pairing found itself up against ideological and political opposition connected to the military purposes of technological research and the considerable financial interests involved. The Vietnam war, and the protests against it from artistic and intellectual quarters, fuelled opposition to the “art and technology” model. “Technology is what we do to the Black Panthers and Vietnamese”, Richard Serra asserted in 1969. [13] Beyond the political sphere, other academics have highlighted the emergence in the late sixties of “anti-computer” sentiment, bound up with enduring concepts such as the romantic vision of the artists and the fear that technology might supersede the individual and undermine the central role of the artist in the creative act. [14] It has also been observed that the critical model underpinning the acknowledgement of the importance of the “art and technology” pairing has encountered varying fortunes. In a 2007 essay, [15] in line with Jack Burnham, Edward A. Shanken asserts that the hermeneutic approach imposed by Alois Riegl and summed up in the concept of Kunstwollen, quashed the theories of Gottfried Semper, according to whom art reflects “economic, technical and social relationships”. In Shanken’s opinion, this approach still endures today, helping to keep New Media Art outside the canons of contemporary art. In the short term, these two prejudices conspired against the operative and interpretative model of the “art and technology” pairing, with a number of significant results: video retired into a niche, despite continuing to have (limited) critical success, above all in works that put formal exploration of the medium in second place, as per the “narcissistic” line plotted by Rosalind Krauss; Kinetic Art and Optical Art, also steeped in technophile rhetoric, vanished completely from the scene, after an initial period of great success, to be rediscovered only relatively recently; even a certain interpretative approach to Conceptual Art – as put forward by Jack Burnham in Software (New York, Jewish Museum 1970) and Kynaston McShine in Information (New York, MoMA 1970) – that relates conceptual work to the advent of information technologies, surrendered to other approaches with less of a technological vein. As for the nascent field of New Media Art, the collaborative model developed by Klüver was well suited to the organization of one-off events, but less to facilitating continuity in artists’ work. Lastly, Computer Art had to come to terms with its aesthetic limitations and the problems involved in actually accessing the machines, which continued to be expensive and bulky. During the seventies computers became more accessible, albeit gradually. Research into increasingly intuitive forms of manmachine interaction made enormous progress, and in 1969 the first distributed network made its appearance, in the shape of Arpanet. In 1971, thanks to the creation of a common protocol among various university and corporate networks, the internet was born. In parallel to this, alongside the cumbersome mainframes, cheaper, more manageable computers appeared: minicomputers (like the PDP-8, distributed as of 1968); microcomputers, like the famed Altair 8800, distributed as of 1975; and home computers, headed up by the equally legendary Apple II (1977), produced by the startup Apple Computer (founded by Steve Wozniak and Steve Jobs in 1976). With the arrival of home computers on the scene, computing branched out of research centers and universities and entered offices and households. A complex, variegated culture sprung up around them, with contributions not only from engineers and high level researchers, but also amateurs and enthusiasts. Many of them had radical political ideas, influenced by Californian counterculture. Much of the New Media Art of the seventies was an expression of this complex cultural milieu. In this context it is not easy to identify figures who can be described simply as “artists”: most of them worked across the disciplines, researchers and employees of the hi-tech industry with an artistic sideline. Douglas Kahn relates, for example, that the first serious attempt to make music with an Altair 8800 was undertaken between 1970 and 1975 by Ned Lagin, who was doing astronaut training at the MIT, but also studied jazz and composition. This work earned him a temporary collaboration with the Grateful Dead. In the same enclave of enthusiasts in the Bay Area there was Paul De Marinis, who worked with Jim Pomeroy and David Tudor on a number of sound installations before starting out on his own artistic career. [16] Visual experimentation received impetus from university and corporate circles. In Stanford University in 1970, the Xerox Corporation opened the Palo Alto Research Centre (PARC), devoted to the development of graphic applications; in the same year, General Electric presented Genigraphics, a graphic system designed for the business world, but used extensively by artists. In 1973, the main computing association in the United States, the ACM (Association for Computing Machinery), set up SIGGRAPH, its “Special Interest Group on GRAPHics and Interactive Techniques”, which organized its first conference in 1974. From then on SIGGRAPH became the main international showcase for developments in computer graphics. This field was to be heavily influenced by the discovery of fractals, described in 1975 by the French-American mathematician Benoît Mandelbrot, then researcher at IBM, as geometric forms that can be split into parts, each a small scale copy of the whole. [17] Throughout the decade, thanks to institutional and corporate support, research into the algorithmic generation of images thus developed, between the more aesthetically and conceptually conscious work of such artists as Charles Csuri, Manfred Mohr and Vera Molnar on the one hand, and the simple deployment of the productive and aesthetic potential of the new tools on the other. Something similar happened with robotics. In 1973 at the University of California San Diego (UCSD), Harold Cohen launched the AARON project, which consisted in developing a form of artificial intelligence capable of painting. Having trained as a painter, over the years Cohen attempted to teach AARON the basic rules of painting, developing its “aesthetic tastes” and decision-making power. The painting done by AARON naturally closely resembles that of Cohen, though the machine did gradually develop its own style over time. In Britain Edward Ihnatowicz, who in 1971 began working as a Research Assistant in the Department of Mechanical Engineering at University College in London, produced his most ambitious project, the cybernetic sculpture The Senster (1970 – 1974), thanks to a commission from Philips, which exhibited it for four years in its permanent exhibition space in Eindhoven, before dismantling it. The sculpture, a 4 meter aluminium structure controlled by a computer, responded to the voices and movements of viewers. In the late seventies and early eighties it was above all telecommunications that lent New Media Art a presence and a profile outside of the corporate/university world. While one to one communication systems (like the telephone) and one to many systems (like mail) elicited the attention of the avant-garde movements and Fluxus, before the advent of the Internet satellite broadcasting was the technology that afforded concrete opportunities to explore the field of communications. In 1973, for the first time in history, satellite technology succeeded in broadcasting a cultural event – Elvis Presley’s concert in Hawaii – to the whole world. On 29 December 1976, with the support of the Contemporary Arts Museum in Houston, the video artist Douglas Davis broadcast the closing minutes of his performance Seven Thoughts to all the IntelSat channels. The following year, thanks to funding from NASA, the Californian artists Kit Galloway and Sherrie Rabinowitz produced Satellite Arts Project ’77, which connected two NASA centers, one on the East Coast and one on the West Coast, via satellite: images of dancers performing in the two centers were filmed and edited, using a simple chroma-key, to form a single live image. In this way, performers physically 3,000 miles apart could act as if dancing together on the same stage. Dance was adopted as a traditional performing art capable of exploring the limitations and potential of technology. [18] In the same year Documenta 6, curated by Manfred Schneckenburger, was devoted to means of communication, with the aim of exploring the position of art in the media society. The exhibition presented photography, video and video installations, and opened up to television by means of satellite broadcasts of performances by Davis, Nam June Paik and Joseph Beuys.

It was above all in the 1980s that artistic work on communication gathered pace, extending to telematics too. 1980 saw two major events, the conference Artists’ Use of Telecommunications, organized by Carl Eugene Loeffler at the Museum of Modern Art in San Francisco, and Hole in Space, a public art project by Galloway and Rabinowitz. The former was an international event that connected up participants in different areas of the globe by satellite, Slow-Scan TV (video broadcast via telephone) or telematic network: from the Center for Advanced Visual Studies at the M.I.T. in Cambridge (USA) to Japan’s Tsukuba University; from the Alternative Media Center of New York to the Trinity Video and Ontario College of Art in Toronto; from the Western Front Society in Vancouver to the Museum des 20 Jahrhunderts in Vienna. Participants included Robert Adrian, Bill Bartlett, Douglas Davis, Carl Loeffler, David Ross, Aldo Tambellini, Norman White, Gene Youngblood and Peter Weibel. The event highlighted the presence of a solid network of traditional art institutions, research centers and media centers. Hole in Space, on the other hand, created a satellite bridge between public areas in two cities (New York and Los Angeles), with large screens installed at the Lincoln Center for the Performing Arts in New York City and the Broadway Department Store in Century City, Los Angeles, respectively. The screens showed live footage from a camera placed beside each one, enabling people in the street, most of whom were unaware that the event was taking place, to interact with others thousands of miles away. The result was a highly participative, spectacular event, that attracted various audiences who explored different levels of interaction and remote communication: relational aesthetics ante-litteram – but also, as it has been defined on YouTube, “the mother of all video chats”. In 1982 it was the turn of The World in 24 Hours, coordinated by Robert Adrian from the Ars Electronica Festival in Linz and featuring a wide range of communications technologies: from phone to fax, Slow-Scan TV and telematic networks, followed in 1983 by La Plissure du Texte by Roy Ascott (Paris, Musée d’Art Moderne de la Ville de Paris), a collaborative text produced by various users connected by BBS, and in 1984 by Good Morning Mr Orwell, a satellite broadcast of video pieces and live performances coordinated by Nam June Paik and produced by WNET TV in New York in collaboration with the Pompidou Center in Paris, seen by more than 10 million people. All these events reveal both the upsurge in interest from traditional art institutions and the great ferment of the field, with the involvement of both companies and specialized centers, some of which came into being in that very decade. The interest from traditional art institutions must however be seen in context. The nascent technologies were the hot topic of the day, and it was not difficult to get sponsorship from the hi-tech industry and television networks. By the early eighties the latter enjoyed an unprecedented presence in society, and critical reflections on the media and their power to manipulate were advanced by artists and intellectuals, and reached the public at large (Sidney Lumet’s film Network, on the power of television, was released in 1976). Moreover, in the decade that saw the return of painting and the explosion of the art market, the institutions took it upon themselves to support less stable, less marketable artistic genres like video, photography and performance. In other words, while conditions were favorable, the reappearance of New Media Art in the establishment art world during the 1980s was conditioned by external factors and was on the whole too fleeting to lead to lasting continuity. All of this emerges clearly if we consider two key events in this decade: the exhibition Les Immateriaux, curated by Jean Francois Lyotard and Thierry Chaput for the Pompidou Center in Paris in 1985; and the 1986 Venice Biennale, coordinated by Maurizio Calvesi and entitled “Art and Science”. The first was not actually an exhibition devoted to the New Media or art numerique, as it is known in France. It started life as a project on the “new materials of creativity”, but the involvement – at a late stage – of Lyotard transformed it into an exploration of post-modern sensibility. As Lyotard said: «It is not our intention to sum up the new technologies in this exhibition [...] or to explain how they work. All it attempts is to discover and raise a sensibility that is specific to post-modernism, and we assume that it exists already». [19] Its press release described it as a “non-exhibition”, and one of its stated aims was to challenge the modern, “prescriptive” model of the exhibition, connected to the 19th century salon and the gallery. In Les Immateriaux works were not hung on the walls: cables attached to the floor and ceiling divided up a decentralised setting, which could be explored in various ways. Visitors were given a walkman with the soundtrack of the exhibition, which played according to their position in the venue: this collage of music, sounds and texts, only some of which actually related to the exhibition, aimed to create a powerful sensation of instability. The event also featured works by conceptual and minimal artists, from Joseph Kosuth to Dan Flavin and Robert Ryman, precursors like Marcel Duchamp and MoholyNagy, and artists working with communication technologies, such as Roy Ascott and Rolf Gelhaar; yet it was the exhibition itself that was designed “as a work of art”, to the point that the actual works on show are rarely mentioned in the numerous comments that the event elicited. [20] Once again, we are faced with a singular contrast: while on one hand Les Immateriaux was of seminal importance for New Media Art, configuring the aesthetic and philosophic categories that were to be its focus in subsequent decades, on the other hand it showed the art crowd that, as Jasia Reichardt commented with regard to Cybernetic Serendipity, this area was yet to produce any definitive outcomes, comparable with those of other artistic tendencies, and was as yet mainly to be appreciated for its aspect of research and experimentation. Similar observations could be made with regard to the 1986 Venice Biennale, where the “Technology and Computing” section curated by Roy Ascott, Don Foresta, Tom Sherman and Tommaso Trini was given a deliberately “workshop” style layout. The central nucleus of this was the Planetary Network, coordinated by Roy Ascott: for three weeks in this workshop in the heart of the Corderie venue, the artists present conducted communicational exchanges of various kinds with other artists in twenty different locations, from Canada to Australia, using three communications protocols: email, fax and Slow-Scan TV. The networking aspect – artists across the globe working together – clearly prevailed over the actual material exchanged: video, images faxed with manual interventions by the artists involved, computer-generated images and texts. According to Ascott, networking and working within a telematic network – with meetings, interactions, negotiations, and visualizations in the electronic arena – was at the core of this show. [21] In the exhibition catalogue, Tom Sherman [22] also returns to the idea of interaction as a founding element of the electronic arts, in an illuminating text that also dwells on their exclusion from the art world in the 1970s and their radical “difference” that continues to make them unpalatable today: their love of machines, feared by the public at large; their propensity for collaborations, which clashes with the rampant careerism of the art world, and the notion of interaction (between artist and machine, between artists via machine, and between machine and public). The 1986 Biennale was undoubtedly a great platform for New Media Art, which in Venice found a unique opportunity to network and succeeded in exploring a large part of its potential. Around the Planetary Network the event featured the most groundbreaking work in computer graphics, as well as less technological, more amateur images; the “first interactive art videodisc” by Lynn Hershman Leeson; a fascinating installation of sounds and coloured lights by Brian Eno, and the sound environment Very Nervous System (1984) by the Canadian David Rokeby: a space controlled by a system of sensors that perceived the presence of the viewer and his or her movements in the area, translated into sounds by a computer. From the 1980s onwards this vast, variegated scene found its first, privileged point of encounter at the Ars Electronica festival in Linz, Austria. [23] Ars Electronica came about in 1979 as a renewed version of the Bruckner Festival, an event devoted to contemporary music accompanied by an academic symposium. The initial idea was to dedicate the symposium to electronic music. But the involvement of the Austrian Broadcasting Corporation (ORF), directed locally by Hannes Leopoldseder, raised the bar. Leopoldseder proposed going beyond the limits of the symposium and creating a permanent festival devoted to technology and its impact on art and society. On 18 September 1979 the first edition of the Ars Electronica festival opened with a spectacular open-air event, in front of an audience of 100,000. The success of this first edition excited the organisers, who began to think about making it a stable thing. The business model behind it had not yet firmed up, and the following editions, up to 1986, took place on a biennial basis. In the meantime the Austrian artist and curator Peter Weibel joined the artistic committee, and from 1986 the event was scheduled to take place every year, with a common theme for the festival and symposium. 1987 saw the launch of the Prix Ars Electronica, a prize – divided into different categories – that was to play a fundamental role in stimulating creativity, as well as establishing a series of critical and qualitative criteria, and developing a hierarchy of merit within the artistic community. In the early 1990s, feasibility studies were undertaken into founding a permanent center, the Ars Electronica Center in Linz, which got off the ground in 1995, accompanied by Ars Electronica Futurelab. The former was conceived as a “Museum of the Future”, gathering and hosting emerging results from the digital medium, while the latter was devoted to production and research, involving artists in courses and workshops and putting the most advanced technologies at their disposal. As emerges from this brief overview, Ars Electronica and the people involved in it were to play a decisive role in establishing New Media Art world as an independent arena. By stimulating debate, proposing categories and criteria of value, facilitating the production and circulation of works, developing a strategic network with other centers, universities and companies and contributing to the development of an economy and model of sustainability for New Media Art, Ars Electronica became its undisputed mecca. Locally, the Ars Electronica model was made possible by the fact that the post-industrial city of Linz was attempting to reinvent itself as the cultural and technological capital of Austria and central Europe. But its success was above all linked to the existence of a flourishing art scene in search of a stable platform for producing and exhibiting its work, not linked to one-off events like the aforementioned 1986 Biennale, and to the slow but ongoing development of an alternative system of festivals and centers like V2_, launched in Hertogenbosch, Holland in 1981 before moving to Rotterdam in 1994, where it stages a biennial festival called the Dutch Electronic Art Festival (DEAF). All these developments are obviously a product of the inexorable progress of technology, which was gradually seeping into everyday life. After the Apple II, various models of home computer appeared on the market: from the Atari 400 to the Commodore VIC-20, the first computer to achieve sales of over a million; from the Sinclair ZX Spectrum to the Commodore 64 and the IBM PC. In 1984, Apple Computer launched the Macintosh, a genuine revolution in the history of the personal computer: relatively cheap (at almost 2,500 dollars), the computer functioned with keyboard and mouse, and featured a graphic interface that replaced the customary green text against a black background. This graphic interface heralded the introduction of common metaphors inspired by the world of the office that the computer was destined for: desktop, wastebasket, windows, files and documents. Lastly, the computer featured a modem, a device that enabled it to connect up to a telematic network via a simple telephone line. Telematic networks also began to spread, and while Internet remained mainly linked to the American university system, some countries (like France with Minitel) created a national network, and on an amateur level BBS (Bulletin Board Systems) took off. These computer systems functioned like electronic noticeboards, with users connecting to them to share or download files and exchange messages. BBS technology first appeared in 1977 and became popular above all thanks to Fidonet, (invented by the American Tom Jennings in 1984), a network of different BBS. But computing did not make its way into households (and the everyday lives of millions) only by means of home computers and networks. In 1961 the MIT labs created Spacewar!, the first videogame in history. It did not take long for the business world to realise that this very basic interactive interface could be the start of a profitable sector of cultural entertainment. In the second half of the 1970s arcade games took off, along with the first home platforms for videogames. From Pong (1972) to Space Invaders (1978) and Pacman (1980), the videogames industry expanded exponentially, and the advent in 1983 of the NES (Nintendo Entertainment System) was to make an indelible mark on the collective consciousness. These developments had conspicuous consequences on the cultural sphere. The 1980s were the decade of hackers, cyberpunk, basic telematics, virtual reality and the start of the free software movement: phenomena which are too complex to be explored in detail here. Cyberpunk, for example, came about as a literary movement in the United States in the early 80s, thanks to the science fiction successes of William Gibson and Bruce Sterling, and the rediscovery of Philip K. Dick, but in Italy it developed as a political movement, attaching onto the substrate of punk, the ferment of the social centers and the left-wing protest movements in 1977. [24] Likewise in California, where a pivotal role was played by figures like Timothy Leary, exponent of counterculture and advocate for psychedelic drugs, who went on to develop videogames, use BBS and become a leading figure of “cyberculture”, and scholar of virtual reality. Both the hacker movement and the Free Software philosophy were rooted in this complex milieu. Artists played an active role in shaping this culture, and enriching its imagery with their works. It is often difficult, if not impossible, to separate the art from the context it is an active, integral part of. The association between New Media and New Media Art formed in the previous decades, but consolidated in the 1980s. This arose perhaps because on one hand, these artists were excluded from – or deliberately avoided – traditional artistic contexts, and on the other because there was a proliferation of hybrid, multidisciplinary figures who did not separate their art from their political activism, or their contribution to the network. In 1986, reviewing an Italian festival, Vittorio Fagone wrote about a “third culture”, distinguishing digital culture from humanistic and scientific culture: a culture in which «engineers, mathematicians, information technologists, architects, musicians and artists (or, if we wish, “visual operators”) and graphic designers live and work together, often exchange not roles but models and objectives. Electronic art occupies this space». [25] In parallel, the system of relationships, events and production centers that conveyed and supported “electronic art”, also firmed up. While in previous decades New Media Art was rooted in the universities and research centers, in the 80s New Media Art became an independent “art world” in its own right and laid the foundations for its continued existence. On the networks debate was conveyed above all on the BBS, while in the real world New Media Art was distributed at temporary events like technology and electronic art festivals, in line with the Linz model. Towards the end of the decade the first “New Media Centers” appeared, really taking off in the early 90s. The advent of these new distribution channels outside of the traditional art world gave the “third culture” fairly sound foundations in terms of visibility, critical debate and preservation. Yet in this regard Italy remained a fairly isolated case. Despite the presence of an active, vibrant art scene (with artists and groups like Tommaso Tozzi, the Giovanotti Mondani Meccanici, Correnti Magnetiche, Mario Canali, Studio Azzurro, Giacomo Verde and, later on, Piero Gilardi and Maurizio Bolognini), the lack of institutional involvement led to a proliferation of autonomous, isolated initiatives, the result of voluntary efforts by curators like Mario Costa and Maria Grazia Mattei, conducted mostly in private venues or peripheral institutional settings. Even now Italy has no Media Centers, and its few active festivals struggle to make a name for themselves internationally.

1989 is a pivotal year in terms of gaining insight into the subsequent fate of New Media Art, and could indeed be taken as the symbolic date in its process of institutionalisation. The initial setting for this was Europe, where specialized institutions (art centers, museums, workshops, archives and festivals) flourished at an unprecedented rate. It was in 1989 that the ZKM (Zentrum für Kunst und Medientechnologie) of Karlsruhe (Germany) was founded, a center that could, broadly-speaking, be seen as the leader of this process. In the same year the fall of the Berlin Wall and the Soviet empire opened an entirely new season, for art too. Russia, together with the countries of Eastern Europe, was obliged to speedily institutionalize contemporary art, which to date had been developing in unofficial situations like squats and private homes. This process was heavily influenced by the billionaire philanthropist George Soros with his Soros Centers of Contemporary Art (SCCA). As Lioudmila Voropai writes, [26] there were some interesting aspects to this process of institutionalization. In the first place, New Media Art had always stressed its “social utility” and contribution to the creative development of the New Media, thus adding to the legacy of confusion between the development of the medium and its use for artistic purposes, between “New Media” and “New Media Art”. This confusion was accompanied by the ambiguous and conflictual relationship between New Media Art and contemporary art, and was indeed one of the reasons behind the conflict: the social utility of New Media Art implicitly opposed the non-utility of contemporary art, which not coincidentally bases its economy on a luxury market.

The conflict between the two became even more pronounced when they were made to coexist in the same institution. The institution in question was the ZKM, the very notion of which speaks volumes about the nature of the relationship between contemporary art and New Media Art in the early 1990s. The two different art worlds coexist here, like a separated couple still sharing the same roof, thanks to an apparently virtuous division into a series of “institutes” and departments, coordinated since 1999 by the director Peter Weibel: the Museum of Contemporary Art, founded in 1999 and also a venue for temporary exhibitions; the Media Museum, which has a permanent, and unique, collection of “interactive media art”, accompanied in recent years by a number of “permanent exhibitions” on the latest developments in New Media Art; the Institute for Visual Media, the center’s “research and development” division (founded and directed by the artist Jeffrey Shaw until 2003); the Institute for Music and Acoustics, the Institute for Media, Education, and Economics, and the Filminstitute. In reality the ZKM only opened its premises, in a converted industrial area, in 1997, but it prepared the terrain with a series of temporary initiatives, like the Multimedia festival of 1989. Its vocation, linked to the orientation of its director (or rather the duo Weibel – Shaw) and its origins in the early 90s, made it into a temple for the interactive, immersive and technologically groundbreaking installations of the last decade of the century, so much so that in Europe the expression “ZKM art” is normally used, tongue in cheek, to refer to this kind of art. [27] Criticism aside, the ZKM has the undisputed merit of being the first in the 90s to raise the question of the “museification” of New Media Art, and issues related to how to preserve it and create a canon, in this way establishing a model for other international players, like Tokyo’s Intercommunication Center (ICC), founded in 1990 and given a permanent venue in 1997. Back in Europe, we have already seen how in the 90s various long-standing institutions like Ars Electronica and V2_ reinforced their position. In the Netherlands sizeable institutional investments in the new media led to the foundation in 1990 of the Inter-Society for the Electronic Arts, or ISEA, that organizes the International Symposium on Electronic Art. This association, which moved its headquarters to Montreal in Québec from 1996 to 2001, before returning to Holland, has an extremely international outlook, as evinced by the itinerant nature of the symposium, always staged in a different location. In Germany, the Institute for New Media (INM) in Frankfurt was set up in 1989 as an experimental workshop in the context of the Art School, before evolving into an independent research platform for post-graduate students. 1988 saw the founding in Britain of the FACT in Liverpool (then known as Moviola), which remains the country’s most important New Media Art institution. These are just a few examples on an international panorama in constant expansion. In this context it is inevitable to take a brief look at what was going on in Eastern Europe, not only for the significant contribution it gave to the development of New Media Art in the 90s, but because what went on there in the space of a decade appears to encapsulate the entire history of New Media Art. In Eastern Europe, up to the 90s, avant-garde art existed entirely outside of the institutional sphere. The Open Society Institute & Soros Foundation Network was the first to make a serious move in this direction. As of 1991 SCCAs were set up in 17 former Soviet block countries. These were relatively shortlived: in 1999, after the Soros foundations were restructured, all the SCCAs became independent non-governmental organizations. For many of them this meant tackling the crucial issue of funding, not always an easy task where public funds for culture were in relatively short supply. But some managed to survive. Supporting New Media Art was one of the key missions of the SCCAs. This came about because in an area where the personal computer was still a rarity and a status symbol, the social utility of the centers lay in their ability to guarantee the population (and the artists) access to the network and the new technologies. In postsocialist countries there was no tradition of New Media Art: information technology was linked to military uses and scientific research, and the embargo which followed the war with Afghanistan effectively prevented Western-made technologies from arriving in Russia. Yet the networking that got under way, and the widespread use of the network, enabled New Media Art to flourish. In 1993 the SCCA in Moscow set up its New Media Art Laboratory, led by Alexei Isaev and Olga Shishko. In 1994 the artist Alexei Shulgin established the Moscow-WWW-Art-Lab, and in the same year Gallery 21, a no-profit venue in the famous quarter of Pushkinaskaya 10 – a squat converted into an art center – opened its doors in St. Petersburg. Leaving Russia, Budapest saw the opening of the C3, the Center for Culture and Communication, which is still up and running, and which combined the traditional functions of an art center with teaching activities, holding courses and workshops on Internet and the new technologies, while Ljubljana opened the Ljudmila Digital Media Lab, promoting festivals and events, and supporting the artistic activities of Vuk osi, one of the pioneers of Net Art. As Voropai notes, the post-Soros era began during the golden age of New Media Art in the West. 1999 was the year of net_condition, a travelling exhibition organised by the ZKM, which opened the season of the major museum exhibitions, destined to continue – above all in the States – until 2002. In Russia the decline of the New Media institutions gave rise to a difficult situation. The affirmation of an uncertain, poorly regulated art market, buoyed up by the new rich, who saw art as a way of laying claim to elite status, did not favour New Media Art, which was held – rightly or wrongly – to be an institutional art form.

This is the situation that has come to pass, in a more recent period, and with the same dynamics, in the West. Here the development of a system of New Media Art, by means of the dynamics we have attempted to illustrate, has gone hand in hand with increasing interest from traditional artistic institutions. Yet the latter tend to be uninterested in the underground tradition of New Media Art, and focus their attention on its most recent results, connected to the mass spread of digital technologies and the advent of the web in the second half of the 90s. Indeed at the start of the decade there were as yet few artists using “domestic” technologies with some degree of awareness, to make art: figures like the Italian Maurizio Bolognini, who in the early 1990s produced installations in a highly conceptual vein by reprogramming and “sealing” personal computers in such a way that their vitality and continued functioning, perceptible as a monotonous hum, could be detected but not visualised through any output devices; [28] or like the German artist Wolfgang Staehle, who in New York in 1991 used various BBS to found The Thing, conceived as a “social sculpture” à la Beuys. And while home computing remained the main arena for the formation of the digital cultures of the 90s, at the start of the decade New Media Art focused above all on immersive systems and virtual reality, telepresence and interactivity (with figures like Jeffrey Shaw, David Rokeby, Paul Sermon and, back in Italy, Mario Canali, Piero Gilardi and Studio Azzurro), technological prostheses and robotics (Eduardo Kac, Stelarc), and 3D graphics and generative algorithms (Karl Sims). But this work involved the use of cutting edge technologies, and was too focused on the latest developments in technology and too detached from the developments in contemporary art in that period to be properly interesting in this context. With the advent of the World Wide Web (Mosaic, the first commercial browser, appeared in 1994), and the mass distribution of the personal computer (1995), this situation changed radically. The computers of the 90s were cheap and featured an intuitive interface; anyone, with a minimum of instruction (which was often undertaken in universities, in the workplace or, for the young generations, by means of videogames) could use them. Processing text, modifying images, and creating sound and video files were relatively simple matters. At the same time the web gave the internet network a multimedia, hypertext interface based on a programming language (html), the basics of which can be picked up in a few days. Making art with a computer no longer required technological training, access to research labs, collaborations with engineers and professionals. Anyone could do it, and not necessarily to make art that was accessible only via computer. So while on one hand computers could be used by any artist, they could also be employed by anyone wishing to exploit the extraordinary communicative, aesthetic and narrative potential of the web. Net Art came about in this very way. It was no longer a question of creating the finest image possible with a given tool, or generating an immersive interface, but about exploring and subverting an elementary language, creating a short circuit in communication, infiltrating a global communications medium. The first net artists did not come from the New Media Art of previous years, but from photography (Alexei Shulgin), post-conceptual art (Vuk osi), film (Olia Lialina), street art (Heath Bunting), painting (Mark Napier) and video (Jodi); they had an artistic, rather than a technological training; some turned to the web out of frustration with the contemporary art world, others were fresh out of art school, and others had links with political activism, which in that very period was beginning to realise the web’s unprecedented potential for media impact (Ricardo Dominguez). Net Art was ironic, subversive and played with the limits of meaning; it looked to the avant-garde and neo avant-garde movements; it practiced pastiche, collage and linguistic games, and it was the output of an era of cultural production that eliminated the difference between original and copy. Net Art originated between 1995 and 1997. In 1997 Documenta, one of the most important dates in the contemporary art calendar, had a section devoted to Net Art. The year before, the Swiss collective etoy won a Golden Nica at the Prix Ars Electronica, in the “World Wide Web” category, for the work Digital Hijack, a spectacular operation of search engine manipulation that diverted hundreds of thousands of internet users onto their site. [29] In the “Computer Animation” category, the first prize went to Pixar, for the animated movie Toy Story (1995), the first movie produced entirely using computer graphics. In the photograph that commemorates the event, an etoy agent with a shaved head and mirror sunglasses, in an orange jacket and black trousers, shares the stage with Japanese interactive artist Masaki Fujihata, Canadian electroacoustic music composer Robert Normandeau, and writer and film director Pete Docter from Pixar: they are all smiling, but they seem to be wondering what they are doing on the same stage. And the question is by no means irrelevant: while 1989 was the key year for consolidating the New Media Art world, 1997 was the annus horribilis of the split between the art and its world: the moment when so called “new media artists” started wondering what they had in common, besides the medium and their under-recognition by mainstream art worlds. The events that we have described, from the eighties onwards, appear to be entirely concentrated in Europe. So what was going on with the States, the homeland of the new technologies and the first artistic experiments in this direction? Lev Manovich accounts for [30] the American delay on this front with two simple considerations. In the first place, the rapidity with which the new technologies were assimilated in the States made them invisible in a very short space of time. In other words, in the US there was no hiatus between the arrival of a new technology and its normalization, the hiatus that enables artists to develop a critical distance from the medium. Secondly, Manovich blames the lack of institutional support, at least compared to areas like Western Europe, Australia and Japan, where the New Media Art world leaned heavily on public funding in the 80s and 90s. In the States the art world is market-driven, and in that context an artistic practice that had always professed its unsaleability had trouble getting by for many years. This, at least, was the case until the late 90s, when the situation changed completely. Universities and art schools set up courses and programs of New Media Art and New Media Design; prestigious academic publishers like the MIT Press began producing books on the subject; renowned institutions like the Princeton Institute for Advanced Studies, the Rockefeller Foundation and the Social Science Research Council set about organizing conferences, prizes and funding, and the major contemporary art museums, from the Whitney Museum of American Art in New York to MoMA, from the San Francisco Museum of Modern Art to the Walker Art Center in Minneapolis to the Guggenheim in New York, together with numerous university museums, got involved with exhibitions, programs and curatorial positions. Even some private galleries, like the Postmasters Gallery in New York, staged solo and group shows of New Media Art. Various no-profit organizations (often led by artists) also appeared, along with specialized institutions like Eyebeam in New York, while existing structures like the Electronic Arts Intermix (EAI) founded by Howard Wise in 1971 and mainly focussed on video, opened up more substantially to the digital media. In other words, interest in New Media Art exploded in the States at a period in which the New Media sector was gaining financial thrust, and New Media Art was becoming financially and technically sustainable for any artist. This phenomenon, however, was fairly short-lived: after the collapse of the New Economy, and the consequent disappearance of the funding that had boosted interest in it, the enthusiasm of American museum system cooled off considerably. At this point the American New Media Art scene was faced with two alternatives, both of which it explored. On one hand it attempted to tackle the arduous task of integrating into the contemporary art system and its market. On the other it looked to Europe with interest, attempting to come up with an alternative model for survival that would enable it to preserve its specific characteristics.

As Arthur Danto wrote, [5] from the sixties onwards (namely from the acceptance of the new “paradigm” introduced by Marcel Duchamp in the 1910s with his first readymades) anything and everything could be art, as long as there was an internal reason for which a given thing should be considered art. Identifying this reason, however, is not always easy. Francesco Bonami, in a book that sets out to explain to the man in the street “why contemporary art really is art”, spectacularly fails in this mission by adopting oblique strategies that constantly avoid the question. In the introduction, Bonami explains that to understand a work of art «all you need is an open-minded approach», curiosity and courage, and that the important thing in art is not the technique, but the idea, which has to be “new” and “right”: «The important thing, in any case and if possible before others get there before you, is to think the right thing at the right time». [6] Yet Bonami does not explain the concept of “new”. In this complete absence of rules, the only one that appears to withstand scrutiny, and that Bonomi returns to frequently, is the central role of the idea. The “right idea”, “good contents”, is the only thing that links Duchamp, who «learned how to generate hot air better than others», and the “reactionary” art of Lucian Freud, who paints «as if Duchamp and Warhol had never existed». I have mentioned Bonami’s dumbed-down aesthetics, rather than more structured theories, because I think it reveals something significant about the arena we are analyzing. One of the most renowned international critics and curators, Bonami does not seem to base his work on a specific “idea of art”. He seems to operate more like a water-diviner, who can see art where others cannot – and is almost always in the right place. Obviously this is possible because when Bonami makes his choice, he has the authority and the means to impose it as the “right” choice to other members of the art world: a consideration that implies a contextual definition of art, according to which art is art because there is a surrounding context that says it is. As Blais and Ippolito explain, [7] this idea is nothing more than intellectual provocation (that of Duchamp) turned intellectual inertia (that of today’s art world). If a work of art is defined by its aura, and if in the age of its technical reproducibility that aura is no longer an integral part of it, the process of “conferring” that aura – namely the work of critics, museums, gallerists and dealers – does not follow but actually precedes the recognition of an object as a work of art. Art is art because critics write about it, museums exhibit it and collectors collect it, not vice versa; the aura is the consequence of this intellectual attention, the interest of the museums, the investments made by collectors, and so on, rather than the cause. [8] This theory, which crops up not infrequently among both those within the art world, and those criticizing it from the outside, is undoubtedly an enthralling one. Also because, once embraced, it is very easy to find evidence to back it up, and very difficult to find arguments against it. By way of example, it is all too easy to look at Damien Hirst, one of the stars of today’s art world, and see the results of canny investments made by an advertising mogul (Charles Saatchi), an extremely solid art world (the English establishment), an unprecedented eye for business (that of the artist) and the concerted efforts of museums, collectors, galleries, critics and curators. It is more difficult to explain why his colored dots mesmerize us, why his butterfly wings fascinate us and why his pharmacies and animals in formaldehyde embody our angst more than many other present day works of art. In other words, it is more difficult to understand whether we would have recognised these pieces as works of art before the art world lent them an aura, variously boosted by the torrents of words used to describe them, the floods of money spent on buying them and the sacral ambiance of the white cube. This problem obviously arises from the weak nature of the few attempts that have been made to come up with a definition of art that transcends the contextual theory. Bonami’s “theory of the right idea” encapsulates this weakness fairly well. Even a vastly more sophisticated theory, like that of the philosopher Mario Perniola (2000) does not seem to yield the results hoped for. Today «we consider it “natural” that some objects are works of art and that some people are artists; any other question seems superfluous», [9] Perniola writes. But just what is it, aside from economic worth and communicative value, that makes art art? According to the philosopher, the answer to this question lies in art’s shadow, «a shady form which contains the most unsettling and enigmatic elements that belong to it». Yet Perniola refuses to define this shadow, conscious that by nature it «disappears when exposed to the light». We can at best identify only a few components of that shadow – the “splendour of the real”, the “sex appeal of the inorganic”, the “logic of dissent”. But shedding light on it necessarily means making it vanish. What seems to emerge from all these “weak” theories is the need for strong contents, art’s ability to home in on an issue, objectivize it and present it for our analysis. This also gives rise to prejudice against media specificity, and art that is not “just art”. This prejudice is linked on one hand to the “damnatio memoriae” that struck Clement Greenberg in the States, and on the other to the fact that art appears to have entered a “postmedia” phase that best manifests itself in multimedia installations, and the nomadic shifting between different media that characterizes the work of many artists. In particular, according to Rosalind Krauss, medium specificity was overcome around the 1970s, on one hand by Marcel Broodthaers with his “eagle principle”, that «simultaneously implodes the idea of an aesthetic medium and turns everything equally into a readymade that collapses the difference between the aesthetic and the commodified»; [10] and on the other by video that, sharing the «television’s “constitutive heterogeneity”», proclaimed the end of medium specificity. «In the age of television, so it broadcast – Krauss writes – we inhabit a post-medium condition». [11] Which does not mean that staying with one medium is inappropriate, or that exploring the specific characteristics of that medium is a cardinal sin. Krauss tries to explain this in another essay, significantly entitled “Reinventing the medium”. According to Krauss, a medium can be rediscovered and reinvented by artists in the post-medium phase when it has fallen into obsolescence: not to explore its creative and aesthetic potential, but to examine it as a “theoretical object” of art.
In Remainder, the first novel by the English artist and writer Tom McCarthy, the main character has survived an accident, followed by a grueling rehabilitation process, that has left him with partial memory loss, but compensation of several million pounds. With this money the character attempts relentlessly to regain the authenticity of some brief episodes of his past and present life by faithfully reconstructing and reenacting them. His first project involves reproducing the atmosphere of a house he believes he has lived in. The setting is reconstructed in great detail (down to the cracks in the walls, the black cats on the roof in front, the sounds and the smells), and various “reenactors” are hired full-time to enable him to relive these moments whenever he feels like it. This is followed by other “projects”, staged with the involvement of hundreds of professionals and “reenactors”: the obsessive reconstruction of a minor accident he once had in a gas station, a murder, a bank robbery. All of this is done to enable him to relive the tingling feeling he experiences when authenticity is achieved. At one point someone asks him: «Does he, perhaps, […] consider himself to be some kind of artist?» To which he replies: «No. I wasn’t any good at art. In school». [13] These lines are telling. They reveal that today’s art is not something you learn at school, and is not necessarily associated with traditional artistic techniques. They also say that art is something visionary and gratuitous; it is not to do with objects, but projects, and it does not produce anything of use, but requires total dedication, generous funds and the involvement of many different kinds of professionals. The artist figure that emerges from this picture is still firmly anchored to the romantic vision of the genius, obviously updated to today’s standards. Figures like Olafur Eliasson, who created waterfalls cascading down the struts of New York’s bridges, and Matthew Barney, who spent five years of his life producing an unprecedented cycle of films, conceived in its entirety as a sophisticated allegory of male genitalia, embody this idea to perfection. The romantic genius acquires celebrity status, and is required to be an excellent entrepreneur of him or herself: think of figures like Damien Hirst, Maurizio Cattelan and Francesco Vezzoli, and further back Jeff Koons and Andy Warhol. If we descend gradually from art’s lofty pinnacles into the complex, variegated fauna of artists, many of these aspects fade away, but the one constant, the one thing we always expect from an artist, is absolute devotion to a project, an idea. With this one lodestar established, everything else is up for discussion, renegotiation. The mythos of complete freedom also admits the option of choosing an entirely reactionary path – that of manual skill, technical prowess, obsessively nurturing a single language. Artists can hide their identities behind a pseudonym or a collective: in this way an academic painter like John Currin can rub shoulders with the likes of Jeff Koons, who has skilled craftsman producing his marble busts. And while the latter, who places himself at the center of many of his works, explores – and reinforces – the cult of the personality of the artist, in contemporary art it is not difficult to come across collaborative platforms, in which individual contributions merge into collective output: the existence of collectives like the Indian RAQS Media Collective – a platform that operates on an artistic, critical and curatorial level – comes as no surprise.
In Mercanti d’aura, Alessandro Dal Lago and Serena Giordano assert that the notion of “purpose” represents an insurmountable barrier to an object being a work of art. If an object has a purpose, it cannot be art, because art serves no purpose; it exists unto itself. And the writers go one further, maintaining that objects created to serve a purpose (therefore the products of worlds such as that of fashion, design and the entire cultural industry) possess disturbing properties that make opposition to them particularly vehement. These objects disturb us because they are artworks in all respects, but also «services marked by the stigma of subordinate work». [14] This theory is undoubtedly a fairly convincing one. Conceived by the aesthetes of the late nineteenth century, the idea of art for art’s sake has stayed with us, in various different forms, in the art and criticism of the twentieth century. Yet continuing to envisage the world of contemporary art as an ivory tower under constant threat from base, secondary practices, is frankly anachronistic. All of the arts have their own “art world”, and most of the artifacts they generate can only be appreciated according to the canons of those worlds. Yet each of these worlds can produce – has produced and continues to produce – a series of artifacts (usually a fairly limited series) able to fulfil the conditions of another world, for example that of contemporary art. This happens for various reasons: because the historic schism between some of these “art worlds” is actually a fairly recent thing, and because certain phenomena that are part of the mythology of contemporary art, like modernism, envisaged a reconciliation that continues to crop up at regular intervals – and, lastly – because the contemporary art world, intended as an arena of free experimentation, unfettered by ulterior motives, has always been particularly receptive to approaches and figures viewed as anomalous by the other art worlds. In other words, the skin of the contemporary art world is much more porous and permeable than that of other worlds, and while it may have proved slightly less porous at some periods in its history, the period that began in 1989, with the fall of the Berlin wall and the recovery of the art market after the recession at the end of the 80s, was undoubtedly particularly open to contamination. In his critical and curatorial work Germano Celant has often highlighted this.
This situation has given rise to two movements: one of appropriation, which encourages artists to engage with other media, be it importing them into the contemporary art world or shifting towards those others worlds, and one of convergence, which sees many hybrid, borderline figures (filmmakers, designers, musicians, etc.) bringing their works into the arena of contemporary art. This does not happen, as might be expected, only on the “borders of the empire”, but at its summit, involving figures of prime importance. Think of Matthew Barney and Shirin Neshat, who have taken works to the Venice Film Festival; think of the numerous artists who have directed Hollywood movies (from Robert Longo to Kathryn Bigelow to Julian Schnabel); or Pierre Bismuth, who won an Oscar for his screenplay for the film Eternal Sunshine Of The Spotless Mind (2004), written with the director Michael Gondry. And think, too, of Takashi Murakami’s collaboration with Vuitton, the double identity of Carsten Nicolai (who also works as a musician, going by the name of Alva Noto), and Peter Greenaway’s nomadism. All of this is also facilitated by internal developments in the contemporary art world, which is increasingly forging a presence as one of the sectors of the cultural industry and show business. And museums and institutions, traditionally more conservative, are facilitating this process, hosting exhibitions devoted to fashion and design, in ways that can be debatable and are indeed debated, but are undeniably forging a trend
The question of how all this is to be reconciled with the traditional conception of the visual work of art, intended as an artifact that is unique (or reproduced in limited editions), collectible, and therefore financially valuable, is constantly being renegotiated, and obviously entails some interesting compromises. In the contemporary art world value is attributed by means of a complex system that includes criticism, museums and other institutions, prizes, exhibitions and the market. Not being able to deal with each of these players singly, I will consider above all the market, which, in my analysis, represents the missing link in the world of New Media Art. The art market has played a key role in the world of visual arts since the nineteenth century, when the arts began gradually severing their ties with the nobility and institutional powers, becoming a private activity mainly destined for the cultured bourgeoisie in search of the social prestige that only a productive relationship with the world of culture can confer. Particularly after the Second World War, art became increasingly bound up with the market: in this way, while the “dematerialization of art” became possible in a period when the market was relatively weak, when the market recovered in the 1980s, and there was a resurgence in demand for marketable artifacts, traditional practices like painting and sculpture rose to the fore once more. The collapse of the stock market in 1989, together with other crucial factors – the new geopolitical situation, and AIDS wiping out an entire generation of artists – played a key role in changing the lie of the land in the early nineties. The phase which followed this, and which is still under way, is a complex one for various reasons. Globalization is bringing forth new art scenarios, new exhibiting platforms and new markets; major temporary art events, like the biennales, are springing up, creating new destinations for cultural tourism; contemporary art museums are being revamped, testing the terrain of the global museum, and becoming artistic objects in their own right, with containers that are often more appealing than their contents, boosting the number of services on offer and becoming focal points of a society in which the services sector, media and culture play a key role; and lastly, the advent of the information society has generated an exponential increase in platforms for criticism, with the launch of dozens of new magazines. The art market spearheads this transformation. Private galleries stage events; by means of contemporary art fairs they increasingly condition the construction of museum collections; by paying for advertising space in art magazines they finance art criticism, and even if the relationship forged between the two is not, at least in the most virtuous cases, a genuine exchange, they inevitably end up conditioning the choices made. Art fairs have grown exponentially in the last decade and some of them (like Art Basel, Frieze or New York’s Armory Show) have established themselves as primary cultural events, key destinations for global tourism, on a par with museum exhibitions and biennales. Lastly, auctions, the main arena for the so-called “secondary market”, have gradually opened up to contemporary art and the so-called “primary market”, their fluctuations influencing the careers of artists. In The Art Fair Age (2008), the Spanish critic Paco Barragán defines art fairs as «Urban Entertainment Centers», [16] and contemporary collecting as a pyramid: on the bottom layer, art is sought after as “social capital”, a source of prestige and affirmation; on the next level art is collected as “financial capital”, namely for its investment value; on the third level of the pyramid we find companies who view art as a “brand” of sure-fire appeal, and include it in their market strategies, while at the top we come to private collectors who seek intellectual fulfillment from art. And the latter are increasingly putting their collections into the public domain, by means of donations to museums (like Giuseppe Panza di Biumo), taking over established institutions (like the new Palazzo Grassi owned by the French entrepreneur François Pinault) or setting up their own foundations (like the Fondazione Sandretto Re Rebaudengo in Turin), thus boosting their influence over the process of institutionalization. The close bond between the contemporary art world and its economy was incisively analysed by the English critic Julian Stallabrass in his book Art Incorporated (2004), which explicitly focuses on the «regulation and incorporation of art in the new world order». [17] According to Stallabrass, art’s micro-economy, governed by a handful of dealers, critics and collectors, is precisely what ensures its freedom from the rules of global capitalism and mass culture. Yet at the same time contemporary art can be seen as a giant metaphor for the capitalist system, with which it has more than one affinity. After demonstrating that the salient characteristics of the art of the 90s – multiculturalism, the success of the installation and the emphasis on youth – are closely linked to its economy, Stallabrass dwells on the way in which the economy of the art world conditions production. The author explains that, while most other art worlds are based on an economy of usage, the core business of contemporary art consists in the «production of rare or unique objects that can only be owned by the very wealthy, whether they are states, businesses or individuals» (p. 102). In recent decades this idiosyncratic economy has had to come to terms with the existence of technically reproducible languages, giving rise to some bizarre compromises: while on one hand photographic works and video exist on the market in very limited series, highly-priced and accompanied by an authentication, on the other, artists like Jeff Koons and Takashi Murakami create digital images which they then get professionals to paint, transforming an infinitely reproducible file into a unique artwork, using a practice (painting) that is manual and entirely traditional. And the ups and downs of the market also obviously influence the type of art that is produced. In the eternal struggle between traditional (and easily marketable) languages, and more difficult forms, the former experience a predictable revival at every economic boom, while the latter emerge more forcefully in every recession, in a «predictable and mechanical process» (p. 107). As for the artists, the idiosyncrasies of the system almost always relegate them to poverty. While there are a few big names who manage to make a killing, most artists are at the lower end of the earning scale. Poverty is at once a side-effect of the particular workings of the system, a contradiction and an ideal: poverty suits art. The artist’s is a high level profession, usually practised by people of high social extraction but low income, who often fund their art with other activities. As Stallabrass concludes: «As a whole, the art market is an archaic, protected enclave, so far immune from the gales of neoliberal modernization that have swept aside so many other less commercial practices. Its status grants it social distinction and a degree of autonomy, even sometimes from the odd market that is at its basis» (p. 114). We might object by asserting that Stallabrass’ vision is a bit too prosaic, that art is something else altogether, something not so exclusively tied to the fortunes of the market. We could object that the present period as it will be reconstructed in two hundred years’ time will have little to do with auction prices, corporate investments and collectors. This is true up to a point, given that the fluctuations of the art economy influence critical debate and the construction of museum collections, as Stallabrass warned us right from the beginning: «the art world is layered vertically and heterogeneous horizontally, comprising many overlapping spheres of association and commerce» (p. 25). This can also be said of the other art worlds, and it is exactly what makes it difficult to reason systematically. At the same time, it is on this horizontal plane that the various worlds intersect, mutually influencing their respective fates.
As we have seen, the world of New Media Art came about to offer artists wishing to experiment with technologies of all kinds the opportunity to do so, removed from the constrictions and limits of a world, the contemporary art world, which is strongly conditioned by its economy and a critical predilection for contents above the exploration of a medium. Far from challenging this configuration, New Media Art criticism merely takes it for granted, and replicates it ad infinitum, to the point of asserting, as Edward Shanken does in Media Art Histories, that contemporary art has never accepted New Media Art because it has always rejected the interpretative model based on the relationship between art, science and technology. [18] Which would imply that it can only be interpreted in this way. In 2006 Gerfried Stocker, director of the Ars Electronica Center and the yearly festival connected to it, returned to discuss this idea of art. The text, rhetorically entitled “The Art of Tomorrow”, [19] is significant from various points of view. Indeed Stocker acknowledges that the current developments in new technologies call for a rethink of the structure and functioning of a festival like Ars Electronica, but does so basically without challenging the idea of art it is based on, namely that art is «a test-drive of the future» (p. 7); that Media Art is «an experiment […] that often brings the creators and proponents of this “new art” into an association with engineers and researchers» (p. 11); and that its basic characteristic is its ability to go beyond an instrumental use of the media as a «medium of representation», making the media not only its tool and medium, but also its subject matter, triumphantly concluding.
In Art of the Digital Age, Bruce Wands [21] depicts the digital artist as someone equipped with technological skills and a good dose of «technological curiosity»; often a programmer, used to working in collaboration with other programmers and IT engineers; attracted to new technologies and viewing art in terms of research and experimentation; a risk-taker who readily veers off the beaten track of established languages and forms to venture into new terrain. Though this definition does not add anything new to what we have said so far, it is an interesting one from various points of view. In the first place, New Media Art appears to have entirely overcome the romantic conception of the artist as genius, and seems to be more interested in returning to the Renaissance models of artist as artisan and artist as scientist. Familiarity with programming also takes the New Media artist into another sociologically interesting terrain: that of hacking (used here in its original sense, freed from the negative connotations attributed by the mass media). It goes without saying that many New Media artists are, and consider themselves to be hackers, to all intents and purposes, and have much in common with hacker ethics: great enthusiasm for their work, limited interest in making a profit, a propensity for knowledge sharing and a belief in the free circulation of information. [22] In 2003, the Net Art group [epidemiC] engaged with this, activating a curious social short circuit. Invited to take part in the Ars Electronica festival, [epidemiC] created Doubleblind Invitation: a program that, if visualized in code form, looked like a beautiful piece of “obfuscated code”, namely formatted like a calligram – a technical feat which holds great kudos in the hacking world, where there are competitions devoted to this particular art form. Yet if executed, [epidemiC]’s code sent out emails – seemingly on behalf of the curator Christiane Paul – to dozens of hackers, fans of obfuscated code, inviting them to take part in the festival. The responses from the invitees, some embarrassed, some enthusiastic, show both the proximity of these two similar cultural niches, and the basic divergence between their two different approaches to programming. This portrait of the New Media artist, albeit an abstract one, appears so far removed from the type of artist cultivated by the contemporary art world that we might be tempted to think that the difference between the two worlds is a question of anthropology rather than history. And while, as we have seen, the contemporary art world is permeable enough to occasionally accept anomalous figures entirely unconnected to the notion of the “career” artist, the appeal of an art world basically without any kind of market economy, devoted to developing knowledge and exploring the arena of digital media, remains strong. Casey Reas is a case in point. Reas is an American artist whose work consists in defining processes and translating them into images. In other words, Reas writes programs that, when executed by a computer, generate animated images that can, if desired, be translated into videos or prints. Unsatisfied with the existing tools, in 2001 Reas, working with the artist and designer Benjamin Fry, created Processing, an open source programming language and freely downloadable program for the creation of images, animations and interactive installations. [23] Processing is now used by a slew of artists, designers and researchers, and obviously Reas himself, who utilizes it in his work. Although Reas works with galleries, he considers himself above all a programmer, designer and researcher: he writes books, holds conferences and coordinates the department of Design and Media Arts at UCLA; and while the resulting products (prints, videos and installations) are produced in limited series, his programs are released with an open source license. He earns his living mainly through teaching and holding workshops on Processing around the world. It is not difficult to come across stories like these in the New Media Art world, just as it is not difficult to meet artists who put their own talent and efforts at the service of temporary collaborative experiments, voluntarily sacrificing their own authorship.
