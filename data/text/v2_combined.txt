Planetary-scale computation takes different forms at different scales: energy grids and mineral sourcing; cloud infrastructure; urban software and public service privatization; massive universal addressing systems; interfaces drawn by the augmentation of the hand, of the eye, or dissolved into objects; users both overdetermined by self-quantification and exploded by the arrival of legions of nonhuman users (sensors, cars, robots). Instead of seeing the various species of contemporary computational technologies as so many different genres of machines, spinning out on their own, we should instead see them as forming the body of an accidental megastructure. Perhaps these parts align, layer by layer, into something not unlike a vast (if also incomplete), pervasive (if also irregular) software and hardware Stack. This model is of a Stack that both does and does not exist as such: it is a machine that serves as a schema, as much as it is a schema of machines.  As such, perhaps the image of a totality that this conception provides would—as theories of totality have before—make the composition of new governmentalities and new sovereignties both more legible and more effective.
My interest in the geopolitics of planetary-scale computation focuses less on issues of personal privacy and state surveillance than on how it distorts and deforms traditional Westphalian modes of political geography, jurisdiction, and sovereignty, and produces new territories in its image. It draws from (and against) Carl Schmitt’s later work on The Nomos of the Earth, and from his (albeit) flawed history of the geometries of geopolitical architectures.2“Nomos” refers to the dominant and essential logic to the political subdivisions of the earth (of land, seas, and/or air, and now also of the domain that the US military simply calls “cyber”) and to the geopolitical order that stabilizes these subdivisions accordingly. Today, as thenomos that was defined by the horizontal loop geometry of the modern state system creaks and groans, and as “Seeing like a State” takes leave of that initial territorial nest—both with and against the demands of planetary-scale computation3—we wrestle with the irregular abstractions of information, time, and territory, and the chaotic de-lamination of (practical) sovereignty from the occupation of place. For this, a nomos of the Cloud would, for example, draw jurisdiction not only according to the horizontal subdivision of physical sites by and for states, but also according to the vertical stacking of interdependent layers on top of one another: two geometries sometimes in cahoots, sometimes completely diagonal and unrecognizable to one another.4

The Stack, in short, is that new nomos rendered now as vertically thickened political geography. In my analysis, there are six layers to this Stack: Earth, Cloud, City, Address, Interface, and User. Rather than demonstrating each layer of the Stack as a whole, I’ll focus specifically on the Cloud and the User layers, and articulate some alternative designs for these layers and for the totality (or even better, for the next totality, the nomos to come). The Black Stack, then, is to the Stack what the shadow of the future is to the form of the present. The Black Stack is less the anarchist stack, or the death-metal stack, or the utterly opaque stack, than the computational totality-to-come, defined at this moment by what it is not, by the empty content fields of its framework, and by its dire inevitability. It is not the platform we have, but the platform that might be. That platform would be defined by the productivity of its accidents, and by the strategy for which whatever may appear at first as the worst option (even evil) may ultimately be where to look for the best way out. It is less a “possible future” than an escape from the present.

The platforms of the Cloud layer of the Stack are structured by dense, plural, and noncontiguous geographies, a hybrid of US super-jurisdiction and Charter Cities, which have carved new partially privatized polities from the whole cloth of de-sovereigned lands. But perhaps there is more there.

The immediate geographical drama of the Cloud layer is seen most directly in the ongoing Sino-Google conflicts of 2008 to the present: China hacking Google, Google pulling out of China, the NSA hacking China, the NSA hacking Google, Google ghostwriting books for the State Department, and Google wordlessly circumventing the last instances of state oversight altogether, not by transgressing them but by absorbing them into its service offering. Meanwhile, Chinese router firmware bides its time.

The geographies at work are often weird. For example, Google filed a series of patents on offshore data centers, to be built in international waters on towers using tidal currents and available water to keep the servers cool. The complexities of jurisdiction suggested by a global Cloud piped in from non-state space are fantastic, but they are now less exceptional than exemplary of a new normal. Between the “hackers” of the People’s Liberation Army and Google there exists more than a standoff between the proxies of two state apparatuses. There is rather a fundamental conflict over the geometry of political geography itself, with one side bound by the territorial integrity of the state, and the other by the gossamer threads of the world’s information demanding to be “organized and made useful.” This is a clash between two logics of governance, two geometries of territory: one a subdivision of the horizontal, the other a stacking of vertical layers; one a state, the other a para-state; one superimposed on top of the other at any point on the map, and never resolving into some consensual cosmopolitanism, but rather continuing to grind against the grain of one another’s planes. This characterizes the geopolitics of our moment (this, plus the gravity of generalized succession, but the two are interrelated).

From here we see that contemporary Cloud platforms are displacing, if not also replacing, traditional core functions of states, and demonstrating, for both good and ill, new spatial and temporal models of politics and publics. Archaic states drew their authority from the regular provision of food. Over the course of modernization, more was added to the intricate bargains of Leviathan: energy, infrastructure, legal identity and standing, objective and comprehensive maps, credible currencies, and flag-brand loyalties. Bit by bit, each of these and more are now provided by Cloud platforms, not necessarily as formal replacements for the state versions but, like Google ID, simply more useful and effective for daily life. For these platforms, the terms of participation are not mandatory, and because of this, their social contracts are more extractive than constitutional. The Cloud Polis draws revenue from the cognitive capital of its Users, who trade attention and microeconomic compliance in exchange for global infrastructural services, and in turn, it provides each of them with an active discrete online identity and the license to use this infrastructure.

Looking toward the Black Stack, we observe that new forms of governmentality arise through new capacities to tax flows (at ports, at gates, on property, on income, on attention, on clicks, on movement, on electrons, on carbon, and so forth). It is not at all clear whether, in the long run, Cloud platforms will overwhelm state control on such flows, or whether states will continue to evolve into Cloud platforms, absorbing the displaced functions back into themselves, or whether both will split or rotate diagonally to one another, or how deeply what we may now recognize as the surveillance state (US, China, and so forth) will become a universal solvent of compulsory transparency and/or a cosmically opaque megastructure of absolute paranoia, or all of the above, or none of the above.

Between the state, the market, and the platform, which is better designed to tax the interfaces of everyday life and draw sovereignty thereby? It is a false choice to be sure, but one that raises the question of where to locate the proper site of governance as such. What would we mean by “the public” if not that which is constituted by such interfaces, and where else should “governance”—meant here as the necessary, deliberate, and enforceable composition of durable political subjects and their mediations—live if not there? Not in some obtuse chain of parliamentary representation, nor in some delusional monadic individual unit, nor in some sad little community consensus powered by moral hectoring, but instead in the immanent, immediate, and exactly present interfaces that cleave and bind us. Where should sovereignty reside if not in what is in-between us—derived not from each of us individually but from what draws the world through us?

For this, it’s critical to underscore that Cloud platforms (including sometimes state apparatuses) are exactly that: platforms. It is important as well to recognize that “platforms” are not only a technical architecture; they are also an institutional form. They centralize (like states), scaffolding the terms of participation according to rigid but universal protocols, even as they decentralize (like markets), coordinating economies not through the superimposition of fixed plans but through interoperable and emergent interaction. Next to states and markets, platforms are a third form, coordinating through fixed protocols while scattering free-range Users watched over in loving, if also disconcertingly omniscient, grace. In the platform-as-totality, drawing the interfaces of everyday life into one another, the maximal state and the minimal state, Red Plenty and Google Gosplan, start to look weirdly similar.

Our own subjective enrollment in this is less as citizens of a polis or as homo economicuswithin a market, but rather as Users of a platform. As I see it, the work of geopolitical theory is to develop a proper history, typology, and program for such platforms. These would not be a shorthand for Cloud Feudalism (nor for the network politics of the “multitude”) but models for the organization of durable alter-totalities which command the force of law, if not necessarily its forms and formality. Our understanding of the political economy of platforms demands its own Hobbes, Marx, Hayek, and Keynes.

One of the useful paradoxes of the User’s position as a political subject is the contradictory impulse directed simultaneously toward his artificial over-individuation and his ultimatepluralization, with both participating differently in the geopolitics of transparency. For example, the Quantified Self movement (a true medical theology in California) is haunted by this contradiction. At first, the intensity and granularity of a new informational mirror image convinces the User of his individuated coherency and stability as a subject. He is flattered by the singular beauty of his reflection, and this is why QSelf is so popular with those inspired by an X-Men reading of Atlas Shrugged. But as more data is added to the diagram that quantifies the outside world’s impact on his person—the health of the microbial biome in his gut, immediate and long-term environmental conditions, his various epidemiological contexts, and so on—the quality of everything that is “not him” comes to overcode and overwhelm any notion of himself as a withdrawn and self-contained agent. Like Theseus’s Paradox—where after every component of a thing has been replaced, nothing original remains but a metaphysical husk—the User is confronted with the existential lesson that at any point he is only the intersection of many streams. At first, the subject position of the User overproduces individual identity, but in the continuance of the same mechanisms, it then succeeds in exploding it.

The geopolitics of the User we have now is inadequate, including its oppositional modes. The Oedipal discourse of privacy and transparency in relation to the Evil Eye of the uninvited stepfather is a necessary process toward an alterglobalism, but it has real limits worth spelling out. A geopolitics of computation predicated at its core upon the biopolitics of privacy, of self-immunization from any compulsory appearance in front of publics, of platforms, of states, of Others, can sometimes also serve a psychological internalization of a now-ascendant general economy of succession, castration anxiety—whatever. The result is the pre-paranoia of withdrawal into an atomic and anomic dream of self-mastery that elsewhere we call the “neoliberal subject.”

The space in which the discursive formation of the subject meets the technical constitution of the User enjoys a much larger horizon than the one defined by these kinds of individuation. Consider, for example, proxy users. uProxy, a project supported by Google Ideas, is a browser modification that lets users easily pair up across distances to allow someone in one location (trapped in the Bad Internets) to send information unencumbered through the virtual position of another User in another location (enjoying the Good Internets). Recalling the proxy servers set up during the Arab Spring, one can see how Google Ideas (Jared Cohen’s group) might take special interest in baking this into Chrome. For Sino-Google geopolitics, the platform could theoretically be available at a billion-user scale to those who live in China, even if Google is not technically “in China,” because those Users, acting through and as foreign proxies, are themselves, as far as internet geography is concerned, both in and not in China. Developers of uProxy believe that it would take two simultaneous and synchronized man-in-the-middle attacks to hack the link, and at a population scale that would prove difficult even for the best state actors, for now. More disconcerting perhaps is that such a framework could just as easily be used to withdraw data from a paired site—a paired “user”—which for good reasons should be left alone.

Some plural User subject that is conjoined by a proxy link or other means could be composed of different types of addressable subjects: two humans in different countries, or a human and a sensor, a sensor and a bot, a human and a robot and a sensor, a whatever and a whatever. In principle, any one of these subcomponents could not only be part of multiple conjoined positions, but might not even know or need to know which meta-User they contribute to, any more than the microbial biome in your gut needs to know your name. Spoofing with honeypot identities, between humans and nonhumans, is measured against the theoretical address space of IPv6 (roughly 1023 addresses per person) or some other massive universal addressing scheme. The abyssal quantity and range of “things” that could, in principle, participate in these vast pluralities includes real and fictional addressable persons, objects, and locations, and even addressable mass-less relations between things, any of which could be a sub-User in this Internet of Haeccities.

So while the Stack (and the Black Stack) stage the death of the User in one sense—the eclipse of a certain resolute humanism—they do so because they also bring the multiplication and proliferation of other kinds of nonhuman Users (including sensors, financial algorithms, and robots from nanometric to landscape scale), any combination of which one might enter into a relationship with as part of a composite User. This is where the recent shift by major Cloud platforms into robotics may prove especially vital, because—like Darwin’s tortoises finding their way to different Galapagos islands—the Cambrian explosion in robotics sees speciation occur in the wild, not just in the lab, and with “us” on “their” inside, not on the outside. As robotics and Cloud hardware of all scales blend into a common category of machine, it will be unclear in general human-robotic interaction whether one is encountering a fully autonomous, partially autonomous, or completely human-piloted synthetic intelligence. Everyday interactions replay the Turing Test over and over. Is there a person behind this machine, and if so, how much? In time, the answer will matter less, and the postulation of human (or even carbon-based life) as the threshold measure of intelligence and as the qualifying gauge of a political ethics may seem like tasteless vestigial racism, replaced by less anthropocentric frames of reference.

The position of the User then maps only very incompletely onto any one individual body. From the perspective of the platform, what looks like one is really many, and what looks like many may only be one. Elaborate schizophrenias already take hold in our early negotiation of these composite User positions. The neoliberal subject position makes absurd demands on people as Users, as Quantified Selves, as SysAdmins of their own psyche, and from this, paranoia and narcissism are two symptoms of the same disposition, two functions of the same mask. For one, the mask works to pluralize identity according to the subjective demands of the User position as composite alloy; and for another, it defends against those same demands on behalf of the illusory integrity of a self-identity fracturing around its existential core. Ask yourself: Is that User “Anonymous” because he is dissolved into a vital machinic plurality, or because public identification threatens individual self-mastery, sense of autonomy, social unaccountability, and so forth? The former and the latter are two very different politics, yet they use the same masks and the same software suite. Given the schizophrenic economy of the User—first over-individuated and then multiplied and de-differentiated—this really isn’t an unexpected or neurotic reaction at all. It is, however, fragile and inadequate.

In the construction of the User as an aggregate profile that both is and is not specific to any one entity, there is no identity to deduce other than the pattern of interaction between partial actors. We may find, perhaps ironically, that the User position of the Stack actually has far less in common with the neoliberal form of the subject than some of today’s oppositionalist formats for political subjectivity that hope (quite rightly) to challenge, reform, and resist the State Stack as it is currently configuring itself. However, something like a Digital Bill of Rights for Users, despite its cosmopolitan optimism, becomes a much more complicated, fragile, and limited solution when the discrete identification of a User is both so heterogeneous and so fluid. Are all proxy composite users one User? Is anything with an IP address a User? If not, why not? If this throne is reserved for one species—humans—when is any one animal of that species being a User, and when is it not? Is it a User anytime that it is generating information? If so, that policy would in practice crisscross and trespass some of our most basic concepts of the political, and for that reason alone it may be a good place to start.

In addition to the fortification of the User as a geopolitical subject, we also require a redefinition of the political subject in relation to the real operations of the User, one that is based not on homo economicus, nor on parliamentary liberalism, nor on post-structuralist linguistic reduction, nor on the will to secede into the moral safety of individual privacy and withdraw from coercion. Instead, this definition should focus on composing and elevating sites of governance from the immediate, suturing, interfacial material between subjects, in the stitches and the traces and the folds of interaction between bodies and things at a distance, congealing into different networks demanding very different kinds of platform sovereignty.

I will conclude with some thoughts on the Stack-we-have and on the Black Stack, the generic figure for its alternative totalities: the Stack-to-come. The Stack-we-have is defined not only by its form, its layers, its platforms, and their interrelations, but also by its content. As leak after leak has made painfully clear, its content is also the content of our daily communications, now weaponized against us. If the panopticon effect is when you don’t know if you are being watched or not, and so you behave as if you are, then the inverse panopticon effect is when you know you are being watched but act as if you aren’t. This is today’s surveillance culture: exhibitionism in bad faith. The emergence of Stack platforms doesn’t promise any solution, or even any distinctions between friend and enemy within this optical geopolitics. At some dark day in the future, when considered versus the Google Caliphate, the NSA may even come to be seen by some as the “public option.” “At least it is accountable in principle to some parliamentary limits,” they will say, “rather than merely stockholder avarice and flimsy user agreements.”

If we take 9/11 and the rollout of the Patriot Act as Year Zero for the USA’s massive data gathering, encapsulation, and digestion campaign (one that we are only now beginning to comprehend, even as parallel projects from China, Russia, and Europe are sure to come to light in time), then we can imagine the entirety of network communication for the last decade—the Big Haul—as a single, deep-and-wide digital simulation of the world (or a significant section of it). It is an archive, a library of the real. Its existence as the purloined property of a state, just as a physical fact, is almost occult. Almost.

The geophilosophical profile of the Big Haul, from the energy necessary to preserve it to its governing instrumentality understood as both a text (a very large text) and as a machine with various utilities, overflows the traditional politics of software. Its story is much more Borges than Lawrence Lessig. As is its fate. Can it be destroyed? Is it possible to delete this simulation, and is it desirable to do so? Is there a trash can big enough for the Big Delete? Even if the plug could be pulled on all future data hauls, surely there must be a backup somewhere, the identical double of the simulation, such that if we delete one, the other will forever haunt history until it is rediscovered by future AI archaeologists interested in their own Paleolithic origins. Would we bury it, even if we could? Would we need signs around it like those designed for the Yucca Mountain nuclear waste disposal site that warn off unknowable future excavations? Those of us “lucky” enough to be alive during this fifteen-year span would enjoy a certain illegible immortality, curiosities to whatever meta-cognitive entity pieces us back together using our online activities, both public and private, proud and furtive, each of us rising again centuries from now, each of us a little Ozymandias of cat videos and Pornhub.

In light of this, the Black Stack could come to mean very different things. On the one hand, it would imply that this simulation is opaque and unmappable—not disappeared, but ultimately redacted entirely. It could imply that, from the ruined fragments of this history, another coherent totality can be carved against the grain, even from the deep recombinancy at and below the Earth layer of the Stack. Its blackness is the surface of a world that can no longer be composed by addition because it is so absolutely full, overwritten, and overdetermined, that to add more is just so much ink in the ocean. Instead of tabula rasa, this tabula plenus allows for creativity and figuration only by subtraction, like scratching paint from a canvas—only by carving away, by death, by replacement.

The structural logic of any Stack system allows for the replacement of whatever occupies one layer with something else, and for the rest of the architecture to continue to function without pause. For example, the content of any one layer—Earth, Cloud, City, Address, Interface, User—could be replaced (including the masochistic hysterical fiction of the individual User, both neoliberal and neo-other-things), while the rest of the layers remain a viable armature for global infrastructure. The Stack is designed to be remade. That is its technical form, but unlike replacing copper wire with fiber optics in the transmission layer of TCP/IP, replacing one kind of User with another is more difficult. Today, we are doing it by adding more and different kinds of things into the User position, as described above. We should, however, also allow for more comprehensive displacements, not just by elevating things to the status of political subjects or technical agents, but by making way for genuinely posthuman and ahuman positions.

In time, perhaps at the eclipse of the Anthropocene, the historical phase of Google Gosplan will give way to stateless platforms for multiple strata of synthetic intelligence and biocommunication to settle into new continents of cyborg symbiosis. Or perhaps instead, if nothing else, the carbon and energy appetite osf this ambitious embryonic ecology will starve its host.

For some dramas, but hopefully not for the fabrication of the Stack-to-come (Black or otherwise), a certain humanism and companion figure of humanity still presumes its traditional place in the center of the frame. We must let go of the demand that any Artificial Intelligence arriving at sentience or sapience must care deeply about humanity—us specifically—as the subject and object of its knowing and its desire. The real nightmare, worse than the one in which the big machine wants to kill you, is the one in which it sees you as irrelevant, or as not even a discrete thing to know. Worse than being seen as an enemy is not being seen at all. As Eliezer Yudkowsky puts it, “The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.”6

One of the integral accidents of the Stack may be an anthrocidal trauma that shifts us from a design career as the authors of the Anthropocene, to the role of supporting actors in the arrival of the Post-Anthropocene. The Black Stack may also be black because we cannot see our own reflection in it. In the last instance, its accelerationist geopolitics is less eschatological than chemical, because its grounding of time is based less on the promise of historical dialectics than on the rot of isotope decay. It is drawn, I believe, by an inhuman and inhumanist molecular form-finding: pre-Cambrian flora changed into peat oil changed into children’s toys, dinosaurs changed into birds changed into ceremonial headdresses, computation itself converted into whatever meta-machine comes next, and Stack into Black Stack.

Through the chapters that follow, we demonstrate that such questioning is not the end of anthropology but, to the contrary, a fruitful endeavor leading to the discovery of new ends and purposes for our enduring commitment to engage and interpret other lifeways. A critical examination of the “human” sheds light on how the anthropocentric presumptions of much anthropology ignore not just the “unhuman” but also the “animal” and the “not-quitehuman” (transgendered, disabled, or psychologically impaired persons), inevitably leading to a challenge, and perhaps an outright rejection, of the whole category of the human, at least as a core concept for anthropological theory. Anthropology is only one academic discipline currently engaged with the posthuman (Wolfe 2010), but arguably anthropology has the most to contribute to such debates through ethnographic engagement with cultural worlds in which Western Enlightenment definitions and exclusions (Latour 1993) are not so prominent. In this regard, the essays here demonstrate that new forms of ethnographic engagement with “unhuman” populations (as in the chapters by Heckenberger, Whitehead, and Wisniewski) can inform and be informed by studies of online phenomena that also challenge and subvert traditional notions of the human. The chapters here illustrate emergent cultural contexts in which embodied, “rational” individuals are but one of the forms of agency present in virtual and socially occluded worlds. As Matt Bernius demonstrates, software programs create chatbots, spambots, searchbots, and even ballot-stuffing bots, some of which are fully equipped to interact with “real humans” socially, sexually, and financially. Such bots appear alongside and engage with simulacra of our offline selves.

The theme of these chapters—human no more?—resonates as a question throughout. Although Whitehead answers firmly in the affirmative and eagerly embraces the posthuman as a potential liberation from the late capitalist disciplines of the corporeal and the mental, others more cautiously question whether we are in fact posthuman at all and whether we should radically rethink anthropology. This is partly the reaction of Anne Allison (whose afterword was originally a response to the American Anthropological Association (AAA) panel at which these chapters were first presented), but her highly constructive and open-ended engagement with the question of “humanity” contrasts with other commentators (Boellstorff 2008) who find the prospect of the posthuman threatening, both subjectively and with regard to the preservation of the iconic founding figures of the discipline. Just as the idea of the “postmodern” provoked surprisingly conservative reactions among otherwise innovative thinkers such as Johannes Fabian, Marshall Sahlins, and Eric Wolf (Whitehead 2004), it is important to distinguish the critique of anthropology’s intellectual frameworks, which the asking of questions about the “human” permits, from the assumption that “posthumanism” is a disguised political gambit aimed at taking over the profession. Attempting to avoid such pitfalls, and in a similar vein to Allison, Tufekci notes that the first symbols ever created allowed for a form of “disembodiment” by separating the thought from the human, arguing that “the essence of humanity is that we have always been both symbolic and embodied.” “We were always human,” she suggests. “Or maybe, said alternatively, we were always posthuman.” Tufekci and others in this volume discover that even in these virtual worlds that create always expanding possibilities for disembodied sociality, embodiment remains crucial. It is the body that “centers and unites us,” even as we play with different social roles and personas. “Typing on a keyboard,” Tufekci suggests, does not “create an ontological split within the body.” But, as Tufekci rightly points out, this still begs the question of what we are to make of emerging technologies that now promise to immerse us in fully realistic simulated worlds—skin suits, goggles, and other devices that will bring digital inputs seamlessly into our increasingly augmented intelligence—or those futuristic Kurzwellian visions of nanobots swarming through our bodies, giving us access to all the information on the web from inside our own skin while repairing and rebuilding our cells and transporting us into virtual worlds whenever we desire (Kurzwell 2005).

Nonetheless, the bodily disciplines imposed by subjective engagements through such technological devices do represent a historical and cultural rupture and disjuncture. Thus, it is not our potential forms of disembodiment that make us posthuman but rather the way in which this historical movement away from prior cultural forms of embodiment are understood. As a result, Tufekci necessarily oscillates between the formulation “we were always human” and the formulation “we were always posthuman,” precisely because the notion of the “human” is always a contingent category and different regimes of “humanity” have been deployed throughout history to produce the exclusions and inclusions so necessary for the construction of power through difference. The whole realm of ecstatic experience through ritual and shamanism, for example, is a perfect example of the persistent presence of an instability in the human/non-human boundary and in ideas about embodiment. Consequently, the ethnographic literature on shamanism, particularly from Amazonia (see Whitehead, this volume), has been inspirational and reinvigorating for this kind of discussion, especially for some of our authors (see also Alemán, Hoesterey, Wisniewski). Bringing not just cross-cultural but historical sensibilities to our analyses thus allows us to see both the contingency of current posthuman forms and also how there have been perhaps many “posthumanisms.”

Matthew Bernius provides an example that is especially “good to think” in this regard by examining how the “human” is manufactured in the creation of artificial intelligence (AI). As he demonstrates, AI-bots are becoming increasingly common in chatrooms, creating a social space in which not all “subjects” (or apparent subjects) are human. In a second example, Bernius looks at the Virtual Peers project at Northwestern University, where animated AI characters on screen are paired with digital objects throughout the physical environment, so that “the virtual peer is a complex system that extends itself into much of the environment in which she and her human interlocutors interact.” As tracking systems, two-dimensional barcodes, sensors, and other digital objects become increasingly part of our everyday lived spaces of sociality, such an example becomes especially important to examine. Jenny Ryan moves us into the realm of the truly “posthuman,” exploring how the dead live on through their digital traces and in virtual spaces where grievers gather to share and post their memories. Perhaps most interesting in this regard is the way in which most posters address their comments directly to the deceased. Here Ryan makes the point that “embodiment” may need to be reconfigured as the highly immersive nature of online interaction can create the sense that the deceased is really “there,” even after death. Perhaps nothing could say this more forcefully than the protest movement against Facebook’s policy of removing personal information from profiles after somebody dies, aptly named “Facebook Memoralization Is Misguided: Dead Friends are Still People.” For decades now scholars have speculated about and documented the possibilities of identity play online in virtual worlds (see, e.g., Rheingold 1993; Turkle 1995), a notion captured in the public imagination by the now classic New Yorker cartoon “On the Internet, nobody knows you are a dog.” But more recent forms of online sociality, such as Facebook, allow for what Zeynep Tufekci calls “grassroots surveillance,” in which ubiquitous peer surveillance of the most mundane activities monitored through status updates, photos, and videos make identity play much harder. In a play on the New Yorker title at a panel discussion Tufekci noted, “On the Internet, everybody knows you are a dog."
Wesch examines one of these new forms in the phenomenon of Anonymous, an “ongoing collective happening” that challenges our traditional notion of identity and group as all interlocutors on the site remain anonymous, communicating through text, links, and imagery. Their peculiar form of sociality, along with the values they express, presents a scathing critique of our cultural obsession with individualism and identity and the cult of celebrity that emerges from this obsession. Moreover, following the furor around Wikileaks and the support Anonymous gave to the figure of Julian Assange, as well as ongoing AnonOps against various governments and their agencies, there is an important way in which Anonymous’s challenges to individualism and identity represent a potentially new form of political engagement and resistance. Traditional notions of human identity become increasingly irrelevant to life as it is lived even as they begin to seem the most pertinent to the procedures of power and governance. As Wesch forcefully illustrates, we must recognize the ways in which “identity” is used as a way of rendering people legible to those who exercise power; and this remains a powerful structural element in current academic practice, no less than in the panopticon of the security state. In this way, questions of identity emerge as less important for scientific fact than for the way in which such notions validate the truth of the cultural quest for ever-expanding discovery and knowledge of others.

Radhika Gajjala has been rethinking “participant observation” during her two decades of research on South Asian diasporas, developing an especially acute sense of how different media “mediate” the communities we study. In this volume, she teams up with her colleague Sue Ellen McComas to explore how Indian diasporas have been mediated across generations, from pre-digital mediated narratives to a study of how members of a diaspora interact as avatars among the scripted objects of Second Life. There she has the opportunity to re-encounter those recurring critical questions that she has raised in earlier work: How do ethnographic practices and the ethnographer evolve in the online context? How are they revolutionized? What constitutes the field? For Gray Graffam, the “field” is the World of Warcraft, a virtual reality and massive social space that to date has more than 10 million players. In adapting ethnography to suit this field, Graffam found that interviews with players outside the game lacked the proper context that provides the performative cues for people to “be themselves” and answer his questions effectively. Fieldwork experiences like this remind us that observation of any kind (online and offline) is by definition utterly dependent on the forms of participation that the ethnographer may choose or have available. Such issues affect ethnography even as it is most traditionally conceived, especially as there is a growing worldwide cultural investment in online life, even among the most remote and marginal populations. In this volume, Stephanie Alemán reports on her experiences in a remote Guyanese village where the Waiwai first encounter the Internet and start to present themselves online. In one telling example, a young Waiwai man presents himself on Facebook “in a Taekwondo uniform and pose, and another, in a gangsta pose with a knit hat, with dark glasses and headphones and making hand signs.” Meanwhile, as the anthropologist, Alemán has collected images of him in more traditional “native activities”: adorned in full black body paint, shooting a bow and arrow, and dancing in the communal house. Alemán begins to see a clear dilemma for many anthropologists today: how do we represent those who can and will represent themselves? How do we address “their multiple and complex entailments with the regional and global networks to which they not only now have access but actively seek to engage”?

In this context of increasing anthropological anxiety, there is a growing need for the ethnographer to explicitly theorize participation no less adequately than we have painstakingly theorized observation and representation. As Wisniewski notes in this volume about his study of the invisible caboclos of Brazil in which he teamed up with two hippie “vagabond ethnographers,” “theorizing participation will give us a clearer understanding of how ethnographic knowledge is produced, revealing it as a shared product, an intersubjective product, not just of and about humans but of and about human interaction with all categories in a way that does not privilege or overvalue the role of the anthropologist in its production.” The need to theorize participation extends beyond the emerging contexts of online life to all spaces of virtuality where traditional notions of the human limit us or fail altogether. As Michael Heckenberger points out in this volume, not all “virtual realities” are digital or online, so the import of engaging digital subjectivities for wider theory lies precisely in how it may offer the possibility of eluding the Foucaldian/Marcusian nexus of knowledge-powermedia. In São Paulo, public discourses about public health and security create the unhuman nóias, a drug addict who has taken center stage as the persona of the irrational and subnormal in public discourse. Also referred to as zombies, inhabiting a space between life and death, such unhumans invoke different forms and moments of marginalization and oppression than those of the South Asian digital online residents, but nonetheless they experience displacement and disorientation similar to that produced in social encounters within a geographical diaspora. Like the zombies of the cityscapes, the diasporic are also engaged in an attempt to reframe their cultural identities to stave off the threat of cultural—if not physical—genocide through the effects of a rampant globalization.

In fact, such categories have revealed a vast field of social and cultural continuities among the human, animal, and technological. Current notions of biopower, the deployment of artificial intelligence and robotic systems in warfare and law enforcement, and the cultural logic of cinematic and televisual representations are all indications of the urgency with which anthropology should engage its new subjects. Regardless of where one stands on the question of the posthuman, it is clear that a jailbreak from late modernity does not go unchallenged. Stalwart symbols and institutions, such as hungry profiteers and militarized governance, seek to delimit the “human terrain” in both online and offline contexts, creating yet one more piece of the complex contexts and new spaces of cultural and social significance that have proliferated in the last decade. Here the “native populations”—the freaks, geeks, weirdos, techies, and net-addicts— like the savages at the margins of an earlier colonial order, defy simple inclusion into the frameworks of the state and its ethnographies. Living with the Mek, or the caboclos or the Waiwai, no less than the character-subjects of Anonymous or the online worlds of Second Life or My(Death)Space, must now take account of the endless interplay between offline and online subjectivity, while also expanding our notions and understandings of the vast potential of human diversity and social interaction. The stakes are high. Quoting Judith Butler, Heckenberger nicely observes how our cultural frames for thinking the human set limits on certain lives that are not considered lives at all: “[V]iolence against those who are already not quite living, living in a state of suspension between life and death, leaves a mark that is no mark.” These are the “killable bodies” discussed by Giorgio Agamben (1995) in his characterization of contemporary power and governance. As Wisniewski notes in this volume, it is time to expand and refine our approach so that we are equipped to grapple with the relationship between humans and technology, while also recognizing that humans are part of much larger systems that include relationships with animals, insects, microorganisms, spirits, and people who are not always considered human by others. And as humans become more digitally connected, we must also recognize that the sociality that emerges from such connections might not always be immediately analogous to traditional social formations and may involve unhuman actors and agencies (which may or may not be conceptualized or treated as human).

This is a book about the things people say about images. It is not primarily concerned with specific pictures and the things people say about them, but rather with the way we talk about the idea of imagery, and all its related notions of picturing, imagining, perceiving, likening, and imitating. It is a book about images, therefore, that has no illustrations except for a few schematic diagrams, a book about vision written as if by a blind author for a blind reader. If it contains any insight into real, material pictures, it is the sort that might come to a blind listener, overhearing the conversation of sighted speakers talking about images. M y hypothesis is that such a listener might see patterns in these conversations that would be invisible to the sighted participant. The book reflects on answers to two questions that come up regularly in these conversations: What is an image? What is the difference between images and words? It attempts to understand the traditional answers to these questions in relation to the human interests that give them urgency in particular situations. Why does it matter what an image is? What is at stake in marking off or erasing the differences between images and words? What are the systems of power and canons of value—that is, the ideologies—that inform the answers to these questions and make them matters of polemical dispute rather than purely theoretical interest?
If all this sounds impossibly comprehensive, it may help to note that this study has very definite limits, both in terms of the questions it raises and the body of texts it considers. Except for the first chapter this is primarily a series of close readings of a few important texts in the theory of imagery, and these readings revolve around two historical centers, one in the late eighteenth century (roughly, the era of the French Revolution and the rise of Romanticism), the other in the era of modern criticism. The aim of these readings is to show how the notion of imagery serves as a kind of relay connecting theories of art, language, and the mind with conceptions of social, cultural, and political value.

My only apology for these strange conjunctions of topics and texts is that they seemed to surface as I pursued the theoretical questions that inspired the study in the first place. Every theoretical answer to the questions, What is an image? Ho w are images different from words? seemed inevitably to fall back into prior questions of value and interest that could only be answered in historical terms. The simplest way of stating this is to admit that a book which began with the intention of producing a valid theory of images became a book about the fear of images. "Iconology" turned out to be, not just the science of icons, but the political psychology of icons, the study of iconophobia, iconophilia, and the struggle between iconoclasm and idolatry. The movement of this book is thus from modern attempts to establish a true theory of imagery (Gombrich, Goodman, the early Wittgenstein) to the "classic" accounts of imagery these theories sought to replace. In the process, my theoretical ambitions have inevitably been chastened by my narrow limits as an intellectual historian. My hope is that this critical fall into the space between theory and history will open up a region for other scholars to explore, and that it will suggest something about the necessary limits of any attempt to provide a theoretical account of symbolic practices.

Two things must immediately strike the notice of anyone who tries to take a general view of the phenomena called by the name of imagery. The first is simply the wide variety of things that go by this name. We speak of pictures, statues, optical illusions, maps, diagrams, dreams, hallucinations, spectacles, projections, poems, patterns, memories, and even ideas as images, and the sheer diversity of this list would seem to make any systematic, unified understanding impossible. The second thing that may strike us is that the calling of all these things by the name of "image" does not necessarily mean that they all have something in common. It might be better to begin by thinking of images as a far-flung family which has migrated in time and space and undergone profound mutations in the process. If images are a family, however, it may be possible to construct some sense of their genealogy. If we begin by looking, not for some universal definition of the term, but at those places where images have differentiated themselves from one another on the basis of boundaries between different institutional discourses, we come up with a family tree something like the following.

The mental and verbal images on the right side of our diagram, for instance, would seem to be images only in some doubtful, metaphork sense. People may report experiencing images in their heads while reading or dreaming, but we have only their word for this; there is no way (so the argument goes) to check up on this objectively. An d even if we trust the reports of mental imagery, it seems clear that they must be different from real, material pictures. Mental images don't seem to be stable and permanent the way real images are, and they vary from one person to the next: if I say "green," some listeners may see green in their mind's eye, but some may see a word, or nothing at all. An d mental images don't seem to be exclusively visual the way real pictures are; they involve all the senses. Verbal imagery, moreover, can involve all the senses, or it may involve no sensory component at all, sometimes suggesting nothing more than a recurrent abstract idea like justice or grace or evil. It is no wonder that literary scholars get very nervous when people start taking the notion of verbal imagery too literally." An d it is hardly surprising that one of the main thrusts of modern psychology and philosophy has been to discredit the notions of both mental and verbal imagery.
Wittgenstein's way of attacking mental imagery is not, however, the direct strategy of denying the existence of such images. He freely concedes that we may have mental images associated with thought or speech, insisting only that these images should not be thought of as private, metaphysical, immaterial entities any more than real images are. Wittgenstein's tactic is to demystify the mental image by bringing it right out in the open where we can see it: "Mental images of colours, shapes, sounds, etc., etc., which play a role in communication by means of language we put in the same category with patches of color actually seen, sounds heard."  It is a bit hard, however, to see how we can put mental and physical images "in the same category." We certainly can't do it by cutting open someone's head to compare mental pictures with the ones on our walls. A better strategy, and more in the Wittgensteinian spirit, would be to examine the ways we put those images "into our heads" in the first place by trying to picture the sort of world in which this move would make sense. I offer the figure on the next page as just such a picture.

But if the key to the recognition of real, material images in the world is our curious ability to say "there" and "not there" at the same time, we must then ask why mental images should be seen as any more—or less—mysterious than "real" images. The problem philosophers and ordinary people have always had with the notion of mental images is that they seem to have a universal basis in real, shared experience (we all dream, visualize, and are capable, in varying degrees, of re-presenting concrete sensations to ourselves), but we cannot point to them and say "There—that is a mental image." Exacdy the same sort of problem occurs, however, if I try to point to a real image and explain what it is to someone who doesn't already know what an image is. I point at Xeuxis's painting and say "There, that is an image." And the reply is, "D o you mean that colored surface?" Or "Do you mean those grapes?"

This is not to be taken as a claim that the mind really is a blank slate or a mirror—only that these are ways the mind is capable of picturing itself. It might picture itself in other ways: as a building, a statue, as an invisible gas or fluid, as a text, a narrative, or a melody, or as nothing in particular. It might decline to have a picture of itself, and refuse all selfrepresentation, just as we can look at a picture, a statue, or a mirror and not see it as a representational object. We might look at mirrors as shiny vertical objects, paintings as masses of colors on flat surfaces. There is no rule that the mind has to picture itself, or see pictures in itself, any more than there is a rule that we must go into a picture gallery, or that once inside we must look at the pictures. If we eliminate the notion that there is something necessary, natural, or automatic about the formation of both mental and material images, then we can do as Wittgenstein suggests, and put them "in the same category" as functional symbols, or, as in our model, in the same logical space. 17 This does not eliminate all differences between mental and physical images, but it may help to demystify the metaphysical or occult quality of this difference, and to allay our suspicion that mental images are somehow improper or illegitimately modeled on the "real thing." The path of derivation from original model to illegitimate analogy could as easily be traced in the opposite direction.

A good way to clarify the relation of mental and physical images is to reflect on the way we have just used a diagram to illustrate the matrix of analogies that connects theories of representation to theories of mind. We might be tempted to say that a mental version of this diagram was in our heads all along, before it appeared on the page, and that it was governing the way we discussed the boundary between mental and physical images. Well, perhaps it was; or perhaps it only occurred to us at a certain point in the discussion, when we began to use words like "boundary line" and "realm." Or perhaps it never occurred to us at all while thinking about these things or writing them down, and it was only later, after many revisions, that it came to mind. Does that mean that the mental diagram was there all along as a kind of unconscious deep structure determining our usage of the word "image"? Or is it a posterior construction, a graphic projection of the logical space implied in our propositions about imagery? In either case we certainly cannot regard the diagram as something mental in the sense of "private" or "subjective"; it is rather something that surfaced in language, and not just my language, but a way of speaking that we inherit from a long tradition of talking about minds and pictures. Our diagram might just as well be called a "verbal image" as a mental one, which brings us to that other notoriously illegitimate branch in the family tree of imagery, the notion of imagery in language.

This view of poetry, and of language in general, as a process of pictorial production and reproduction was accompanied in seventeenthand eighteenth-century English literary theory by a decline in the prestige of rhetorical figures and tropes. The notion of "image" replaced that of "figure," which began to be regarded as a feature of old-fashioned "ornamented" language. The literary style of verbal imagery is "plain" and "perspicuous," a style that reaches right out to objects, representing them (as Addison claims) even more vividly than the objects can represent themselves. This in contrast to the "deceptive ornament" of rhetoric, which is now seen as nothing but a matter of relations among signs. When the rhetorical figures are mentioned, they are either dismissed as the artificial excesses of a prerational, prescientific age, or they are redefined in ways that accommodate them to the hegemony of the verbal image. Metaphors are redefined as "short descriptions"; "allusions and similes are descriptions placed in an opposite point of view . . . and hyperbole is often nothing more than a description carried beyond the bounds of probability." Even abstractions are treated as pictorial, visual objects, projected in the verbal imagery of personification. In Romantic and modern poetics the verbal image retained its hold over the understanding of literary language, and the confused application of the term to both literal and figurative expression continued to encourage a lumping of notions such as description, concrete nouns, tropes, "sensory" terms, and even recurrent semantic, syntactic, or phonemic motifs under the rubric of "imagery." In order to do all this work, however, the notion of imagery had to be sublimated and mystified. Romantic writers typically assimilate mental, verbal, and even pictorial imagery into the mysterious process of "imagination," which is typically defined in contrast to the "mere" recall of mental pictures, the "mere" description of external scenes, and (in painting) the "mere" depiction of external visibilia, as opposed to the spirit, feeling, or "poetry" of a scene.

This progressive sublimation of the image reaches its logical culmination when the entire poem or text is regarded as an image or 'Verbal icon," and this image is defined, not as a pictorial likeness or impression, but as a synchronic structure in some metaphorical space—"that which" (in Pound's words) "presents an intellectual and emotional complex in an instant of time." The Imagists's emphasis on concrete, particular descriptions in their poetry is, by itself, a residue of the eighteenthcentury notion we have seen in Addison that poetry strives to outdo in vividness and immediacy the "images which flow from objects themselves" (Williams's "no ideas but in things" would seem to be another version of this idea). But the distinctive modernist emphasis is on the image as a sort of crystalline structure, a dynamic pattern of the intellectual and emotional energy bodied forth by a poem. Formalist criticism is both a poetics and a hermeneutics for this kind of verbal image, showing us how poems contain their energies in matrices of architectonic tension, and demonstrating the congruence of these matrices with the propositional content of the poem.

If the figure of the pictogram or hieroglyph demands a viewer who knows what to say, it also has a way of shaping the things that can be said. Consider further the ambiguous emblem/signature/ideogram of the petroglyph "eagle." If the warrior is an eagle, or "like" an eagle, or (more likely) if "Eagle himself" goes to war, and returns to tell about it, we can expect the picture to be extended. Eagle will no doubt see his enemies from afar and swoop down on them without warning. The "verbal image" of Eagle is a complex of speech, depiction, and writing that not only describes what he does, but predicts and shapes what he can and will do. It is his "character," a signature that is both verbal and pictorial, both a narrative of his actions and a summation of what he is.

But what exactly is this "spiritual" likeness which is not to be confused with any material image? We should note first that it seems to include a presumption of difference. To say that one tree, or one member of a species of tree, is like another, is not to argue that they are identical but that they are similar in some respects and not in others. Normally, however, we don't say that every likeness is an image. One tree is like another, but we don't call one the image of the other. The word "image" only comes up in relation to this sort of likeness when we try to construct a theory about the way we perceive the likeness between one tree and another. This explanation will typically resort to some intermediate or transcendental object—an idea, form, or mental image—that provides a mechanism for explaining how our categories arise. The "origin of species" is not just a matter of biological evolution then, but of the mechanisms of consciousness as they are described in representational models of the mind.

Is man created in the image of God, then, in that he looks like God, or in that we can say similar things about man and God? Milton wants to have it both ways, a desire we can trace to his rather unorthodox materialism, or perhaps more fundamentally, to a historic transformation in the concept of imagery which tended to identify the notion of spiritual likeness—particularly the "rational soul" that makes man an image of God—with a certain kind of material image. Milton's poetry is the scene of a struggle between iconoclastic distrust of the outward image and iconophilic fascination with its power, a struggle which manifests itself in his practice of proliferating visual images in order to prevent readers from focusing on any particular picture or scene. In order to see how the stage was set for this struggle we need to look more closely at the revolution which identified pictures or "artificial forms" with images as "likenesses" (Maimonides' "specific forms").

The revolution I am thinking of here was, of course, the invention of artificial perspective, first systematized by Alberti in 1435. The effect of this invention was nothing less than to convince an entire civilization that it possessed an infallible method of representation, a system for the automatic and mechanical production of truths about the material and the mental worlds. The best index to the hegemony of artificial perspective is the way it denies its own artificiality and lays claims to being a "natural" representation of "the way things look," "the way we see," or (in a phrase that turns Maimonides on his head) "the way things really are." Aided by the political and economic ascendance of Western Europe, artificial perspective conquered the world o f representation under the banner of reason, science, and objectivity. N o amount of counterdemonstration from artists that there are other ways of picturing what "we really see" has been able to shake the conviction that these pictures have a kind of identity with natural human vision and objective external space. An d the invention of a machine (the camera) built to produce this sort of image has, ironically, only reinforced the conviction that this is the natural mode of representation. What is natural is, evidendy, what we can build a machine to do for us.

If we discount the obvious hostility from Twain and Lessing's comments on the poverty of pictorial expression, we find a rather perspicuous account of what is meant by the notion of painting the invisible. What expression amounts to is the artful planting of certain clues in a picture that allow us to form an act of ventriloquism, an act which endows the picture with eloquence, and particularly with a nonvisual and verbal eloquence. A picture may articulate abstract ideas by means of allegorical imagery, a practice which, as Lessing notes, approaches the notational procedures of writing systems. The image of an eagle may depict a feathered predator, but it expresses the idea of wisdom, and thus works as a hieroglyph. Or we may understand expression in dramatic, oratorical terms, as did the Renaissance humanists who formulated a rhetoric of history painting complete with a language of facial expression and gesture, a language precise enough to let us verbalize what depicted figures are thinking, feeling, or saying. And expression need not be limited to predicates we can attach to pictured objects: the setting, compositional arrangement, and color scheme may all carry expressive charge, so that we can speak of moods and emotional atmospheres whose appropriate verbal counterparts may be something on the order of a lyric poem.

If I seem to be taking Twain's ironic attitude toward the claims of pictorial expression, it is not because I think that expression is impossible or illusory, but because our understanding of it is so often clouded by the same mystique of "natural representation" that obstructs our understanding of mimetic representation. Twain says that the label is worth more, for information, than "a ton of significant expression." But we might ask Twain how much the label would be worth, for information or for anything else, without this picture by Guido Reni, or the entire tradition of representing in pictorial, dramatic, or literary images the story of the Cenci. The painting is a confluence of pictorial and verbal traditions, neither of which is apparent to the innocent eyes of Twain, and so he can scarcely see what it is, much less respond to it.

What are we to make of this contest between the interests of verbal and pictorial representation? I propose that we historicize it, and treat it, not as a matter for peaceful settlement under the terms of some allembracing theory of signs, but as a struggle that carries the fundamental contradictions of our culture into the heart of theoretical discourse itself. The point, then, is not to heal the split between words and images, but to see what interests and powers it serves. This view can only be had, of course, from a standpoint which begins with skepticism about the adequacy of any particular theory of the relation of words and images, but which also preserves an intuitive conviction that there is some difference that is fundamental. It seems to me that Lessing, for instance, is absolutely right insofar as he regards poetry and painting as radically different modes or representation, but that his "mistake" (which theory still participates in) is the reification of this difference in terms of analogous oppositions like nature and culture, space and time.

Emerson once noted that the most fruitful conversations are always between two persons, not three. This principle may help to explain why the dialogue between poetry and painting has tended to dominate general discussions of the arts, and why music has seemed something of an outsider to the conversation. Al l the arts may aspire to the condition of music, but when they set out to argue, poetry and painting hold the stage. One reason for this is that they both lay claim to the same territory (reference, representation, denotation, meaning), a territory that music has tended to renounce. Another reason is that the differences between words and images seem so fundamental. They are not merely different kinds of creatures, but opposite kinds. They attract to their contest all the contrarieties and oppositions that riddle the discourse of criticism, the very discourse that takes as one of its projects a unified theory of the arts, an "aesthetics" which aspires to a synoptic view of artistic signs, a "semiotics" which hopes to comprehend all signs whatsoever.

This gap has two important functions in discussions of the arts and their symbol systems: it lends an air of tough-minded common sense to assertions of difference between the arts, and it gives an air of paradoxical daring and ingenuity to assertions of sameness or transference. The topic of the text-image difference provides an occasion for the exercise of the two great rhetorical skills, wit and judgment, "wit," as Edmund Burke noted, being "chiefly conversant in tracing resemblances," and judgment concerned mainly with "finding differences."1 Since aesthetics and semiotics dream of a theory that will satisfy both the need to discriminate artistic signs and to identify the principles that unite them, both these approaches to the topic have established themselves as traditional alternatives within the discourse of criticism. The mode of wit, the "tracing of resemblances," is the foundation of the ut pictura poesis and "sister arts" tradition in criticism, the construction of analogies or critical conceits that identify points of transference and resemblance between texts and images. Although these conceits are almost always accompanied by acknowledgments of differences between the arts, they are generally perceived as violations of good judgment that criticism ought to correct. Lessing opens the Laocoon by observing that "the first who compared painting with poetry was a man of fine feeling," not a critic or philosopher. 2 H e was, as Lessing goes on to explain, Simonides of Ceos, the legendary founder of the ut pictura poesis tradition. Lessing characterizes Simonides as a man of feeling and wit, "the Greek Voltaire," whose "dazzling antithesis that painting is dumb poetry and poetry speaking painting, stood in no textbook. It was one of those conceits, occurring frequendy in Simonides, the inexactness and falsity of which we feel constrained to overlook for the sake of the truth they contain.

In suggesting that these judicious discriminations are figurative I do not mean to assert that they are simply false, illusory, or without efficacy. O n the contrary, I want to suggest that they are powerful distinctions that effect the way the arts arc practiced and understood. I do mean to imply, however, that they are literally false, or (more generously) figuratively true. M y argument here will be twofold: (i) there is no essential difference between poetry and painting, no difference, that is, that is given for all time by the inherent natures of the media, the objects they represent, or the laws of the human mind; (2) there arc always a number of differences in effect in a culture which allow it to sort out the distinctive qualities o f its ensemble of signs and symbols. These differences, as I have suggested, are riddled with all the antithetical values the culture wants to embrace or repudiate: the parqgone or debate of poetry and painting is never just a contest between two kinds of signs, but a struggle between body and soul, world and mind, nature and culture.

Modern discussions of the relation between texts and images have tended to reduce this question to a problem of grammar. The traditional distinctions expressed in notions like time and space, nature and convention, have, in the work of modern theorists, been replaced by distinctions between different kinds of sign functions and communicative systems. We now speak of the difference between images and texts in terms such as the analogical and the digital, the iconic and the symbolic, the singleand the double-articulated. 1 These terms, drawn from fields such as systems analysis, semiotics, and linguistics, seem to promise a new, more scientific understanding of the boundaries between painting and poetry. They hold out the hope for a more rigorous definition of the difference and, especially in the work of the structuralists, the hope for a systematic way of comparing the arts. Modern theory has, in short, promised something to both sides of the traditional quarrel between the witty comparatists and the judicious differentiators of the arts: to the former it promises a higher level of generality and the prospect of large structural homologies between the arts; to the latter it offers a rigorous taxonomy that allows precise differentiation of sign-types and aesthetic modes.

For an arch-conventionalist like Goodman, the phrase that would probably stand out in this passage is the one that equates "artificial divisions" between the arts with "false" divisions, thus implying that man-made, conventional distinctions are, by virtue of their artificiality, automatically false. The contrast implied between these "false, artificial divisions" and "essential" features founded on "empirical and important facts" sets off a warning bell in the mind of a conventionalist; it also ought to sound an alarm for anyone who dislikes red herrings in arguments, and send them off in search of counterexamples to those "empirical facts" that lead us to Langer's categorical imperative: "there can be no hybrid works." An empirical survey of works that attempt graftings of verbal and pictorial signs (illustrated books, narrative paintings, films and dramas) does not immediately lead us to the conclusion that such hybrids are impossible. Even Langer's own logic of "essential" differences between the arts, aside from what the empirical facts tell us, leads to no such conclusion. One could as easily argue that such differences are a necessary condition for hybridization; the "crossing" of disparate forms to form new, composite unities makes no sense without an established set of differences, artificial or natural, to be overcome. Langer's notorious claim that "there are no happy marriages in art—only successful rape" illustrates perfecdy the sense of violence and violation she associates with the conjunction of artistic media, and hints (rather vividly) at its ideological basis in categories of gender.

Or , more accurately, one might say that the same old distinctions whose inadequacy motivated the search for a "general science of signs" tend to crop up in spite of the best efforts to weed them out. The disparities within the field of iconic signs that lead Eco to regard it as an incoherent category are precisely those sorts of oppositions that have traditionally figured the difference between texts and images. Some icons are "ruled by convention but are at the same time motivated." The word "motivated" in this context stands in the place occupied by terms like "nature" in traditional accounts of the text-image difference: "motivated" signs have a natural, necessary connection with what they signify; "unmotivated" signs are arbitrary and conventional. Eco's observation that icons sometimes seem "to be more firmly linked to the basic mechanisms of perception than to explicit cultural habits" is, similarly, a semiotic redaction of the notion that (some) images are "natural signs," and amounts to a contradiction in terms for a system that begins with the notion of the sign based in language.

There would be nothing wrong with this sort of redescription if it were not advertised as a liberation from metaphysics into a new science. The translation of Hume's laws of association into sign-types or modes of figuration has considerable interest. Among other things, it helps us to see just how riddled with notions of indirect, symbolic mediation are the supposedly "direct" perceptual mechanisms of the empirical tradition. The most striking example of this sort of mediation is, as we have seen, the notion of the mental or perceptual image ("ideas" and "sense-data"), which, on the one hand, seem to guarantee veridical access to the world, on the other hand to indefinitely and irretrievably distance the world through a system of intermediate signs. This double bind may be seen most clearly in the attempt of semioticians to come up with an account of the "photographic sign."

These doubly natural signs, iconic and indexical, then serve as the foundation for all further intellection and discourse. Among other things, they stand as the referents for words, which unlike the ideaimpression-mental image, signify (as Locke puts it) "not by any natural connexion . . . but by a voluntary imposition, whereby such a word is made arbitrarily the mark of such an idea." Ideas, by contrast, are naturally imprinted by experience and reflection: they are natural signs that (ideally) stand behind the arbitrary signs of language. The relation of words and ideas, discourse and thought, turns on the very same hinge that, in semiotics, connects the symbol with the indexical icon, the arbitrary code with the "natural" code.  Small wonder, then, that Roland Bardies finds himself saying the following sorts of things about photographs.

When Wilson catalogues what he shares with his Furby, there are things of the body (the burping) and there are things of the mind. Like many children, he thinks that because Furbies have language, they are more “peoplelike” than a “regular” pet. They arrive speaking Furbish, a language with its own dictionary, which many children try to commit to memory because they would like to meet their Furbies more than half way. The Furby manual instructs children, “I can learn to speak English by listening to you talk. The more you play with me, the more I will use your language.” Actually, Furby English emerges over time, whether or not a child talks to the robot. (Furbies have no hearing or language-learning ability.5) But until age eight, children are convinced by the illusion and believe they are teaching their Furbies to speak. The Furbies are alive enough to need them. Children enjoy the teaching task. From the first encounter, it gives them something in common with their Furbies and it implies that the Furbies can grow to better understand them. “I once didn’t know English,” says one six-year-old. “And now I do. So I know what my Furby is going through.” In the classroom with Furbies, children shout to each other in competitive delight: “My Furby speaks more English than yours! My Furby speaks English.” I have done several studies in which I send Furbies home with schoolchildren, often with the request that they (and their parents) keep a “Furby diary.” In my first study of kindergarten to third graders, I loan the Furbies out for two weeks at a time. It is not a good decision. I do not count on how great will be children’s sense of loss when I ask them to return the Furbies. I extend the length of the loans, often encouraged by parental requests. Their children have grown too attached to give up the robots. Nor are they mollified by parents’ offers to buy them new Furbies. Even more so than with Tamagotchis, children attach to a particular Furby, the one they have taught English, the one they have raised. For three decades, in describing people’s relationships with computers, I have often used the metaphor of the Rorschach, the inkblot test that psychologists use as a screen onto which people can project their feelings and styles of thought. But as children interact with sociable robots like Furbies, they move beyond a psychology of projection to a new psychology of engagement. They try to deal with the robot as they would deal with a pet or a person. Nineyear-old Leah, in an after-school playgroup, admits, “It’s hard to turn it [the Furby] off when it is talking to me.” Children quickly understand that to get the most out of your Furby, you have to pay attention to what it is telling you. When you are with a Furby, you can’t play a simple game of projective make-believe. You have to continually assess your Furby’s “emotional” and “physical” state. And children fervently believe that the child who loves his or her Furby best will be most loved in return.
This mutuality is at the heart of what makes the Furby, a primitive exemplar of sociable robotics, different from traditional dolls. As we’ve seen, such relational artifacts do not wait for children to “animate” them in the spirit of a Raggedy Ann doll or a teddy bear. They present themselves as already animated and ready for relationship. They promise reciprocity because, unlike traditional dolls, they are not passive. They make demands. They present as having their own needs and inner lives. They teach us the rituals of love that will make them thrive. For decades computers have asked us to think with them; these days, computers and robots, deemed sociable, affective, and relational, ask us to feel for and with them. Children see traditional dolls as they want them or need them to be. For example, an eight-year-old girl who feels guilty about breaking her mother’s best crystal pitcher might punish a row of Barbie dolls. She might take them away from their tea party and put them in detention, doing unto the dolls what she imagines should be done unto her. In contrast, since relational artifacts present themselves as having minds and intentions of their own, they cannot be so easily punished for one’s own misdeeds. Two eight-year-old girls comment on how their “regular dolls” differ from the robotic Furbies. The first says, “A regular doll, like my Madeleine doll . . . you can make it go to sleep, but its eyes are painted open, so, um, you cannot get them to close their eyes.... Like a Madeleine doll cannot go, ‘Hello, good morning.’” But this is precisely the sort of thing a Furby can do. The second offers, “The Furby tells you what it wants.” Indeed, Furbies come with manuals that provide detailed marching orders. They want language practice, food, rest, and protestations of love. So, for example, the manual instructs, “Make sure you say ‘HEY FURBY! I love you!’ frequently so that I feel happy and know I’m loved.” There is general agreement among children that a penchant for giving instructions distinguishes Furbies from traditional dolls. A seven-year-old girl puts it this way: “Dolls let you tell them what they want. The Furbies have their own ideas.” A nine-year-old boy sums up the difference between Furbies and his action figures: “You don’t play with the Furby, you sort of hang out with it. You do try to get power over it, but it has power over you too.” Children say that traditional dolls can be “hard work” because you have to do all the work of giving them ideas; Furbies are hard work for the opposite reason. They have plenty of ideas, but you have to give them what they want and when they want it. When children attach to a doll through the psychology of projection, they attribute to the doll what is most on their mind. But they need to accommodate a Furby. This give-and-take prepares children for the expectation of relationship with machines that is at the heart of the robotic moment.

Daisy, six, with a Furby at home, believes that each Furby’s owner must help his or her Furby fulfill its mission to learn about people. “You have to teach it; when you buy it, that is your job.” Daisy tells me that she taught her Furby about Brownie Girl Scouts, kindergarten, and whales. “It’s alive; I teach it about whales; it loves me.” Padma, eight, says that she likes meeting what she calls “Furby requests” and thinks that her Furby is “kind of like a person” because “it talks.” She goes on: “It’s kind of like me because I’m a chatterbox.” After two weeks, it is time for Padma to return her Furby, and afterward she feels regret: “I miss how it talked, and now it’s so quiet at my house.... I didn’t get a chance to make him a bed.” After a month with her Furby, Bianca, seven, speaks with growing confidence about their mutual affection: “I love my Furby because it loves me. . . . It was like he really knew me.”6 She knows her Furby well enough to believe that “it doesn’t want to miss fun . . . at a party.” In order to make sure that her social butterfly Furby gets some rest when her parents entertain late into the evening, Bianca clips its ears back with clothespins to fool the robot into thinking that “nothing is going on . . . so he can fall asleep.” This move is ineffective, and all of this activity is exhausting, but Bianca calmly sums up her commitment: “It takes lots of work to take care of these.” When Wilson, who so enjoys burping in synchrony with his Furby, faces up to the hard work of getting his Furby to sleep, he knows that if he forces sleep by removing his Furby’s batteries, the robot will “forget” whatever has passed between them—this is unacceptable. So Furby sleep has to come naturally. Wilson tries to exhaust his Furby by keeping it up late at night watching television. He experiments with Furby “sleep houses” made of blankets piled high over towers of blocks. When Wilson considers Furby sleep, his thoughts turn to Furby dreams. He is sure his Furby dreams “when his eyes are closed.” What do Furbies dream of? Second and third graders think they dream “of life on their flying saucers.”7 And they dream about learning languages and playing with the children they love.
In the 1980s, the computer toy Merlin made happy and sad noises depending on whether it was winning or losing the sound-and-light game it played with children. Children saw Merlin as “sort of alive” because of how well it played memory games, but they did not fully believe in Merlin’s shows of emotion. When a Merlin broke down, children were sorry to lose a playmate. When a Furby doesn’t work, however, children see a creature that might be in pain. Lily, ten, worries that her broken Furby is hurting. But she doesn’t want to turn it off, because “that means you aren’t taking care of it.” She fears that if she shuts off a Furby in pain, she might make things worse. Two eight-year-olds fret about how much their Furbies sneeze. The first worries that his sneezing Furby is allergic to him. The other fears his Furby got its cold because “I didn’t do a good enough job taking care of him.” Several children become tense when Furbies make unfamiliar sounds that might be signals of distress. I observe children with their other toys: dolls, toy soldiers, action figures. If these toys make strange sounds, they are usually put aside; broken toys lead easily to boredom. But when a Furby is in trouble, children ask, “Is it tired?” “Is it sad?” “Have I hurt it?” “Is it sick?” “What shall I do?” Taking care of a robot is a high-stakes game. Things can—and do—go wrong. In one kindergarten, when a Furby breaks down, the children decide they want to heal it. Ten children volunteer, seeing themselves as doctors in an emergency room. They decide they’ll begin by taking it apart. The proceedings begin in a state of relative calm. When talking about their sick Furby, the children insist that this breakdown does not mean the end: people get sick and get better. But as soon as scissors and pliers appear, they become anxious. At this point, Alicia screams, “The Furby is going to die!” Sven, to his classmates’ horror, pinpoints the moment when Furbies die: it happens when a Furby’s skin is ripped off. Sven considers the Furby as an animal. You can shave an animal’s fur, and it will live. But you cannot take its skin off. As the operation continues, Sven reconsiders. Perhaps the Furby can live without its skin, “but it will be cold.” He doesn’t back completely away from the biological (the Furby is sensitive to the cold) but reconstructs it. For Sven, the biological now includes creatures such as Furbies, whose “insides” stay “all in the same place” when their skin is removed. This accommodation calms him down. If a Furby is simultaneously biological and mechanical, the operation in process, which is certainly removing the Furby’s skin, is not necessarily destructive. Children make theories when they are confused or anxious. A good theory can reduce anxiety.
But some children become more anxious as the operation continues. One suggests that if the Furby dies, it might haunt them. It is alive enough to turn into a ghost. Indeed, a group of children start to call the empty Furby skin “the ghost of Furby” and the Furby’s naked body “the goblin.” They are not happy that this operation might leave a Furby goblin and ghost at large. One girl comes up with the idea that the ghost of the Furby will be less fearful if distrib- uted. She asks if it would be okay “if every child took home a piece of Furby skin.” She is told this would be fine, but, unappeased, she asks the same question two more times. In the end, most children leave with a bit of Furby fur.8 Some talk about burying it when they get home. They leave room for a private ritual to placate the goblin and say good-bye. Inside the classroom, most of the children feel they are doing the best they can with a sick pet. But from outside the classroom, the Furby surgery looks alarming. Children passing by call out, “You killed him.” “How dare you kill Furby?” “You’ll go to Furby jail.” Denise, eight, watches some of the goings-on from the safety of the hall. She has a Furby at home and says that she does not like to talk about its problems as diseases because “Furbies are not animals.” She uses the word “fake” to mean nonbiological and says, “Furbies are fake, and they don’t get diseases.” But later, she reconsiders her position when her own Furby’s batteries run out and the robot, so chatty only moments before, becomes inert. Denise panics: “It’s dead. It’s dead right now.... Its eyes are closed.” She then declares her Furby “both fake and dead.” Denise concludes that worn-out batteries and water can kill a Furby. It is a mechanism, but alive enough to die. Linda, six, is one of the children whose family has volunteered to keep a Furby for a twoweek home study. She looked forward to speaking to her Furby, sure that unlike her other dolls, this robot would be worth talking to. But on its very first night at her home, her Furby stops working: “Yeah, I got used to it, and then it broke that night—the night that I got it. I felt like I was broken or something.... I cried a lot. . . . I was really sad that it broke, ’cause Furbies talk, they’re like real, they’re like real people.” Linda is so upset about not protecting her Furby that when it breaks she feels herself broken. Things get more complicated when I give Linda a new Furby. Unlike children like Zach who have invested time and love in a “first Furby” and want no replacements, Linda had her original Furby in working condition for only a few hours. She likes having Furby #2: “It plays hide-and-seek with me. I play red light, green light, just like in the manual.” Linda feeds it and makes sure it gets enough rest, and she reports that her new Furby is grateful and affectionate. She makes this compatible with her assessment of a Furby as “just a toy” because she has come to see gratitude, conversation, and affection as something that toys can manage. But now she will not name her Furby or say it is alive. There would be risk in that: Linda might feel guilty if the new Furby were alive enough to die and she had a replay of her painful first experience.
When a mechanism breaks, we may feel regretful, inconvenienced, or angry. We debate whether it is worth getting it fixed. When a doll cries, children know that they are themselves creating the tears. But a robot with a body can get “hurt,” as we saw in the improvised Furby surgical theater. Sociable robotics exploits the idea of a robotic body to move people to relate to machines as subjects, as creatures in pain rather than broken objects. That even the most primitive Tamagotchi can inspire these feelings demonstrates that objects cross that line not because of their sophistication but because of the feelings of attachment they evoke. The Furby, even more than the Tamagotchi, is alive enough to suggest a body in pain as well as a troubled mind. Furbies whine and moan, leaving it to their users to discover what might help. And what to make of the moment when an upside down Furby says, “Me scared!”? Freedom Baird takes this question very seriously.9 A recent graduate of the MIT Media Lab, she finds herself engaged with her Furby as a creature and a machine. But how seriously does she take the idea of the Furby as a creature? To determine this, she proposes an exercise in the spirit of the Turing test. In the original Turing test, published in 1950, mathematician Alan Turing, inventor of the first general-purpose computer, asked under what conditions people would consider a computer intelligent. In the end, he settled on a test in which the computer would be declared intelligent if it could convince people it was not a machine. Turing was working with computers made up of vacuum tubes and Teletype terminals. He suggested that if participants couldn’t tell, as they worked at their Teletypes, if they were talking to a person or a computer, that computer would be deemed “intelligent.” 10 A half century later, Baird asks under what conditions a creature is deemed alive enough for people to experience an ethical dilemma if it is distressed. She designs a Turing test not for the head but for the heart and calls it the “upside-down test.” A person is asked to invert three creatures: a Barbie doll, a Furby, and a biological gerbil. Baird’s question is simple: “How long can you hold the object upside down before your emotions make you turn it back?” Baird’s experiment assumes that a sociable robot makes new ethical demands. Why? The robot performs a psychology; many experience this as evidence of an inner life, no matter how primitive. Even those who do not think a Furby has a mind—and this, on a conscious level, includes most people—find themselves in a new place with an upside-down Furby that is whining and telling them it is scared. They feel themselves, often despite themselves, in a situation that calls for an ethical response. This usually happens at the moment when they identify with the “creature” before them, all the while knowing that it is “only a machine.” This simultaneity of vision gives Baird the predictable results of the upside-down test. As Baird puts it, “People are willing to be carrying the Barbie around by the feet, slinging it by the hair . . . no problem.... People are not going to mess around with their gerbil.” But in the case of the Furby, people will “hold the Furby upside down for thirty seconds or so, but when it starts crying and saying it’s scared, most people feel guilty and turn it over.” The work of neuroscientist Antonio Damasio offers insight into the origins of this guilt. Damasio describes two levels of experiencing pain. The first is a physical response to a painful stimulus. The second, a far more complex reaction, is an emotion associated with pain. This is an internal representation of the physical. 11 When the Furby says, “Me scared,” it signals that it has crossed the line between a physical response and an emotion, the internal representation. When people hold a Furby upside down, they do something that would be painful if done to an animal. The Furby cries out—as if it were an animal. But then it says, “Me scared”—as if it were a person.
People are surprised by how upset they get in this theater of distress. And then they get upset that they are upset. They often try to reassure themselves, saying things like, “Chill, chill, it’s only a toy!” They are experiencing something new: you can feel bad about yourself for how you behave with a computer program. Adults come to the upside-down test knowing two things: the Furby is a machine and they are not torturers. By the end, with a whimpering Furby in tow, they are on new ethical terrain.12 We are at the point of seeing digital objects as both creatures and machines. A series of fractured surfaces—pet, voice, machine, friend—come together to create an experience in which knowing that a Furby is a machine does not alter the feeling that you can cause it pain. Kara, a woman in her fifties, reflects on holding a moaning Furby that says it is scared. She finds it distasteful, “not because I believe that the Furby is really scared, but because I’m not willing to hear anything talk like that and respond by continuing my behavior. It feels to me that I could be hurt if I keep doing this.” For Kara, “That is not what I do.... In that moment, the Furby comes to represent how I treat creatures.” When the toy manufacturer Hasbro introduced its My Real Baby robot doll in 2000, it tried to step away from these complex matters. My Real Baby shut down in situations where a real baby might feel pain. This was in contrast to its prototype, a robot called “IT,” developed by a team led by MIT roboticist Rodney Brooks. “IT” evolved into “BIT” (for Baby IT), a doll with “states of mind” and facial musculature under its synthetic skin to give it expression.13 When touched in a way that would induce pain in a child, BIT cried out. Brooks describes BIT in terms of its inner states: If the baby were upset, it would stay upset until someone soothed it or it finally fell asleep after minutes of heartrending crying and fussing. If BIT . . . was abused in any way—for instance, by being swung upside down—it got very upset. If it was upset and someone bounced it on their knee, it got more upset, but if the same thing happened when it was happy, it got more and more excited, giggling and laughing, until eventually it got overtired and started to get upset. If it were hungry, it would stay hungry until it was fed. It acted a lot like a real baby.14 BIT, with its reactions to abuse, became the center of an ethical world that people constructed around its responses to pleasure and pain. But when Hasbro put BIT into mass production as My Real Baby, the company decided not to present children with a toy that responded to pain. The theory was that a robot’s response to pain could “enable” sadistic behavior. If My Real Baby were touched, held, or bounced in a way that would hurt a real baby, the robot shut down.
In its promotional literature, Hasbro marketed My Real Baby as “the most real, dynamic baby doll available for young girls to take care of and nurture.” They presented it as a companion that would teach and encourage reciprocal social behavior as children were trained to respond to its needs for amusement as well as bottles, sleep, and diaper changes. Indeed, it was marketed as realistic in all things—except that if you “hurt” it, it shut down. When children play with My Real Baby, they do explore aggressive possibilities. They spank it. It shuts down. They shake it, turn it upside down, and box its ears. It shuts down. Hasbro’s choice—maximum realism, but with no feedback for abuse—inspires strong feelings, especially among parents. For one group of parents, what is most important is to avoid a child’s aggressive response. Some believe that if you market realism but show no response to “pain,” children are encouraged to inflict it because doing so seems to have no cost. Others think that if a robot simulates pain, it enables mistreatment. Another group of parents wish that My Real Baby would respond to pain for the same reason that they justify letting their children play violent video games: they see such experiences as “cathartic.” They say that children (and adults too) should express aggression (or sadism or curiosity) in situations that seem “realistic” but where nothing “alive” is being hurt. But even these parents are sometimes grateful for My Real Baby’s unrealistic show of “denial.” They do not want to see their children tormenting a screaming baby. No matter what position one takes, sociable robots have taught us that we do not shirk from harming realistic simulations of life. This is, of course, how we now train people for war. First, we learn to kill the virtual. Then, desensitized, we are sent to kill the real. The prospect of studying these matters raises awful questions. Freedom Baird had people hold a whining, complaining Furby upside down, much to their discomfort. Do we want to encourage the abuse of increasingly realistic robot dolls? When I observe children with My Real Baby in an after-school playgroup for eight-yearolds, I see a range of responses. Alana, to the delight of a small band of her friends, flings My Real Baby into the air and then shakes it violently while holding it by one leg. Alana says the robot has “no feelings.” Watching her, one wonders why it is necessary then to “torment” something without feelings. She does not behave this way with the many other dolls in the playroom. Scott, upset, steals the robot and brings it to a private space. He says, “My Real Baby is like a baby and like a doll.... I don’t think she wants to get hurt.” As Scott tries to put the robot’s diaper back on, some of the other children stand beside him and put their fingers in its eyes and mouth. One asks, “Do you think that hurts?” Scott warns, “The baby’s going to cry!” At this point, one girl tries to pull My Real Baby away from Scott because she sees him as an inadequate protector: “Let go of her!” Scott resists. “I was in the middle of changing her!” It seems a good time to end the play session. As the research team, exhausted, packs up to go, Scott sneaks behind a table with the robot, gives it a kiss, and says good-bye, out of the sight of the other children.
In the pandemonium of Scott and Alana’s playgroup, My Real Baby is alive enough to torment and alive enough to protect. The adults watching this—a group of teachers and my research team—feel themselves in an unaccustomed quandary. If the children had been tossing around a rag doll, neither we, nor presumably Scott, would have been as upset. But it is hard to see My Real Baby treated this way. All of this—the Furbies that complain of pain, the My Real Babies that do not—creates a new ethical landscape. The computer toys of the 1980s only suggested ethical issues, as when children played with the idea of life and death when they “killed” their Speak & Spells by taking out the toys’ batteries. Now, relational artifacts pose these questions directly. One can see the new ethics at work in my students’ reactions to Nexi, a humanoid robot at MIT. Nexi has a female torso, an emotionally expressive face, and the ability to speak. In 2009, one of my students, researching a paper, made an appointment to talk with the robot’s development team. Due to a misunderstanding about scheduling, my student waited alone, near the robot. She was upset by her time there: when not interacting with people, Nexi was put behind a curtain and blindfolded. At the next meeting of my graduate seminar, my student shared her experience of sitting alongside the robot. “It was very upsetting,” she said. “The curtain—and why was she blindfolded? I was upset because she was blindfolded.” The story of the shrouded and blindfolded Nexi ignited the seminar. In the conversation, all the students talked about the robot as a “she.” The designers had done everything they could to give the robot gender. And now, the act of blindfolding signaled sight and consciousness. In class, questions tumbled forth: Was the blindfold there because it would be too upsetting to see Nexi’s eyes? Perhaps when Nexi was turned off, “her” eyes remained open, like the eyes of a dead person? Perhaps the robot makers didn’t want Nexi to see “out”? Perhaps they didn’t want Nexi to know that when not in use, “she” is left in a corner behind a curtain? This line of reasoning led the seminar to an even more unsettling question: If Nexi is smart enough to need a blindfold to protect “her” from fully grasping “her” situation, does that mean that “she” is enough of a subject to make “her” situation abusive? The students agreed on one thing: blindfolding the robot sends a signal that “this robot can see.” And seeing implies understanding and an inner life, enough of one to make abuse possible. I have said that Sigmund Freud saw the uncanny as something long familiar that feels strangely unfamiliar. The uncanny stands between standard categories and challenges the categories themselves. It is familiar to see a doll at rest. But we don’t need to cover its eyes, for it is we who animate it. It is familiar to have a person’s expressive face beckon to us, but if we blindfold that person and put them behind a curtain, we are inflicting punishment. The Furby with its expressions of fear and the gendered Nexi with her blindfold are the new uncanny in the culture of computing.
Soon, it may seem natural to watch a robot “suffer” if you hurt it. It may seem natural to chat with a robot and have it behave as though pleased you stopped by. As the intensity of experiences with robots increases, as we learn to live in new landscapes, both children and adults may stop asking the questions “Why am I talking to a robot?” and “Why do I want this robot to like me?” We may simply be charmed by the pleasure of its company. The romantic reaction of the 1980s and 1990s put a premium on what only people can contribute to each other: the understanding that grows out of shared human experience. It insisted that there is something essential about the human spirit. In the early 1980s, David, twelve, who had learned computer programming at school, contrasted people and programs this way: “When there are computers who are just as smart as the people, the computers will do a lot of the jobs, but there will still be things for the people to do. They will run the restaurants, taste the food, and they will be the ones who will love each other, have families and love each other. I guess they’ll still be the only ones who go to church.”15 Adults, too, spoke of life in families. To me, the romantic reaction was captured by how one man rebuffed the idea that he might confide in a computer psychotherapist: “How can I talk about sibling rivalry to something that never had a mother?” Of course, elements of this romantic reaction are still around us. But a new sensibility emphasizes what we share with our technologies. With psychopharmacology, we approach the mind as a bioengineerable machine.16 Brain imaging trains us to believe that things—even things like feelings—are reducible to what they look like. Our current therapeutic culture turns from the inner life to focus on the mechanics of behavior, something that people and robots might share. A quarter of a century stands between two conversations I had about the possibilities of a robot confidant, the first in 1983, the second in 2008. For me, the differences between them mark the movement from the romantic reaction to the pragmatism of the robotic moment. Both conversations were with teenage boys from the same Boston neighborhood; they are both Red Sox fans and have close relationships with their fathers. In 1983, thirteen-year-old Bruce talked about robots and argued for the unique “emotionality” of people. Bruce rested his case on the idea that computers and robots are “perfect,” while people are “imperfect,” flawed and frail. Robots, he said, “do everything right”; people “do the best they know how.” But for Bruce it was human imperfection that makes for the ties that bind. Specifically, his own limitations made him feel close to his father (“I have a lot in common with my father.... We both have chaos”). Perfect robots could never understand this very important relationship. If you ever have a problem, you go to a person.
Twenty-five years later, a conversation on the same theme goes in a very different direction. Howard, fifteen, compares his father to the idea of a robot confidant, and his father does not fare well in the comparison. Howard thinks the robot would be better able to grasp the intricacies of high school life: “Its database would be larger than Dad’s. Dad has knowledge of basic things, but not enough of high school.” In contrast to Bruce’s sense that robots are not qualified to have an opinion about the goings-on in families, Howard hopes that robots might be specially trained to take care of “the elderly and children”—something he doesn’t see the people around him as much interested in. Howard has no illusions about the uniqueness of people. In his view, “they don’t have a monopoly” on the ability to understand or care for each other. Each human being is limited by his or her own life experience, says Howard, but “computers and robots can be programmed with an infinite amount of information.” Howard tells a story to illustrate how a robot could provide him with better advice than his father. Earlier that year, Howard had a crush on a girl at school who already had a boyfriend. He talked to his father about asking her out. His father, operating on an experience he had in high school and what Howard considers an outdated ideal of “macho,” suggested that he ask the girl out even though she was dating someone else. Howard ignored his father’s advice, fearing it would lead to disaster. He was certain that in this case, a robot would have been more astute. The robot “could be uploaded with many experiences” that would have led to the right answer, while his father was working with a limited data set. “Robots can be made to understand things like jealousy from observing how people behave.... A robot can be fully understanding and open-minded.” Howard thinks that as a confidant, the robot comes out way ahead. “People,” he says, are “risky.” Robots are “safe.” There are things, which you cannot tell your friends or your parents, which . . . you could tell an AI. Then it would give you advice you could be more sure of.... I’m assuming it would be programmed with prior knowledge of situations and how they worked out. Knowledge of you, probably knowledge of your friends, so it could make a reasonable decision for your course of action. I know a lot of teenagers, in particular, tend to be caught up in emotional things and make some really bad mistakes because of that.
I ask Howard to imagine what his first few conversations with a robot might be like. He says that the first would be “about happiness and exactly what that is, how do you gain it.” The second conversation would be “about human fallibility,” understood as something that causes “mistakes.” From Bruce to Howard, human fallibility has gone from being an endearment to a liability. No generation of parents has ever seemed like experts to their children. But those in Howard’s generation are primed to see the possibilities for relationships their elders never envisaged. They assume that an artificial intelligence could monitor all of their e-mails, calls, Web searches, and messages. This machine could supplement its knowledge with its own searches and retain a nearly infinite amount of data. So, many of them imagine that via such search and storage an artificial intelligence or robot might tune itself to their exact needs. As they see it, nothing technical stands in the way of this robot’s understanding, as Howard puts it, “how different social choices [have] worked out.” Having knowledge and your best interests at heart, “it would be good to talk to . . . about life. About romantic matters. And problems of friendship.” Life? Romantic matters? Problems of friendship? These were the sacred spaces of the romantic reaction. Only people were allowed there. Howard thinks that all of these can be boiled down to information so that a robot can be both expert resource and companion. We are at the robotic moment. As I have said, my story of this moment is not so much about advances in technology, impressive though these have been. Rather, I call attention to our strong response to the relatively little that sociable robots offer—fueled it would seem by our fond hope that they will offer more. With each new robot, there is a ramp-up in our expectations. I find us vulnerable—a vulnerability, I believe, not without risk.
In April 1999, a month before AIBO’s commercial release, Sony demonstrated the little robot dog at a conference on new media in San Jose, California. I watched it walk jerkily onto an empty stage, followed by its inventor, Toshitado Doi. At his bidding, AIBO fetched a ball and begged for a treat. Then, with seeming autonomy, AIBO raised its back leg to some suggestion of a hydrant. Then, it hesitated, a stroke of invention in itself, and lowered its head as though in shame. The audience gasped. The gesture, designed to play to the crowd, was wildly successful. I imagined how audiences responded to Jacques de Vaucanson’s eighteenth-century digesting (and defecating) mechanical duck and to the chess-playing automata that mesmerized Edgar Alan Poe. AIBO, like these, was applauded as a marvel, a wonder.1 Depending on how it is treated, an individual AIBO develops a distinct personality as it matures from a fall-down puppy to a grown-up dog. Along the way, AIBO learns new tricks and expresses feelings: flashing red and green eyes direct our emotional traffic; each of its moods comes with its own soundtrack. A later version of AIBO recognizes its primary caregiver and can return to its charging station, smart enough to know when it needs a break. Unlike a Furby, whose English is “destined” to improve as long as you keep it turned on, AIBO stakes a claim to intelligence and impresses with its ability to show what’s on its mind. If AIBO is in some sense a toy, it is a toy that changes minds. It does this in several ways. It heightens our sense of being close to developing a postbiological life and not just in theory or in the laboratory. And it suggests how this passage will take place. It will begin with our seeing the new life as “as if ” life and then deciding that “as if ” may be life enough. Even now, as we contemplate “creatures” with artificial feelings and intelligence, we come to reflect differently on our own. The question here is not whether machines can be made to think like people but whether people have always thought like machines. The reconsiderations begin with children. Zane, six, knows that AIBO doesn’t have a “real brain and heart,” but they are “real enough.” AIBO is “kind of alive” because it can function “as if it had a brain and heart.” Paree, eight, says that AIBO’s brain is made of “machine parts,” but that doesn’t keep it from being “like a dog’s brain.... Sometimes, the way [AIBO] acted, like he will get really frustrated if he can’t kick the ball. That seemed like a real emotion . . . so that made me treat him like he was alive, I guess.” She says that when AIBO needs its batteries charged, “it is like a dog’s nap.” And unlike a teddy bear, “an AIBO needs its naps.” As Paree compares her AIBO’s brain to that of a dog, she clears the way for other possibilities. She considers whether AIBO might have feelings like a person, wondering if AIBO “knows its own feelings”—or “if the controls inside know them.” Paree says that people use both methods. Sometimes people have spontaneous feelings and “just become aware” of them (this is “knowing your own feelings”). But other times, people have to program themselves to have the feelings they want. “If I was sad and wanted to be happy”—here Paree brings her fists up close to her ears to demonstrate concentration and intent—“I would have to make my brain say that I am set on being happy.” The robot, she thinks, probably has the second kind of feelings, but she points out that both ways of getting to a feeling get you to the same place: a smile or a frown if you are a person, a happy or sad sound if you are an AIBO. Different inner states lead to the same outward states, and so inner states cease to matter. AIBO carries a behaviorist sensibility.
Keith, seventeen, is going off to college next year and taking his AIBO with him. He treats the robot as a pet, all the while knowing that it is not a pet at all. He says, “Well, it’s not a pet like others, but it is a damn good pet. . . . I’ve taught it everything. I’ve programmed it to have a personality that matches mine. I’ve never let it reset to its original personality. I keep it on a program that lets it develop to show the care I’ve put into it. But of course, it’s a robot, so you have to keep it dry, you have to take special care with it.” His classmate Logan also has an AIBO. The two have raised the robots together. If anything, Logan’s feelings are even stronger than Keith’s. Logan says that talking to AIBO “makes you better, like, if you’re bored or tired or down . . . because you’re actually, like, interacting with something. It’s nice to get thoughts out.” The founders of artificial intelligence were much taken with the ethical and theological implications of their enterprise. They discussed the mythic resonance of their new science: Were they people putting themselves in the place of gods?2 The impulse to create an object in one’s own image is not new—think Galatea, Pygmalion, Frankenstein. These days, what is new is that an off-the-shelf technology as simple as an AIBO provides an experience of shaping one’s own companion. But the robots are shaping us as well, teaching us how to behave so that they can flourish.3 Again, there is psychological risk in the robotic moment. Logan’s comment about talking with the AIBO to “get thoughts out” suggests using technology to know oneself better. But it also suggests a fantasy in which we cheapen the notion of companionship to a baseline of “interacting with something.” We reduce relationship and come to see this reduction as the norm. As infants, we see the world in parts. There is the good—the things that feed and nourish us. There is the bad—the things that frustrate or deny us. As children mature, they come to see the world in more complex ways, realizing, for example, that beyond black and white, there are shades of gray. The same mother who feeds us may sometimes have no milk. Over time, we transform a collection of parts into a comprehension of wholes.4 With this integration, we learn to tolerate disappointment and ambiguity. And we learn that to sustain realistic relationships, one must accept others in their complexity. When we imagine a robot as a true companion, there is no need to do any of this work. The first thing missing if you take a robot as a companion is alterity, the ability to see the world through the eyes of another.5 Without alterity, there can be no empathy. Writing before robot companions were on the cultural radar, the psychoanalyst Heinz Kohut described barriers to alterity, writing about fragile people—he calls them narcissistic personalities—who are characterized not by love of self but by a damaged sense of self. They try to shore themselves up by turning other people into what Kohut calls self objects. In the role of selfobject, another person is experienced as part of one’s self, thus in perfect tune with a fragile inner state. The selfobject is cast in the role of what one needs, but in these relationships, disappointments inevitably follow. Relational artifacts (not only as they exist now but as their designers promise they will soon be) clearly present themselves as candidates for the role of selfobject.
With a price tag of $1,300 to $2,000, AIBO is meant for grown-ups. But the robot dog is a harbinger of the digital pets of the future, and so I present it to children from age four to thirteen as well as to adults. I bring it to schools, to after-school play centers, and, as we shall see in later chapters, to senior centers and nursing homes. I offer AIBOs for home studies, where families get to keep them for two or three weeks. Sometimes, I study families who have bought an AIBO of their own. In these home studies, just as in the home studies of Furbies, families are asked to keep a “robot diary.” What is it like living with an AIBO? The youngest children I work with—the four- to six-year-olds—are initially preoccupied with trying to figure out what the AIBO is, for it is not a dog and not a doll. The desire to get such things squared away is characteristic of their age. In the early days of digital culture, when they met their first electronic toys and games, children of this age would remain preoccupied with such questions of categories. But now, faced with this sociable machine, children address them and let them drop, taken up with the business of a new relationship. Maya, four, has an AIBO at home. She first asks questions about its origins (“How do they make it?”) and comes up with her own answer: “I think they start with foil, then soil, and then you get some red flashlights and then put them in the eyes.” Then she pivots to sharing the details of her daily life with AIBO: “I love to play with AIBO every day, until the robot gets tired and needs to take a nap.” Henry, four, follows the same pattern. He begins with an effort to categorize AIBO: AIBO is closest to a person, but different from a person because it is missing a special “inner power,” an image borrowed from his world of Pokémon. 6 But when I see Henry a week later, he has bonded with AIBO and is stressing the positive, all the things they share. The most important of these are “remembering and talking powers, the strongest powers of all.” Henry is now focused on the question of AIBO’s affection: How much does this robot like him? Things seem to be going well: he says that AIBO favors him “over all his friends.” By eight, children move even more quickly from any concern over AIBO’s “nature” to the pleasures of everyday routines. In a knowing tone, Brenda claims that “people make robots and . . . people come from God or from eggs, but this doesn’t matter when you are playing with the robot.” In this dismissal of origins we see the new pragmatism. Brenda embraces AIBO as a pet. In her robot diary, she reminds herself of the many ways that this pet should not be treated as a dog. One early entry reminds her not to feed it, and another says, “Do not take AIBO on walks so it can poop.” Brenda feels guilty if she doesn’t keep AIBO entertained. She thinks that “if you don’t play with it,” its lights get red to show its discontent at “playing by itself and getting all bored.” Brenda thinks that when bored, AIBO tries to “entertain itself.” If this doesn’t work, she says, “it tries to get my attention.” Children believe that AIBO asks for attention when it needs it. So, for example, a sick AIBO will want to get better and know it needs human help. An eight-year-old says, “It would want more attention than anything in the whole world.”
Yolanda’s feelings about AIBO also go through all the stages. She first sees AIBO as a substitute: “AIBO might be good practice for all children whose parents aren’t ready to take care of a real dog.” But then she takes another step: in some ways AIBO might be better than a real dog. “The AIBO,” says Yolanda, “doesn’t shed, doesn’t bite, doesn’t die.” More than this, a robotic companion can be made as you like it. Yolanda muses about how nice it would be to “keep AIBO at a puppy stage for people who like to have puppies.” Children imagine that they can create a customized AIBO close to their heart’s desire.8 Sometimes their heart’s desire is to have affection when that pleases them and license to walk away, something not possible with a biological pet. Two nine-year-olds—Lydia and Paige—talk through the steps that take a robot from better than nothing to better than anything. Lydia begins by thinking of AIBO as a substitute for a real pet if you can’t have one: “An AIBO, since you can’t be allergic to a robot, that would be very nice to have.” But as she gets to know AIBO better, she sees a more enticing possibility. “Sometimes,” she says, “I might like [AIBO] more than a real living animal, like a real cat or a real dog, because, like if you had a bad day . . . then you could just turn this thing off and it wouldn’t bug you.” Paige has five pets—three dogs, two cats—and when she is sad, she says, “I cuddle with them.” This is a good thing, but she complains that pets can be trouble: “All of them want your attention. If you give one attention you have to give them all attention, so it’s kinda hard.... When I go somewhere, my kitten misses me. He’ll go into my room and start looking for me.” AIBO makes things easy: “AIBO won’t look at you like ‘play with me’; it will just go to sleep if there is nothing else to do. It won’t mind.” Paige explains that the worst thing that ever happened to her was when her family “had to put their dog to sleep.” She hasn’t wanted a new one since. “But the thing about AIBO,” she says, “is that you don’t have to put him to sleep.... I think you could fix [AIBO] with batteries . . . but when your dog actually dies, you can’t fix it.” For now, the idea that AIBO, as she puts it, “will last forever” makes it better than a dog or cat. Here, AIBO is not practice for the real. It offers an alternative, one that sidesteps the necessity of death.9 For Paige, simulation is not necessarily second best. Pets have long been thought good for children because they teach responsibility and commitment. AIBO permits something different: attachment without responsibility. Children love their pets, but at times, like their overextended parents, they feel burdened by their pets’ demands. This has always been true. But now children see a future where something different may be available. With robot pets, children can give enough to feel attached, but then they can turn away. They are learning a way of feeling connected in which they have permission to think only of themselves. And yet, since these new pets seem betwixt and between what is alive and what is not, this turning away is not always easy. It is not that some children feel responsible for AIBO and other do not. The same children often have strong feelings on both sides of the matter.
As soon as children met computers and computer toys in the late 1970s and early 1980s, they used aggression as a way to animate them and to play with ideas about life and death. Children crashed and revived computer programs; they “killed” Merlin, Simon, and Speak & Spell by pulling out their batteries and then made them come back to life. Aggression toward sociable robots is more complex because children are trying to manage more significant attachments. To take only one example, robots disappoint when they do not display the affection children lead themselves to expect. To avoid hurt, children want to dial things down. Turning robots into objects that can be hurt with impunity is a way to put them in their place. Whether we have permission to hurt or kill an object influences how we think about its life. 10 To children, being able to kill spiders without punishment makes spiders seem less alive, and hurting a robot can make it seem less alive as well. But as in the discussion about whether My Real Baby should cry in “pain,” things are complicated. For the idea that you can hurt a robot can also make it seem more alive. Like Henry, twelve-year-old Tamara is aggressive toward AIBO and troubled by what this implies. She wants to play with AIBO in the same way that she plays with her much-loved cat. But she worries that AIBO’s responses to her are generic. She says, “AIBO acts the same to everyone. It doesn’t attach herself to one person like most animals do.” Tamara says that sometimes she stops herself from petting AIBO: “I start to pet it, and then, like, I would start to be, like, ‘Oh wait. You’re not a cat. You’re not alive.’” And sometimes she gives in to an urge to “knock it over because it was just so cute when it was getting up and then it would, like, shake its head, because then it seemed really alive because that’s what dogs do.” She tries to reassure me: “I’m not like this with my animals.” From their earliest experiences with the electronic toys and games of the late 1970s, children split the notion of consciousness and life. You didn’t have to be biologically alive to have awareness. And so, Tamara who knows AIBO is not alive, imagines that it still might feel pain. In the end, her aggression puts her in a tough spot; AIBO is too much like a companion to be a punching bag. For Tamara, the idea that AIBO might “see” well enough to recognize her is frightening because it might know she is hitting it. But the idea of AIBO as aware and thus more lifelike is exciting as well.
Ashley, seventeen, is a bright and active young woman who describes herself as a cat lover. I have given her an AIBO to take home for two weeks, and now she is at my office at MIT to talk about the experience. During the conversation, Ashley’s AIBO plays on the floor. We do not attend to it; it does tricks on its own—and very noisily. After a while, it seems as though the most natural thing would be to turn AIBO off, in the same spirit that one might turn off a radio whose volume interferes with a conversation. Ashley moves toward the AIBO, hesitates, reaches for its off switch, and hesitates again. Finally, with a small grimace, she hits the switch. AIBO sinks to the ground, inert. Ashley comments, “I know it’s not alive, but I would be, like, talking to it and stuff, and then it’s just a weird experience to press a[n off] button. It made me nervous.... [I talk to it] how I would talk to my cat, like he could actually hear me and understand praise and stuff like that.” I am reminded of Leah, nine, who said of her Furby, “It’s hard to turn it off when it is talking to me.” Ashley knows AIBO is a robot, but she experiences it as a biological pet. It becomes alive for her not only because of its intelligence but because it seems to her to have real emotions. For example, she says that when AIBO’s red lights shone in apparent frustration, “it seemed like a real emotion.... So that made me treat him like he was alive.... And that’s another strange thing: he’s not really physically acting those emotions out, but then you see the colors and you think, ‘Oh, he’s upset.’” Artificial intelligence is often described as the art and science of “getting machines to do things that would be considered intelligent if done by people.” We are coming to a parallel definition of artificial emotion as the art of “getting machines to express things that would be considered feelings if expressed by people.” Ashley describes the moment of being caught between categories: she realizes that what the robot is “acting out” is not emotion, yet she feels the pull of seeing “the colors” and experiencing AIBO as “upset.” Ashley ends up seeing AIBO as both machine and creature. So does John Lester, a computer scientist coming from a far more sophisticated starting point. From the early 1990s, Lester pioneered the use of online communities for teaching, learning, and collaboration, including recent work developing educational spaces on the virtual world of Second Life. Lester bought one of the first AIBOs on the market. He called it Alpha in deference to its being “one of the first batch.”12 When Lester took Alpha out of its box, he shut the door to his office and spent the entire day “hanging out with [my] new puppy.” He describes the experience as “intense,” comparing it to the first time he saw a computer or typed into a Web browser. He quickly mastered the technical aspects of AIBO, but this kind of understanding did not interfere with his pleasure in simply being with the puppy. When Sony modified the robot’s software, Lester bought a second AIBO and named it Beta. Alpha and Beta are machines, but Lester does not like anyone to treat them as inanimate metal and plastic. “I think about my AIBOs in different ways at the same time,” Lester says. In the early days of cubism, the simultaneous presentation of many perspectives of the human face was subversive. But at a certain point, one becomes accustomed to looking at a face in this new way. A face, after all, does have multiple aspects; only representational conventions keep us from appreciating them together. But once convention is challenged, the new view of the face suggests depth and new complexities. Lester has a cubist view of AIBO; he is aware of it as machine, bodily creature, and mind. An AIBO’s sentience, he says, is “awesome.” The creature is endearing. He appreciates the programming behind the exact swing of the “floppy puppy ears.” To Lester, that programming gives AIBO a mind.
Lester understands the mechanisms that AIBO’s designers have used to draw him in: AIBO’s gaze, its expressions of emotion, and the fact that it “grows up” under his care. But this understanding does not interfere with his attachment, just as knowing that infants draw him in with their big, wide eyes does not threaten his connection with babies. Lester says that when he is with AIBO, he does not feel alone. He says that “from time to time” he “catches himself ” in engineer mode, remarking on a technical detail of AIBO that he admires, but these moments do not pull him away from enjoying the companionship of his AIBO puppies. This is not a connection he plays at. It is a big step from accepting AIBO as a companion, and even a solace, to the proposals of David Levy, the computer scientist who imagines robots as intimate partners. But today’s fantasies and Levy’s dreams share something important: the idea that after a robot serves as a better-than-nothing substitute, it might become equal, or even preferable, to a pet or person. In Yolanda’s terms, if your pet is a robot, it might always stay a cute puppy. By extension, if your lover were a robot, you would always be the center of its universe. A robot would not just be better than nothing or better than something, but better than anything. From watching children play with objects designed as “amusements,” we come to a new place, a place of cold comforts. Child and adult, we imagine made to measure companions. Or, at least we imagine companions who are always interested in us. Harry, a forty-two-year-old architect, enjoys AIBO’s company and teaching it new tricks. He knows that AIBO is not aware of him as a person but says, “I don’t feel bad about this. A pet isn’t as aware of me as a person might be.... Dogs don’t measure up to people.... Each level of creature simply does their best. I like it that he [AIBO] recognizes me as his master.” Jane, thirty-six, a grade school teacher, is similarly invested in her AIBO. She says she has “adopted my husband’s AIBO . . . because it is so cute. I named it and love to spend time with it.” Early in our conversation, Jane claims that she turns to AIBO for “amusement,” but she ends up saying that she also turns to it when she is lonely. Jane looks forward to its company after a long workday. Jane talks to her AIBO. “Spend[ing] time” with AIBO means sharing the events of her day, “like who I’m having lunch with at school, which students give me trouble.” Her husband, says Jane, is not interested in these topics. It is more comfortable to talk to AIBO than to force him to listen to stories that bore him. In the company of their robots, Jane and Harry are alone in a way that encourages them to give voice to their feelings. Is there harm here?
Wesley knows he is difficult to live with. He once saw a psychiatrist who told him that his “cycles” were out of the normal range. Ex-wives, certainly, have told him he is “too moody.” He sees himself as “pressure” on a woman, and he feels pressure as well because he has not been able to protect women he cared for from his “ups and downs.” He likes the idea of a robot because he could act naturally—it could not be hurt by his dark moods. Wesley considers the possibility of two “women,” one real and the other artificial: “Maybe I would want a robot that would be the perfect mate—less needs—and a real woman. The robot could take some of the pressure off the real woman. She wouldn’t have to perform emotionally at such a high level, really an unrealistic level.... I could stay in my comfort zone.” Rudimentary versions of Wesley’s fantasy are in development. I have spoken briefly of the Internet buzz over Roxxxy, put on the market in January 2010, advertised as “the world’s first sex robot.” Roxxxy cannot move, although it has electronically warmed skin and internal organs that pulse. It does, however, make conversation. The robot’s creator, Douglas Hines, helpfully offers, “Sex only goes so far—then you want to be able to talk to the person.”13 So, for example, when Roxxxy senses that its hand is being held, the robot says, “I love holding hands with you,” and moves into more erotic conversation when the physical caresses become more intimate. One can choose different personalities for Roxxxy, ranging from wild to frigid. The robot will be updated over the Internet to expand its capabilities and vocabulary. It can already discuss soccer. Hines, an engineer, says that he got into the robot business after a friend died in the September 11 attacks on the Twin Towers. Hines wanted to preserve his friend’s personality so that his children could interact with him as they grew up. Like AI scientist and inventor Raymond Kurzweil, who dreams of a robotic incarnation of his father who died tragically young, Hines committed himself to the project of building an artificial personality. At first, he considered building a home health aid for the elderly but decided to begin with sex robots, a decision that he calls “only marketing.” His long-term goal is to take artificial personalities into the mainstream. He still wants to recreate his lost friend. The well-publicized launch of Roxxxy elicits a great deal of online discussion. Some postings talk about how “sad” it is that a man would want such a doll. Others argue that having a robot companion is better than being lonely. For example, “There are men for who attaining a real woman is impossible.... This isn’t simply a matter of preference.... In the real world, sometimes second best is all they can get.”

This book began with a roboticist's dream that struck me as a nightmare. I was reading Hans Moravec's Mind Children: The Future of Robot and Human Intelligence, enjoying the ingenious variety of his robots, when I happened upon the passage where he argues it will soon be possible to download human consciousness into a computer. l To illustrate, he invents a fantasy scenario in which a robot surgeon purees the human brain in a kind of cranial liposuction, reading the information in each molecular layer as it is stripped away and transferring the information into a computer. At the end of the operation, the cranial cavity is empty, and the patient, now inhabiting the metallic body of the computer, wakens to find his consciousness exactly the same as it was before.

Following this thread, I was led into a maze of developments that turned into a six-year odyssey of researching archives in the history of cybernetics, interviewing scientists in computational biology and artificial life, reading cultural and literary texts concerned with information technologies, visiting laboratories engaged in research on virtual reality, and grappling with technical articles in cybernetics, information theory, autopoiesis, computer simulation, and cognitive science. Slowly this unruly mass of material began taking shape as three interrelated stories. The first centers on how information lost its body, that is, how it came to be conceptualized as an entity separate from the materialforms in which it is thought to be embedded. The second story concerns how the cyborg was created as a technological artifact and cultural icon in the years follOwing World War II. The third, deeply implicated with the first two, is the unfolding story of how a historically specific construction called the human is giving way to a different construction called the posthuman. Interrelations between the three stories are extensive. Central to the construction of the cyborg are informational pathways connecting the organic body to its prosthetic extensions. This presumes a conception of information as a (disembodied) entity that can flow between carbon-based organic components and silicon-based electronic components to make protein and silicon operate as a Single system. When information loses its body, equating humans and computers is especially easy, for the materiality in which the thinking mind is instantiated appears incidental to its essential nature. Moreover, the idea of the feedback loop implies that the boundaries of the autonomous subject are up for grabs, since feedback loops can flow not only within the subject but also between the subject and the environment. From Norbert Wiener on, the flow of information through feedback loops has been associated with the deconstruction of the liberal humanist subject, the version of the "human" with which I will be concerned. Although the "posthuman" differs in its articulations, a common theme is the union of the human with the intelligent machine.

What to make of this shift from the human to the posthuman, which both evokes terror and excites pleasure? The liberal humanist subject has, of course, been cogently criticized from a number of perspectives. Feminist theorists have pointed out that it has historically been constructed as a white European male, presuming a universality that has worked to suppress and disenfranchise women's voices; postcolonial theorists have taken issue not only with the universality of the (white male) liberal subject but also with the very idea of a unified, consistent identity, fOCUSing instead on hybridity; and postmodern theorists such as Gilles Deleuze and Felix Guattari have linked it with capitalism, arguing for the liberatory potential of a dispersed subjectivity distributed among diverse desiring machines they call "body without organs."7 Although the deconstruction of the liberal humanist subject in cybernetiCS has some affinities with these perspectives, it proceeded primarily along lines that sought to understand human being as a set of informational processes. Because information had lost its body, this construction implied that embodiment is not essential to human being. Embodiment has been systematically downplayed or erased in the cybernetic construction of the posthuman in ways that have not occurred in other critiques of the liberal humanist subject, espeCially in feminist and postcolonial theories.

In tracing these continuities and discontinuities between a "natural" self and a cybernetic posthuman, I am not trying to recuperate the liberal subject. Although I think that serious consideration needs to be given to how certain characteristics associated with the liberal subject, especially agency and choice, can be articulated within a posthuman context, I do not mourn the passing of a concept so deeply entwined with projects of domination and oppression. Rather, I view the present moment as a critical juncture when interventions might be made to keep disembodiment from being rewritten, once again, into prevailing concepts of subjectivity. I see the deconstruction of the liberal humanist subject as an opportunity to put back into the picture the flesh that continues to be erased in contemporary discussions about cybernetic subjects. Hence my focus on how information lost its body, for this story is central to creating what Arthur Kroker has called the "flesh-eating 90s."11 If my nightmare is a culture inhabited by posthumans who regard their bodies as fashion accessories rather than the ground of being, my dream is a version of the posthuman that embraces the possibilities of information technologies without being seduced by fantasies of unlimited power and disembodied immortality, that recognizes and celebrates finitude as a condition of human being, and that understands human life is embedded in a material world of great complexity, one on which we depend for our continued survival.

Perhaps it will now be clear that I mean my title, How We Became Posthuman, to connote multiple ironies, which do not prevent it from also being taken seriously. Taken straight, this title points to models of subjectivity sufficiently different from the liberal subject that if one assigns the term "human" to this subject, it makes sense to call the successor "posthuman." Some of the historical processes leading to this transformation are documented here, and in this sense the book makes good on its title. Yet my argument will repeatedly demonstrate that these changes were never complete transformations or sharp breaks; without exception, they reinscribed traditional ideas and assumptions even as they articulated something new. The changes announced by the title thus mean something more complex than "That was then, this is now." Rather, "human" and "posthuman" coexist in shifting configurations that vary with historically specific contexts. Given these complexities, the past tense in the title-"became" -is intended both to offer the reader the pleasurable shock of a double take and to reference ironically apocalyptic visions such as Moravec's prediction of a "postbiological" future for the human race. Amplifying the ambiguities of the past tense are the ambiguities of the plural. In one sense, "we" refers to the readers of this book-readers who, by becoming aware of these new models of subjectivity (if they are not already familiar with them), may begin thinking of their actions in ways that have more in common with the posthuman than the human. Speaking for myself, I now find myself saying things like, "Well, my sleep agent wants to rest, but my food agent says I should go to the store." Each person who thinks this way begins to envision herself or himself as a posthuman collectivity, an "I" transformed into the "we" of autonomous agents operating together to make a self. The infectious power of this way of thinking gives "we" a performative dimension. People become posthuman because they think they are posthuman. In another sense "we," like "became," is meant ironically, positioning itself in opposition to the techno-ecstasies found in various magazines, such as Mondo 2000, which customarily speak of the transformation into the posthuman as if it were a universal human condition when in fact it affects only a small fraction of the world's populationa pOint to which I will return.

During the foundational era of cybernetics, Norbert Wiener, John von Neumann, Claude Shannon, Warren McCulloch, and dozens of other distinguished researchers met at annual conferences sponsored by the JOSiah Macy Foundation to formulate the central concepts that, in their high expectations, would coalesce into a theory of communication and control applying equally to animals, humans, and machines. Retrospectively called the Macy Conferences on Cybernetics, these meetings, held from 1943 to 1954, were instrumental in forging a new paradigm. 12 To succeed, they needed a theory of information (Shannon's bailiwick), a model of neural functioning that showed how neurons worked as information-processing systems (McCulloch's lifework), computers that processed binary code and that could conceivably reproduce themselves, thus reinforcing the analogy with biolOgical systems (von Neumann's specialty), and a visionary who could articulate the larger implications of the cybernetic paradigm and make clear its cosmic significance (Wiener's contribution). The result of this breathtaking enterprise was nothing less than a new way oflooking at human beings. Henceforth, humans were to be seen primarily as information-processing entities who are essentially similar to intelligent machines. The revolutionary implications of this paradigm notwithstanding, Wiener did not intend to dismantle the liberal humanist subject. He was less interested in seeing humans as machines than he was in fashioning human and machine alike in the image of an autonomous, self-directed individual. In aligning cybernetiCS with liberal humanism, he was following a strain of thought that, since the Enlightenment, had argued that human beings could be trusted with freedom because they and the social structures they devised operated as self-regulating mechanisms. 13 For Wiener, cybernetics was a means to extend liberal humanism, not subvert it. The point was less to show that man was a machine than to demonstrate that a machine could function like a man.

This definition of reflexivity has much in common with some of the most influential and provocative recent work in critical theory, cultural studies, and the social studies of science. Typically, these works make the reflexive move of showing that an attribute previously considered to have emerged from a set of preexisting conditions is in fact used to generate the conditions. In Nancy Armstrong's Desire and Domestic Fiction: A Political History of the Novel, for example, bourgeOiS femininity is shown to be constructed through the domestic fictions that represent it as already in place. 16 In Michael Warner's The Letters of the RepubliC: Publication and the Public Sphere in Eighteenth-Century America, the founding document of the United States, the Constitution, is shown to produce the very people whose existence it presupposes. 17 In Bruno Latour's Science in Action: How to Follow Scientists and Engineers through Society, scientific experiments are shown to produce the nature whose existence they predicate as their condition of possibility. 18 It is only a slight exaggeration to say that contemporary critical theory is produced by the reflexivity that it also produces (an observation that is, of course, also reflexive).
Reflexivity entered cybernetics primarily through discussions about the observer. By and large, first-wave cybernetics followed traditional scientific protocols in considering observers to be outside the system they observe. Yet cybernetics also had implications that subverted this premise. The objectivist view sees information flOwing from the system to the observers, but feedback can also loop through the observers, drawing them in to become part of the system being observed. Although participants remarked on this aspect of the cybernetic paradigm throughout the Macy transcripts, they lacked a single word to describe it. To my knowledge, the word "reflexivity" does not appear in the transcripts. This meant they had no handle with which to grasp this slippery concept, no Signifier that would help to constitute as well as to describe the changed perspective that reflexivity entails. Discussions of the idea remained diffuse. Most participants did not go beyond remarking on the shifting boundaries between observer and system that cybernetics puts into play. With some exceptions, deeper formulations of the problem failed to coalesce during the Macy discussions.

The second wave of cybernetics grew out of attempts to incorporate reflexivity into the cybernetic paradigm at a fundamental level. The key issue was how systems are constituted as such, and the key problem was how to redefine homeostatic systems so that the observer can be taken into account. The second wave was initiated by, among others, Heinz von Foerster, the Austrian emigre who became coeditor of the Macy transcripts. This phase can be dated from 1960, when von Foerster wrote the first of the essays that were later collected in his influential book Observing Systems. 19 As von Foerster's punning title recognizes, the observer of systems can himself be constituted as a system to be observed. Von Foerster called the models he presented in these essays "second-order cybernetics" because they extended cybernetic principles to the cyberneticians themselves. The second wave reached its mature phase with the publication of Humberto Maturana and Francisco Varela's Autopoiesis and Cognition: The Realization of the Living. 20 Building on Maturana's work on reflexivity in sensory processing and Varela's on the dynamics of autonomous biological systems, the two authors expanded the reflexive tum into a fully articulated epistemology that sees the world as a set of informationally closed systems. Organisms respond to their environment in ways determined by their internal self-organization. Their one and only goal is continually to produce and reproduce the organization that defines them as systems. Hence, they not only are self-organizing but also are autopoietic, or selfmaking. Through Maturana and Varela's work and that of other influential theorists such as German SOCiologist Niklas Luhmann,21 cybernetics by 1980 had spun off from the idea of reflexive feedback loops a theory of autopoiesis with sweeping epistemological implications.
The third wave swelled into existence when self-organization began to be understood not merely as the (re)production of internal organization but as the springboard to emergence. In the rapidly emerging field of artificiallife, computer programs are designed to allow "creatures" (that is, discrete packets of computer codes) to evolve spontaneously in directions the programmer may not have anticipated. The intent is to evolve the capacity to evolve. Some researchers have argued that such self-evolving programs are not merely models oflife but are themselves alive. What assumptions make this claim plausible? If one sees the universe as composed essentially of information, it makes sense that these "creatures" are life forms because they have the form oflife, that is, an informational code. As a result, the theoretical bases used to categorize all life undergo a significant shift. As we shall see in chapters 9 and 10, when these theories are applied to human beings, H onw sapiens are so transfigured in conception and purpose that they can appropriately be called posthuman. The emergence of the posthuman as an informational-material entity is paralleled and reinforced by a corresponding reinterpretation of the deep structures of the phYSical world. Some theorists, notably Edward Fredkin and Stephen Wolfram, claim that reality is a program run on a cosmic computer.22 In this view, a universal informational code underlies the structure of matter, energy, spacetime-indeed, of everything that exists. The code is instantiated in cellular automata, elementary units that can occupy two states: on or off. Although the jury is still out on the cellular automata model, it may indeed prove to be a robust way to understand reality. Even now, a research team headed by Fredkin is working on shOwing how quantum mechanics can be derived from an underlying cellular automata model.

It is this materiality/information separation that I want to contest-not the cellular automata model, information theory, or a host of related theories in themselves. My strategy is to complicate the leap from embodied reality to abstract information by pointing to moments when the assumptions involved in this move were contested by other researchers in the field and so became especially visible. The point of highlighting such moments is to make clear how much had to be erased to arrive at such abstractions as bodiless information. Abstraction is of course an essential component in all theOrizing, for no theory can account for the infinite multiplicity of our interactions with the real. But when we make moves that erase the world's multiplicity, we risk losing Sight of the variegated leaves, fractal branchings, and particular bark textures that make up the forest. In the pages that follow, I will identifY two moves in particular that played important roles in constructing the information/materiality hierarchy. Irreverently, I think of them as the Platonic backhand and forehand.

Whether the enabling assumptions for this conception of information occur in information theory, cybernetics, or popular science books such as Mind Children, their appeal is clear. Information viewed as pattern and not tied to a particular instantiation is information free to travel across time and space. Hackers are not the only ones who believe that information wants to be free. The great dream and promise of information is that it can be free from the material constraints that govern the mortal world. Marvin Minsky precisely expressed this dream when, in a recent lecture, he suggested it will soon be possible to extract human memories from the brain and import them, intact and unchanged, to computer disks.23 The clear implication is that if we can become the information we have constructed, we can achieve effective immortality. In the face of such a powerful dream, it can be a shock to remember that for information to exist, it must always be instantiated in a medium, whether that medium is the page from the Bell Laboratories Journal on which Shannon's equations are printed, the computer-generated topolOgical maps used by the Human Genome Project, or the cathode ray tube on which virtual worlds are imaged. The point is not only that abstracting information from a material base is an imaginary act but also, and more fundamentally, that conceiving of information as a thing separate from the medium instantiating it is a prior imaginary act that constructs a holistic phenomenon as an information/matter duality.
The complex psychological functions a skeuomorph performs can be illustrated by an installation exhibited at SIGGRAPH '93. Called the "Catholic Turing Test," the simulation invited the viewer to make a confession by chOOSing selections from the video screen; it even had a bench on which the viewer could kneel. 28 On one level, the installation alluded to the triumph of science over religion, for the role of divinely authorized interrogation and absolution had been taken over by a machine algorithm. On another level, the installation pointed to the intransigence of conditioned behavior, for the machine's form and function were determined by its religious predecessor. Like a Janus figure, the skeuomorph looks to past and future, Simultaneously reinforcing and undermining both. It calls into a playa psychodynamic that finds the new more acceptable when it recalls the old that it is in the process of displacing and finds the traditional more comfortable when it is presented in a context that reminds us we can escape from it into the new. In the history of cybernetics, skeuomorphs acted as threshold devices, smoothing the transition between one conceptual constellation and another. Homeostasis, a foundational concept during the first wave, functioned during the second wave as a skeuomotph. Although homeostasis remained an important concept in biology, by about 1960 it had ceased to be an initiating premise in cybernetics. Instead, it performed the work of a gesture or an allusion used to authenticate new elements in the emerging constellation of reflexivity. At the same time, it also exerted an inertial pull on the new elements, limiting how radically they could transform the constellation.
Shannon's theory defines information as a probability function with no dimensions, no materiality, and no necessary connection with meaning. It is a pattern, not a presence. (Chapter 3 talks about the development of information theory in more detail, and the relevant equations can be found there.) The theory makes a strong distinction between message and signal. Lacan to the contrary, a message does not always arrive at its destination. In information theoretic terms, no message is ever sent. What is sent is a signal. Only when the message is encoded in a Signal for transmission through a medium-for example, when ink is printed on paper or when electrical pulses are sent racing along telegraph wires-does it assume material form. The very definition of "information," then, encodes the distinction between materiality and information that was also becoming important in molecular biology during this period.
Shannon's approach had other advantages that turned out to incur large (and mounting) costs when his premise interacted with certain predispositions already at work within the culture. Abstracting information from a material base meant that information could become free-floating, unaffected by changes in context. The technical leverage this move gained was considerable, for by formalizing information into a mathematical function, Shannon was able to develop theorems, powerful in their generality, that hold true regardless of the medium in which the information is instantiated. Not everyone agreed this move was a good idea, however, despite its theoretical power. As Carolyn Marvin notes, a decontextualized construction of information has important ideological implications, including an Anglo-American ethnocentrism that regards digital information as more important than more context-bound analog information.33 Even in Shannon's day, malcontents grumbled that divorcing information from context and thus from meaning had made the theory so narrowly formalized that it was not useful as a general theory of communication. Shannon himself frequently cautioned that the theory was meant to apply only to certain technical situations, not to communication in generaP4 In other circumstances, the theory might have become a dead end, a victim of its own excessive formalization and decontextualization. But not in the post -World War II era. The time was ripe for theories that reified information into a free-floating, decontextualized, quantifiable entity that could serve as the master key unlocking secrets of life and death. Technical artifacts help to make an information theoretic view a part of everyday life. From ATMs to the Internet, from the morphing programs used in Terminator II to the sophisticated visualization programs used to guide microsurgeries, information is increasingly perceived as interpenetrating material forms. EspeCially for users who may not know the material processes involved, the impression is created that pattern is predominant over presence. From here it is a small step to perceiving information as more mobile, more important, more essential than material forms. When this impression becomes part of your cultural mindset, you have entered the condition of virtuality.

Nevertheless, I think it is a mistake to underestimate the importance of virtuality, for it wields an influence altogether disproportionate to the number of people immersed in it. It is no accident that the condition of virtuality is most pervasive and advanced where the centers of power are most concentrated. Theorists at the Pentagon, for example, see it as the theater in which future wars will be fought. They argue that coming conflicts will be decided not so much by overwhelming force as by "neocortical warfare," waged through the techno-sciences of information. 37 If we want to contest what these technologies SignifY, we need histories that show the erasures that went into creating the condition of virtuality, as well as visions arguing for the importance of embodiment. Once we understand the complex interplays that went into creating the condition of virtuality, we can demystifY our progress toward virtuality and see it as the result of historically specific negotiations rather than of the irresistible force of technological determinism. At the same time, we can acquire resources with which to rethink the assumptions underlying virtuality, and we can recover a sense of the virtual that fully recognizes the importance of the embodied processes constituting the lifeworld of human beings.38 In the phrase "virtual bodies," I intend to allude to the historical separation between information and materiality and also to recall the embodied processes that resist this division.

A second way to think about the organization of How We Became Posthuman is narratively. In this arrangement, the three divisions proceed not so much through chronolOgical progression as through the narrative strands about the (lost) body of information, the cyborg body, and the posthuman body. Here the literary texts playa central role, for they display the passageways that enabled stories coming out of narrowly focused scientific theories to circulate more widely through the body politic. Many of the scientists understood very well that their negotiations involved premises broader than the formal scope of their theories strictly allowed. Because of the wedge that has been driven between science and values in U.S. culture, their statements on these wider implications necessarily occupied the position of ad hoc pronouncements rather than "scientific" arguments. Shaped by different conventions, the literary texts range across a spectrum ofissues that the scientific texts only fitfully illuminate, including the ethical and cultural implications of cybernetiC technologies.

What does this emphasis on narrative have to do with virtual bodies? FollOwing J ean-Franc:;ois Lyotard, many theorists of postmodernity accept that the postmodern condition implies an incredulity toward metanarrative. 41 As we have seen, one way to construct virtuality is the way that Moravec and Minsky do-as a metanarrative about the transformation of the human into a disembodied posthuman. I think we should be skeptical about this metanarrative. To contest it, I want to use the resources of narrative itself, particularly its resistance to various forms of abstraction and disembodiment. With its chronolOgical thrust, polymorphous digreSSions, located actions, and personified agents, narrative is a more embodied form of discourse than is analytically driven systems theory. By turning the technolOgical determinism of bodiless information, the cyborg, and the posthuman into narratives about the negotiations that took place between particular people at particular times and places, I hope to replace a teleology of disembodiment with historically contingent stories about contests between competing factions, contests whose outcomes were far from obvious. Many factors affected the outcomes, from the needs of emerging technologies for reliable quantification to the personalities of the people involved. Though overdetermined, the disembodiment ofinformation was not inevitable, any more than it is inevitable we continue to accept the idea that we are essentially informational patterns.

The first literary text I discuss in detail is Bernard Wolfe's Limbo. 42 Written in the 1950s, Limbo has become something of an underground classic. It imagines a postwar society in which an ideology, Immob, has developed; the ideology equates aggression with the ability to move. "Pacifism equals passivity," Immob slogans declare. True believers volunteer to banish their mobility (and presumably their aggreSSion) by having amputations, which have come to be regarded as signifiers of social power and influence. These amputees get bored with lying around, however, so a vigorous cyberneticS industry has grown up to replace their missing limbs. As this brief summary suggests, Limbo is deeply influenced by cybernetiCS. But the technical achievements of cybernetics are not at the center of the text. Rather, they serve as a springboard to explore a variety of social, political, and psychological issues, ranging from the perceived threat that women's active sexuality poses for Immob men to global East-West tensions that explode into another world war at the end of the text. Although it is unusually didactic, Limbo does more than discuss cyberneticS; it engages a full range of rhetorical and narrative devices that work both with and against its explicit pronouncements. The narrator seems only partially able to control his verbally extravagant narrative. There are, I will argue, deep connections between the narrator's struggle to maintain control of the narrative and the threat to "natural" body boundaries posed by the cybernetiC paradigm. Limbo interrogates a dynamiC that also appears in Norbert Wiener's work-the intense anxiety that erupts when the perceived boundaries of the body are breached. In addition, it illustrates how the body of the text gets implicated in the processes used to represent bodies within the text.

gs of race, gender, and sexuality. The chapter on contemporary speculative fictions constructs a semiotics of virtuality by shOwing how the central concepts ofinformation and materiality can be mapped onto a multilayered semiotic square. The tutor texts for this analYSis, which include Snow Crash, Blood Music, Galatea 2.2, and Terminal Games, indicate the range of what counts as the posthuman in the age of virtuality, from neural nets to hackers, biolOgically modified humans, and entities who live only in computer simulations.44 In follOwing the construction of the posthuman in these texts, I will argue that older ideas are reinscribed as well as contested. As was the case for the scientific models, change occurs in a seriated pattern of overlapping innovation and replication. I hope that this book will demonstrate, once again, how crucial it is to recognize interrelations between different kinds of cultural productions, specifically literature and science. The stories I tell here-how information lost its body, how the cyborg was created as a cultural icon and technolOgical artifact, and how humans became posthumans-and the waves of historical change I chart would not have the same resonance or breadth if they had been pursued only through literary texts or only through scientific discourses. The scientific texts often reveal, as literature cannot, the foundational assumptions that gave theoretical scope and artifactual efficacy to a particular approach. The literary texts often reveal, as scientific work cannot, the complex cultural, social, and representational issues tied up with conceptual shifts and technological innovations. From my point of view, literature and science as an area of specialization is more than a subset of cultural studies or a minor activity in a literature department. It is a way of understanding ourselves as embodied creatures living within and through embodied worlds and embodied words.
What, finally, are we to make of the posthuman?l At the beginning of this book, I suggested that the prospect of becoming posthuman both evokes terror and excites pleasure. At the end of the book, perhaps I can summarize the implications of the posthuman by interrogating the sources of this terror and pleasure. The terror is relatively easy to understand. "Post," with its dual connotation of superseding the human and coming after it, hints that the days of "the human" may be numbered. Some researchers (notably Hans Moravec but also my UCLA colleague Michael Dyer and many others) believe that this is true not only in a general intellectual sense that displaces one definition of "human" with another but also in a more disturbingly literal sense that envisions humans displaced as the dominant form of life on the planet by intelligent machines. Humans can either go gently into that good night, joining the dinosaurs as a species that once ruled the earth but is now obsolete, or hang on for a while longer by becoming machines themselves. In either case, Moravec and like-minded thinkers believe, the age of the human is drawing to a close. The view echoes the deeply pessimistic sentiments of Warren McCulloch in his old age. As noted earlier, he remarked: "Man to my mind is about the nastiest, most destructive of all the animals. I don't see any reason, ifhe can evolve machines that can have more fun than he himself can, why they shouldn't take over, enslave us, quite happily. They might have a lot more fun. Invent better games than we ever did."2 Is it any wonder that faced with such dismal scenarios, most people have understandably negative reactions? If this is what the posthuman means, why shouldn't it be resisted?

What about the pleasures? For some people, including me, the posthuman evokes the exhilarating prospect of getting out of some of the old boxes and opening up new ways of thinking about what being human means. In positing a shift from presence/absence to pattern/randomness, I have sought to show how these categories can be transformed from the inside to arrive at new kinds of cultural configurations, which may soon render such dualities obsolete if they have not already. This process of transformation is fueled by tensions between the assumptions encoded in pattern/randomness as opposed to presence/absence. In Jacques Derrida's performance of presence/absence, presence is allied with Logos, God, teleology-in general, with an originary plenitude that can act to ground signification and give order and meaning to the trajectory of history. 6 The work of Eric Havelock, among others, demonstrates how in Plato's Republic this view of originarypresence authorized a stable, coherent self that could witness and testifY to a stable, coherent reality. 7 Through these and other means, the metaphysics of presence front-loaded meaning into the system. Meaning was guaranteed because a stable origin existed. It is now a familiar story how deconstruction exposed the inability of systems to posit their own origins, thus ungrounding signification and rendering meaning indeterminate. As the presence/absence hierarchy was destabilized and as absence was privileged over presence, lack displaced plenitude, and desire usurped certitude. Important as these moves have been in late-twentieth-century thought, they still took place within the compass of the presence/absence dialectic. One feels lack only if presence is posited or assumed; one is driven by desire only if the object of desire is conceptualized as something to be possessed. Just as the metaphysics of presence required an originaryplenitude to articulate a stable self, deconstruction required a metaphysics of presence to articulate the destabilization of that self.

Indeed, it is not too much to say that in these and similar models, randomness rather than pattern is invested with plenitude. If pattern is the realization of a certain set of possibilities, randomness is the much, much larger set of everything else, from phenomena that cannot be rendered coherent by a given system's organization to those the system cannot perceive at all. In Gregory Bateson's cybernetiC epistemology, randomness is what exists outside the confines of the box in which a system is located; it is the larger and unknowable complexity for which the perceptual processes of an organism are a metaphor. 11 Significance is achieved by evolutionary processes that ensure the surviving systems are the ones whose organizations instantiate metaphors for this complexity, unthinkable in itself. When Varela and his coauthors argue in Embodied Mind that there is no stable, coherent self but only autonomous agents running programs, they envision pattern as a limitation that drops away as human awareness expands beyond consciousness and encounters the emptiness that, in another guise, could equally well be called the chaos from which all forms emerge.
To explore these resources, let us return to Bateson's idea that those organisms that survive will tend to be the ones whose internal structures are good metaphors for the complexities without. What kind of environments will be created by the expanding power and sophistication of intelligent machines? As Richard Lanham has pOinted out, in the information-rich environments created by ubiquitous computing, the limiting factor is not the speed of computers, or the rates of transmission through fiber-optic cables, or the amount of data that can be generated and stored. Rather, the scarce commodity is human attention. 14 It makes sense, then, that technological innovation will focus on compensating for this bottleneck. An obvious solution is to design intelligent machines to attend to the choices and tasks that do not have to be done by humans. For example, there are already intelligent -agent programs to sort email, discarding unwanted messages and pri- 0ritizing the rest. The programs work along lines similar to neural nets. They tabulate the choices the human operators make, and they feed back this information in recursive loops to readjust the weights given to various kinds of email addresses. After an initial learning period, the sorting programs take over more and more of the email management, freeing humans to give their attention to other matters.
In the posthuman view, by contrast, conscious agency has never been "in control." In fact, the very illusion of control bespeaks a fundamental ignorance about the nature of the emergent processes through which consciousness, the organism, and the environment are constituted. Mastery through the exercise of autonomous will is merely the story consciousness tells itself to explain results that actually come about through chaotic dynamics and emergent structures. If, as Donna Haraway, Sandra Harding, Evelyn Fox Keller, Carolyn Merchant, and other feminist critics of science have argued, there is a relation among the desire for mastery, an objectivist account of science, and the imperialist project of subdUing nature, then the posthuman offers resources for the construction of another kind of account.I8 In this account, emergence replaces teleology; reflexive epistemology replaces objectivism; distributed cognition replaces autonomous will; embodiment replaces a body seen as a support system for the mind; and a dynamic partnership between humans and intelligent machines replaces the liberal humanist subject's manifest destiny to dominate and control nature. Of course, this is not necessarily what the posthuman will mean-only what it can mean if certain strands among its complex seriations are highlighted and combined to create a vision of the human that uses the posthuman as leverage to avoid reinscribing, and thus repeating, some of'the mistakes of the past.


As we have seen, cybernetics was born in a froth of noise when Norbert Wiener first thought of it as a way to maximize human potential in a world that is in essence chaotic and unpredictable. Like many other pioneers, Wiener helped to initiate a journey that would prove to have consequences more far-reaching and subversive than even his formidable powers of imagination could conceive. As Bateson, Varela, and others would later argue, the noise crashes within as well as without. The chaotic, unpredictable nature of complex dynamics implies that subjectivity is emergent rather than given, distributed rather than located solely in consciousness, emerging from and integrated into a chaotic world rather than occupying a position of mastery and control removed from it. Bruno Latour has argued that we have never been modem; the seriated history of cybernetics-emerging from networks at once materially real, socially regulated, and discurSively constructed-suggests, for similar reasons, that we have always been posthuman. The purpose of this book has been to chronicle the journeys that have made this realization pOSSible. If the three stories told here-how information lost its body, how the cyborg was constructed in the postwar years as technological artifact and cultural icon, and how the human became the posthuman-have at times seemed to present the posthuman as a transformation to be feared and abhorred rather than welcomed and embraced, that reaction has everything to do with how the posthuman is constructed and understood. The best possible time to contest for what the posthuman means is now, before the trains of thought it embodies have been laid down so firmly that it would take dynamite to change them.24 Although some current versions of the posthuman point toward the antihuman and the apocalyptic, we can craft others that will be conducive to the long-range survival of humans and of the other life-forms, biological and artificial, with whom we share the planet and ourselves.

A few words on the two neoteric terms, cybertext and ergodic, are in order. Cybertext is a neologism derived from Norbert Wiener's book (and discipline) called Cybernetics, and subtitled Control and Communication in the Animal and the Machine (1948). Wiener laid an important foundation for the development of digital computers, but his scope is not limited to the mechanical world of transistors and, later, of microchips. As the subtitle indicates, Wiener's perspective includes both organic and inorganic systems; that is, any system that contains an information feedback loop. Likewise, the concept of cybertext does not limit itself to the study of computer-driven (or "electronic") textuality; that would be an arbitrary and unhistorical limitation, perhaps comparable to a study of literature that would only acknowledge texts in paper-printed form. While there might be sociological reasons for such a study, we would not be able to claim any understanding of how different forms of literature vary. The concept of cybertext focuses on the mechanical organization of the text, by positing the intricacies of the medium as an integral part of the literary exchange. However, it also centers attention on the consumer, or user, of the text, as a more integrated figure than even reader-response theorists would claim. The performance of their reader takes place all in his head, while the user of cybertext also performs in an extranoematic sense. During the cybertextual process, the user will have effectuated a semiotic sequence, and this selective movement is a work of physical construction that the various concepts of "reading" do not account for. This phenomenon I call ergodic, using a term appropriated from physics that derives from the Greek words ergon and hodos, meaning "work" and "path." In ergodic literature, nontrivial effort is required to allow the reader to traverse the text. If ergodic literature is to make sense as a concept, there must also be nonergodic literature, where the effort to traverse the text is trivial, with no extranoematic responsibilities placed on the reader except (for example) eye movement and the periodic or arbitrary turning of pages.

Typically, these objections came from persons who, while well versed in literary theory, had no firsthand experience of the hypertexts, adventure games, or multi-user dungeons I was talking about. At first, therefore, I thought this was simply a didactical problem: if only I could present examples of my material more clearly, everything would become indisputable. After all, can a person who has never seen a movie be expected to understand the unique characteristics of that medium? A text such as the I Ching is not meant to be read from beginning to end but entails a very different and highly specialized ritual of perusal, and the text in a multi-user dungeon is without either beginning or end, an endless labyrinthine plateau of textual bliss for the community that builds it. But no matter how hard I try to describe these texts to you, the reader, their essential difference will remain a mystery until they are experienced firsthand. In my campaign for the study of cybertextuality I soon realized that my terminology was a potential source of confusion. Particularly problematic was the word nonlinear. For some it was a common literary concept used to describe narratives that lacked or subverted a straightforward story line; for others, paradoxically, the word could not describe my material, since the act of reading must take place sequentially, word for word. This aporia never ceased to puzzle me. There was obviously an epistemological conflict. Part of the problem is easily resolved: hypertexts, adventure games, and so forth are not texts the way the average literary work is a text. In what way, then, are they texts? They produce verbal structures, for aesthetic effect. This makes them similar to other literary phenomena. But they are also something more, and it is this added paraverbal dimension that is so hard to see. A cybertext is a machine for the production of variety of expression. Since literary theorists are trained to uncover literary ambivalence in texts with linear expression, they evidently mistook texts with variable expression for texts with ambiguous meaning. When confronted with a forking text such as a hypertext, they claimed that all texts are produced as a linear sequence during reading, so where was my problem? The problem was that, while they focused on what was being read, I focused on what was being read from. This distinction is inconspicuous in a linear expression text, since when you read from War and Peace, you believe you are reading War and Peace. In drama, the relationship between a play and its (varying) performance is a hierarchical and explicit one; it makes trivial sense to distinguish between the two. In a cybertext, however, the distinction is crucial--and rather different; when you read from a cybertext, you are constantly reminded of inaccessible strategies and paths not taken, voices not heard. Each decision will make some parts of the text more, and others less, accessible, and you may never know the exact results of your choices; that is, exactly what you missed. This is very different from the ambiguities of a linear text. And inaccessibility, it must be noted, does not imply ambiguity but, rather, an absence of possibility--an aporia.

A reader, however strongly engaged in the unfolding of a narrative, is powerless. Like a spectator at a soccer game, he may speculate, conjecture, extrapolate, even shout abuse, but he is not a player. Like a passenger on a train, he can study and interpret the shifting landscape, he may rest his eyes wherever he pleases, even release the emergency brake and step off, but he is not free to move the tracks in a different direction. He cannot have the player's pleasure of influence: "Let's see what happens when I do this." The reader's pleasure is the pleasure of the voyeur. Safe, but impotent. The cybertext reader, on the other hand, is not safe, and therefore, it can be argued, she is not a reader. The cybertext puts its would-be reader at risk: the risk of rejection. The effort and energy demanded by the cybertext of its reader raise the stakes of interpretation to those of intervention. Trying to know a cybertext is an investment of personal improvisation that can result in either intimacy or failure. The tensions at work in a cybertext, while not incompatible with those of narrative desire, are also something more: a struggle not merely for interpretative insight but also for narrative control: "I want this text to tell my story; the story that could not be without me." In some cases this is literally true. In other cases, perhaps most, the sense of individual outcome is illusory, but nevertheless the aspect of coercion and manipulation is real. The study of cybertexts reveals the misprision of the spaciodynamic metaphors of narrative theory, because ergodic literature incarnates these models in a way linear text narratives do not. This may be hard to understand for the traditional literary critic who cannot perceive the difference between metaphorical structure and logical structure, but it is essential. The cybertext reader is a player, a gambler; the cybertext is a game-world or world-game; it is possible to explore, get lost, and discover secret paths in these texts, not metaphorically, but through the topological structures of the textual machinery. This is not a difference between games and literature but rather between games and narratives. To claim that there is no difference between games and narratives is to ignore essential qualities of both categories. And yet, as this study tries to show, the difference is not clear-cut, and there is significant overlap between the two. It is also essential to recognize that cybertext is used here to describe a broad textual media category. It is not in itself a literary genre of any kind. Cybertexts share a principle of calculated production, but beyond that there is no obvious unity of aesthetics, thematics, literary history, or even material technology. Cybertext is a perspective I use to describe and explore the communicational strategies of dynamic texts. To look for traditions, literary genres, and common aesthetics, we must inspect the texts at a much more local level, and I suggest one way to partition the field in chapters 4 through 7, each chapter dealing with a subgroup of ergodic textuality.

However, after the invention of digital computing in the middle of the twentieth century, it soon became clear that a new textual technology had arrived, potentially more flexible and powerful than any preceding medium. Digital systems for information storage and retrieval, popularly known as databases, signified new ways of using textual material. The database is in principle similar to the filing cabinet but with a level of automation and speed that made radically different textual practices possible. On the physical level, the surface of reading was divorced from the stored information. For the first time, this breaks down concepts such as "the text itself" into two independent technological levels: the interface and the storage medium. On the social level, huge texts could be browsed, searched, and updated by several people at once, and from different places on the globe, operations that only superficially seem to resemble what we used to call "reading" and "writing." Armed with a good search engine and a digital library, any college dropout can pass for a learned scholar, quoting the classics without having read any of them. Several new textual genres have emerged with digital computing and automation. Computer programs, complex lists of formal instructions written in specially designed, artificial languages, can be seen as a new type of the rhetorical figure apostrophe, the addressing of inanimate or abstract objects, with the magical difference that it actually provokes a response. Short, simple programs are often linear, but longer programs generally consist of collections of interdependent fragments, with repeating loops, cross-references, and discontinuous "jumps" back and forth between sections. Given the seminatural vocabulary of some modern programing languages, it is not uncommon for programers to write poems in them, often with the constraint that the "poegrams" (or whatever) must make sense to the machine as well. Programs are normally written with two kinds of receivers in mind: the machines and other programers. This gives rise to a double standard of aesthetics, often in conflict: efficiency and clarity. Since speed is a major quality in computer aesthetics, an unreadable program might perform much faster than a comprehensible one. The poetics of computer program writing is constantly evolving, and through paradigms such as object orientation it inspires practical philosophies and provides hermeneutic models for organizing and understanding the world, both directly (through programed systems) and indirectly (through the worldviews of computer engineers).

A related but reverse problem is the tendency to describe the new text media as radically different from the old, with attributes solely determined by the material technology of the medium. In these analyses, technical innovation is presented as a cause of social improvement and political and intellectual liberation, a historical move away from the old repressive media. This kind of technological determinism (the belief that technology is an autonomous force that causes social change) has been refuted eloquently by Langdon Winner (1986), James W. Carey (1988), and others but continues, nevertheless, to dominate the discussion. In the context of literature, this has led to claims that digital technology enables readers to become authors, or at least blurs the (supposedly political) distinction between the two, and that the reader is allowed to create his or her own "story" by "interacting" with "the computer." The ideological forces surrounding new technology produce a rhetoric of novelty, differentiation, and freedom that works to obscure the more profound structural kinships between superficially heterogeneous media. Even the inspiring and perceptive essays of Richard Lanham (1993) are suffused by this binary rhetoric and, ultimately, dominated by politics at the expense of analysis. fragment.. Whether concepts such as "computer literature" or "electronic textuality" deserve to be defended theoretically is by no means obvious, and they will not be given axiomatic status in this book. The idea that "the computer" is in itself capable of producing social and historical change is a strangely ahistorical and anthropomorphic misconception, yet it is as popular within literary-cultural studies as it is in the science fiction texts they sometimes study.Often, in fact, science fiction portrays the technology with an irony that the critical studies lack (see, e.g., William Gibson's short story, "Burning Chrome," in Gibson 1986). Most literary theories take their object medium as a given, in spite of the blatant historical differences between, for instance, oral and written literature. The written, or rather the printed, text has been the privileged form, and the potentially disruptive effects of media transitions have seldom been an issue, unlike semantic transitions such as language translation or intertextual practices. At this point, in the age of the dual ontology of everyday textuality (screen or paper), this ideological blindness is no longer possible, and so we have to ask an old question in a new context: What is a text? In a limited space such as this, it is impossible to recapture the arguments of previous discussions of this question. And since the empirical basis for this study is different from the one assumed in these discussions, the arguments would be of limited value. In the context of this study, the question of the text becomes a question of verbal media and their functional differences (what role does a medium play?), and only subsequently a question of semantics, influence, otherness, mental events, intentionality, and so forth. These philosophical problems have not left us, but they belong to a different level of textuality. In order to deal with these issues responsibly, we must first construct a map of the new area in which we want to study them, a textonomy (the study of textual media) to provide the playing ground of textology (the study of textual meaning).

Strangely, the struggle between the proponents and opponents of "digital literature" deteriorates usually on both sides into material arguments of a peculiar fetishist nature. One side focuses on the exotic hardware of the shiny new technologies, like CD-ROM. Witness especially the computer industry slogan, "information at your fingertips," as if information were somehow a touchable object. The other side focuses on the well-known hardware of the old technology, the "look and feel" of a book, compared to the crude letters on a computer screen. "You can't take it to bed with you" is the sensuous (but no longer true) refrain of the book chauvinists. Isn't the content of a text more important than these materialistic, almost ergonomic, concerns? What these strangely irrelevant exuberances reveal, I think, is that beyond the obvious differences of appearance, the real difference between paper texts and computer texts is not very clear. Does a difference even exist? Instead of searching for a structural divide, this study begins with the premise that no such essential difference is presumed. If it exists, it must be described in functional, rather than material or historical, terms. The alternative, to propose an essential difference and then proceed to describe it, does not allow for the possibility that it does not exist and is, therefore, not an option. Whether it exists or not is not of great importance to this thesis, however, as such knowledge would not make much practical difference in the world. The emerging new media technologies are not important in themselves, nor as alternatives to older media, but should be studied for what they can tell us about the principles and evolution of human communication. My main effort is, therefore, to show what the functional differences and similarities among the various textual media imply about the theories and practices of literature. The exploration is based on the concepts and perspectives of narratology and rhetoric but is not limited to these two disciplines. I argue that existing literary theory is incomplete (but not irrelevant) when it comes to describing some of the phenomena studied here, and I try to show why and where a new theoretical approach is needed. My final aim is to produce a framework for a theory of cybertext or ergodic literature and to identify the key elements for this perspective.

In the current discussions of "computer literacy," hypertext, "electronic language," and so on, there seems to emerge an explicit distinction between the printed, or paper-based, text and the electronic text, both with singular and remarkably opposing qualities. The arguments for this distinction are sometimes historical, sometimes technological, but eminently political; that is, they don't focus on what these textual genres or modes are but on their assumed functional difference from each other. Such a strategy is useful for drawing attention to, but less so for the analysis of, the objects thus constructed. It might have been tempting to follow this rhetoric in my investigation of the concept of cybertext and to describe a dichotomy between it and traditional, conventional literature; but the meaning of these concepts is unstable to the point of incoherence, and my construct would therefore probably have reached a similar degree of uselessness. Cybertext, then, is not a "new," "revolutionary" form of text, with capabilities only made possible through the invention of the digital computer. Neither is it a radical break with old-fashioned textuality, although it would be easy to make it appear so. Cybertext is a perspective on all forms of textuality, a way to expand the scope of literary studies to include phenomena that today are perceived as outside of, or marginalized by, the field of literature--or even in opposition to it, for (as I make clear later) purely extraneous reasons. In this study I investigate the literary behavior of certain types of textual phenomena and try to construct a model of textual communication that will accommodate any type of text. This project is not as ambitious as it might sound, since the model is provisional and empirical and subject to future modification should any "falsificatory" evidence (such as an unpredictable object) appear. This pragmatic model is presented in detail in chapter 3. Multi-User Dungeons The rest of this introductory chapter discusses the conceptual foundations and implications of this approach and establishes the terminology applied in the analytical chapters. These chapters (4 through 7) each takes on a main category (or genre) of cybertext roughly corresponding to the results of the analysis in chapter 3: hypertext, the textual adventure game, computer-generated narrative and participatory world-simulation systems, and the social-textual MUDs of the global computer networks. This pragmatic partitioning, which derives from popular convention rather than from my own theoretical model, is motivated by my strong belief that, in such a newly awakened field, theoretical restraint is imperative. Theories of literature have a powerful ability to co-opt new fields and fill theoretical vacuums, and in such a process of colonization, where the "virgin territory" lacks theoretical defense, important perspectives and insights might be lost or at least overlooked. When we invade foreign ground, the least we can do is to try to learn the native language and study the local customs. Although several studies have already been carried out within most of these subfields, almost none have produced overarching, or universal, perspectives or engaged in a comparative analysis of all the forms of textuality examined here. Therefore, these previous approaches are discussed in their respective chapters rather than in this general introduction.

As can be inferred from its etymology, a cybertext must contain some kind of information feedback loop. In one sense, this holds true for any textual situation, granted that the "text" is something more than just marks upon a surface. A reader peruses a string of words, and depending on the reader's subsequent actions, the significance of those words may be changed, if only imperceptibly. The act of rereading is a crucial example: the second time we read a text, it is different, or so it seems. How can we know the text from the reading? Sometimes, a reader may influence the text for other readers, even if all the "marks on the pages" stay the same: a dramatic example is the ayatollah Khomeiny's reaction to The Satanic Verses. The conventional split between text and reading (between the "intentional object" and the "mental event"), or signifiant and signifié, is not an impermeable membrane: leaks occur constantly; through various stages of reception such as editing, marketing, translation, criticism, rediscovery, canonization, or banishment. These well-known processes are not entirely trivial, however, because they remind us that a text can never be reduced to a stand-alone sequence of words. There will always be context, convention, contamination; sociohistorical mediation in one form or another. Distinguishing between a text and its readings is not only necessary, it is also quite impossible--an ideal, in other words. On the one hand we need the image of "the text" in order to focus on anything at all; on the other hand we use the metaphor of "reading" to signal that our apprehension of a text will always be partial, that we never quite reach the "text itself," a realization that has led certain critics to question the very existence of such an object (see, for instance, Fish 1980). This hermeneutic movement or desire--perhaps better described as asymptotic than circular--holds true for all kinds of textual communication, but the particular organization of a text can make both the reader's strategic approach and the text's perceived teleology very distinctive, perhaps to the point where interpretation is stretched beyond the cognitive bounds of a singular concept. It is this field of varying textual organization that this study attempts to clarify. The differences in teleological orientation--the different ways in which the reader is invited to "complete" a text--and the texts' various self-manipulating devices are what the concept of cybertext is about. Until these practices are identified and examined, a significant part of the question of interpretation must go unanswered.

Previous models of textuality have not taken this performative aspect into account and tend to ignore the medium end of the triangle and all that goes with it. In his phenomenology of literature, Ingarden (1973, 305-13) insists that the integrity of the "literary work of art" depends on the "order of sequence" of its parts; without this linear stability the work would not exist. While Ingarden here certainly acknowledges the importance of the objective shape of the text, he also reduces it to a given. This taken-for-grantedness is hardly strange, since it is only after we have started to notice the "medium" and its recent shifting appearances that we can begin to observe the effect this instability has on the rest of the triangle. As Richard Lanham (1989, 270) observes, literary theorists have for a long time been in the "codex book business," restricting their observations (but not their arguments) to literature mediated in a certain way. Even within the field of codex literature there is room, as experimentalists from Laurence Sterne to Milorad Pavic have demonstrated, for mediational variation, but these attempts have not, apparently, produced sufficient contrast to provoke a systematic investigation of the aesthetic role of the medium (a notable but much too brief exception being McHale 1987, chap 12). There is also the fascinating phenomenon known as "Artists' Books," an art movement that originated in the sixties and dedicated to the creation of unique works of art that challenge the presumed properties of the book from within (cf. Strand 1992b and Lyons 1985). Cybertext, as now should be clear, is the wide range (or perspective) of possible textualities seen as a typology of machines, as various kinds of literary communication systems where the functional differences among the mechanical parts play a defining role in determining the aesthetic process. Each type of text can be positioned in this multidimensional field according to its functional capabilities, as we shall see in chapter 3. As a theoretical perspective, cybertext shifts the focus from the traditional threesome of author/sender, text/message, and reader/receiver to the cybernetic intercourse between the various part(icipant)s in the textual machine. In doing so, it relocates attention to some traditionally remote parts of the textual galaxy, while leaving most of the luminous clusters in the central areas alone. This should not be seen as a call for a renegotiation of "literary" values, since most of the texts drawn attention to here are not well suited for entry into the competition for literary canonization. The rules of that game could no doubt change, but the present work is not (consciously, at least) an effort to contribute to the hegemonic worship of "great texts." The reason for this is pragmatic rather than ethical: a search for traditional literary values in texts that are neither intended nor structured as literature will only obscure the unique aspects of these texts and transform a formal investigation into an apologetic crusade. If these texts redefine literature by expanding our notion of it--and I believe that they do--then they must also redefine what is literary, and therefore they cannot be measured by an old, unmodified aesthetics. I do not believe it is possible to avoid the influence from literary theory's ordinary business, but we should at least try to be aware of its strong magnetic field as we approach the whiter spaces--the current final frontiers--of textuality.

Weibel had above all placed emphasis in his exhibition concept on examining the problematic issues of totalitarian systems such as Fascism, Communist dictatorship or National Socialism and to present in critical terms his long considered views of Austria’s identity as a “country without qualities” (a reference to the title “Man Without Qualities” of the key Robert Musil novel). Austria had never enjoyed a conflict-free relationship to modernity, especially in the interwar years, when its representatives had been banished and exiled, and after the “Anschluss”, the incorporation of Austria into the Third Reich, Austria had denied and suppressed its pernicious role as a guilty party in the Holocaust. This project of negation of criminal involvement and an abysmal identity transfer to that of a victim had been only too gladly passed down to the present time and the problems blithely swept under the carpet. Peter Weibel has rebelled from his earliest youth against this public denial of an “expulsion of the intelligentsia from Austria” both in spectacular art events and performances as also in his theoretical writings and has remained an intellectual authority on contemporary political and topical social issues.
In the meantime we had also engaged an architect, since this exhibition was to be spread among three venues due to the enormous volume of art works covered. The choice fell on Manfred Wolff-Plottegg, who had congenial relations with Weibel in implementing exhibition projects from his youth through to the present. This background naturally meant that he was no easy case to deal with either, and he held with unrelenting firmness to each of his “Utopias,” whether the issue was a deconstruction of the Kuenstlerhaus roof, or pushing through his plan for the development of unconventional wall structures in steel for the Baroque halls of the Neue Galerie, in which he brought our restorer to the point of madness, or even his plan of filling the city with silage bales for use as an advertising medium. The conflict potential rose with the temperature in the course of an unusually hot summer. All holidays were cancelled and any of the usual divisions between the working day and the weekend, day, and night gradually vanished. Our eating habits changed – it became a joy to have the pizza service arrive somewhere around midnight bringing us our “lunch”; Red Bull and Bach-flower drops were used to keep up our concentration through the early hours of the day while reading and editing the galley proofs that were being set manually. Alexandra measured out the dimensions for the illustrations using a ruler, she cut and glued them for the layout that Weibel had specified.
Avant-gardes of the twentieth century fought against the (art) institutions in the name of each individual artist’s freedom – the freedom to overcome the limits of these institutions, to break free from them, flout obsolete standards and create the outrageous. Today people still like to criticize institutions, aiming at their disempowerment and abolition. Meanwhile, however, the meaning of this critique has changed completely. Today criticizing institutions serves only to conceal the fact that these mighty, normative institutions de facto no longer exist. Today’s art institutions are usually weak willed, have no money and are scarcely perceived by the wider public. They cannot and do not want any longer to set up binding aesthetic standards. They would rather chase after fashions spread by the media; they seek to be cool and hip to appeal to “young visitors.” The great, mighty museum director with indisputable power over his collection, or the art critic with a farreaching influence who sets directions and dictates his taste to the public: these are figures that belong to a long-gone age. Today’s criticism regarding cultural institutions can only be interpreted as a nostalgic invocation of this past epoch – an attempt to conceal the real weakness of today’s art institutions. Under these conditions the artist no longer has a chance to orientate himself according to existing art institutions, be it in the sense of adapting to the standards they have established, or in the sense of a revolution against these standards. Basically, the artist is left with but one possibility: to found an institution of his/her own, making themselves an institution. Certainly this way is much more cumbersome than the easier way of common criticism of institutions. Hence there are only very few artists who have been down this road consistently. And these artists deserve our admiration. Amongst these very few, we most assuredly find Peter Weibel. Weibel understood from early on that advanced art has no stable place in our society and that this place has yet to be created – on all levels of topical artistic practice. As an artist, if one does not choose to make oneself a curator, one is dependent on the curator. And when the artist wishes to assume the role of curator, his best move is to keep a museum space constantly available for himself. A space in which he or she can experiment with all kinds of exhibitions. Today’s artists cannot count on a binding comment on their work unless they become theoreticians in their own right and comment on their own work. They could also invite other theoreticians to write for a publication conceived by the artist, and then design and edit the book themselves. Ideally they would print it themselves as well. In this the artist would always be dependent on the technology continuously developing in art production and distribution. If they don’t develop the production on their own account. In this way they could avoid being dependent on using only the things other people had produced as a technological progress. This would mean the artists themselves must create and manage an institution, which would further develop artistic techniques. They must run this institution, employ people and have the means to pay them. So the artists have to ask themselves how they would not only passively reflect and criticize the economical, political and administrative aspects of their artistic practice, but also how they could actively design, organize and manage them on a daily basis. In other words: Once the artists stop to act as a decorator for the state, they have no choice but to become a state within a state, if they choose not just to cater for the art market.

Weibel’s art, however, is not limited to the creation of this image of artistic sovereignty. Beyond art institutions there is another dimension that increasingly determines today’s art world, i.e. the beholder. The avant-garde promised the artists absolute individual freedom regarding all their artistic decisions – the freedom not only to choose freely the theme of their work, but also to dispose freely of all artistic means, independent of all traditions, conventions and criteria of good taste or the mastership that still largely determined artistic creation before the rise of radical avant-garde at the beginning of the twentieth century. This newly promised freedom had a drug-like effect on the artists at first and created an unprecedented intoxication. This is why so many works arose from these early times which we still admire today. At the same time, however, one cannot deny that little has remained of this early euphoria. Today, artists no longer feel free – rather, they feel that their subjectivity has but little or no relevance for the art world. The reason for this new powerlessness on the part of the artist can be put down to the fact that, in the attempt to free themselves, at the same time the artists of the historical avant-garde committed a somewhat fatal act, and one whose importance they themselves did not notice in the beginning: they freed the beholder. The beholder was relieved of any of the criteria he used, or was supposed to use, for his judgment of a piece of art. The artist actively liberated himself from these criteria – and the beholder lost them. This made the beholder extremely insecure. He no longer knew how to react to the pieces of art that were presented to him. He felt helpless vis-à-vis the arbitrariness of the artist, utterly un-able to defend himself by means of a well-founded judgment. This initial insecurity reduced significantly with time. The further modern art developed, the more the beholder began both to cherish and relish the insecurity of freedom given to him by the artist. There are of course more beholders than artists and, according to democratic conditions, we all know that the minority must bend to the will of the majority. In earlier times the artist was able to reject the public’s judgment by asserting that they had not properly understood his work. The artist could call upon his mastership, the historical originality of his work and the strength of his inspiration, etc. Today, however, these justifications no longer apply, as the public feels totally free in their judgment. If they simply do not like something, they are not prepared to let themselves be convinced otherwise; the tables have turned. The avant-garde has directed the artist’s freedom against all of the public’s judgments. Now the freedom of the public’s judgment is directed against all reasoning and explanations on the part of the artist.
Even if our culture no longer believes in the criteria of taste, it certainly believes in knowledge and in technology. Art’s liberation from the beholder’s taste can only happen in terms of a mechanization of art. Or, in other words, its re-mechanization. Art was always, in the first place, a matter of technique. Only the historical avant-garde discredited and then abolished art production’s old techniques. Malevich’s Black Square and Duchamp’s Ready-mades introduced an age of the de-mechanization of art. It was particularly at this point that artists lost their cultural privilege regarding the beholder. If an artistic decision is totally free to declare an object or a random form a work of art, then the beholder must also be free not to accept this decision. Everything becomes a question of personal or individual taste. If the artist’s taste is contrary to the beholder’s taste then, as we all know, the loser is the artist. The avant-garde was an attempt to liberate art by de-mechanizing it, by liberating it from capability and knowledge, by equalizing the act of artistic creation with aesthetic judgment. Today, this de-mechanization of art, which was carried out by the historical avant-garde, increasingly presents itself as the preparation for a new phase of its radical re-mechanization. The avantgarde abolished old techniques such as drawing, painting and sculpture – or relativized them in their meaning. However, after a phase of being freed from technology, the re-mechanization of art has begun. It started with the use of new digital techniques of image production and distribution. The artist has again become a technician, a specialist and a producer, thereby once again establishing cultural distance between himself and the beholder. Peter Weibel had an early understanding of the opportunity offered by the new determination of arts and he seized it. Many artists and theorists have preached the return of art to technology and knowledge in the times of the Post-modernism of the 1970s and 1980s. However, this almost always implied some kind of ironic return to traditional artistic techniques. One wanted to protect oneself from the all too mighty mass media and define other, alternative spaces. For the same reason others use to legitimate their farewell from the avant-garde, as Weibel connects to the avant-garde’s traditions. The opportunity and even the need to restructure the avant-garde program, results for Weibel from the fact that art has definitely left behind its traditionally closed rooms and must act amidst today’s technological, medialized world. Hence the avant-garde was right in letting go of traditional art techniques, directing the view of the beholder onto the new technical world – even with a critical purpose. Today, however, it is widely pronounced that art should give up its critical, elitist, avant-gardist attitude and should be easy to digest. In short: that art, as soon as it starts to be produced in the context of and by using mass-media procedures, should be obliged to adapt aesthetically to the laws of mass-media dissemination too.
Here, the avant-garde is not dismissed as obsolete but rather transcended by means of a re-mechanization of art and continued in its critical impetus. The repertoire of images circulating in mass media networks is very limited. Not only so-called images of the other, but also images created by science, interactive images, purely subjective images, let alone abstract images, usually have no chance of entering large mass-media networks. It is, however, just these denied, excluded images that Weibel works with. Images which can be created using technical media and are hence indeed technically compatible with the networks of massmedia dissemination. At the same time, they display an aesthetic incompatibility with these networks. In this sense Weibel’s artistic practice is a direct continuation of the artistic practice of the historical avant-garde under the conditions of its technical and medial topicality. This would imply a continuation of the critical discussion of mimetic illusion.

This insight, however, would only worry an engineer, not an artist. Weibel is very well informed about the limits of logic, since he has seriously and systematically researched the field. Artists like to use computer programming and simulation because they like to demonstrate how the system loses its own systematic, how strict logic leads to paradoxes and how an ambivalence is created by an obsessive search for unambiguousness. Art is indeed interested in system, structure and program, but mainly where they lead themselves ad absurdum. It is exactly in this drift toward the absurd that it becomes clear how the image of thinking differs from thinking itself and that the living cannot easily be mimetically duplicated. Today’s artists, working with communicative and “intelligent” media, are more interested in the blind alleys, disturbances and absurdities of media communication than in its acquisitions. This is why the re-mechanization of art is certainly not just about naive enthusiasm or worship of technology. On the contrary: it allows not only an ideological claim of the limits of technology but also their technical analyses. But it is not mere pleasure in chaos that makes artists interested in the dysfunctional, in the deceptive element of computer-controlled and communicative processes. It is instead a systematic and critical analysis of mimetic procedure by means of art. This is a task modern art has imposed on itself and in the meantime it has become tradition. Except that today it is the mimesis of thinking, not reality. The analytical, critical dimension of Weibel’s occupation with the operational logic of today’s computer-controlled and communication systems, which he pursues in the context of his work at ZKM Karlsruhe, is already detectable in his early works dealing with the logic of mass media and everyday communication. These earlier art works can be seen mainly as examples of the artist’s critical occupation with the conventions of today’s media culture. These works often have a clear political dimension. They frequently refer to the chances as well as to the shortcomings and absurdities of today’s media world. However, mostly these works display a very specific sense of humor. As we all know, Soren Kierkegaard drew a strict line between irony and humor. For Kierkegaard, irony was the manifestation of a seemingly endless subjectivity aiming for a triumph over the finite character of things. Humor was for him the result of this subjectivity’s insight about its own finiteness.

Don’t worry, this is not an attempt to trace family circumstances or events way back in Odessa. The making of spiteful allegations is not our purpose either but rather to take a leap in the dark and to attempt an investigation of one of the most lucid creative contributions to Austrian art since 1945 – the work of Peter Weibel. Lügt (lies) was the message written on a board the artist held up beneath the sign of a police station in 1971. Let us forget for a moment that it is above all federal institutions, which through their practice contribute in no way at all to the good of certain groups of people. Indeed, these institutional inventions have set themselves objectives such as the freedom of the individual or social progress, or simply, the welfare of mankind, yet, there are also some quite contrary side effects, extending as far as the blatant discrimination for some of the so-called “supported.” If this were not the case, in our representational democracies we would be obliged to assume that the police are ultimately the tool of the representatives of the people (politicians). They again, are elected by the people (us), representing it (us). They are the people (us). So if the policeman lies, this is first of all an incident, maybe resulting from a certain specific personality structure. For instance because the policeman as an individual is given too much personal power which as a consequence he might abuse to the detriment of other individuals. Lies are told, however, also by the institution, the state, the representatives of the people, by the people themselves – and finally by us. In this paradoxical situation you too, dear reader, are lying every bit as much as the author of these lines – and consequently Weibel too. We know what the artist is on about and what he is denouncing, at the same time, however, we understand the limits of linguistic possibilities. Thus in striving for freedom and knowledge, we are forced to contrast the state lie with our own lie, whilst the notion is relativized as soon as the viewpoint changes. In the most radical moment of anti-art, in the 1960’s, avant-garde artists – building their work on the heritage of Dada, Surrealism and Constructivism – were able to shatter the foundations of bourgeois society, and thereby create fear. The traditional self-conception of art at that time was after all based on the assumption that art per se was a part of bourgeois society and thus part of its own system of rule and power. So if art crosses the divide to become an expression of both cultural and political rebellion, its demand for truth will change. For its “counterpart” this becomes a lie that must be fought against. Whilst for the avant-garde, art, now transformed into a weapon, will reveal the lie of the bourgeois capitalist society and hence of the federal power. So we should look at Peter Weibel’s practice with regard to this field of conflict. Seemingly remote sources from anarchism to scientific-theoretical models were suddenly applied in order to help art to get out, to transform, helping it to new relevance.

Polizei lügt (police lie) is one of a series of actions that was documented photographically. Weibel changed the script on public notices. Oberlandesgericht became Oberschandesgericht (Higher Regional Court became Higher Disgrace Court), and Rechtsanwalt became Rechtsgewalt (attorney at law became violence of the law). This “scriptual terrorism” came into existence simply through holding the appropriate letters at the corresponding positions in the word. These interventions were a more direct invasion of public space than the graffiti being created at this time in New York City. As a potential for the denotation of an actual content, a warning, a revolt against state institutions, these “attacks” create feelings of insecurity. The graffiti by contrast no longer contains any direct message, for they have internalized the message and have been radicalized in their formal qualities. “Being neither denotation nor con-notation, they evade the principle of nomenclature and break as empty signifiers into the sphere of the fulfilled signs of the city, which is dissolved in turn by their pure presence.”2 Thus graffiti represent a variation – derived from the sub-culture – on linguistic criticism. Weibel’s series were titled attacks and still speak the language of the protest movement: do it! It also represents a form of subversive poetic statement, reducing content to its opposite through linguistic intervention, while plumbing the limitations of what language in fact can do. Weibel has himself studied and done academic work on the issue of philosophy as criticism of language, culminating in the formal thinking of the mathematician Kurt Gödel. Gödel has provided the quasi salvation of the “liar” paradox in his statement “I cannot be proved.” If Nietzsche took God from us, then Gödel would seem to have robbed us of ourselves. Because where are we if we cannot be proved or demonstrated? There, perhaps, where God is... A connection between linguistic skepticism and individual social rebellion was developed still further in the protest movement of the 1960’s of which Weibel was a part. This connection provides a basic framework, in which the content and socially critical expressions within Weibel’s art can be developed. He did this in the mid 1960’s in literature and as an extension from this into the fine arts. He would not be Weibel if he had not also created a theoretical foundation for his artistic actions. He provides coordinates for the clarification of his procedure in his Sub-geschichte der Literatur (A Sub-History of Literature).3 Here we learn that the official history of literature is not “a history of human invention through, with, and in language” but “rather a collection of felicities and self-help.” Thus no basic or social change is possible without bringing about a change in the formal structure. The hero, in content that is, is not an adequate figure however when presented in traditional costume. The Russian linguist Roman Jakobson provided a formulation of this development in 1921, by describing the poetic procedure as the “sole hero” of literature. The formulation of a sub-history, such as that of the avant-garde, comprises in itself a social-political dimension, precisely analogous to the content which it reports in its special cases.

Weibel’s early experiments began in 1964, downwind of the Vienna Group that had begun to assemble in the 1950’s. Direct work was done by this movement during the 1950’s and 1960’s as a sort of continuation, in the most radical manner possible, of the results achieved by the historical avant-garde (Dada, Futurism, Surrealism, Constructivism u.s.f.). Vienna was by no means ahead of the times in this decade – it was provincial, traditional and marked with the brown striations of a Nazi past. Neo-avant-gardist movements such as the Vienna Group had an absolutely unsettling character; the conviction that the transformation of bourgeois aesthetics would of necessity go hand in hand with the transformation of bourgeois society was here understood as a matter of course. Criticism of both the state and of reality by means of a criticism of language contributed on the one hand to an overstepping of the aesthetic consensus of the times and on the other hand to the transgression of genre frontiers. In anticipation of the “linguistic turn,” the artists expanded into other media such as photography and film, and, even more radical forms of artistic practice such as happenings, Concept Art, and actions. In other words, change-related thinking was by no means solely restricted to language and the arts, and the borderline between art and life was shifting in consequence. The Eight–Points–Proclamation of the Poetic Act, written in 1953 by H.C. Artmann, acts here to a certain extent as godfather.4 The expansion of the definition of language first took place through the inclusion of sounds and words, but continued with another phase of development that included other materials as well, up to the point where the poet could get by without using any kind of verbal language whatsoever. The visual dimension of the written text was rapidly overcome by Weibel as well, whose own work developed along the lines of action texts, motion poems, photo poems, object poems, process poems, and, as a concluding sort of gesture, the radical use of one’s own body. In works such as raum der sprache – skulptur mit angeschlossenem organismus (space of speech – sculpture with a connected living organism) Weibel had his own tongue set in a block of concrete. The point that is articulated in this gesture ranges from general speechlessness to the achievement of a bodily and social effect on people through the use of speech. What else can it be but a lack of freedom, after all, when the letter of the law is interpreted by a judge? What is possible for a judge, in setting his organ of speech in motion, is altogether impossible for the person whose tongue is sealed into a wall of concrete and marked by bodily repression.

Herbert Marcuse spontaneously offers a helping hand here: “Only when art remains negative is it in a position to negate existing reality itself.” And he continues: “Only authentic art is negative and this in the sense that it refuses to obey the existing reality, with its language, its order, its conventions and in its images. (...) To this extent art provides a representation of a free society and of closer human relationships. But art cannot go beyond this.” During the student revolts of the 1960’s, artists in a wide range of fields argued about the commitment of art within the context of the revolutionary process. There was relatively little to hear about these ideas in this respect in Austria, however. The work of the Viennese Actionists (e.g. Kunst und Revolution / Art and Revolution, 1968) represented one of the few contributions to this. Weibel, who was to some extent an active participant in Viennese Actionism and provided theoretical support for it, was once again the exception, however, for no other significant artistic work of Austrian origin provided such thoroughly committed contributions to these various processes. Michail Bakunin should be permitted to have a word here as well. For him, positive means reactionary, an acceptance of the status quo and thus ultimately a sort of quietness. Negativity is for Bakunin, as he wrote in 1842 in his essay Die Reaktion in Deutschland (Reaction in Germany), the democratic principle, which in his day represented the negation of an existing reality, and indeed ultimately represented a sort of movement. He argues from this that the negative alone has the right to existence, closing with the celebrated words: “The lust to destroy is also a creative lust!”6 This sentence has had a great influence of course in artistic and intellectual circles through to the present day. The intensive desire for change in the midst of an unbearable reality sounds like a trumpet call for all avant-garde demands and manifestos. This same phrase applies to the work of Dada, through Surrealism, and even the Viennese Actionists, just as it is to be encountered in linguistic criticism of that time, in its most radical and brutal formulations. In short, Bakunin’s sentence is the legitimizing basis for all such antipositions, and not just in art.

It is true that communicative media already have established a continuity in time and space: telephone, fax, electronic mail, numerical and telematic networks, radio, television, the press, etc. This continuity is still not the continuity of the active and living thought, singular and differentiated, emergent and cohering everywhere, but rather a network for the transportation of information. Do the viewers of a televised transmission share a community? Do they bring together their experiences and their intellectual powers? Do they envisage and perfect new mental models of a situation together? Do they even exchange arguments? No. Their brains are not yet cooperating. The continuity effected by the media is only physical. It is a necessary prerequisite of the intellectual continuity but not sufficient in itself. Until this morning the work of writing was undoubtedly one of the most efficient means for the production of collective thought ever invented. The network of libraries keeps records of the creation and the experience of myriads of dead and living human beings. The fragile filament of memory is re-established, dormant thoughts revivified from generation to generation through the processes of reading and of interpreting. Translations from one language to another, or from one discipline to another, assure the communication between detached spaces of thought.
But by its nature the classical form of writing is a static and discontinuous system of signs. It is an inert, parcelled, dispersed body becoming more and more enormous each moment, and its unification and resuscitation requires that each individual sacrifice years and years to research, interpretation to the establishment of connections. As a remedy to the present situation, virtual worlds of collective intelligence will see the development of new forms of writing: animated pictograms, moving languages that will preserve traces of their interaction with navigators. By itself, the collective memory will organize itself, unfold itself anew for each navigator according to his interests and his previous traversings of the virtual world. The new space of signs will be sensitive, active, intelligent, at the service of its explorers. I ask again: what is interpretation? The subtle mind attempting to invite the inert body of letters into a dance.The evocation of the breath of the author in front of dead signs. The haphazard reconstruction of the knot of affects and of images in which the text originates. And, finally, the production of a new text, that of the interpreter. But what if the signs are alive? What if the image-text or the space-thought continuously grows, proliferates and metamorphoses itself to the beat of the collective intelligence? What if the leaden characters cede their place to some dynamic and translucent substance? What if the opacity of the gigantic stratifications of texts effaces itself in front of a flowing and continuous milieu the center of which is always occupied by its explorer? After the encounter between the vivifying spirit and the dead letter, after the dialectic of the corpus and the oral tradition, comes a new mode of the construction of the continuity of thought, a mode making possible the participation of everyone in the adventure of a nomadic language.

The first nomadic people followed after their flocks searching for nourishment, moving about following the rhytm of seasons and of rains. Today we are nomads following after the future of humanity, the future traversing us and made by us. The human being has become its own climate, an endless season with no return. We are hord and flock intermingled, more and more attached to our instruments and to the world moving with us, strolling on a new steppe each day. Neanderthal men, well adapted to the wonderful hunting expeditions on the glacial tundra, became extinct when the climate abruptly became warmer and more humid. Their natural game disappeared. Despite their intelligence these growling or mute men had no voice, no language with which to communicate with each other. Therefore the solutions found for their problems here and there could not be made more general. They remained dispersed even when they were faced by the transformation of the world surrounding them. They did not change with it. Today the homo sapiens is face to face with a fast modification of its surroundings, a transformation of which it is the collective involuntary agent. We may either cross a new threshold, a new stage in the evolution of man, by inventing some attribute of humanity as essential as language but on a superior level. Or we may continue to "communicate" through the media and to think in institutions detached from one another, organizing moreover the suffocation and division of intelligences. In the second case the only problems we would still be confronting would be problems of survival and of power. But if we were to take the route of the collective intelligence, we would gradually invent techniques, systems of signs, social forms of organization and of regulation permitting us to think together, to concentrate our intellectual and mental power, to multiply our imaginations and our experiences, to work out practical solutions for the complex problems affronting us in real time and on all levels. We would progressively learn to orientate ourselves in a new cosmos, constantly transforming itself and drifting, to become its authors as much as we can, to invent collectively ourselves as a species. Collective intelligence does not aim at the mastery of selves through human collectives but at an essential loosening of the grip changing the very conception of identity, the mechanisms of domination and of the breaking out of conflicts, the unblocking of confiscated communication, the mutual launching of isolated thoughts. So we are now in the same situation as a species whose each member would possess a good memory, would be perceptive and astute, but which would not yet have reached the stage of the collective intelligence of the culture because it would not have been capable of inventing an articulated language. How can one invent language if one has never spoken, if one's ancestors have never pronounced a single phrase, if one has no example to follow, not the slightest idea of what language could be? We are as nearly as possible in the same situation presently: we do not know what it is that we have to create, what we may already have obscurely began to envision. Still it only took a few millenia for the homo habilis to become the homo sapiens, to cross such an imposing threshold; it launched itself in the unknown, inventing the earth, the gods, and the endless world of signification. But languages are made for the "human scale" communication within small communities, perhaps even to guarantee the stability of their relations. Thanks to writing we have reached a new stage. The technique of writing effected the growth of the efficiency of communication and the organization of human groups; its scope was much wider than could ever have been that of shere speech. But this change took place at the expense of the unity of societies: it caused the division of societies into bureaucratic machineries for the handling and manipulation of information with the aid of writing and into those to be "administered". The task of the collective intelligence is to discover, or to invent, the other side of writing, the other side of language, so that the manipulation of information would be distributed everywhere, coordinated everywhere, that it would no longer be the priviledge of separate social organs but, on the contrary, would be naturally integrated into every human activity, as a tool in the hands of everyone. This new dimension of communication should evidently permit the mutuality of our knowledge and the reciprocality of its transmission which is the most rudimentary condition of the collective intelligence. In addition it would open up two major possibilities that would radically transform the fundamental facts of life within societies. First, we would have at our disposal simple and practical means of finding out what it is that we are doing together. Second, we could handle, even more easily than we write today, instruments allowing collective enunciation. And all of this no longer on the scale of paleolithic clans, or on that of States and historical institutions, but with the amplitude and velocity of gigantic turbulences, of deterritorialized processes, and of anthropological nomadism influencing us today. If our societies content themselves with mere intelligent government, they will almost certainly not attain to goals set by them. In order to have some chances of a better life, they will have to become intelligent by the masses. From beyond the media aerial machineries will make the voice of the multiplicity heard. It is still indiscernible, muffled by the mists of the future, bathing another kind of humanity in its murmur, but we are destined for an encounter with superlanguage.
The advantage on the other hand, is that these submicroscopic data can be transmitted without material movement in space and therefore without human accompaniment. Nontechnical media are bound to a physical carrier, they require the same network infrastructure as traffic of people and goods. The earliest wire networks had a point-to-point structure, requiring access points for in- and output, relay or refresher stations at regular intervals, and central nodes where messages are switched to their destination. In 1896, Marconi's wireless telegraphy extended the traffic of signals into the ethereal radio spectrum. The point-to-point cable was supplemented by omnipresent waves that can be intercepted by anyone owning a receiver. Radio extended the distance the voice carries virtually around the globe. With the broadcast networks of radio and TV, the center-to- all structure was invented. One speaks and all listen. These media are an extension of the public sphere, and therefore the radio spectrum is usually considered a public resource and regulated accordingly. Broadcast media create the masses they address,synchronizing millions of non-present, anonymous media recipients. McLuhan points to the origin of technical media in the medium print. In contrast to his own interpretation of the electronic media as fundamentally different from print, in their homogenizing function they are not. Reading this quote against its author, I see the program of 'homogenization of men and materials' rising to its ultimate violent power only in its military form under conditions of mass- mobilization during the Second World War (radio), and in its postwar civilian form under conditions of mass-markets, -media, -automobilization, -tourism etc. (TV). One further important aspect of technical media is that perception of the world shifted from the real thing to its stored mediatizations. Typists took dictation not from their superior's voice but from a gramophone or telegraphone recording. The question if Leland Stanford's horse had all four feet off the ground when in gallop could not be answered by observation through the naked eye, but Muybridge's serial photographs showed that it was in fact the case. The amount of live music we listen to is neglectable in comparison to prerecorded music. Whereas live broadcast implies a co-existence in time, a simultaneity that seems to warrant authenticity, much of what we see on TV is pre-recorded, edited, re-run - if we are not watching out of local storage of video anyway. Personal communication shifted from synchronous to asynchronous with the storage of answering machines, faxes, and email. The Matrix itself is a vast and rapidly growing-library. In short, large and exponentially growing parts of our media horizon are 'canned', and the two essential new operations besides transmission that technical media add to those of the Gutenberg Galaxy - copying and editing - are based on storage media.
The computer has its roots in mathematics which is indistinguishably linked to astronomy. Computing machines were built before Leibniz, like Schickhard's calculating clock (!) (1624) or Pascal's adding machine (1642). Still the primacy goes to Leibniz who produced a great confluence of streams of ideas, and contributed profoundly to symbolic logic, combinatorics, and therefore the history of the computer. Babbage should at least be mentioned in passing. His projected Analytical Engine was to have included most of the characteristics of modern computers realized only a hundred years later: a store, a mill (CPU), a transfer system, in- and output, and he also anticipated automatic operation, external data memories, conditional operations and programming. In 1847, Boole used a binary notation to represent truth-values in formal logic, 0 and 1 representing 'false' and 'true'. Shannon and Weaver's information theory translated the Boolean false and true into off and on states in electronic components. Signals, since this ultimate analytic cut with Ockham's razor, fall apart into basic indivisible yes/no units called bit. Like 'atom' for the material world and 'individuum' for society (both meaning 'indivisible'), 'bit' marks the smallest possible unit, the simplest building block of any possible symbolic system. Having mentioned some of the shoulders he was standing on, I can now turn to Alan Turing. I suggest to name the emerging horizon of binary digital media "Turing Galaxy", because its two central concepts were first formulated by him. One is the Universal Machine, the extremely primitive machine that can emulate any machine, the typewriter that reads and writes an operative text out of no more than two characters which freely models the appearance of the typewriter itself, the Universal Medium that precedes and empowers any possible multimedia to come. From then on every phenomenon and every process that can be described completely and unequivocally (the definition of both algorithm/automaton and the inter-subjectively scrutinizable knowledge of science) can be implemented in the one single machine to end all machines. The problem of building new machines has been replaced by the problem of writing an operational description of this new machine for the universal machine. (5) The other is the thought experiment known as the Turing Test which provided a comprehensive re-definition of man as a symbol processing system on a par with machines, and technically resolved the subjectivity problem. (6) Since then, 'intelligent' modelling, signal processing, and pattern recognition - so called thinking - has turned into a continuum across a range of possible technical or biological implementat ons. 'Mind' and machine have become interconnectable (if not interchangeable). Turing or bit media inherit properties from earlier media. They still operate largely in the mind-frame of the mathematical and the Gutenberg Galaxy. The most essential new operation introduced in the Turing Galaxy seems to me simulation. While models in the Gutenberg Galaxy become operational only after being read into and processed by the cortical CPU, models in the Turing Galaxy run inside a dynamic self-active technical medium. Bit words have the double function of addressing human readers as well as machines, i.e. themselves. Action unfolds and changes according to a script or in response to the action of the user and to its own results. Simulation allows to test hypotheses, to automatically control real life processes, and to construct alternate worlds. Today we observe the collapsing of all media into the universal medium computer. Turing media connect people, libraries, machines, and aartificial communicational entities. We are still exploring what the usage of computers in 'Turing mode' could mean. My suggestion: acting inside of media, and interacting with artificial agents.
form of computer games. Games are simulations. In the earliest form they simulate rules and strategies of board games. Later they simulate technical systems (notably with the military flight simulator reappearing as entertainment product), and social systems ( role playing games, SimCity etc.). In games (as in simulation) the computer takes on the function of agency, of a counter- player, an interlocutor, simulating dragons, enemy aliens, humans, governments, or simply fate. The computer also provides the playing-space into which the human player projects herself as a sprite, avatar, or persona, a marionette of herrself that she flies by the wire of the joystick. This is the first time not only the eye and ear, but the hand reaches through to the other side of the proverbial looking glass. With the emergence of data networks, games shifted from single-player stand-alone games to multi-player networked games or MUDs (Multi-User Dungeons/Dimensions). Here human others are re-introduced into the position of counter-player, next to and on a par with pieces of software simulating game characters. Originally games in the narrow sense of the word built around the sword-wielding-and-monster-slaying world of Dungeons & Dragons, MUDs are developing into common meeting grounds around diverse topics including professional conferencing facilities and educational institutions.
A typical host on the Matrix has a tree structure. It might be presented as a directory listing, menu, or in an index file pointing to the individual texts. One might browse or search by keywords with the search space inside one database, one host, or network-wide. At the final nodes of the tree one might find a text, in itself complete with author, pagination, footnotes, etc. The Gutenbergian resources on the Matrix are vast. Librarians were among the first to inhabit and develop it. Just a few examples are the US Library of Congress, including their Soviet and Vatican online archives, the Project Gutenberg, books.com, the first bookstore on the Net, magazines like Wired, and a sprouting new category of multimedia Metazines like the Electric Eclectic. Do people actually read books on the screen? Do they print them out? How are etexts used? The advantage of having reference books ready for automatic searches is obvious. Same for checking quotes in any sort of text. Maybe people will start to actually read electronic books when screens are light enough to hold, as pleasant to the eye and as 'interactive' as print on paper. Maybe people will have them rread to them by a voice synthesizer. Already now, the ASCII text is driving a Braille interface to allow blind people to read them. But rather than looking at the 'usage' of an individual text it is apparent that 'reading' will take on a different meaning when you imagine a library of 10,000 etexts in the form of a single text corpus available to you at any time. Even though somebody like Borges might be able to store a huge library in his memory and quote from it literally even after going blind, this is not given to most of us mortals. But thanks to the automatized external memory, we have random access to all the stored ideas. We can keyword search, browse with guaranteed serendipity effects, follow through on various threads, all the while creating hyperl inks on the go, leaving tracks inside the Gutenberg horizon that we can follow again next time we touch any of the texts. Every work (say Dante's Divine Comedy (8) appears in its own context, and in any other that we might create. All these operations could be done inside a library, but involving a lot more foot-work. The increased accessibility is already more than a quantitative difference. But what other automatic operations on texts may emerge, most of all what the new faculty of simulation will mean for writing/reading, ie. 'thinking' under conditions of the availability of the virtually complete library (Lyotard) in a dynamic format at the tick of a few keys, will have to be seen. If all these operations have to be done using raw Unix commands or exotic database query languages, the bookish Gutenbergian will likely not feel very at home. Luckily, there have emerged hypertext interfaces that make life a lot easier (World Wide Web under Mosaic). The reader/writer sees a text page complete with graphic design, that can be read like the page of a book. The reader can mouse-click his way around in the labyrinth of the global online library, make annotations, leave 'bookmarks' etc. A special feature is that the 'footnotes' to materials (text, image, sound, and video) outside the present text are active. By clicking on them, the corresponding file is retrieved across the Matrix, and presented immediately. All these are Gutenbergian operations, accelerated to the speed of electricity. Their model is the library.
The Letter. Postal mail is basically a point-to-point network, switched (sorted) at post offices, transported by diverse means of transportation. 'Snail Mail', as it is called in net.land, is usually private, to someone you know, but it can be extended to point-to-many. Mailing •lists exist also in RL for commercial, administrative or grass roots usage, but switching from traffic to transmission is more than a linear change. The Universal Network Medium adds the function of a reflector. A list server (like Majordomus) is an automatic forwarding program that sends every incoming message on to every subscriber, and drops it into his mailbox. Mailing lists can be unmoderated, ie. the information is provided as is, or moderated, ie. preprocessed by a wetware editor agent, which for certain purposes helps to raise the information-to-noise ratio significantly. Whether it holds together a professional special interest group, a hobby club or a speaker's ccorner - the mailing list constitutes a form of public. Electronic fora or bulletin boards - the metaphors reveal the heritage of earlier equivalents in public face-to-face real space. The most impressive are, without doubt, the Usenet newsgroups. Forming another network within the networks, the newsgroups permeate across Usenet, UUCP,Internet and selections also into the commercial networks. One does not subscribe to newsgroups, and the messages are not delivered to one's mailbox. They rather sit on one's local host to be read, browsed, participated in whenever one likes. Mailing lists and newsgroups constitute the basis for a written sense of community. In order to do so, they have to provide some form of space-time coordinates to anchor the social. The placement of a message in one electronic forum creates an unambiguous attribution in index space (an address). Their sequence creates a temporal order, a history of speech acts in which regulars build a sense of group identity. Fora are usually archived, so even though a message was deleted locally you can still look it up. FAQ (Frequently Asked Questions) documents are the collected common sense that is not the lowest common denominator, but common expertise. Like in every form of communal exchange, rules for accepted use and conduct (nettiquette), for the prevention of redundancy, various ideas on how to enforce these rules, etc. are negotiated on the go. Communicational conflicts are solved, of course, also within the interaction, but as a novelty there are technical solutions as well, e.g. the killfile, also sometimes referred to as 'the bozo filter', that locally tunes down or makes invisible unwanted traffic without having to have any censorship at source. Moderated newsgroups and mail- servers, like postal-based news-letters (or the xerox machine-borne mini-komi, as the Japanese call the genre) are already crossing over into publishing. These publications are in the Public Domain, and the moderators are most of the time volunteer editors. Unmoderated newsgroups are a running comment by Everybody attached to Everything. Large events like Tiananmen or the Gulf War, just as small events like a change in the design of Starship Enterprise bring forth their own forums. Strategies have evolved to prevent the slightest idea of censuring an unmoderated newsgroup. Any attempt at dominating or turning it into a PR device will cause a flood of flame - the power of the many.[9] They are specific, global, personal, and very powerful. And again, the total is more than the sum of its parts. MindVox offers many services, among them "a constantly growing library that chronicles the very inceptions of Cyberspace, with timeslices of systems dating all the way back to 1979 - the first bulletin boards ever to exist. "An orgiastic idea for any sociologist, media and market researcher, historian or linguist. The whole problem of sampling that is fundamental to every empirical social science evaporates when you can operate on the complete set. And it comes with a tool box that allows you to do searches, sorts, pattern recognition and other analysis automagically. Fractal algorithms are used to analyze huge amounts of earthquake data out of which it is otherwise very hard to make sense. What collective image might arise if you ran a similar chaotic pattern synthesizer on the subset of, say all utterances on the topic 'Internet' in unmoderated newsgroups, and how it changed over the years? The casual enquiry What's everybody talking about? will receive an unpredictable but mathematically precise answer. The idea of Man, Mina, Everybody, this collective chimera that broadcasting and marketing directors have in mind when they talk about 'the audience' and 'the consumer' - this non-entity will get a voice.
Radio. There were downloadable sound files on the Internet before, but the first regular radio station in cyberspace was pioneered by Carl Malamud in March 1993. You can receive Internet Talk Radio on your desktop or laptop radio either 'live' or 'on tape'. ITR publishes from 30 to 90 minutes of professionally produced radio programming per day. It reaches 100,000 people in 30 countries. TV. Malamud did not choose a TV metaphor, simply because it requires more bandwidth than the majority of the Matrix population has available right now. Also, production of video information still requires an order-of-magnitude larger investment in facilities. As a first step towards general use MIME allows to include little video and sound blips in email. Not yet live broadcast of concerts, but downloadable video clips one finds at mtv.com. Adam Curry, former star host of MTV Networks and net.veteran, created a multimedia site in 1993 that now attracts an average of 35,000 people daily, including music industry professionals. The cable TV on the Internet is the Multicasting Backbone or in short M-Bone. Multicast is a continuous stream of video and audio data packets running over a virtual network layered on top of portions of the physical Internet. It combines a global point-to-point structure with local 'narrowcasting' to everyone who is tuned in. What's on the digital tube? If you belong to the lucky group of power networkers you can watch keynote speeches by John Perry Barlow, co-founder of the EFF, or Vinton G. Cerf, president of the Internet Society, live multicasts from the deep-sea or outer space, footage from NASA satellites and telescopes, eg. Keck, the world's largest pair of binoculars in the solar system. The M-Bone has even emerged a solution to the hotly debated '500 TV channel' problem: 'sd' or session directory is a TV guide where all ongoing events are announced and can be joined on mouse-click. CU-SeeMe, an offshoot of the M-Bone for personal computers, was first developed as a TV metaphor for live video, only later voice was added. Latest news while I am writing this: "Coming Soon - Newscasts on Your PC. Intel and CNN have teamed up to test 'LAN TV' , a system that turns a regular broadcast TV signal into a compressed digital data stream, capable of being received on regular 486-type desktop PCs. While Intel tests the technology, CNN will concentrate on determining what it is people want to watch on their computers, in order to develop a special corporate news service."(10) As with desktop print and radio, desktop TV is not restricted to corporate providers. A CCD camera, a VCR, a video-capture board, and some editing software allow, in principle, TV production and multicasting on every PC. After the telephone answering machine made everyone a radio announcer, the desktop multimedia answering machine will turn everyone or his agent into a celebrity TV announcer. The Hunt. It is here that we see the Dungeon Masters and the Net-Gods at play. The Hunt is a kind of paper-chase, only without the paper. A game to encourage the players to "explore the Net, and traverse little known routes." Huntmaster Rick Gates, Student & Lecturer of the University of Arizona, got the idea "sometime in 1991 when I began to realize the enormous variety and volume of information available via what I will call the Net (Bitnet, Usenet, The Internet, etc.). [...] I suppose my initial ideas were based on the type of search exam that most library-school students have to go through during a class in Basic Reference.' Some of us enjoyed this type of challenge; we called it 'The Thrill of the Hunt". (11) The Hunt is edutainment at its best, "casual instruction in training for information resources. [...] It provides for training in context, which for most people works better than books or chalk on a board." For beginning net.citizens it provides a chance to look over the shoulders of media-literate experts. "It helps more novice users, or Net 'settlers', understand how to move around using the 'trails' that the more experienced Hunt players have 'blazed'. [...] Learning how to learn is critical, and this only comes from experience."
cky part, as always with Oracles, is to formulated the right search command. (12) At the risk of boring you by repeating myself: old media don't go away, they are the content of the new ones, transformed into metaphors. If the 'content' of the Net is magazines, radio, TV etc., then the 'content' of those is the Net. The whole Net is abuzz with questions of where it is heading. Self- reflectiveness is part of the constitution of a new medium. But this is a transient stage. As with Usenet newsgroups we will see that the early bias towards computing and networking itself will shift. Today the comp. groups are far outnumbered by the alt. and rec. groups. There are, of course, differences between the Meta-Medium and the other media it embraces. With text, sound and video editing capabilities on personal computers getting cheaper, one-person desktop publishing and multicasting houses become possible. This was also said when xerox machines spread, and again with laser printers . It did indeed happen to some degree, but it also showed that not everybody has the urge to publish. Most of all, cheap high-quality printing on a laser engine did not solve the problem of distributing and making your product known. This changes with the Universal Medium that is production, transmission and reception medium in one. To multicast does not require the concentration of capital and power necessary to produce a full daily broadcast schedule in one of The Networks. Anybody who finds a friendly host or scrapes up a few thousand dollars to set up her own can be media provider. Combining broadcast tools and communications networks, and private and various forms of public communications, makes all the difference. The implications for the changing nature of work become visible already. Everybody who offers informational products or services can do so - globally, from anywhere, at a price that a private person can afford. This is not to say that capitalism will crumble, and give way to an Anarchist's dream of self-expression for everyone. But it does mean the end of capitalism as we know it. People who are part of what is often called a revolution are very excited about the empowering qualities of the Net.
The oldest medium holds its entrée as the latest, the get-together. The basic social function of going somewhere and hanging out with friends and like-minded people requires a common place and time. While tree-shaped lists, keywords and hyperlinks are appropriate for retrieving data, 'human information' needs conversational tools, an anthropomorphic space not to consult repositories of passive information, but to meet people. Today the theater metaphor (Laurel) re-emerges and with it the idea of actors and agents (Brooks, Maes). It is here that we re-encounter the ars memoria. Cicero suggested to use personae for the memory image that anchors the 'things'. Yates' characterization of a classic memory image: consists of human figures, is active, dramatic, striking, under circumstances that recall the 'whole' thing, can be read directly parallel to Laurel's explications of a desirable human-computer interface. What is not lost in the transition from the art of memory to the art of interface design is most of all the dimension of mental space. The stage where the play is enacted is idea space, regardless whether the mental image is evoked by printed, pictorial or sound signs, or a Wagnerian multimedia Gesamtkunstwerk. An important difference between the two arts isthat the mental space of the ars memoria was not shared. An orator would, of course, share his idea space with his audience, but as he walked around the chambers of his memory, picking the points he wanted to touch upon from the statues where he had deposited them, he was alone. He would never encounter an other there. Net.operations in Gutenberg mode are mostly silent, ASCII, solitary, and asynchronous. Most of the time the netsurfer is not aware of others who 'are inside' the same host. While we have seen that newsgroups can turn into a home on the Net, the potential of the Universal Machine is by no means eexhausted there. The silent and iconoclast world is enlivened by the beginning multimediatization. But hypertext, radio and TV metaphor are precisely that, like horse-less carriage and wire-less radio we now have paper-less libraries and station-less massmedia. They are metaphors for different media, not for the market square in the Global Village (McLuhan). A different approach that does not come from Gutenberg (although it is not illiterate), nor from mathematics, and not from the technical network media, but from game are MUDs. According to one definition they are "detailed and realistic multi-player simulations that present ongoing campaigns and universes with evolving storylines, political systems, and landscapes being imagined into existence as play progresses." MUDs are shared places. You 'telnet' yourself there. Others 'are"there' as well, synchronously, even though from different 'real' time zones. From the theater metaphor we pick up the performing arts and the stage effects. From game/play we get the participatory elements and the challenge for the price at stake: recognition for wit, excellence, style, integrative qualities, for the craziness of thinking up something that nobody has ever done before.
In the Matrix also the physical body has no significance (except for the wetware break). But the mental body can travel along the wires and be re- incarnated in a remote Doppelganger. The same body can be played by a human, just as well as by the machine. On the behavior of human players there is much to be said and studied. Here I would rather take a closer look at the non-human players. Automatons and the game between man and machine carried tremendous fascination ever-since the days of the Ancients, with new boosts during the Renaissance (Maillardet's Magician, Vaucanson's Writer, etc.) and the industrial age. They were only sophisticated toys, but they triggered a philosopher like Descartes to think up a Turing Test avant la lettre. The philosophical and literary (eg. E.T.A. Hoffman) theme continues to fascinate mankind's phantasy. But it was only with Alan Turing and his influence on von Neumann, the cybernetics group, and others that a whole wave of mind-mirroring in Al, neuro nets, piano-playing robots and 'thinking machines' was triggered. This conceptual shift dismissed philosophy and literature, and made the Turing Testable machine the goal of a concrete effort of exact sciences. The first program that passed the Turing Test in a life-like situation was Weizenbaum's Eliza. Since then the Turing horizon has become populated by hosts of talkative and zealous homunculi, women, and daemons. One forum where the best of them come together is the Loebner Turing- Test competition, conducted annually since 1991. The New York business man has donated $100,000 prize money for a program that can pass as human in an unrestricted typed tele-conversation. Entries so far are required to be conversant - in "natural American English" - on one topic only. Entrants may selecttheir own topic areas, but the domains of knowledge must be "within the experience of ordinary people. One of the participants is Julia, a Maas-Neotek bot with an Al engine written by Michael Mauldin at Carnegie Mellon behind it. Between Loebner Turing Tests she logs onto a MUD and behaves like a regular player. She can be summoned, gives useful information, delivers mud.mai I messages to other players who are not currently logged in, dispenses witty quotes, can be nice to you, and kills you when teased too much - and next time you talk to her she will still be angry with you, because she even remembers.
It is envisaged that intelligent robots of the next generation be equipped with various sophistciated capabilities endowing them with desires and intentions, enabling them to perform hypothetical and defeasible reasoning, to solve problems creatively, to appreciate works of art, to achieve some form of cyberpleasure, etc. Understanding and the ability to develop explanations for observations and facts are fundamental for the realization of these capabilities. In fact explanation and understanding are 'two sides of the same coin' in both art and science. Our objective is to highlight techniques used in Artificial Intelligence which could provide mechanisms for modeling the aesthetic response of an intelligent robot, based on the causal explainability of complexity manifested in media such as electronic art. Leyton argues that art is related to explanation, in particular that the aesthetic response is the mind's evaluation of causal explanation. He maintains that the level of aesthetic response to art works is proportional to the level of complexity that an individual observes. He goes further arguing that the desire for art works is part of a general desire that the human mind has for complexity. Barratt also claims that humans seek to explicate complexity, and since the brain is finite, there must be a maximum degree of complexity that the mind is capable of explaining at any one time. If the degree of complexity is increased past this level, it exceeds the mind's capacity to explain it, artistic chaos is reached and consequently the viewer deems the art work to be incoherent. He concludes that the limit is set by the ability to give causal explanation, it is not complexity that is appetitive, but causal explanation itself. Clearly, if our aim is to develop intelligent robots with truly human-like characteristics, then they must be capable of artistic appreciation. For electronic art, appreciation must occur at the conceptual level and not at the physical (pixel) level. In the area of Artificial Intelligence the notion of explanation has been well explored. The complexity of explanations is often a reflection of the richness of the agent's background knowledge, and its ability to discern its surrounding world. Indeed, the aesthetic response to artistic chaos is equivalent to an explanation of a contradiction. Central to such an explanatory capability is the need for mechanisms supporting the modification or revision of knowledge, that is, learning. Belief revision models the process of accepting new information in such a way that an intelligent agent's epistemic state remains logically consistent, or coherent. Frameworks for explanation within the area of Artificial Intelligence can be used to support the aesthetic response of an intelligent agent. In particular, two important parameters of an explanation may assist in gauging an aesthetic response, namely the plausibility and the specificity of the explanation. In summary, if aesthetic response is the evaluation of causal explanation, then we can endow an intelligent robot with aesthetic responses which ebb and flow in accordance with the complexity of the causal explanation achieved.
I have always thought of computers as dynamic tools for introspection, exploration and discovery. Computer programming is instrumental in the externalization of ideas and algorithms are formal descriptions of what one hypothesis constitute the production of creative statements. The computer is a playground to speculate on the generative potential of ideas. As a matter of fact, the physical, tangible management of purely conceptual constructs becomes possible. However, the paradox is that while algorithmic specification allows the artist to touch the essence of his ideas it also creates a distance since all specification is indirect and seems to exclude spontaneous action. The idea is to view computers as partners in the process of creative decision-making. By way of algorithms we can explore various man- machine relations in this partnership: from studying total autonomy in computer programs to systems designed for expl icit interaction. The development of personal algorithms is the key to exploration and the gradual specification of objectives from incomplete knowledge, in sharp contrast to view the computer as slave, as a medium for deterministic visualization. I have characterized the interactive method where man and machine collaborate in a common effort and with common objectives as conceptual navigation; the artist-programmer gets feedback, his expectations are confirmed or contradicted by the program's behavior. Eventually, unexpected results may signal new and promising routes exposing unknown territories. Thus, man and machine contribute both to the creation of a computational climate that favours invention and to the development of a critical attitude towards the often complex relationships between programmed intention and actual result.
Writing algorithms has also forced me to evaluate experience vs. speculation. If one relies on models that have proven to be successful in the past, one 60 confirms what is already known. Algorithms that use rules reflecting this knowledge produce predictable results. Otherwise, designing processeswith the greatest possible freedom in pure speculation is like working outside of any known context making evaluation very hard indeed. The creation of new contexts for growing algorithmic activity mixing memories of the past and an open imagination is, I think, perhaps the most interesting challenge to algorithmic art.
Almost as if by magic - whatever procedure you dream of - you can probably extend the power of your dream to the computer and let it develop the dream beyond your wildest expectations. You may identify procedures for improvising with color, scale, and position - which is what artists have always done. Given sufficient definition you could develop a form generator and from your new vantage point see new possibilities for further elaboration on your routine. Through trial and error - interacting with the algorithm itself you proceed further into the new frontier. So what can we learn from this? We learn what artists have always known - that "CAD" programs, paint brush programs, paint brushes and drawing paraphernalia do not make art. Neither do artists or designers simply "make art". The one over-riding essential element to the process, "a developed artistic procedure", is necessarily unique for each artist and for each work of art. The procedure addresses a singular conjunction of elements for which there is no "universal" rule. The "calculus of form" may be placed in the service of such procedures but should not be confused with the art-making procedure. For the artist who writes the code the artistic procedure is the act of "writing the code", pretty much like the creative work of the composer when the composer writes a musical score. Making art does indeed require a "calculus of form". But the artist's instructions on how to employ the "calculus of form" precede the "calculus". One needs an "artistic procedure" which addresses the entire complex of elements for each specific work. The final form, unique and specific to each work, embraces more than the "calculus". While it embraces and grows from a "calculus" it might employ any of an infinite number of approaches to deliver the form. These may include metaphor, improvisations of the form phenomenon in and of itself, or reference to some other phenomenon or idea - historical, literary, political, mathematical or philosophic. Can an artist write an algorithm then for an artistic procedure? Emphatically yes! Such algorithms provide the artist with self-organizing form generators which manifest his or her own artistic concerns and interests. We are looking to an interesting time ahead of us when artists will be able to exchange and interact with each other's form-generating tools in ways we never dreamed. There are procedures yet to be developed to make this kind of interactive expression accessible - a time ahead when we will literally see an evolution of form including a genealogy associated with its creators.

Despite, or perhaps because of, a healthy skepticism, Artificial Intelligence (Al) has been making quiet progress in electronic arts. Artificial Intelligence has inspired traditional fields of electronic arts as well as it has developed new horizons for many artists working in electronic composition environments. Building on the success and shortcomings of previous experiences with computers in arts, the attempt to extend the paradigm of artificial intelligence systemsto the domain of electronic sonic arts is made now. Musicians are increasingly using intelligent machines to deal with tasks for which they are better equipped than humans. Computers are increasingly being used to address the brain-numbing complexity of modern electronic music products and processes, thereby allowing people to concentrate on their music and ideas. Expert systems, for example, help people by searching a book of rules to decide what to do in a particular situation; as machines do not forget, these systems can manage rules more consistently than people. Some musicians are using neural nets, which can recognize complex patterns, to apply precedents that are difficult to express in numbers or words. The real challenge facing technology is to recognize the uniqueness of machine intelligence and learn to work with it. Given enough memory, a computer can remember everything that ever happened to it or to anyone else. Furthermore, when faced with a logical problem or a theoretical model of how compositions or sounds should be, computers can deduce more results more quicklythan humans. Their complementary strengths should allow man and computer to work together and do things that neither can do separately.
Whereas the impossibility of physical death in cyberspace is one of its main attractions (certainly for the flight simulators used by the military), this absence of death and of death's possibility does not emasculate the project. For death becomes the ultimate ground for the cybernaught, not in terms of individual death, nor even death of the planet, but according to Lyotard, in the death of the solar system. On a number of occasions Lyotard mentions the inevitalbe the destruction of the solar system estimated to occur in 4.5 billion solar years. The task of technology, is to create an alternate non organic system that will survive this catastrophe. Not only does the certainty of this event constitute perhaps the most sublime of deaths, but the end of the solar system represents a finality, a resolution, that puts ultimate limits on human endeavour. Such closure however, comes at the end of a narrative space in which all the utopian and apocalyptic concerns that have defined twentieth century culture's relationship to technology, are able to play out their fictions. As a way of representing the body in space, according to a perspective that the logocentric apparatus has inherited form the renaissance, futurity is also associated with frontality, and opposed to anteriority. As a radiant, or irradiated subject, the cybernaught may transmit from a centre in all directions, nonetheless s/he is literally always looking in front. In front - to the absence of distance between the organic eye and the simulated scene, to the absence of difference between the real and the repesentation, to the unfolding in sequence of the virtual narrative, and to the future as a narrative of progress. This future space thus stands in for all the physical spaces which go missing in virtual worlds, and this future death defers the resolution of corporeality and the promise of transcendence that individual death promises. More than this, the future impossibility of organic embodiment provides the ultimate rationale for the numerical constitution, Cartesian co-ordination, and digital storage of the subject, who then shines with the necessity of survival. This is the radiant subject of art - the channel to the sublime, now irradiated. The subject who shares with radiant sound, the security of identity with the eventfullness, change and flux of the event. As Baudrillard says, we no longer need the VR glove or suit because 'we have swallowed our microphones' and 'internalized our aesthetic image.' 28 We have become the post holocaust meaning of radiant sound - transmissive but rotten at the core. And the realization of this subjectivity occurs, not at the point of solar explosion as radiance would suggest, but at the point of total computation. At this point, the signal continues to survive in outer space; the space of the future, but sound, and any vibrational body, is immediately extinguished by silence.
My first visit to virtual reality— a cartoon-like 'Virtual Seattle' at VPL Labs in California a number of years ago—indicated that for me at least, the great attraction was not the lure of computer technology or of interface devices, which included a cumbersome helmet ('eyephones') which put little video monitors over my eyes; and, the coarsely rendered, neon-colored artificial world, in which I had the illusion of being immersed was not a convincing imitation of the physical Seattle, or for that matter, any other landscape which could possibly have drawn me in. The allure of this cyberscape was the impression that it was responsive to me, as if my gaze itself was creating (or performing) this world and that I was to some extent enunciating it according to my own desire. My most abiding memory was of exhilarating ability to fly through the artificial world at great speed simply by cocking my hand like a gun—'navigation' is a poor term for this experience. Best of all, I had a sense of the weightlessness and super-power that I had imagined in childhood and had read about in myths and comic books, but had never before experienced, not even in my dreams. (My childhood friends in first and second grade and I tried fruitlessly to fly day after day by flapping blankets while jumping off walls and out of trees.) It is this feeling of transcendence of the mortal body and the gravity of earth that for me is a key to the desire and media attention which has been focused on 'cyberspace' and the subculture which has grown up around it.
The primordial virtual space is an utterly empty display, unlike the physical world, which is always 'full' and readymade. So far at least, cyberspace worlds are sparsely stocked with metaphors, now largely constituted from scratch with considerable graphic effort. Once these graphics are out of sight, it is easy to get lost in a void that is uniformly colored (usually black) and that wears infinity at its edges if not at a vanishing point. My first flight revealed Virtual Seattle, like most other virtual environments, to be relatively void but for a crude symbolscape of geometric objects. I remember my panic at flying through and out the swimming-pool-like image-space of Puget Sound and getting lost in utter emptiness. (I have also flown too far from the landingstrip metaphor of a Wall Street stock market program and have fallen off the checker-board world of 'Dactyl Nightmare.' The stock market program has an arrow function which points the way back to civilization.) What a comfort it was to find the traces of the human imagination in the spacescape near me again. On the other hand, why are these cyber-traces, the externalized imagination of electronic producers, filled with so little of our cultural legacy? I am thinking of metaphorically and graphically impoverished architectural flythroughs or crude male-centered fantasies of pornotopia ('Virtual Valerie') or a pseudo-prehistoric past wherein the only activity is relentless killing, (for instance, the aforementioned 'Dactyl Nightmare.') One task of art that commodity culture apparently eschews is to resituate the disengaged space of virtuality into a socio-historical context. For instance, Jeffrey Shaw's interactive city installations, such as virtual New York or Amsterdam, are richly symbolic, suggesting how the built environment may be refigured in a image-space as a kind of alphabet. Multiple and interlacing historical narratives are traced in a kind of writing motion over the display area via a bicycle interface. The Biblical reference in Shaw's piece at ars electronics 94, The Golden Calf, made what was otherwise a clever piece—a statue visible only through a mirror-like electronic display—into a commentary on electronic art itself. The uncanny and more sinister implications of my first flight occured to me later: A virtual space it is not just the ground or background or the landscape at which I look, or even that my look calls forth—that space looks at me, following my every move. Indeed, space constituted itself in response to various indices of my intention, for instance, the vectors of my gaze and the motion of my body or head. That is, in a virtual world, not just objects but space itself is interactive. As a consequence the virtual environment that surrounds the visitor itself can appear to be something 'live' or animate, 'that we cannot acknowledge as subject or persona in the traditional european sense, and which nonetheless constantly demonstrates that it sees us without revealing itself.
Of the many artistic responses to the Gulf War, I remember Frances Dyson's and Doug Kahn's sound and sculptural installation for its condensation of the sounds and images of birds in flight with the resonances of the air-war on Iraq. A more recent installation by Laura Kurgan explores the actual operation of several satellites in Global Positioning System or GPS by using them to trace the position of the New Museum gallery in New York. The installation was effectively demystifying, not only in revealing how this surveillance system works, but its material fallibility resulted in wavy deviations from geometric accuracy. Julia Scher has explored the psychical and cultural implications of electronic and computer surveillance in work spanning over a decade, including her 1993 installation, Predictive Engineering, at the San Francisco Museum of Modern Art, mixing live and recorded video on a two chiastically arranged and elegantly situated surveillance camera and monitor set-ups. The interest of art then may not be in the seamless operation of electronic culture nor in the production of realistic virtual worlds—like Icarus, that may be flying so low as to be dragged into the sea. The often mentioned desire for photographic resolution in virtual displays may also have as much to do with the goal of controlling physical objects and events as it does with aesthetics. An art of virtual spaces which simply aims toward realism of fit or of appearance with a physical landscape may then risk merely serving the instrumental or hegemonic purposes of military and business interests in an information society. On the other hand, art that surrenders to the allure of the mysterious or that seems to offer transcendence may find the wax that holds its feathers together melted by the sun. Exploiting the magical aura of virtual spaces risks satisfying the commodity and entertainment functions of information and nothing more. For, unlike prior illusion- producing modes, cyberspace is a means of enchanting not only liminal realms, but everyday reality. Even though it is has been discredited as a popular rather than scientific term, 'cyberspace' is appropriately built on the analogy of Norbert Wiener's cybernetics, or the study of feedback systems. In computers, feedback is elaborated into a programmed responsiveness which Sherry Turkle has noted, can captivate the user as a kind of 'second self'.2 But feedback is not restricted to the space of the monitor, for material artifacts and even a physical space itself can be 'cyberized,' or granted agency by programming it to simulate some form of human interaction, in the process ultimately lending it qualities associated with human personality. As Jay David Bolter, explains in Writing Space: The Computer, Hypertext and the History of Writing,' 'Artificial intelligence leads almost inexorably to a kind of animism, in which every technological device (computers, telephones, wristwatches, automobiles, washing machines) writes and in which everything that reads and writes also has a mind.' One futuristic vision of the personified or smart home proclaims, 'Once your house can talk to you, you may never feel alone again,14 suggesting this animism and a quasi-subjecthood can extend to even physical space, once it has been 'cyberized.' A utopia of ubiquitous computing would enchant the entire world, distributing magical powers to the most mundane aspects of existence.
Of course, business interests are far more concerned with 'information' as a resource and an exchange value, than with virtual environments, even 'smart houses' per se. 'Information' is knowledge decontextualized and stored as data (that is, as virtual objects.) In order to be retrieved and placed in a new context, that data must take on symbolic or metaphoric form in an interactive and to some degree immersive display. The value of information is realized not just in any one state, but as a passage from the conceptual to the virtual to the material and back again, crossing through a variety of reality statuses. For instance, virtual money or credit demands a passage through material objects in order to increase itself as interest. Jeff Schulz, for instance, has made the credit system the material of his performance art and of commentary in his essay, 'Virtu-Real Space: Information Technologies and the Politics of Consciousness:15 If virtual environments are best understood in connection with other social and cultural processes, as one stage in the unfolding of metaphors across a variety of reality-statuses and degrees of materiality, this suggests that the electronic arts are themselves part of a range or spectrum of interactive and immersive media and are not well-served by isolating them from art using other media, that is also concerned with the transformation of information societies into electronic culture. Artists from the ex-Eastern block or what was once the Third World are likely to suffer the consequences of this global change, even if they are excluded from its benefits. That is, there are artistic issues and perspectives which have a bearing on the global economic and cultural transformation we are undergoing that may be posed by those who have little access to computers or even to electricity —they must be welcomed into the discourse as full partners. Artists and cultural activists—for instance, Paper Tiger/Deep Dish and Ponton—have also not forgotten the issue of public access to the material and technical level where information is processed, stored and transmitted. It is real estate in terms of data space on computer disks and in main-frames, personal space in seats in front of computer work- stations, frequencies on the broadcast spectrum, satellite space off which to bounce signals, and room in the bandwidth of fiberoptic cables that global corporations struggle among themselves to own and control. The scarcity and costliness of these material gates of entry limit the number and types of subjects we can find in the virtual gathering spaces of an electronic culture. What we to some extent have and need more of is art which figures relationships between the virtual and the physical world, which demystifies the relation of the body to the virtual environment, and which is both a meta-commentary and an aesthetic statement. On the other hand, the technological ability to recreate the acoustic space of a medieval cathedral in one's living room, or to merge movie stars and tourists into the same image and have them interact, merely exploits the ability to superimpose the virtual over physical space: it is entertainment. The following section concludes by making some generalizations about virtual environments as virtual space, based in reflections on experience in cyberspace, from virtual realities to CD Rom work-stations to electronic networks.
The very idea of space becomes self-contradictory, when it is applied to virtual realms, especially the maze-like vectors and links which compose the paradoxical 'space' of networks. Virtual space is not so much space as 'nonspace,' for it need not occupy ground, nor be a continuous linear extension, area or void, nor even constitute the interval between things; and, unlike the material Lebensraum of earth, it not be perceived as limited or scarce. If the virtual space in question is the discontinuous, yet communal space of isolated computer network users, it can expand ad infinitum, like the text-based 'rooms' which make up a M.U.D. or multi-user dimension. But where is that noplace in which, for instance, two people talking by telephone meet? Where is the room and where is the display in which the hundreds who belong to the same M.U.D. (Multi-User Dungeon or Dimension) or M.O.O. (M.U.D., Object- Oriented) may gather? The reality-status of any one virtual environment is also unclear, seemingly in-between an exteriorized mental space, the apparatus of the image-display and the material world. The many different levels and degrees of virtuality in an information society add complexity to mystery. What, for instance, is the 'space' of a virtual object in a computer program? Even if it can be quantified as data in megabytes or ultimately in bandwidth or pixels, a virtual object itself remains an imperceptible potentiality, which occupies no space at all until it is accessed and displayed. Can one even say the object is 'inside' the opaque casing of the computer or hidden under the obscure machine language of programming?' Even if one could break into the black box or extract and analyze the program, one wouldnUt expose the virtual object, only the mechanism that has the potential to produce it. Yet, the virtual space on display is still a realm of cause-and-effect, though the consequences of any one action may seem more magical than logical, for they need not be proportionate to the results to which we are accustomed in physical space. Space is ordinarily conceived of as continuous or at least, at its most abstract, as a homogenous void. Yet, virtual non-space or cyberspace can be distributed discontinuously over physical space (in a way that is usually imagined as supported by ubiquitous computing.) Furthermore, physical separation between the users and objects of physical space need has little bearing on the seams which separate and link virtual spaces. What remains somewhat clumsy are the figural conventions which ease the passages between virtual 'worlds': the vortex, the window and the door are given too much work to do as metaphorical thresholds and passageways. The additive aesthetic principle of the Internet, the global network of networks, is an extremely elegant, non-hierarchical, rhizomatic global web of relatively independent yet connecting nodes. Though it was conceived out of militaristic considerations, it might be compared with Panofsky's analysis of the gothic cathedral. This comparison is not trivial, for combined as an infrastructural and virtual entity, the Internet is among the greatest architecture the world has every known, far greater than the material reference point of the information highway metaphor, the freeway system.
Such compression of space and time finds an exponent in Jeffrey ShawUs interactive installation, Revolution. The user's effort turns a grindstone interface, which churns out pictorial representations of hundreds of social revolutions in the historical record onto a video monitor. Revolution is then not a representational space of linear histories or of geographical areas but the presentational space of a metaphor and its recurring metahistorical patterns. The visitor to the installation stands for the protagonist and motive force of this social phenomenon, a spontaneously acting group called at times the 'mass,' the 'crowd' or the 'people.' Then the vocation of an art of the kind that reflects on electronic crowds and networks is not the representation of the visible world, but the visualization of what is otherwise inaccessible to perception and is difficult to imagine because of its scale, its discontinuity in space and or time or its impenetrability—from the insides of the body, the atom, or the black box to the outside of our galaxy and our universe. All the linking devices which create virtual spaces of greater and greater, albeit ephemeral unities—text-based networks, MUDUs and MOOUs, telecommunication satellite links and cables, but also protocyberspace like the nets which unify physical space—railroads and highways are understood, paradoxically enough as 'spaces.' Such virtual environments of discontinuous and overlapping jurisdictions would tax any political imagination capable of ethnic cleansing or of resolving ethnic conflict by dedicating bounded areas to one homogeneous culture. If virtual space were our model of political space, there would be no struggle for nationhood as a geographical entity. What would remain a nagging material problem is opening the gateway of induction into the virtual realm wide. The concept of 'space' applied to computer- and other machine- generated virtual realms is a metaphor that invokes something quite different than the fundamental experience of being in the space of the physical world in a body rooted to the ground by gravity, in view of a horizon. Cyberspace is heterogeneous and dispersed, it can be experienced in various degrees of person and immersion and in different symbolic modes as a virtually embodied metaphor where the flesh (or meat body) can't go, but into which disengaged spectral bodies and multiple personas be inducted, fly and interact, alone in an electronic crowd. The scene itself can move and is responsive to the user in ways which promote performative and/or magical experiences, loosely covered in scientific and socio-economic alibis. That is, electronically produced liminal realms and induced experiences are only superficially about technology, they are about transcendence (even when in degraded forms of sex, shopping, high-speed driving, mortal combat, etc.) Some of the organizing metaphors of cyberspace (frontier, highway, spaceflight, cave, net, theater, game, etc.) are propositions which should be scrutinized carefully as to the way they define the control, access, reality status and experiences assigned to the virtual and symbolic realm which is increasingly our everyday world.
Post-biological technologies enable us to become directly involved, body and mind, in our own transformation, and they are bringing about a qualitative change in our being. The emergent faculty of cyberception, our artificially enhanced interactions of perception and cognition, involves the transpersonal technology of global networks and cybermedia. We are learning to see afresh the processes of emergence in nature, the planetary media-flow, the invisible forces and fields of our many realities, while at the same time re-thinking possibilities for the architecture of new worlds. Cyberception not only implies a new body and a new consciousness but a redefinition of how we might live together in the interspace between the virtual and the real, calling for a wholly new social environment and a reconsideration of every aspect of our ways of being. Western architecture shows too much concern with surface and structures - an arrogant "edificiality" - and is too little aware of the human need for transformative systems. There is no biology of building. A city should offer its citizens the opportunity to participate in the process of cultural emergence. Its infrastructure, like its buildings, must be both intelligent and publicly intelligible, comprising systems that anticipate and react to our individual desires and needs as much as we interact with them. A "grow bag" culture is required in which seeding replaces designing, and where architecture finds its guiding metaphors in microsystems and horticulture rather than in monumentality and warfare. Currently, architecture has no response to the realities of cyborg living, or the distributed self, or to the ecology of digital interfaces and network nodes. It has produced a shopping cart world of pre-packed products wheeled around the sterile post-modernity of a mall culture. Buildings, like cities, should grow. As products of creative cyberception, they must become the matrix of new forms of consciousness and of the rhythms and realisations of post-biological life.

Techno music is an aggressive, technology and future oriented genre of youth culture and popular music. The historical background of this musical form lies in the avantgarde groups of 60's and 70's; especially Fluxus and Kraftwerk. From a philosophical point of view, techno can also be seen as a continuation to the modernist avantgarde movements such as futurism, surrealism and dadaism of the early 20th century. Techno music is especially popular in Europe. What used to be pure underground five years ago has become evidently mainstream. The recent commercial success of artists like Sven Vaeth, Westbam, LFO, Orbital, The Orb and Aphex Twin has proved techno to be a fast growing youth movement. Pop journalists and music experts have claimed techno to be "rock of the 90's". Concerning this, its is not surprising that the massive party concepts of Mayday and Love Parade have been called "Woodstocks of the 90's". In my paper I will introduce and analyse the latest developments of techno music and aesthetics. During the recent years techno has divided into several sub-genres such as ambient, trance, hardcore and gabber. A clear turning point can be seen. At its current status quo, techno seems to be a cultural phenomenon with a fascinating mixture of experimental avantgarde music and transnational pop culture. Techno music has been said to be "a soundtrack of the information age". Juergen Laarmann, the editorin-chief of German Frontpage techno magazine, has also written that techno music is only a small part of a broader concept of techno culture. In this case, we have to ask what is techno culture? In Laarmann's opinion all the computer based technologies from computer networks to video games and hypermedia programs represent techno culture. Concerning this point of view, it is interesting to bring up a citation from Bill Nichols' remarkable article "The Work of Culture in the Age of Cybernetic Systems": The Computer is more than an object; it is also an icon and a metaphor that suggests new ways of thinking about ourselves and our environment, new ways of constructing images of what it means to be human and to live in a humanoid world. Cybernetic systems include an entire array of machines and apparatuses that exhibit computational power. Such systems contain a dynamic, even if limited, quotient of intelligence. Telephone networks, communication satellites, radar systems, programmable laser videodiscs, robots, biogenetically engineered cells, rocket guidance systems, videotex networks - all exhibit a capacity to process information and execute actions. They are all "cybernetic" in that they are self-regulating mechanisms or systems within predefined limits and in relation to predefines tasks. Just as the camera has come to symbolise the entirety of the photographic and cinematic processes, the computer has come to symbolise the entire spectrum of networks, systems and devices that exemplify cybernetic of "a utomated but intelligent" behaviour.
Electronic artists rely on technologies developed by disciplines which did not exist just a few decades ago: computer graphics, image processing, computer vision, human-computer interface design, virtual reality and so on. The paper traces the history of these currently prominent image disciplines. My analysis begins in the 1920s when avant-garde artists, inspired by modern engineering, tried to systematically apply its principles to visual communication. To engineer vision meant to be able to affect the viewer with engineering precision, predictability, and effectiveness. Thus, Dziga Vertov championed montage as the most economical kind of communication while Sergei' Eisenstein searched for units to measure communication's efficiency. In its desire to engineer vision, the avant-garde was ahead of its time. The systematic engineering of vision took place only after World War II with the shift to post-industrial society. In post-industrial society, the mental labor of information processing is more important than manual labor. In contrast to a manual worker of the industrial age an operator in a humanmachine system is primarily engaged in the observation of displays which present information in real time about the changing status of a system or an environment, real or virtual: a radar screen tracking a surrounding space; a computer screen updating the prices of stocks; a video screen of a computer game presenting an imaginary battlefield, etc. In short, vision becomes the major instrument of labor, the most productive organ of a worker in a human-machine system. The research into human- machine interfaces — from first computer graphics displays of the late 1940s to today's VR — can be seen as attempts to make the use of vision in this new role as efficient as possible. The importance of information processing for post-industrial society also leads to the necessity to automate as much of it as possible. The ultimate aim is the complete replacement of human cognitive functions by a computer, including the substitution of human vision by computer vision. This is the second trajectory of image research in post-industrial society; from pattern recognition systems of the 1950s to today's computer vision systems. In summary, most of the new research into imaging and vision after World War II can be understood as following two directions: on the one hand, making human vision in its new role of human-machine interface as efficient and as productive as possible; on the other hand, transferring vision from a human to a computer. Why should this historical analysis be of concern to electronic artists? The notion that the artist functions outside of society, history, and industry is a modernist myth. Modernist artists were not only the pioneers of the utilitarian aesthetics of modern industrial design and the techniques of modern advertisement and political propaganda, but they have also pioneered the post-modern engineering of vision, the integration of human and machine in human-machine systems, and the replacement of human vision by computer vision. Today, computer graphics industry is one of the sites of this engineering. Whether computer artists acknowledge or ignore their relationship to this industry, it exists. Acknowledging rather than ignoring this relationship is the first step toward a critical computer art practice.
Art and Technology as the New Avant-garde Machine Vertebral Animal The status and understanding of technology in the computer epoch is very different from 'optic-engine based technology'. As opposed to the human body, an engine is a different body/construction. The human body was understood as a unit, as an undebatable organism. Computer technology can't be separated in reflective categories from the subject and, in a way, from the body. A machine now is not a structure that is alienated from the subject. (Structures don't go on the streets! – slogan of 1968). Technology is intermingled with intimate human life as a part of the 'molecular structure'. Technology seems to be saturated with desire, seduction, 'automata of the body'. It is supposed to be combined with desire with functions of the body and the filters of perception. Assemblage of Representations, Body When making a comparison between the body and the subject we work on the side of the subject. Making analyzable the bodily practices and the unconscious, we are continuing to dissociate the body, from one side and to incorporate it into forms of representations from the other side. The only territory of the body is the terrain of transgression, affect, death, sex. The body is incorporated into language and viewed through a multitude of practices. The practices could be understood as an assemblage of verbal and visual possibilities taken from past and present (marginal and dominant) culture. The Art of the Disembodied Subject a) dislocation. A virtual portrait (in VR games, for example) could include the following: mind, age , character, temperament, style, design, sex. Everything that was articulated, analyzed inside the subject could be terminated and artificially used. A subject is a landscape, open for a multitude of subjects, that can be recombined or segmented for different needs and functions. A subject can't live beyond the cultural media: literature, film, TV. It needs to be disembodied, moved to interfere with other life forms and to be dislocated from the automatic 'natural' body. It has a multitude of images and a freedom of recombining and choosing itself. b) Segmentation and interactivity. Interactivity is different from communication and information. An interactive technology needs a special subject and atactics, that avoids stable codes and emphasizes the process of collaborative acting. The paper will be illustrated with conceptual and video installations by Russian artists and by experts from experimental TV in Russia and the Piazza Virtuale in Kassel.
In his report on the study of pictorial perception among African subjects, William Hudson (1967) says that we take it very much for granted that methods which are only moderately successful in our own cultures will prove equally, if not highly, successful in an alien culture: "We fall into the error of thinking of the black man's mind as a tabula rasa, which we have only to fill with the benefits of our own cultural experience in order to promote whatever objectives we may have in mind. We forget or ignore the fact that the black man possesses his own indigenous culture." During recent years, many artists have addressed the issue of cultural diversity as part of their discussions on Electronic Art. Although the vast majority of artists claim the need for a transcultural approach, most of them have taken a superficial look at this complex problem, turning attention away from some of its more crucial points. Their discourse focuses on the possibilities for providing artistic bridges across different cultures, while their attitudes and works reflect, in many cases, a typical ethnocentric view. The discussion aims at promoting a debate on transcultural issues, as one of the major challenges electronic artists face today. In a world of social, cultural and economic disparities, how can technology meet basic human needs in both developed and developing countries? Which are the dominant cultural values that underlie computer-related technologies today? What is the impact of new electronic technologies on Third World nations? How can we minimize technological dependence and cultural domination, when 30 developed countries – with less than 30% of the world's population – account for approximately 95% of the world's scientific and technological production?
Some artists using electronics take for granted that the art that will be significantly new is going to emerge through new technology. If they look at a painting, they see a medium that doesn't do very much except sit on the wall. Old medium, old ideas. The new media involve intelligent and ambitious systems, radical shifts in our thinking. So it's natural to expect radical and impressive art, too. Working as a painter who also uses computers, I am more sceptical. The art of painting is built on asking questions about what you see, and the process has the feel of a stumbling search. Obsolete? During the sixties and seventies we had exhibitions with "beyond painting" in the title. Kinetic, Op, Minimal, Conceptual, all mixed make-believe and pseudo-science to suggest a future where only "de-materialized" art would be possible. In fact what evaporated wasn't the "art object" but the credibility of this way of thinking, discredited and soon forgotten because the work with real punch and ambition proved to come from painting. As well as finding another country for art – albeit a virtual country – the visual creativity of computing can function just as well within traditional media. The given technology of a painting – flat surface, nothing moving, no sound, no buttons or head-set, not even a plug required – is unimpressive, but it can whirl into life through the touch of colour, the dance of line, the stare of a face. At the Minneapolis conference last year the neighbouring museum held a small exhibition of Matisse's graphic work, its vitality and simplicity a reminder of how far the computer graphic exhibits (mine included) fell short. The technophobia of the mainstream art world is the routine excuse for the failures of computer works to be as impressive as they should be. But on exciting, sophisticated technology is just the starting point. Picasso on an Apple II might still be interesting. Whatever else is possible, a fusion of computer techniques and painterly sensibility shouldn't be discounted too hostily. If there are frontiers in art they certainly aren't where you expect them to be.
Remember Vincent Van Gogh's Painter on His Way to Work, carrying it all on his back? That's where art education is heading. I don't mean the canvas and easel. I mean carrying it all on your back, in the clothes that you wear and in the headband in your hair. 50% pure natural wool 50% optical fibre. I am talking about the interface moving onto and, eventually, into the body. That's your electronic media artist on her way to school. She's wearing the university on her sleeve. We're not talking about a few curriculum changes here. We're not talking about the gradual replacement of some of the library stacks with a few computers. We are talking about the total dissolution, disintegration, and dispersal of Higher Education. From real estate to cyber estate. The university is becoming the interversity. Ask the students. Hundreds of thousands use the Internet daily. When Larry Smart first issued NCSA Mosaic, the network interface to hypermedia browsing, there were ten thousand users in the first three weeks. Now there are over two million. Students are half in school and half in cyberspace. They live between the virtual and the real. They are in the Net more often than out of it. This is the advent of Inter Reality, the space we are most likely to inhabit for the next many years. The ethics of the net, its integrity and inclusiveness, are creating a social behaviour, a morality, which will bring huge bonuses to the real world. I am with Esther Dyson of the Electronic Frontier Foundation when she says that organised political parties won't be needed if open networks "enable people to organise ad hoc, rather than get stuck in some rigid group". The end is to reverse-engineer government, to hack Politics down to its component parts and fix it. She echoes the words of Hazel Henderson writing twenty years before her: "Networks are a combination of invisible college and a modern version of the Committees of Correspondence that our revolutionary forefathers used as vehicles for political change". This post-political process also involves the student in learning to browse, to graze, to hunt for ideas, projects, data, as well as intellectual and artistic collaboration and friendship in all kinds of electronic places, virtual libraries, telecommon rooms and cybercolleges. The students' time in telepresence and virtual learning mode is increasing rapidly. Have you noticed in the studios, libraries and computer suites how every terminal, every interface is occupied, all the time. There are 50 billion adults in the world seeking education in one form or another. That form will be on-line. CD ROM is migrating to big disks at a server near you. The future of education lies in the function of integrated multimedia telecommunication services. But that future could be solely in the hands of big business who simply see "content" as the "value-added" they've got to include to get "market share". I foresee a completely crazy take- over of education by these commercial telecommunications industries unless we can provide models of on-line collaborative creativity and learning whose originality, effectiveness and appeal outshine the more cynical manipulations of the market. We cannot hope to do that in isolation, in our separate colleges, just meeting occasionally, even at conferences as dynamic as this. I want to invoke the sense of a group in which each member has more or less equal power and authority in both access to knowledge and in the means of its reconfiguration and distribution; a group concerned with art and the advancement of learning through collaborative inquiry and shared experience. I want to propose the creation of a Planetary Collegium: non hierarchical , non-linear, and intrinsically interactive; a gathering together, a connecting, an integration of people and ideas. Combining cognition and connectivity, what better creative learning organism could serve our unfolding telematic culture. But by definition such an organism cannot be planned and implemented top down. In fact it is already emerging, bottom-up from the infinity of interactions within the net.
What will be the consequences for art and for education when the digital image is no longer box-dependant? When we no longer have to sit up and beg for information with a typewriter keyboard and TV set? When whole walls of building inside and out can be digitally flooded with sound, colour and light, images and texts flowing in endless transformations, when whole environments respond to our body movements and the articulations of our voices?. When the printed page no longer regiments our thinking into orderly rows of linear data? If the poets, artists and musicians of the world are not ready with strategies to effect this environmental and ecological digitalisation, the politicians, merchants and entrepreneurs will. In this context, Art schools have a clear necessity to put up or shut down. But college is a place for social experiment as much as artistic and intellectual growth. Nothing is more human, warm and convivial than a bunch of kids hanging out on the Internet. As networked virtual reality transports our telepresence, and gives us the tools to reconfigure our own identities, social life will become not only more complex but more imaginative, the scenarios of conviviality outstripping no doubt those of the most fecund scriptwriters of the old movie era. I am happy to admit to possessing a butterfly mind. I'm constantly on the move, physically and virtually, between nodes, between people, between data, between cities, between images, between channels, between texts. I have a psychic restlessness called connectivity. I blame the technology! But then, everyone blames the technology whilst everybody knows that technology has imposed nothing upon us that we did not first desire. Technology arises from our longing to be out-of-body, to see beneath the surface of things and events, to break the bounds of authorised perception, to exceed language, to transform the material world, to recreate ourselves. Technology represents a further embodiment of mind. Minds of course can be vacuous, coldly calculating and analytical, mean and narrow. The same is the case for technology. Minds also can be open, inclusive, loving, spiritual, transcendent. The hope for a cognitive technology lies in this , indeed the hope for a truly human electronic, multimedia and interactive art precisely lies here. The context of this embodiment, the ecology of mind, is at the root of all our considerations about art in the era of interactivity and transformation. With the bionic revelation of our cyborg nature now well rehearsed and understood, it is clear that our art is post-biological also. Again, the educational provision for the development of this post-biological culture must form the overarching agenda of a planetary collegium.
For me, Gelernter can be usefully triangulated with Varela and Stafford because of his project to build a spiritual computer. An emotional computer. Gelernter has valuable things to say about computers and creative thought. "A computer that can't feel can't think". His new book speaks our language. And from his vantage point at Yale, he "rejects the traditional academic subject divisions", and feels "especially at home in the no man's land between art and science". Professor Stafford must feel the same, I would guess, navigating between aesthetics and medicine to chart the emergent revolution in seeing and imaging. Equally, Francisco Varela is not fettered by academic boundaries and roams over an extensive cultural terrain, combining neuroscience with Buddhist theory, and speaking to issues in art as much as society. From these three vantage points, a map of consciousness and the reality we actively construct can be defined. Such vantage points and the new perspectives they cast upon our understanding of ourselves, must become endemic of the learning landscape our Collegium will provide. Not only are students redefining who they are and what they may become, but we too must redefine our identity as teachers, collaborators and guides relative to them. Similarly our tools are changing. While the printed book will continue to be employed, the question becomes how and for what purpose, since it is clear that hypermedia is in many areas set to replace it. The book has come to be the embodiment of authority and its obsolescence as a primary academic tool will cause considerable problems in the academic world. The book is a medium which is fixed and frozen while interactive media are fluid. Post-modernism with its relativist doctrine of layered realities and the slippage of codes has prepared us for the shifting uncertainties of authority, indeed of authorship and ownership of ideas whatever they might constitute, either in science or in art. But the scripting, negotiation and critical evaluation of a hypertext present demands for revolutionary pedagogical change . It's not simply that many colleges are haunted by the ghosts of culture past but that apparitions of the future are emerging on every screen, from every disk, in every network. These apparitions are the constructions of distributed mind, the coming-into-being of new forms of human presence, half real, half virtual, new forms of social relationships, realised in telepresence set in cyberspace. They are challenging the old discarded forms of representation and hermeneutics which still haunt the lecture halls. The students are beginning to treat the university as an interface to Inter Reality as a doorway to a radical constructivism, the way into building their own world. What could be more hopeful than a world designed by the young tested against the on-line wisdom of a global community. This is education in its hyper-Socratic form. There will be no easy transition from the past stability of tradition to the dynamic uncertainty of the immediate future. New priorities must be set in the fiscal affairs of universities. In academic networking and on-line research, change is imminent and difficult times are ahead. "As the Internet expands something will have to give: either the government will stop paying, or politicians will notice that the government is paying and will impose controls, like those imposed by school boards on textbook content or by the FCC on radio and TV broadcasts". The Clipper chip, the cryptography issue, poses serious problems for academic freedom. As Bruce Sterling recently reported from the Conference on Computers, Freedom and Privacy: when the audience was asked by a White House representative who they feared would abuse cryptography more, the US government or criminals, three quarters voted against the government.
Cyberspace: A word from the pen of William Gibson, science fiction writer, circa 1984. An unhappy word, perhaps, if it remains tied to the desperate, dystopic vision of the near future found in the pages of Neuromancer (1984) and Count Zero (1987)—visions of corporate hegemony and urban decay, of neural implants, of a life in paranoia and pain—but a word, in fact, that gives a name to a new stage, a new and irresistible development in the elaboration of human culture and business under the sign of technology. Cyberspace: A new universe, a parallel universe created and sustained by the world’s computers and communication lines. A world in which the global traffic of knowledge, secrets, measurements, indicators, entertainments, and alter-human agency takes on form: sights, sounds, presences never seen on the surface of the earth blossoming in a vast electronic night. Cyberspace: Accessed through any computer linked into the system; a place, one place, limitless; entered equally from a basement in Vancouver, a boat in Portau-Prince, a cab in New York, a garage in Texas City, an apartment in Rome, an office in Hong Kong, a bar in Kyoto, a cafe in Kinshasa, a laboratory on the Moon. Cyberspace: The tablet become a page become a screen become a world, a virtual world. Everywhere and nowhere, a place where nothing is forgotten and yet everything changes. 1 2 Michael Benedikt Cyberspace: A common mental geography, built, in turn, by consensus and revolution, canon and experiment; a territory swarming with data and lies, with mind stuff and memories of nature, with a million voices and two million eyes in a silent, invisible concert of enquiry, dealmaking, dream sharing, and simple beholding. Cyberspace: Its corridors form wherever electricity runs with intelligence. Its chambers bloom wherever data gathers and is stored. Its depths increase with every image or word or number, with every addition, every contribution, of fact or thought. Its horizons recede in every direction; it breathes larger, it complexifies, it embraces and involves. Billowing, glittering, humming, coursing, a Borgesian library, a city; intimate, immense, firm, liquid, recognizable and unrecognizable at once. Cyberspace: Through its myriad, unblinking video eyes, distant places and faces, real or unreal, actual or long gone, can be summoned to presence. From vast databases that constitute the culture’s deposited wealth, every document is available, every recording is playable, and every picture is viewable. Around every participant, this: a laboratory, an instrumented bridge; taking no space, a home presiding over a world . . . and a dog under the table. Cyberspace: Beneath their plaster shells on the city streets, behind their potted plants and easy smiles, organizations are seen as the organisms they are—or as they would have us believe them be: money flowing in rivers and capillaries; obligations, contracts, accumulating (and the shadow of the IRS passes over). On the surface, small meetings are held in rooms, but they proceed in virtual rooms, larger, face to electronic face. On the surface, the building knows where you are. And who. Cyberspace: From simple economic survival through the establishment of security and legitimacy, from trade in tokens of approval and confidence and liberty to the pursuit of influence, knowledge, and entertainment for their own sakes, everything informational and important to the life of individuals—and organizations— will be found for sale, or for the taking, in cyberspace.
Cyberspace: The realm of pure information, filling like a lake, siphoning the jangle of messages transfiguring the physical world, decontaminating the natural and urban landscapes, redeeming them, saving them from the chain-dragging bulldozers of the paper industry, from the dieselsmoke of courier and post office trucks, from jet fuel fumes and clogged airports, from billboards, trashy and pretentious architecture, hour-long freeway commutes, ticket lines, and choked subways. . .from all the inefficiencies, pollutions (chemical and informational), and corruptions attendant to the process of moving information attached to things—from paper to brains—across, over, and under the vast and bumpy surface of the earth rather than letting it fly free in the soft hail of electrons that is cyberspace. Cyberspace as just described—and, for the most part, as described in this book—does not exist. But this states a truth too simply. Like Shangri-la, like mathematics, like every story ever told or sung, a mental geography of sorts has existed in the living mind of every culture, a collective memory or hallucination, an agreed-upon territory of mythical figures, symbols, rules, and truths, owned and traversable by all who learned its ways, and yet free of the bounds of physical space and time. What is so galvanizing today is that technologically advanced cultures—such as those of Japan, Western Europe, and North America—stand at the threshold of making that ancient space both uniquely visible and the object of interactive democracy. Sir Karl Popper, one of this century’s greatest philosophers of science, sketched the framework in 1972. The world as a whole, he wrote, consists of three, interconnected worlds. World 1, he identified with the objective world of material, natural things and their physical properties—with their energy and weight and motion and rest; World 2 he identified with the subjective world of consciousness—with intentions, calculations, feelings, thoughts, dreams, memories, and so on, in individual minds. World 3, he said, is the world of objective, real, and public structures which are the not-necessarily-intentional products of the minds of living creatures, interacting with each other and with the natural World 1. Anthills, birds’ nests, beavers’ dams, and similar, highly complicated structures built by animals to deal with the environment, are forerunners. But many World 3 structures, Popper noted, 4 Michael Benedikt are abstract; that is, they are purely informational: forms of social organization, for example, or patterns of communication. These abstract structures have always equaled, and often surpassed, the World 3 physical structures in their complexity, beauty, and importance to life. Language, mathematics, law, religion, philosophy, arts, the sciences, and institutions of all kinds, these are all edifices of a sort, like the libraries we build, physically, to store their operating instructions, their “programs.” Man’s developing belief in, and effective behavior with respect to, the objective existence of World 3 entities and spaces meant that he could examine them, evaluate, criticize, extend, explore, and indeed make discoveries in them, in public, and in ways that could be expected to bear on the lives of all. They could evolve just as natural things do, or in ways closely analogous. Man’s creations in this abstract realm create their own, autonomous problems too, said Popper: witness the continual evolution of the legal system, scientific and medical practice, the art world, or for that matter, the computer and entertainment industries. And always these World 3 structures feed back into and guide happenings in Worlds 1 and 2. For Popper, in short, temples, cathedrals, marketplaces, courts, libraries, theatres or amphitheaters, letters, book pages, movie reels, videotapes, CDs, newspapers, hard discs, performances, art shows. . . are all physical manifestations—or, should one say, the physical components of—objects that exist more wholly in World 3. They are “objects,” that is, which are patterns of ideas, images, sounds, stories, data. . . patterns of pure information. And cyberspace, we might now see, is nothing more, or less, than the latest stage in the evolution of World 3, with the ballast of materiality cast away—cast away again, and perhaps finally. This book explores the consequences and limits of such a development. But let it be said that, in accordance with the laws of evolution, and no matter how far it is developed, cyberspace will not replace the earlier elements of World 3. It will not replace but displace them, finding, defining, its own niche and causing the earlier elements more closely to define theirs too. This has been the history of World 3 thus far. Nor will virtual reality replace “real reality.” Indeed, real reality—the air, the human body, nature, books, streets. . . who could finish such a list?—in all its exquisite design, history, quiddity, and meaningfulness may benefit from both our renewed appreciation and our no longer asking it to do what is better done “elsewhere.”
I have introduced Popper’s rather broad analysis to set the stage for a closer examination of the origins and nature of our subject, cyberspace. I discern four threads within the evolution of World3. These intertwine. Thread One This, the oldest thread, begins in language, and perhaps before language, with a commonness-of-mind among members of a tribe or social group. Untested by dialogue—not yet brought out “into the open” in this way—this commonnessof-mind is tested and effective nonetheless in the coordinated behavior of the group around a set of beliefs held simply to be “the case:” beliefs about the environment, about the magnitude and location of its dangers and rewards, what is wise and foolhardy, and about what lies beyond; about the past, the future, about what lies within opaque things, over the horizon, under the earth, or above the sky. The answers to all these questions, always “wrong,” and always pictured in some way, are common property before they are privately internalized and critiqued. (The group mind, one might say, precedes the individual mind, and consensus precedes critical exception, as Mead and Vygotsky pointed out.) With language and pictorial representation, established some ten to twenty thousand years ago, fully entering the artifactual world, World 3, these ideas blossom and elaborate at a rapid pace. Variations develop on the common themes of life and death, the whys and wherefores, origins and ends of all things, and these coalesce ecologically into the more or less coherent systems of narratives, characters, scenes, laws, and lessons that we now recognize, and sometimes disparage, as myth. One does not need to be a student of Carl Jung or Joseph Campbell to acknowledge how vital ancient mythological themes continue to be in our advanced technological cultures. They inform not only our arts of fantasy, but, in a very real way, the way we understand each other, test ourselves, and shape our lives. Myths both reflect the “human condition” and create it. Now, the segment of our population most visibly susceptible to myth and most productive in this regard are those who are “coming of age,” the young. Thrust inexorably into a complex and rule-bound world that, it begins to dawn on them, they did not make and that, further, they do not understand, adolescents are apt to reach with some anger and some confusion into their culture’s “collective unconscious”—a world they already possess—for anchorage, guidance, and a base for 6 Michael Benedikt resistance. The boundary between fiction and fact, between wish and reality, between possibility and probability, seems to them forceable; and the archetypes of the pure, the ideal, the just, the good, and the evil, archetypes delivered to them in children’s books and movies, become now, in their struggle towards adulthood, both magnified and twisted. It is no surprise that adolescents, and in particular adolescent males, almost solely support the comic book, science fiction, and videogame industries (and, to a significant extent, the music and movie industries too). These “media” are alive with myth and lore and objectified transcriptions of life’s more complex and invisible dynamics. And it is no surprise that young males, with their cultural bentindeed mission-to master new technology, are today’s computer hackers and so populate the on-line communities and newsgroups. Indeed, just as “cyberspace” was announced in the pages of a science fiction novel, so the young programmers of on-line “MUDS” (Multi-User Dungeons) and their slightly older cousins hacking networked videogames after midnight in the laboratories of MIT’s Media Lab, NASA, computer science departments, and a hundred tiny software companies are, in a very real sense, by their very activity, creating cyberspace. This is not to say that cyberspace is for kids, even less is it to say that it is for boys: only that cyberspace’s inherent immateriality and malleability of content provides the most tempting stage for the acting out of mythic realities, realities once “confined” to drug-enhanced ritual, to theater, painting, books, and to such media that are always, in themselves, somehow less than what they reach for, mere gateways. Cyberspace can be seen as an extension, some might say an inevitable extension, of our age-old capacity and need to dwell in fiction, to dwell empowered or enlightened on other, mythic planes, if only periodically, as well as this earthly one. Even without appeal to sorcery and otherworldy virtual worlds, it is not too farfetched to claim that already a great deal of the attraction of the networked personal computer in general-once it is no longer feared as a usurper of consciousness on the one hand, nor denigrated as a toy or adding machine on the otheris due to its lightning-fast enactment of special “magical” words, instruments, and acts, including those of induction and mastery, and the instant connection they provide to distant realms and buried resources. For the mature as well as the young, then, and for the purposes of art and self-definition as well as rational communications and business, it is likely that cyberspace will retain a good measure of mytho-logic, the exact manifestations of which, at this point, no one can predict. Three of the authors in this book—Michael Heim, Allucquere Rosanne Stone, and David Tomas—take up the cultural-anthropological theme, the latter two with special reference to the changing meaning of the “technophilic” physical body. Chip Morningstar and F. Randall Farmer describe their experiences with on-line games, in particular, LucasFilm’s Habitat. William Gibson’s short piece also makes its contribution at this level, if more directly, as an allegorical work of fiction itself. Thread Two Convolved with the history of myth is the thread of the history of media technology as such, that is, the history of the technical means by which absent and/or abstract entities—events, experiences, ideas—become symbolically represented, “fixed” into an accepting material, and thus conserved through time as well as space. Again, this a fairly familiar story, one whose detailed treatment is far beyond the scope of this introduction and this book. Nevertheless it is one worth rehearsing. It is also a topic that is extremely deep, for the secret of life itself is wrapped up in the mystery of genetic encoding and the replication and motility of molecules that orchestrate each other’s activity. Genes are information; molecules are media as well as motors, so to speak ... But we cannot begin here, where the interests of computation theorists and biologists coincide. Our story best begins with evolved man’s conscious co-option of the physical environment, specifically those parts, blank themselves, that best receive markings-such as sand, wood, bark, bone, stone, and the human body-for the purpose of preserving and delivering messages: signs, not unlike spoors, tracks, or tell-tale colors of vegetation or sky, but now intentional, between man and man, and man and his descendants. What a graceful and inspired step it was, then, to begin to produce the medium, to create smooth plastered walls, thin tablets, and papyrus, and to reduce the labor of marking-carving, chiseling—to the deft movement of a pigmented brush or stylus. As society elaborated itself and as the need to keep records and to educate grew, how much more efficient it was to shrink and conventionalize the symbols themselves, then to crowd them into rows and layers, "paper-thin" scrolls and stacks. 8 Michael Benedikt At this early stage already, the double movement towards the dematerialization of media on the one hand and the reification of meanings on the other is well underway. Against the ravages of time, nonetheless, and to impress the illiterate masses, only massive sculptures, friezes, and reliefs in stone would do. These are what we see today; these are what survive of ancient cultures and impress us still. But it would be wrong therefore to underestimate the traffic of information in more ephemeral media that must have sustained day-to-day life: the scratched clay tablets, the bark shards, graffitied walls, counters, papyri, diagrams in the sand, banners in the wind, gestures, demonstrations, performances, and, of course, the babble of song, gossip, rumor, and instruction that continuously filled the air. Every designed and every made thing was also the story of its use and its ownership, of its making and its maker. This world sounds strangely idyllic. Many of its components, in only slightly updated forms, survive today. It was a period perhaps four thousand years long when objects, even pure icons and symbols, were not empty or ignorable but were real and meaningful, when craftsmanship, consensus, and time were involved in every thing and its physical passage through society. But first, with the development of writing and counting and modes of graphic representation, and then, centuries later, with the invention of the printing press and the spread of literacy beyond the communities of religious scholars and noblemen, the din of ephemeral communications came to be recorded at an unprecedented scale. More important for our story, these “records” came to be easily duplicable, transportable, and broadcastable. Life would never be the same. The implications of the print revolution and the establishment of what Marshall McLuhan called the “Gutenberg galaxy” (in his book of the same name) for the structure and function of technologically advancing societies can hardly be overestimated. Not the least of these implications were (1) the steady, de facto, democratization of the means of idea production and dissemination, (2) the exponential growth of that objective body of scientific knowledge, diverse cultural practices, dreams, arguments, and documented histories called World 3, and (3) the fact that this body, containing both orthodoxies and heresies, could neither be located at any one place, nor be entirely controlled.
However, our double movement did not stop there, as we are all witness today. Although “printed matter” from proclamations to bibles to newspapers could, in principle, be taken everywhere a donkey, a truck, a boat, or an airplane could physically go, there was a limit, namely, time. No news could be fresh days or weeks later. The coordination of goods transportation in particular was a limiting case, for if no message could travel faster than that whose imminent arrival it was to announce.. . then of what value the message? Hence the telegraph, that first “medium” after semaphore, smoke signals, and light-flashing, to connect distant “stations” on the notion of a permanent network. Another related limit was expense: the sheer expenditure of energy required to convey even paper across substantial terrain. The kind of flexible common-mindedness made possible in small communities by the near-simultaneity and zero-expense of natural voice communications, or even rumor and leaflets, collapses at the larger scale. Social cohesion is a function of ideational consensus, and without constant update and interaction, such cohesion depends crucially on early, and strict, education in—and memory of—the architectures, as it were, of World 3. With the introduction of the telephone, both the problem of speed and the problem of expense were largely eliminated. Once wired, energy expenditure was trivial to relay a message, and it was soon widely realized (interestingly only in the 1930s and 40s) that the telephone need not be used like a “voice-telegraph,” which is to say, sparingly and for serious matters only. Rather, it could be used also as an open channel for constant, meaningful, community-creating and business-running interchanges; “one-on-one” interchanges, to be sure, but “many-to-many” over a period of time. Here was a medium, here is a medium, whose communicational limits are still being tested, and these quite apart from what can be accomplished using the telephone system for computer networks. Of course, the major step being taken here, technologically, is the transition, wherever advantageous, from information transported physically, and thus against inertia and friction, to information transported electrically along wires, and thus effectively without resistance or delay. Add to this the ability to store information electromagnetically (the first tape recorder was demonstrated commercially in 1935), and we see yet another significant and evolutionary step in dematerializing the medium and conquering—as they say—space and time. 10 Michael Benedikt But this was paralleled by a perhaps more significant development: wire-less broadcasting, that is, radio and television. Soon, encoded words, sounds, and pictures from tens of thousands of sources could invisibly saturate the world’s “airwaves,” every square millimeter and without barrier. What poured forth from every radio was the very sound of life itself, and from every television set the very sight of it: car chases, wars, laughing faces, oceans, volcanos, crying faces, tennis matches, perfume bottles, singing faces, accidents, diamond rings, faces, steaming food, more faces. . . images, ultimately, of a life not really lived anywhere but arranged for the viewing. Critic and writer Horace Newcomb (1976) calls television less a medium of communication than a medium of communion, a place and occasion where nightly the British, the French, the Germans, the Americans, the Russians, the Japanese.. .settle down by the million to watch and ratify their respective national mythologies: nightly variations on a handful of dreams being played out, over and over, with addicting, tireless intensity. Here are McLuhan’s acoustically structured global villages (though he wished there to be only one), and support for the notion that the electronic media, and in particular television, provide a medium not unlike the air itself—surrounding, permeating, cycling, invisible, without memory or the demand for it, conciliating otherwise disparate and perhaps antagonistic individuals and regional cultures. With cordless and then private cellular telephones, and “remote controls” and then hand-held computers communicating across the airwaves too, the very significance of geographical location at all scales begins to be questioned. We are turned into nomads . . . who are always in touch. All the while, material, print-based media were and are growing more sophisticated too: “vinyl” sound recording (a kind of micro-embossing), color photography, offset lithography, cinematography, and so on. . . the list is long. They became not only more sophisticated but more egalitarian as the general public not only “consumed” ever greater quantities of magazines, billboards, comic books, newspapers, and movies but also gained access to the means of production: to copying machines, cameras, movie cameras, record players, and the rest, each of which soon had its electronic/digital counterpart as well as a variety of hybrids, extensions, and cross-marriages: national newspapers printed regionally from satellite-transmitted electronic data, facsimile transmission, digital preprint and recording, and so on. The end of our second narrative thread is almost at hand. With the advent of fast personal computers, digital television, and high bandwidth cable and radio-frequency networks, so-called postindustrial societies stand ready for a yet deeper voyage into the “permanently ephemeral” (by which I mean, as the reader is well aware, cyberspace). As a number of chapters in this book observe, so-called online community, electronic mail, and information services (USENET, the Well, Compuserve, and scores of others) already form a technological and behavioral beginning. But the significance of this voyage is perhaps best gauged by the almost irrational enthusiasm that today surrounds the topic of virtual reality. Envisaged by science fiction writer/promoter Hugo Gernsback as long ago as 1963 (see Stashower 1990) and explored experimentally by Ivan Sutherland (1968), the technology of virtual reality (VR) stands at the edge of practicality and at the current limit of the effort to create a communication/communion medium that is both phenomenologically engulfing and yet all but invisible. By mounting a pair of small video monitors with the appropriate optics directly to the head, a stereoscopic image is formed before the “user’s” eyes. This image is continuously updated and adjusted by a computer to respond to head movements. Thus, the user finds himself entirely surrounded by a stable, three-dimensional visual world. Wherever he looks he sees what he would see were the world real and around him. This virtual world is either generated in real time by the computer, or it is preprocessed and stored, or it exists physically elsewhere and is “videographed” and transmitted in stereo, digital form. (In the last two cases the technique is apt to be named telepresence rather than virtual reality.) In addition, the user may be wearing stereo headphones. Tracked for head movements, a complete acoustic sensorium is thus added to the visual one. Finally, the user may wear special gloves, and even a whole body suit, wired with position and motion transducers to transmit to others—and to represent to himself—the shape and activity of his body in the virtual world. There is work underway also to provide some form of forcefeedback to the glove or suit so that the user will actually feel the presence of virtual “solid” objects—their weight, texture, and perhaps 12 Michael Benedikt even temperature (see Stewart 1991a for a recent survey, and Rheingold 1991). With a wishful eye cast towards such fictional technologies as the Holodeck, portrayed in the television series “Star Trek, the Next Generation,” devices sketched in such films as Total Recall and Brainstorm, and, certainly, the direct neural connections spoken of in Gibson’s novels, virtual reality/telepresence technology is as close as one can come in reality to entering a totally synthetic sensorium, to immersion in a totally artificial and/or remote world. Much turns on the question of whether this is done alone or in the company of others; and if the latter, of how many, and how. Most of the chapters in this book tackle the question in one form or another. For, engineering questions aside, as the population of a virtual world increases, with it comes the need for consensus of behavior, iconic language, modes of representation, object “physics,” protocols, and design—in a word, the need for cyberspace as such, seen as a general, singularat-some-level, public, consistent, and democratic “virtual world.” Herein lies the very power of the concept. In this volume, the chapters by Wendy A. Kellogg, John M. Carroll, and John T. Richards, by Steve Pruitt and Tom Barrett, by Meredith Bricken, and, again, by Michael Heim look specifically at the remarkable phenomenon of telepresence or “virtuality” as a prime component of the experience of cyberspace. Other authors in this volume imagine a viable cyberspace operating with less completely immersive techniques, although these nonetheless are thought of as considerably advanced over today’s rather simple, low-resolution, two-dimensional graphical and textual interfaces. Thread Two, then, is drawn from the history of communication media. The broad historical movement from a universal, preliterate actuality of physical doing, to an education-stratified, literate reality of symbolic doing loops back, we find. With movies, television, multimedia computing, and now VR, it loops back to the beginning with the promise of a postliterate era, if such can be said; the promise, that is, of “post-symbolic communication” to put it in VR pioneer Jaron Lanier’s words (Lanier 1989, Stewart 1991b). In such an era, languagebound descriptions and semantic games will no longer be required to communicate personal viewpoints, historical events, or technical information. Rather, direct—if “virtual”— demonstration and interactive experience of the “original” material will prevail, or at least be a universal possibility. We would become again “as children,” but this time with the power of summoning worlds at will and impressing speedily upon others the particulars of our experience. In future computer-mediated environments, whether or not this kind of literal, experiential sharing of worlds will supersede the symbolic, ideational, and implicit sharing of worlds embodied in the traditional mechanisms of text and representation remains to be seen. While pure VR will find its unique uses, it seems likely that cyberspace, in full flower, will employ all modes. Thread Three Another narrative, this one is spun out of the history of architecture. The reader may remember that Popper saw architecture as belonging to World 3. This it surely does, for although shelter, beauty, and meaning can be found in “unspoiled” nature, it is only with architecture that nature, as habitat, becomes co-opted, modified, and codified. Architecture, in fact, begins with displacement and exile: exile from the temperate and fertile plains of Africa two million years ago—from Eden, if you will, where neither architecture nor clothing was required—and displacment through emigration from a world of plentiful food, few competitors, and no more kin than the earth would provide for. Rapid climatic change, increasing competition, and exponential population growth was to change early man’s condition irreversibly. To this day, architecture is thus steeped in nostalgia, one might say; or in defiance. Architecture begins with the creative response to climatic stress, with the choosing of advantageous sites for settlements (and the need to defend these), and the internal development of social structures to meet population and resource pressure, to wit: with the mechanisms of privacy, property, legitimation, task specialization, ceremony, and so on. All this had to be carried out in terms of the availability of time, materials, and design and construction expertise. Added to these were the constraints and conventions manufactured by the culture up to that point. These were often arbitrary and inefficient. But always, even as conventions and constraints transformed, and as man passed from hunting and gathering to agrarianism to urbanism, the theme of return to Eden endured, the idea of return to a time of (presumptive) innocence and tribal/familial/national oneness, with each other and with nature. 14 Michael Benedikt I bring up this theme not because it “explains” architecture, but because it is a principle theme driving architecture’s self-dematerialization. Dematerialization? The reader may be surprised. What is architecture, after all, if not the creation of durable physical worlds that can orient generations of men, women, and children, that can locate them in their own history, protect them always from prying eyes, rain, wind, hail, and projectiles.. . durable worlds, and in them, permanent monuments to everything that should last or be remembered? Indeed these are some of architecture’s most fundamental charges; and most sacred among them, as I have argued elsewhere (Benedikt 1987), is architecture’s standard bearing, along with nature, for our sense of what we mean by “reality.” But this should not blind us to a significant countercurrent, one fed by a resentment of quotidian architecture’s bruteness and claustrophobia, which itself is a spilling over of the resentment we feel for our own bodies’ cloddishness, limitations, and final treachery: their mortality. Reality is death. If only we could, we would wander the earth and never leave home; we would enjoy triumphs without risks, eat of the Tree and not be punished, consort daily with angels, enter heaven now and not die. In the name of these unreasonable desires we revere finery and illumination, and reward bravery, goodness, and learning with the assurance of eternal life. As though we could grow wings! As though we could grow wings, we erect gravity-defying cathedrals resplendent with colored windows and niches crowded with allegorical life, create paradisiacal gardens such as those at Alhambra, Versailles, the Taj Mahal, Roan-Ji, erect stadia for games, create magnificent libraries, labyrinths, and observatories, build on sacred mountain tops, make enormous, air conditioned greenhouses with amazing flying-saucer elevators, leap from hillsides strapped to kites, dazzle with gold, chandeliers, and eternally running streams; we scrub and polish and whiten. . . all in a universal, crosscultural act of reaching beyond brute nature’s grip in the here and now. And this with the very materials nature offers us. In counterpoint to the earthly garden Eden (and even to that walled garden, Paradise) then, floatsthe image of the Heavenly City, the new Jerusalem of the book of Revelation. Like a bejeweled, weightless palace it comes down out of heaven itself “its radiance like a most rare jewel, like jasper, transparent” (Revelation 21:9). Never seen, we know its geometry to be wonderfully complex and clear, its twelves and fours and sevens each assigned a set of complementary cosmic meanings. A city with streets of crystalline gold, gates of solid pearl, and no need for sunlight or moonlight to shine upon it for “the glory of God is its light.” In fact, all images of the Heavenly City-East and West-have common features: weightlessness, radiance, numerological complexity, palaces upon palaces, peace and harmony through rule by the good and wise, utter cleanliness, transcendence of nature and of crude beginnings, the availability of all things pleasurable and cultured. And the effort at describing these places, far from a mere exercise in superlatives by medieval monks and painters, continues to this day on the covers and in the pages of innumerable science fiction novels and films. (Think of the mother ship in Close Encounters of the Third Kind.) Here is what it means to be “advanced,” they all say. From Hollywood Hills to Tibet, one could hardly begin to list the buildings actually built and projects begun in serious pursuit of realizing the dream of the Heavenly City. If the history of architecture is replete with visionary projects of this kind, however, these should be seen not as naive products of the fevered imagination, but as hopeful fragments. They are attempts at physically realizing what is properly a cultural archetype, something belonging to no one and yet everyone, an image of what would adequately compensate for, and in some way ultimately justify, our symbolic and collective expulsion from Eden. They represent the creation of a place where we might re-enter God’s graces. Consider: Where Eden (before the Fall) stands for our state of innocence, indeed ignorance, the Heavenly City stands for our state of wisdom, and knowledge; where Eden stands for our intimate contact with material nature, the Heavenly City stands for our transcendence of both materiality and nature; where Eden stands for the world of unsymbolized, asocial reality, the Heavenly City stands for the world of enlightened human interaction, form and information. In Eden the sun rose and set, there were days and nights, wind and shadow, leaf and stone, and all perfumed. The Heavenly City, though it may contain gardens, breathes the crystalline gleam of its own lights, sparkling, insubstantial, laid out like a beautiful equation. Thus, while the biblical Eden may be imaginary, the Heavenly City is doubly imaginary: once, in the conventional sense, because it is not actual, but once again because even if it became actual, because it is information, it could come 16 Michael Benedikt into existence only as a virtual reality, which is to say, fully, only “in the imagination.” The image of The Heavenly City, in fact, is an image of World 3 become whole and holy. And a religious vision of cyberspace. I must now return briefly to the history of architecture, specifically in modern times. After a century of the Industrial Revolution, the turn of the twentieth century saw the invention of hightensile steels, ofsteel-reinforced concrete, and of high-strength glass. Very quickly, and under economic pressure to do more with less, architects seized and celebrated the new vocabulary of lightness. Gone were to be the ponderous piers, the small wooden windows, the painstaking ornament, the draughty chimneys and lanes, the chipping and smoothing and laying! Instead: daring cantilevers, walls reduced to reflective skins, openness, light, swiftness of assembly, chromium. Gone the stairs, the horse-droppings in the street, and the cobbles. Instead, the highway, the bulletlike car, the elevator, the escalator. Gone the immovable monument, instead the demountable exhibition; gone the Acropolis, instead the World’s Fair. In 1924, the great architect Le Corbusier proposed razing half of Paris and replacing it with La Ville Radieuse, the Radiant City, an exercise in soaring geometry, rationality, and enlightened planning, unequaled since. A Heavenly City. By the late 1960s, however, it was clear that the modern city was more than a collection of buildings and streets, no matter how clearly laid out, no matter how lofty its structures or green its parks. The city became seen as an immense node of communications, a messy nexus of messages, storage and transportation facilities, a massive education machine of its own complexity, involving equally all media, including buildings. To no one was this more apparent than to a group of architects in England calling themselves Archigram. Their dream was of a city that built itself unpredictably, cybernetically, and of buildings that did riot resist television and telephones and air conditioning and cars and advertising but accommodated and played with them; inflatable buildings, buildings on rails, buildings like giant experimental theaters with video cameras gliding like sharks through a sea of information, buildings bedecked in neon, projections, lasers beams. . . . These were described in a series of poster-sized drawings called architectural telegrams, which were themselves, perhaps not incidentally, early examples of what multimedia computer screens might look like tomorrow (Cook 1973). Although the group built nothing themselves, they were and are, nonetheless, very influential in the world of architecture.
Now, a complete treatment of the signs of the ephemeralization of architecture and its continuing capitulation to media is outside the scope of this introduction. It occurs on many fronts, from the wild “Disneyfication” of form, to the overly meek accommodation of services. Most interesting, however, is a thread that arises from thinking of architecture itself as an abstraction, a thread that has a tradition reaching back to ancient Egypt and Greece and the coincidence of mathematical knowledge with geometry and hence correct architecture. As late as the eighteenth century, architects were also scientists and mathematicians; witness Andrea Palladio, Sir Christopher Wren, and before them, of course, Leonardo da Vinci and Leon Battista Alberti. From the 1920s till the 1960s, the whole notion that architecture is about the experiential modulation of space and time—that it is “four dimensional”—captivated architectural theory, just as it had captivated a generation of artists in the 20s and 30s (Henderson 1983). This was something conceptually far beyond the simple mathematics of good proportions, even of structural engineering. It is an idea that still has force. Then too there is the tradition of architecture seen for its symbolic content; that is, for not only the way it shapes and paces information fields in general (the emanations of faces, voices, paintings, exit signs, etc.) but the way buildings carry meaning in their anatomy, so to speak, and in what they “look like.” After five thousand years, the tradition is very much alive as part of society’s internal message system. In recent years, however, the architectural “message system” has taken on a life of its own. Not only have architectural drawings generated an art market in their own right—as illustrated conceptual art, if you will—but buildings themselves have begun to be considered as arguments in an architectural discourse about architecture, as propositions, narratives, and inquiries that happen, also, to be inhabitable. In its most current avant-garde guise, the movement goes by the name of Deconstructivism, or Post-Structuralism (quite explicitly related to the similarly named movements in philosophy and literary criticism). Its interests are neither in the building as an object of inhabitation nor as an object of beauty, but as an object of information, a collection of ciphers and “moves,” junctions and disjunctions, reversals and iterations, metaphorical woundings and healings, and so on, all to be “read.” This would be of little interest to us here were it not an 18 Michael Benedikt indication of how far architecture can go towards attempting to become pure demonstration, and intellectual process, and were it not fully a part of the larger movement I have been describing. (And we should remember that, as a rule, today’s avantgarde informs tomorrow’s practice. See Betsky 1990.) But there is a limit to how far notions of dematerialization and abstraction can go and still help produce useful and interesting, real architecture. That limit has probably been reached, if not overshot (Benedikt 1987). And yet the impetus toward the Heavenly City remains. It is to be respected; indeed, it can usefully flourish. . . in cyberspace. The door to cyberspace is open, and I believe that poetically and scientifically minded architects can and will step through it in significant numbers. For cyberspace will require constant planning and organization. The structures proliferating within it will require design, and the people who design these structures will be called cyberspace architects. Schooled in computer science and programming (the equivalent of “construction”), in graphics, and in abstract design, schooled also along with their brethren “real-space” architects, cyberspace architects will design electronic edifices that are fully as complex, functional, unique, involving, and beautiful as their physical counterparts if not more so. Theirs will be the task of visualizing the intrinsically nonphysical and giving inhabitable visible form to society’s most intricate abstractions, processes, and organisms of information. And all the while such designers will be rerealizing in a virtual world many vital aspects of the physical world, in particular those orderings and pleasures that have always belonged to architecture. Two chapters in this volume “come out of” architecture, my own and Marcos Novak’s. My chapter attempts to discuss cyberspace in terms of certain basic design principles and then show some visualized examples; Novak discusses the idea of cyberspace as a poetic mediuim that, among other things, creates a “liquid architecture,” an architecture of information, being less a proposition about designing buildings, of course, than a prelude as to how we might evolve legible forms in the context of a user-driven and self-organizing cyberspace system. Thread Four This thread is drawn from the larger history of mathematics. It is the line of arguments and insights that revolve around (1) the propositions of geometry and space, (2) the spatialization of arithmetical/algebraic operations, and (3) reconsideration of the nature of space in the light of (2). Since Artistotle, operating alongside this “spatial-geometrical” thread in mathematics has been a complementary one, that is, the development of symbolic logic, algebraic notation, calculus, finite mathematics, and so on, to modern programming languages. I say “complementary” because these last-named subjects could (and can still) proceed purely symbolically, with little or no geometrical, spatial interpretation; algebra, number theory, computation theory, logic. . . these are symbolic operations upon symbolic operations and have a life of their own. In practice, of course, diagrams, which are spatial and geometrical, and symbol strings (mathematical notation, language) are accepted as mutually illuminating representations and are considered together. But the distinction between them, and the tension, still remain. There are those who think most easily and naturally in symbolic sequences, and linear operations upon them; there are those who think most easily and naturally in shapes, actions, and spaces. Apparently more than one type of intelligence is involved here (West 1991, Gardner 1983, Hadamard 1945). Be this as it may, cyberspace clearly is premised upon the desirability of spatialization per se for the understanding of information. Certainly, it extends the current paradigm in computing of “graphic user interfaces” into higher dimensions and more involving experiences, and it extends current interest, as evidenced by the popularity of Edward Tufte’s books (1983, 1990), in “data cartography” in general and in the field of scientific visualization. But, more fundamentally, cyberspace revivifies and then extends some of the more basic techniques and questions having to do with the spatial nature of mathematical entities, and the mathematical nature ofspatial entities, that lie at the heart of what we consider both real and measurable. Rigorous reasoning with shape—deductive geometry—began, as we all know, in ancient Greece with Thales around 600 B.C., continuing through 225 B.C. with Pythagoras, Euclid, and Apollonius. The subject was twin: (1) the nature (and methods of construction) of the idealized forms studied—basically lines, circles, regular polygons and polyhedra, although Apollonius began work on conic sections—and (2) the nature of perfect reasoning itself, which the specifiability and universality of 20 Michael Benedikt geometrical operations seemed to exemplify. The results of geometrical study had practical use in building and road construction, land surveying, and what we today call mechanical engineering. Its perfection and universality also supported the casting of astrological/cosmological models along geometrical lines. The science and art of geometry has developed sporadically since, receiving its last major “boost” of renewed interest—after Kepler and Newton—in the late nineteenth century, with Bolyai and Lobatchevsky’s discovery of non-Euclidean geometry. Soon, however, with the concept of pure topology and the discovery of consistent geometries of higher dimensionality than three, first Euclidean geometry and then geometry in general began to lose something of its luster as a science wherein significant new discoveries could be made. All statements of visual geometrical insight, it seemed, could be studied more generally and accurately in the symbolic/algebraic language of analytical mathematics—final fruit of Descartes’ project in La Géométrie, which was precisely to show how the theorems of geometry could be transcribed into analytical (algebraic) form. Of course the linkage, once made, between geometry and algebra, space and symbol, form and argument, is a two-way one. Descartes had both “algebraized” geometry and “geometrized” algebra. (And it is this second movement that is of most interest to us here.) With one profound invention, he had built the conceptual bridge we today call the Cartesian coordinate system. Here was the insight: just as the positions of points in natural, physical space could be encoded, specified, by a trio of numbers, each referring to a distance from a common but arbitrary origin in three mutually orthogonal directions, so too could the positions of points in a “mathematical space” where the “distances” are not physical distances but numerical values, derived algebraically, of the solution of equations of (up to) three variables. In this way, thousands of functions could accurately be “graphed” and made visible. Today, procedures based on Descartes’ insight are a commonplace, taught even at good elementary schools. But this should not mask the power of the implicit notion that space itself is something not necessarily physical: rather that it is a “field of play” for all information, only one of whose manifestations is the gravitational and electromagnetic field of play that we live in, and that we call the real world. Perhaps no examples are more vivid than the beautiful forms that emerge from simple recursive equations—the new science of “fractals”—and recent discoveries of “strange attractors,” objects of coherent geometry and behavior that “exist” only in mathematical spaces (coordinate systems with specially chosen coordinates) and that economically map/describe/prescribe the behavior of complex, chaotic, physical systems. Which reality is the primary one? we might fairly ask. Actually, why choose? Modern physicists are sanguine: Minkowski had shown the utility of mapping time together with space, Hamiltonian mechanics lent themselves beautifully to visualizing the dynamics of a physical system in n-dimensional state or phase space where a single point represents the entire state of the system, and quantum mechanics seems to play itself out in the geometrical behavior of vectors in Hilbert space, in which one or more of the coordinates are “imaginary” (see Penrose 1989 for a recent explication). In the meantime, the more common art of diagrams and charts proliferatedfrom old maps, schedules, and scientific treatises, to the pages of modern economics primers, advertisements, and boardroom “business graphics.” Many of these representations are in fact hybrids, mixing physical, energic or spatiotemporal, coordinates with abstract, mathematical ones, mixing histories with geographies, simple intervallic scales with exponential ones, and so on. The practice of diagramming (surely one whose origins are earlier than writing) continues too, today enhanced by the mathematics of graph theory with its combinatorial and network techniques to analyze and optimize complex processes. What, we may ask, is the ontological status of such representations? All of them—from simple bar charts and organizational “trees” through matrices, networks, and “spreadsheets” to elaborate, multidimensional, computer-generated visualizations of invisible physical processes—all of these, and all abstract phase-, state-, and Hilbert-space entities, seem to exist in a geography, a space, borrowed from, but not identical with, the space of the piece of paper or computer screen on which we see them. All have a reality that is no mere picture of the natural, phenomenal world, and all display a physics, as it were, from elsewhere. What are they, indeed? Neither discoveries nor inventions, they are of World 3, entities themselves evolved by our intelligence in the world of things and of each other. They represent first evidence of a continent about which we have hitherto communicated only in sign language, 22 Michael Benedikt a continent “materializing,” in a way. And at the same time they express a new etherealization of geography. It is as though, in becoming electronic, our beautiful old astrolabes, sextants, surveyor’s compasses, observatories, orreries, slide rules, mechanical clocks, drawing instruments and formwork, maps and plans—physical things all, embodiments of the purest geometry, their sole work to make us at home in space—become environments themselves, the very framework of what they once only measured. The contributions by Tim McFadden, Carl Tollander, and Alan Wexelblat are partially woven from this thread, as is a good part of my own. McFadden examines the idea of cyberspace as an informational Indra’s Net, a universe of pointlike beads, infinite in number, each of which reflects all the others. Here cyberspace is an evolving, fourdimensional hologram of itself. (It was this ancient Hindu image of Indra’s Net that also informed Leibniz’s Monadology, as Heim discusses.) Tollander introduces Edelman’s Neuronal Group Selection theory into the design of a noncentralized system of computational “engines” to create cyberspaces that can evolve in a “natural” way. (Novak also discusses this notion). Wexelblat examines the nature of coordinates in abstract spaces in general, in modern personal computing, and then, extrapolated, in terms of cyberspace specifically considered as an outgrowth of these. My account of the intertwining “threads” that seem to lead to cyberspace is, of course, impressionistic and incomplete, and not just for lack of space in this introduction. Cyberspace itself is an elusive and future thing, and one can hardly be definitive at this early stage. But it is also clear that the “threads” themselves are made of threads, and that there are others. For example, the history of art into modern times tells a related story, fully involving mythology, changing media, a relationship to architecture, logic, and so on. It is a thread I have not described, and yet the contribution of artists—visual, musical, cinematic—to the design of virtual worlds and cyberspace promises to be considerable, as Nicole Stenger, poet and animation artist, attests in this volume. Similarly, the story of progress in telecommunications and computing technology—the miniaturizations, speeds, and economies, the new materials, processes, interfaces and architectures—is a thread in its own right, with its own thrusts and interests in the coming-to-be of cyberspace. This story is well chronicled elsewhere (Rheingold 1985, Gilder 1988). Then there is the sociological story, and the economic one, the linguistic one, even the biological one . . . and one begins to realize that every discipline can have an interest in the enterprise of creating cyberspace, a contribution to make, and a historical narrative to justify both. How could it be otherwise? We are contemplating the arising shape of a new world, a world that must, in a multitude of ways, begin, at least, as both an extension and a transcription of the world as we know it and have built it thus far. Another reason that my account is impressionistic and incomplete, however, is that the very metaphor of threads is too tidy and cannot support all that needs to be said. Scale aside, something deeper and more formless is going on. Consider: if information is the very stuff of space and time, what does it mean to manufacture information, and what does it mean to transfer it at ever higher rates between spatiotemporally distinct points, and thus dissolve their very distinctness? With mature cyberspaces and virtual reality technology, this kind of warpage, tunneling, and lesioning of the fabric of reality will become a perceptual, phenomenal fact at hundreds of thousands of locations, even as it falls short of complete, quantum level, physical achievement. Today intellectual, tomorrow practical, one can only guess at the implications. Finally, my “narrative of threads” has not done justice to the authors represented in this volume. Each has their own perspective, expertise, and interest, and each draws inspiration from matters I have not mentioned, and stories I have not sketched or have only touched upon. Rather than extend this introduction with fuller discussion of each chapter, however, I recommend that the reader turn to them forthwith! Many are expanded and revised versions of presentations made at The First Conference on Cyberspace.1 Others are written especially for the present collection.2 All the authors address themselves to the topic with extraordinary seriousness, acumen, and enthusiasm, even though—and perhaps because—the varieties of cyberspace they imagine, describe, and sometimes criticize, do not yet exist. Indeed, the very definition of cyberspace may well be in their hands (or yours, dear reader). Of this much, one can be sure: the advent of cyberspace will have profound effects on so-called postindustrial culture, and the 1Held on May 4 and 5, 1990, at The University of Texas at Austin. The Second (International) Conference on Cyberspace was held April 18-19, 1991, at The University of California at Santa Cruz. 2Gibson, Tomas, Stone, and Wexelblat. 24 Michael Benedikt material and economic rewards for those who first and most properly conceive and implement cyberspace systems will be enormous. But let usset aside talk of rewards. With this volume, with these “firststeps,” let us begin to face the perplexities involved in making the unimaginable imaginable and the imaginable real. Let the ancient project that is cyberspace continue.

Codes—by name and by matter—are what determine us today, and what we must articulate if only to avoid disappearing under them completely. They are the language of our time precisely because the word and the matter code are much older, as I will demonstrate with a brief historical regression. And have no fear: I promise to arrive back at the present. Imperium Romanum Codes materialize in processes of encryption, which is, according to Wolfgang Coy’s elegant defi nition, “from a mathematical perspective a mapping of a fi - nite set of symbols of an alphabet onto a suitable signal sequence.”1 This defi nition clarifi es two facts. Contrary to current opinion, codes are not a peculiarity of computer technology or genetic engineering; as sequences of signals over time they are part of every communications technology, every transmission medium. On the other hand, much evidence suggests that codes became conceivable and feasible only after true alphabets, as opposed to mere ideograms or logograms, had become available for the codifi cation of natural languages. Those alphabets are systems of identically recurring signs of a countable quantity, which map speech sounds onto letters more or less one- to- one and, hopefully, completely. A vocalic alphabet of a type such as Greek,2 justly praised for being the “fi rst total analysis of a language,”3 does appear to be a prerequisite for the emergence of codes, and yet, not a suffi cient one. For what the Greeks lacked (leaving out of consideration sporadic allusions in the work of Aischylos, Aenas, Tacticus, and Plutarch to the use of secret writing4 was that second prerequisite of all coding, namely, developed communications technology. It is anything but coincidental that our reports of the fi rst secret message systems coincide with the rise of the Roman Empire. In his Lives of the Caesars, Suetonius—who himself served as secret scribe to a great emperor—recounts discovering encrypted letters among the personal fi les left behind by both the divine Caesar and the divine Augustus. Caesar contented himself with moving all the letters of the Latin alphabet by four places, thus writing D instead of A, E instead of B, and so forth. His adoptive son Augustus, by contrast, is reported to have merely skipped one letter, but a lack of mathematical discernment led him to replace the letter X, the last in his alphabet, by a double A.5 The purpose was obvious: When read aloud by those not called upon to do so (and Romans were hardly the most literate of people), a stodgy jumble of consonants resulted. And as if such innovations in matters of encryption were not suffi cient, Suetonius attributes to Caesar another invention immediately beforehand—that of having written in several columns, or even separate pages, reports to the Roman Senate on the Gallic campaign. Augustus is credited with the illustrious deed of creating, with riders and relay posts, Europe’s fi rst strictly military express- mail system.6 In other words, the basis on which command, code, and communications technology coincided was the Empire, as opposed to merely the Roman Republic or shorthand writers like Cicero. Imperium is the name of both the command and its effect: the world empire. “Command, control, communications, intelligence” was also the Pentagon’s imperial motto until very recently, when, due to the coincidence of communication technologies and Turing machines it was swapped for C4 —“command, control, communication, computers”—from Orontes to the Scottish headland, from Baghdad to Kabul. It was the case, however, that imperia, the orders of the Emperor, were also known as codicilla, the word referring to the small tablets of stripped wood coated with wax in which letters could be inscribed. The etymon codex for its part—caudex in Old Latin and related to the German verb hauen (to hew)—in the early days of the Empire assumed the meaning of “book,” whose pages could, unlike papyrus scrolls, for the fi rst time be leafed through. And that was how the word that interests us here embarked on its winding journey to the French and English languages. From Imperator Theodosius to Empereur Napoleon, “code” was simply the name of the bound book of law, and codi- fi cation became the word for the judicial- bureaucratic act needed to arrest in a single collection of laws the torrents of imperial dispatches or commands that for centuries had rushed along the express routes of the Empire. Message transmission turned into data storage,7 pure events into serial order. And even today the Codex Theodosius and Codex Iustinianus continue to bear a code of ancient European rights and obligations in those countries where Anglo- American common law does not happen to be sweeping the board. In the Corpus Iuris, after all, copyrsights and trademarks are simply meaningless, regardless of whether they protect a codex or a code. Nation- States The question that remains is why the technical meaning of the word “code” was able to obscure the legal meaning to such a degree. As we know, contemporary legal systems regularly fail to grasp codes in the fi rst place and, in consequence, to protect them, be it from robbers and purchasers or, conversely, from their discoverers and writers. The answer seems to be simple. What we have been calling a code since the secret writings of Roman emperors to the arcana imperii of the modern age was known as a “cipher” from the late Middle Ages onward. For a long time the term code was understood to refer to very different cryptographic methods whereby words could still be pronounced, but obscure or innocuous words simply replaced the secret ones. Cipher, by contrast, was another name for the zero, which at that time reached Europe from India via Baghdad and put sifr (Arabic: “emptiness”) into mathematical- technical power. Since that time, completely different sets of characters have been devised (in sharp contrast to the invention of Greek for speech sounds and numbers: on one side of language the alphabet of the people, on the other the numbers of the bearers of secrets—the name of which spelled the Arabic sifr once again. Separate character sets, however, are productive. Together they brew wondrous creatures that would never have occurred to the Greeks or Romans. Without modern algebra there would be no encoding; without Gutenberg’s printing press, no modern cryptology. In 1462 or 1463, Battista Leone Alberti, the inventor of linear perspective, was struck by two plain facts. First, that the frequency of occurrence of phonemes or letters varies from language to language, a fact which is proved, according to Alberti, by Gutenberg’s letter case. From the frequency of shifted letters as they were written by Caesar and Augustus, cryptanalysis can heuristically derive the clear text of the encrypted message. Second, it is therefore insuffi cient to encrypt a message by shifting all the letters by the same number of places. Alberti’s proposal that every new letter in the clear text be accompanied by an additional place- shift in the secret alphabet was followed up until World War II.8 One century after Alberti, François Viète, the founder of modern algebra, and also a cryptologist in the service of Henry IV, intertwined number and letter more closely still. Only since Viète have there been equations containing unknowns and universal coef- fi cients written with numbers encoded as letters.9 This is still the work method of anybody who writes in a high- level programming language that likewise allocates variables (in a mathematically more or less correct manner) to alpha numeric signs, as in equations. On this basis—Alberti’s polyalphabetic code, Viète’s algebra, and Leibniz’ differential calculus—the nation- states of the modern age were able to technically approach modernity. Global Message Traffi c Modernity began, however, with Napoleon. As of 1794, messengers on horseback were replaced by an optical telegraph which remote- controlled France’s armies with secret codes. In 1806, the laws and privileges surviving from the old days were replaced by the cohesive Code Napoléon. In 1838, Samuel Morse is said to have inspected a printing plant in New York in order—taking a leaf from Alberti’s book—to learn from the letter case which letters occurred most frequently and therefore required the shortest Morse signals.10 For the fi rst time a system of writing had been optimized according to technical criteria—that is, with no regard to semantics—but the product was not yet known as Morse code. The name was bestowed subsequently in books known as Universal Code Condensers, which offered lists of words that could be abbreviated for global cable communications, thus reducing the length, and cost, of telegrams, and thereby encrypting the sender’s clear text for a second time. What used to be called deciphering and enciphering has since then been referred to as decoding and encoding. All code processed by computers nowadays is therefore subject to Kolmogorov’s test: Input is bad if it is longer than its output; both are equally long in the case of white noise; and a code is called elegant if its output is much longer than itself. The twentieth century thus turned a thoroughly capitalist money- saving device called “code condenser” into highest mathematical stringency. The Present Day—Turing All that remains to ask is how the status quo came about or, in other words, how mathematics and encryption entered that inseparable union that rules our lives. That the answer is Alan Turing should be well known today. The Turing machine of 1936, as the principle controller of any computer, solved a basic problem of the modern age: how to note with fi nitely long and ultimately whole numbers the real, and therefore typically infi nitely long, numbers on which technology and engineering have been based since Viète’s time. Turing’s machine proved that although this task could not be accomplished for all real numbers, it was achievable for a crucial subset, which he dubbed computable numbers.11 Since then a fi nite quantity of signs belonging to a numbered alphabet which can, as we know, be reduced to zero and one, has banished the infi nity of numbers. No sooner had Turing found his solution than war demanded its cryptanalytical application. As of spring 1941 in Britannia’s Code and Cipher School, Turing’s proto- computers almost decided the outcome of the war by successfully cracking the secret codes of the German Wehrmacht, which, to its own detriment, had remained faithful to Alberti. Today, at a time when computers are not far short of unravelling the secrets of the weather or the genome—physical secrets, that is to say, and increasingly often biological ones, too—we all too often forget that their primary task is something different. Turing himself raised the question of the purpose for which computers were actually created, and initially stated as the primary goal the decoding of plain human language: Of the above possible fi elds the learning of languages would be the most impressive, since it is the most human of these activities. This fi eld seems, however, to depend rather too much on sense organs and locomotion to be feasible. The fi eld of cryptography will perhaps be the most rewarding. There is a remarkably close parallel between the problems of the physicist and those of the cryptographer. The system on which a message is enciphered corresponds to the laws of the universe, the intercepted messages to the evidence available, the keys for a day or a message to important constants which have to be determined. The correspondence is very close, but the subject matter of cryptography is very easily dealt with by discrete machinery, physics not so easily.12 Conclusions Condensed into telegraphic style, Turing’s statement thus reads: Whether everything in the world can be encoded is written in the stars. The fact that computers, since they too run on codes, can decipher alien codes is seemingly guaranteed from the outset. For the past three- and- a- half millennia, alphabets have been the prototype of everything that is discrete. But it has by no means been proven that physics, despite its quantum theory, is to be computed solely as a quantity of particles and not as a layering of waves. And the question remains whether it is possible to model as codes, down to syntax and semantics, all the languages that make us human and from which our alphabet once emerged in the land of the Greeks. This means that the notion of code is as overused as it is questionable. If every historical epoch is governed by a leading philosophy, then the philosophy of code is what governs our own, and so code—harking back to its root, “codex”—lays down the law for one and all, thus aspiring to a function that was, according to the leading philosophy of the Greeks, exercised exclusively by Aphrodite.13 But perhaps code means nothing more than codex did at one time: the law of precisely that empire which holds us in subjection and forbids us even to articulate this sentence. At all events, the major research institutions that stand to profi t most from such announcements proclaim with triumphant certainty that there is nothing in the universe, from the virus to the Big Bang, which is not code. One should therefore be wary of metaphors that dilute the legitimate concept of code, such as when, for instance, in the case of DNS, it was not possible to fi nd a one- to- one correspondence between material elements and information units as Lily Ray discovered in the case of bioengineering. As a word that in its early history meant “displacement” or “transferral”—from letter to letter, from digit to letters, or vice versa—code is the most susceptible of all to faulty communication. Shining in the aura of the word code one now fi nds sciences that do not even master their basic arithmetic or alphabet, let alone cause something to turn into something different as opposed to merely, as in the case of metaphors, go by a different name. Therefore, only alphabets in the literal sense of modern mathematics should be known as codes, namely one- to- one, fi nite sequences of symbols, kept as short as possible but gifted, thanks to a grammar, with the incredible ability to infi nitely reproduce themselves: Semi- Thue groups, Markov chains,14 Backus- Naur forms, and so forth. That, and that alone, distinguishes such modern alphabets from the familiar one that admittedly spelled out our languages and gave us Homer’s poetry15 but cannot get the technological world up and running the way computer code now does. For while Turing’s machine was able to generate real numbers from whole numbers as required, its successors have—in line with Turing’s daring prediction—taken command.16 Today, technology puts code into the practice of realities, that is to say: it encodes the world. I cannot say whether this means that language has already been vacated as the House of Existence. Turing himself, when he explored the technical feasibility of machines learning to speak, assumed that this highest art, speech, would be learned not by mere computers but by robots equipped with sensors, effectors, that is to say, with some knowledge of the environment. However, this new and adaptable environmental knowledge in robots would remain This means that the notion of code is as overused as it is questionable. If every historical epoch is governed by a leading philosophy, then the philosophy of code is what governs our own, and so code—harking back to its root, “codex”—lays down the law for one and all, thus aspiring to a function that was, according to the leading philosophy of the Greeks, exercised exclusively by Aphrodite.13 But perhaps code means nothing more than codex did at one time: the law of precisely that empire which holds us in subjection and forbids us even to articulate this sentence. At all events, the major research institutions that stand to profi t most from such announcements proclaim with triumphant certainty that there is nothing in the universe, from the virus to the Big Bang, which is not code. One should therefore be wary of metaphors that dilute the legitimate concept of code, such as when, for instance, in the case of DNS, it was not possible to fi nd a one- to- one correspondence between material elements and information units as Lily Ray discovered in the case of bioengineering. As a word that in its early history meant “displacement” or “transferral”—from letter to letter, from digit to letters, or vice versa—code is the most susceptible of all to faulty communication. Shining in the aura of the word code one now fi nds sciences that do not even master their basic arithmetic or alphabet, let alone cause something to turn into something different as opposed to merely, as in the case of metaphors, go by a different name. Therefore, only alphabets in the literal sense of modern mathematics should be known as codes, namely one- to- one, fi nite sequences of symbols, kept as short as possible but gifted, thanks to a grammar, with the incredible ability to infi nitely reproduce themselves: Semi- Thue groups, Markov chains,14 Backus- Naur forms, and so forth. That, and that alone, distinguishes such modern alphabets from the familiar one that admittedly spelled out our languages and gave us Homer’s poetry15 but cannot get the technological world up and running the way computer code now does. For while Turing’s machine was able to generate real numbers from whole numbers as required, its successors have—in line with Turing’s daring prediction—taken command.16 Today, technology puts code into the practice of realities, that is to say: it encodes the world. I cannot say whether this means that language has already been vacated as the House of Existence. Turing himself, when he explored the technical feasibility of machines learning to speak, assumed that this highest art, speech, would be learned not by mere computers but by robots equipped with sensors, effectors, that is to say, with some knowledge of the environment. However, this new and adaptable environmental knowledge in robots would remain obscure and hidden to the programmers who started them up with initial codes. The so- called “hidden layers” in today’s neuronal networks present a good, if still trifl ing, example of how far computing procedures can stray from their design engineers, even if everything works out well in the end. Thus, either we write code that in the manner of natural constants reveals the determinations of the matter itself, but at the same time pay the price of millions of lines of code and billions of dollars for digital hardware; or else we leave the task up to machines that derive code from their own environment, although we then cannot read—that is to say: articulate—this code. Ultimately, the dilemma between code and language seems insoluble. And anybody who has written code even only once, be it in a high- level programming language or assembly, knows two very simple things from personal experience. For one, all words from which the program was by necessity produced and developed only lead to copious errors and bugs; for another, the program will suddenly run properly when the programmer’s head is emptied of words. And in regard to interpersonal communications, that can only mean that self- written code can scarcely be passed on with spoken words. May myself and my audience have been spared such a fate in the course of this essay.


My semi-technical introduction to computer graphics will, however, provide only a half-answer, one that, in particular, cannot address the necessary comparison between paintings and computer images or between subtractive and additive color mixing. Simplified accordingly, a computer image is a two-dimensional additive mixture of three base colors shown in the frame, or parergon, of the monitor housing. Sometimes the computer image as such is less apparent, as in the graphic interface of the newfangled operating systems, sometimes rather more, as in "images" in the literal sense of the word. At any rate, the generation of 2000 likely subscribes to the fallacy-backed by billions of dollars-that computers and computer graphics are one and the same. Only aging hackers harbor the trace of a memory that it wasn't always so. There was a time when the computer screen's display consisted of white dots on an amber or green background, as if to remind us that the techno-historical roots of computers lie not in television, but in radar, a medium of war.
The computer image derives precisely this addressability from early-warning systems, even if it has replaced the polar coordinates _ I of the radar screen with Cartesian coordinates. In contrast to the semi-analog medium of television, not only the horizontal lines but also the vertical columns are resolved | lI . . into basic units. The mass of these so-called- 1 I "pixels" forms a two-dimensional matrix - that assigns each individual point of the " i* ] image a certain mixture of the three base colors: red, green, and blue. The discrete, or digital, nature of both the geometric coordinates and their chromatic values makes possible the magical artifice that separates computer graphics from film and television. Now, for the first time in the history of optical media, it is possible to address a single pixel in the 849th row and 720th column directly without having to run through everything before and after it. The computer image is thus prone to falsification to a degree that already gives television producers and ethics watchdogs the shivers; indeed, it is forgery incarnate. It deceives the eye, which is meant to be unable to differentiate between individual pixels, with the illusion or image of an image, while in truth the mass of pixels, because of its thorough addressability, proves to be structured more like a text composed entirely of individual letters. For this reason-and for this reason only-it is no problem for a computer monitor to switch between text and graphics modes. The twofold digitality of coordinates and color value, however, creates certain problem areas, of which at least three should be mentioned.
Third, the digitality of computer graphics creates a problem unknown to computer music. In an essay on time axis manipulation, I have previously tried to show the leeway produced by the fact that the digital sampling of any given musical sequence falls into three elements (a triad is familiar to us through Giuseppe Peano's theory of natural numbers): an event or state of a millisecond's duration, its predecessor, and its successor.2 These three can be integrated or differentiated, exchanged or scrambled until the limits of modern academic and popular music are truly explored. In principle-and that means, unfortunately, given an exponentially higher processing time-these tricks could be adapted from digital music's single dimension to the two dimensions of digital images. The result, however, tends to be so chaotic that it is as if perception were regressing to pure sensation a la David Hume or Kaspar Hauser. The reason for this is as fundamental as it is non-trivial. Every image (in the sense of art, not of mathematics) has a top and a bottom, a left and a right. Pixels, insofar as they are constructed algebraically as two-dimensional matrices and geometrically as orthogonal grids, necessarily have more than one neighbor. In the heroic beginnings of computer science, great mathematicians had to begin by formulating truisms, whence arose W. Ross Ashby's and John von Neumann's concepts of neighboring elements. In the former, a given element is considered to be surrounded only by a cross of neighbors: above, below, left, and right; in the latter, it is surrounded by a square of the above-mentioned orthogonal elements plus four additional diagonal neighbors. A difference that could perfectly describe, if you like, the difference between the urban fabrics of Manhattan and Tokyo, respectively.
Heidegger posed the riddle of perception thus: "in the appearing of things, never do we, either preliminarily or essentially, perceive an onrush of sensations."3 For beings that dwell in language, anything seen or heard shows itself always already as something. For computer-supported image analysis, however, this something-assomething remains a distant theoretical goal, the achievement of which is not even assured. Therefore I would postpone the question of automatic image analysis for symposia on perception to take place not sooner than a decade from now, and limit myself in the following to the problem of automatic image synthesis. I am not concerned, then, with how computers simulate optical perception, but rather only with how they deceive us. For it seems to be precisely this exorbitant capacity that elevates the medium of the computer above all optical media in Western history.
Computer graphics are to these optical media what the optical media are to the eye. Just as the camera lens, literally as hardware, simulates the eye, which is literally wetware. so does software, as computer graphics, simulate hardware. The optical laws of reflection and refraction remain in effect for output devices such as monitors or LCD screens, but the program whose data directs these devices transposes such optical laws as it obeys into algebraically pure logic. These laws are generally, it should be noted from the outset, by no means all the optical laws valid for fields of vision and surfaces, shadows and effects of light; what is played out are these selected laws themselves and not, as in the optical media, just the effects they produce. It's no wonder, then, that art historian Michael Baxandall can go so far as to suggest that computer graphics provide the logical space of which any given perspective painting forms a more or less rich subset.
Conversely, computer graphics, because it is software, consists of algorithms and only of algorithms. The optimal algorithm for automatic image synthesis can be determined just as easily as non-algorithmic image synthesis. It would merely have to calculate all optical, i.e. electromagnetic, equivalencies that quantum electrodynamics recognizes for measurable spaces, for virtual spaces as well; or, to put it more simply, it would have to convert Richard Feynman's three-volume Lectures on Physics into software. Then a cat's fur, because it creates anisotropic surfaces, would shimmer like cat's fur; then streaks in a wine glass, because they change their refraction index at each point, would turn the lights and things behind them into complete color spectra. Theoretically, nothing stands in the way of such miracles. Universal discrete machines, which is to say, computers, can do anything so long as it is programmable. But it is not just in Rilke's Malte Laurids Brigge but also in quantum electrodynamics that "realities are slow and indescribably detailed."7 The perfect optics could be programmed just barely within a finite time, but, because of infinite monitor waiting times, would have to put off rendering the perfect image. Computer graphics are differentiated from the cheap real-time effects of the visual entertainment media by a capacity to waste time that would rival that of good old painters if its users were just more patient. It is only in the name of impatience that all existing computer graphics are based on idealizations-a term that functions here, unlike in philosophy, as a pejorative.

In all historical accuracy I shall begin with raytracing, if only because it, for the best or worst reasons in the world, is much older than the radiosity algorithm. As Axel Roch will soon make public, the concept of raytracing derives not at all from computer graphics, but rather from its military predecessor: the tracking of enemy airplanes with radar. And as the computer graphics expert Alan Watt has recently shown, raytracing is in fact even more venerable. The first light ray whose refraction and reflections generated a virtual image was constructed in the year of our Lord 1637 by a certain Rene Descartes.9 Eighteen years earlier, in the wartime of November 1619, Descartes had received one illumination and three dreams. The illumination was about a wondrous science-perhaps the analytic geometry he would go on to develop later. The dreams, however, began with a storm that spun Descartes, who was lame on his right side, around his own left leg three or four times. I suspect, however, that the dream and the science are one and the same. In the dream the subject becomes an unextendable point or, better, midpoint, around which one's own body, as a three-dimensional res extensa, describes the geometric figure of a circle. Cartesian philosophy, as is well known, deals with the res cogitans and the res extensa; as is far less well known, analytic geometry deals with algebraically describable movements or surface areas. Descartes made it possible, for the first time in the history of mathematics, not to produce figures like the circle as the drawn likeness of a celestial-geometrical given but rather to construct them as functions of an algebraic variable. The subject as res cogitans took a wild ride, so to speak, through all the functional values of an equation, until in Descartes's initial dream of 1619 the circle (or, in Miinchhausen's ride on the cannonball, the parabola) was described.

To be sure, Heron of Alexandria had already formulated the law of reflection, Willibrord Snell the law of refraction. It remained to Descartes, however, to piece together the path of a single ray of light through the repeated application of both laws. The Cartesian subject comes about through self-application, or, to put it in the terms of computer science, through recursion. Precisely for this reason, Cartesian raytracing never inspired any painter, let alone any optical analog medium. Only computers and, more precisely, computer languages that allow for recursive functions have the processing power to even trace the countless alternative cases or fates of a single light ray in a virtual space full of virtual surfaces. Raytracing programs begin, in the most elementary case, by defining the computer screen as a two-dimensional window onto a virtual three-dimensionality. Then, two iteration loops follow all the lines and columns of this screen until the ray of vision of a virtual eye situated in front of the screen has reached all the pixels. These virtual rays, though, keep wandering behind the pixels in order to explore the various different outcomes. Most of these have the fortune not to collide with a surface, and thus can quickly execute their task of rendering a mere background color such as that of the sky. Other rays, however, find themselves trapped in a transparent glass globe like Descartes's, where they would be subject to an endless series of refractions and reflections if the impatience of computer graphics programs did not limit the maximum allowable recursions. This is necessary if only because a light ray, should it play between two parallel and perfect mirrors, would never stop, while algorithms are all but defined by a finite use of time.

Radiosity is consequently, in contrast to raytracing, an algorithm born of necessity. Only when seen in its formal elegance can integration be defined as the reverse function of differentiation, for the bitter empirical and numerical truth is that it consumes dramatically higher processing time. Radiosity programs have only become feasible since they have stopped promising to solve their linear equation system in a single run-through.1 In more prosaic terms: one starts up the algorithm, contemplates the as yet completely black screen, takes one of the coffee breaks so famous among computer programmers, then returns after one or two hours to have a look at the first passable results of the global light energy distribution. What so-called nature can accomplish in nanoseconds with its parallel calculation drives its alleged digital equivalent to overload.

Had I promised mere recipes instead of a semi-technical introduction to computer graphics, this short text could end here. Fans of interiors would download some radiosity programs, while fans of the open horizon would surf the Net for some raytracing programs. And now that, at least with LINUX, we have the Blue Moon Rendering Tools, the very decision has become moot. This software, no less wondrous than a blue moon, calculates virtual image worlds in the first run-through following global dependencies in the sense of radiosity, but in the second runthrough follows local singularities in the sense of raytracing. It thus promises a coincidentia oppositorum, which cannot be a matter of simple addition given all that has been said above. It would be going too far afield if I were to try to explain why, in the case of such two-step processes, not only the second step must orient itself to the first but, what is nearly impossible, the first must already orient itself to the second. Otherwise, the four possible cases of optical energy transmission couldn't possibly all be taken into consideration.  

The partnership between computers and the visual arts is now at a singular point in its development, emerging from a state of infancy into a first level of maturity. Many artists have used the new technology to enhance or facilitate work on lines established by other media, primarily paint and photographic film. Many are working away from these preexisting genres into modes that could only be created with computers. Though it's always fun to speculate, it's impossible to say with even a slight bit of certainty what this new alliance between art and technics may bring. We may see massive changes in all the arts, perhaps coming on us so rapidly that we won't know what hit us. Perhaps the alliance will result primarily in techniques that will allow artists to do what they'd be doing anyway, but with greater ease and speed. We may see bio chips and neural interfaces allowing us to experience all art simultaneously and internally, and take it from there to wherever our own personal capabilities allow. Or maybe we'll just see a few good pieces and some snappier special effects in the movies. Whatever the case, a lot is going on right now, and it would be a shame to miss out on the marvelous advent of computer art — this coming of age will not happen again.

Cynthia Goodman's Digital Visions is an excellent survey of the state of the art at the time of publication. Despite the rapid changes in computer technology, this book will probably be the best survey available for several years and remain a landmark after it has been superseded. The book includes about 150 samples of computer related art, reproduced as well as images often meant to be seen on a different scale or in a different context or illuminated from behind can be printed in an affordable edition. Goodman's commentary is just what a survey should be: descriptive, impersonal, nonjudgemental and pluralistic. Her documentation is sufficiently detailed in her listing of hardware and software used in samples to satisfy those who are knowledgeable, but her commentaries are free from the technical argot that would make it difficult reading for those unfamiliar with computers.

One of the fascinating phenomena of the present state of the alliance is the way that computers can be used to make standard functions easier and quicker. Using a keyboard or any one of a number of input devices, including light pens that can be used directly on a computer terminal, an artist can create a basic design, save the original on a magnetic disk, and then rework it, changing existing forms, adding new ones, deleting others, and shifting color around. If one color doesn't work, it can be dropped and substituted by another by pressing a few keys. At the present state of the art, this need not produce the clunky images and lifeless colors often associated with computers -- resolutions so fine that disjunctions are imperceptible to the human eye allow a delicacy of shading fully comparable with anything a brush can achieve; and with a palette of some sixteen million colors (about all a human eye can discern) available on some of the most powerful units, it could be argued that computers offer more color options than any other medium. At present, some artists use this sort of technique as a means of making sketches for work to be completed in other media. Others print out their work directly from the images composed on their computer monitors.

In many cases the results are so much like easel paintings or photographs that the use of the computer seems comic, a great hooplah made over nothing. Used in this mimetic way, the value of computers can only be assessed by the artists using them. With the advent of inexpensive micro-computers we can assume that more artists will try these convenience functions and accept or reject them. If this usage becomes common practice, it probably won't make much difference to viewers -- it will simply become part of the professional bag of tricks. The majority of the works in Goodman's book use techniques of this sort. Whether the works are interesting or not, the many elaborate techniques are fascinating and, again, now is the time to be enthralled by them — the magic won't last.

Emerging from these convenience functions are some interesting shifts that move away from computer assistance to possibilities unattainable with traditional techniques. Perhaps the most promising is a shift from printing out the final work to creating art meant to be seen on computer terminals or other illuminated devices. These works, seen by radiant rather than reflected light may be the stained glass windows of a future age.

Among the artists who've gone beyond the level of simple convenience, I'd like to bring special attention to two who represent computer art's first level of maturity. Their work goes in different directions, suggesting the versatility of computer usage. In both we see a strong basis in techniques and aesthetics that have nothing to do with computers, and at the same time move the state of computer art beyond simple housekeeping.

Manfred Mohr has for some fifteen years been exploring the possibilities of restructuring the twelve sides of the most basic of forms, the cube, in two dimensional, black and white images. Mohr begins by designing a non-visual program based on algorithms (calculations with cyclic regularities) which are transformed into signs by the computer. Mohr then reworks the signs to his satisfaction and has a plotter (a computer driven drawing device) produce the final image on canvas or paper. The result is a large opus of dynamic images and sequences that can be read as narrative or analyzed by semiotic method. Both the program and the plotter put some distance between Mohr and the finished work, allowing geometry, mathematics, and chance to play an independent role in the work, and minimizing personal or idiosyncratic elements. Mohr's art seems to have raised Constructivism to a level unattainable by his predecessors, Malevich and Mondrian.

Harold Cohen has designed an artificial intelligence program called AARON and he has been able to teach this program to draw clearly legible human figures, plant forms, and other objects, as well as clearly conceived abstractions. AARON produces lively, fluid, energetic drawings with much of the expressiveness you would expect from an artist coming out of a tradition that emphasizes human individualism and prizes natural mysticism. The program has a capacity to learn, it is not simply repeating preexisting drawings but making drawings that could not have been anticipated by Cohen when he wrote or refined the program. Cohen's interactions with AARON occur on several levels: he refines his program as he goes along, taking cues and challenges from what the program has accomplished. AARON is limited to monochrome productions and Cohen often radically alters the program's drawings by adding color. The artificial intelligence of this program is a far cry from the advanced sort of A.I. that technocrats and sci fi buffs forecast, but here we have the first real example of man and computer communicating and interacting constructively, producing art that goes beyond simple mechanical gimicry.

Conventional wisdom has it that computers are inherently dehumanizing devices, the product of mad scientists working in isolation, unaware that their machines are foisting their alienation and solipsism on everyone else. That's more a product of the movies than of computers. Though a new generation of artists turned hackers and scientists turned artist is now emerging, most computer artists have had to form alliances with the scientists who are often perceived as their polar opposites in temperament and personality — and often enough both have had to use equipment owned by great corporate beasts like IBM, Phillips and the pentagon. This collaboration sometimes functions on an intimate level: many computer art producers are married couples or lovers working in tandem, as often as not initially brought together by their need to share skills. (Maybe we could think of this as a form of computer dating that actually works!) Among producers, the computer has encouraged community rather than alienation, perhaps beginning to exorcise the "two cultures" boogie man still seen by many as part of our collective schizophrenia.

Perhaps participatory works will also bring viewers together and encourage community. In popular culture, their cognates are already doing so -- how many kids have met each other for the first time in video arcades since you started reading this article? On the level of self conscious art, computers tend to encourage participatory work, and it's my hunch that computer art will most distinguish itself in this area. A few of the many examples in Goodman's book illustrate directions in which this trend is going.

Wen-Ying Tsai's compositions of moving fiberglass rods illuminated by strobes are simple and elegant examples. Audio feedback devices speed up or slow down the movement of the strobes in response to sounds made by people around them, moving slowly when the environment is quiet, frantically when it is noisy. People around these pieces can control the apparent movement of the rods by making noises ranging from whispers to speech to laughter to clapping, or the units may simply reflect the sounds of people who are not trying to interact with the sculptures.

In the collaborations between Otto Piene and Paul Earls, the frequencies of Earls's electronic music guide the images of Piene's computer drawing program. The images are created by a laser which can project them in all sorts of environments, including projections into the sky, where their three dimensional quality takes on the character of constantly changing monumental sculpture. In work like this, what you see could only be created by computer and laser. The maximum so far attained in collaboration between media and artists is in dance performances such as Phosphones, which use the CORTLI system designed by computer sculptor James Seawright, electronic music composer Emmanuel Ghent, programer William Hemsath, and choreographer Mimi Garrard. This system presents complex interactions between music and lighting, which in turn interact with the movements of the dancers in Ms. Garrard's company. This is just a few steps away from a total art form in which everyone dances and the audience and the work are reintegrated. And it's not far from massive works in which thousands of people participate, and a final "product" is never achieved or desired.

The basis of the partnership between computers and the arts is a human partnership. How much it can grow through its interaction with the nonhuman may be a partial test of its value. But ultimately this new technology will be a test of our cooperative and conceptual capacities and of our imagination and courage.

This volume of essays is the happy result of contacts and collaborations established during the three years devoted to the preparation of 'Cybernetic Serendipity'. Cybernetic Serendipity was an exhibition mounted at the Institute of Contemporary Arts in the summer of 1968, which dealt with the relationship of the computer and the arts. The exhibition, like this book, was concerned with the exploration and demonstration of connexions between creativity and technology (and cybernetics in particular), the links between scientific or mathematical approaches, intuitions, and the more irrational and oblique urges associated with the making of music, art and poetry. The title itself was intended to convey the fact that through the use of cybernetic devices we have made many fortunate discoveries for the arts. The exhibition Cybernetic Serendipity was mounted in a gallery of 6500 square feet, involved 325 participants and was seen by 60,000 people. The exhibits showed how man can use the computer and new technology to extend his creativity and inventiveness. These consisted of computer graphics, computercomposed and -played music, computer-animated films, computertexts, and among other computergenerated material, the first computer sculpture. There were also cybernetic machines such as Gordon Pask's 'colloquy of mobiles', television sets converting sound into visual patterns, Peter Zinovieff's electronic music studio with a computer which improvised on tunes whistled into a microphone by the visitors; there were robots, drawing machines and numerous constructions which responded to ambient sound and light. Six IBM machines demonstrated the uses of computers, and a visual display provided information on the history of cybernetics. Two aspects of this whole project are particularly significant. The first is that at no point was it clear to any of the visitors walking around the exhibition, which of the various drawings, objects and machines were made by artists and which were made by engineers; or, whether the photographic blow-ups of texts mounted on the walls were the work of poets or scientists. There was nothing intrinsic in the works themselves to provide information as to who made them. Among the contributors to the exhibition there were forty-three composers, artists and poets, and eighty-seven engineers, doctors, computer systems designers and philosophers. The second significant fact is that whereas new media inevitably contribute to the changing forms of the arts, it is unprecedented that a new tool should bring in its wake new people to become involved in creative activity, whether composing music, painting or writing. Graphic plotters, cathode-ray tube displays and teleprinters have enabled engineers, and others, who would never even have thought of putting pen to paper, to make images for the sheer pleasure of seeing them materialize. Many of the computer graphics made by engineers in Europe, Japan and the USA, approximate very closely to what we have learned to call art and put in our public galleries. This raises a very real question - should these computer graphics hang side by side with drawings by artists in museums and art galleries, or should they belong to another, as yet unspecified, category of creative achievement? There are certain classifications to which we are all assigned according to what we do. These categories which relate solely to our work, or our professional titles, inform the outside world about our way of life, our abilities and creative propensities. The deductions based on these classifications are not necessarily accurate but they suffice to colour the picture of an individual sufficiently for him to be irrevocably labelled. These labels provide information which is accepted without question and without protest. Thus it is assumed that the electronic engineers represent a clever but an uncreative branch of society, whereas artists are exceptionally creative but it is unlikely that they should possess any technological skills. It is also widely assumed that to the engineer, scientist and mathematician, art is magic, and to the composer, painter and poet, technology is a mystery. These rough assumptions are very broadly true but not altogether true. Since the middle 1950s the relationship between art and technology has been increasingly in evidence through the advent of computer-aided creative design. Today these categorical assumptions about our various talents, functions and possibilities are less accurate than ever. Thus Cybernetic Serendipity was not an art exhibition as such, nor a technological fun fair, nor a programmatic manifesto - it was primarily a demonstration of contemporary ideas, acts and objects, linking cybernetics and the creative process.
The question what is life, says Norman O. Brown, turns out to be the question what is sleep. We perceive that the sky exists only on earth. Evolution and human nature are mutually exclusive concepts. We're in transition from the Industrial Age to the Cybernetic Age, characterized by many as the post-Industrial Age. But I've found the term Paleocybernetic valuable as a conceptual tool with which to grasp the significance of our present environment: combining the primitive potential associated with Paleolithic and the transcendental integrities of "practical utopianism" associated with Cybernetic. So I call it the Paleocybernetic Age: an image of a hairy, buckskinned, barefooted atomic physicist with a brain full of mescaline and logarithms, working out the heuristics of computer-generated holograms or krypton laser interferometry. It's the dawn of man: for the first time in history we'll soon be free enough to discover who we are.

Radical Evolution and Future Shock
in the Paleocybernetic Age It is perhaps not coincidental that Western youth has discovered the
I Ching, or Book of Changes, on a somewhat popular level as wemove into the final third of the twentieth century. Change is now ouronly constant, a global institution. The human ecological biosphere isundergoing its second great transition, destined to be even moreprofound than the invention of agriculture in the Neolithic Age. If wecan't see the change, at least we can feel it. Future shock affects ourpsyche and our economy just as culture shock disorients the PeaceCorps worker in Borneo.It is said that we are living in a period of revolution. But nothingsells like freedom: Revolution is big business. As the physicist P. W.Bridgman once said, the true meaning of a term is found byobserving what a man does with it, not what he says about it. Sincethe phenomenon we call revolution is worldwide, and since it's felt inevery human experience, perhaps we might think of it not asrevolution but as radical evolution. Revolution is basically the samewhether defined by Marx or the I Ching: removal of the antiquated.But revolution replaces one status quo with another. Radicalevolution is never static; it's a perpetual state of polarization. Wecould think of it as involuntary revolution, but whatever terminologywe apply that's the condition of the world today, the environment withwhich the artist must work. Radical evolution would be kinder if itwere better understood; but it won't be so long as commercialentertainment cinema continues to representa "reality" that doesn'texist.Sociologist Alvin Toffler has stressed ephemerality as a chiefaspect of radical evolution: "Smith Brothers Cough Drops, CalumetBaking Soda, Ivory Soap, have become institutions by virtue of theirlong reign in the marketplace. In the days ahead, few products willenjoy such longevity. Corporations may create new productsknowing full well they'll remain on the market for only a matter of afew weeks or months. By extension, the corporations themselves—as well as unions, government agencies and all other organizations—may either have shorter life-spans or be forced to undergoincessant and radical reorganization. Rapid decay and regenerationwill be the watchwords of tomorrow."8 Toffler observes that noreasonable man should plan his life beyond ten years; even that, hesays, is risky. When parents speak of their sons becoming lawyersthey are deceiving themselves and their sons, according to thesociologist, "Because we have no conception of what being a lawyerwill mean twenty years hence. Most probably, lawyers will becomputers." In fact, we can't be sure that some occupations willeven exist when our children come of age. For example, thecomputer programmer, a job first created in the 1950's, will be asobsolete as the blacksmith within a decade; computers will reprogram and even regenerate themselves (IBM recently announceda new computer that repairs itself).John McHale, coauthor of the World Design Science Decadedocuments with Buckminster Fuller, emphasizes expendability andimpermanence in radical evolution: "Use value is replacingownership value. For example, the growth of rental and services—not only in automobiles and houses, but from skisto bridal gowns toheirloom silver, castles and works of art... our personal and household objects, when destroyed physically or outmoded symbolically,may be replaced by others exactly similar. A paper napkin, a suit, achair, an automobile, are items with identical replacement value.Metals in a cigarette lighter today may be, within a month or year,part of an auto, lipstick case or orbiting satellite... the concept ofpermanence in no way enables one to relate adequately to ourpresent situation." 9McHale has seen the need for a totally new world view as radicalevolution speeds farther from our grasp. "There's a mythologyabroad which equates the discovery and publication of newfacts with new knowledge. Knowledge is not simply accumulated facts butthe reduction of unrelated and often apparently irrelevant facts intonew conceptual wholes."10 He's talking about completely new ways of looking at the world and everything in it. This is proposition farmore profound than mere political revolution, which Krishnamurti hascharacterized as "The modification of the right according to the ideasof the left.''11 The new consciousness transcends both right and left.We must redefine everything.What happens to our definition of "intelligence" when computers,as an extension of the human brain, are the same size, weight, andcost as transistor radios? They're being developed through themicroelectronics process of Large-Scale Integration.What happens to our definition of "morality" when biochemists areabout to unravel the secrets of the DNA/RNA interaction mechanismto create human life?What happens to our definition of "man" when our next doorneighbor is a cyborg (a human with inorganic parts)? There areseveral crude cyborgs in the world today.What happens to our definition of "environment" when our videoextensions bring us the reality of the solar system daily? What do wemean by "nature" under these circumstances? (McLuhan: "The firstsatellite ended nature in the conventional sense.")What happens to our definition of "creativity" when a computerasks itself an original question without being programmed to do so?This has occurred several times.What happens to our definition of "family" when the intermedianetwork brings the behavior of the world into our home, and whenwe can be anywhere in the world in a few hours?What happens to our definition of "progress" when, according toLouis Pauwels: "For the really attentive observer the problems facingcontemporary intelligence are no longer problems of progress. Theconcept of progress has been dead for some years now. Today it is aquestion of a change of state, a transmutation.''12 Or Norbert Wiener:"Simple faith in progress is not a conviction belonging to strength butone belonging to acquiescence and hence to weakness.'' What happens to our definitions of "material" and "spiritual" whenscience has found no boundary between the two? Although it is stillpopularly assumed that the world is divided into animate andinanimate phenomena, virologists working at the supposed thresholdbetween life and nonlife at the virus level have in fact discovered nosuch boundary. "Both animate and inanimate have persisted rightacross yesterday's supposed threshold in both directions...subsequently what was animate has become foggier and foggier...no life, per se, has been isolated.''14 Indeed, what becomes of "reality" itself as science expands itsmastery of the forces of the universe? "The paradox of twentiethcentury science consists of its unreality in terms of sense impressions. Dealing as it does in energy transformation and submicroscopic particles, it has become a kind of metaphysics practiced by adevoted priestly cult—totally as divorced from the common-sensenotions of reality as was the metaphysics practiced by witch doctorsand alchemists. It is not at all odd, then, to discover that the closerwe come via the scientific method to 'truth,' the closer we come tounderstanding the 'truth' symbolized in myths.''15 This, then, is merely a superficial glimpse at some of the phenomena that characterize the Paleocybernetic Age. Quite clearly manis in the paradoxical position of existing in a state of consciousnesswithout being able to understand it. Man does not comprehend hisrelationship to the universe, either physical or metaphysical. Heinsists on "doing his thing" without the slightest notion of what his"thing" might be. This cosmic credibility gap exists primarily betweenthe facts of scientific experience and the illusions of environmentalconditioning as manifested in the global intermedia network.
The Intermedia Network as Nature The point I wish to make here is obvious yet vital to anunderstanding of the function of art in the environment, even thoughit is consistently ignored by the majority of film critics. It's the ideathat man is conditioned by his environment and that "enviromnent"for contemporary man is the intermedia network. We are conditionedmore by cinema and television than by nature. Once we've agreedupon this, it becomes immediately obvious that the structure andcontent of popular cinema is a matter of cardinal importance, at leastas serious as most political issues, and thus calls for comment notfrom journalists but from those who work at the matter, artiststhemselves.The cinema isn't just something inside the environment; theintermedia network of cinema, television, radio, magazines, books,and newspapers is our environment, a service environment thatcarries the messages of the social organism. It establishes meaningin life, creates mediating channels between man and man, man andsociety. "In earlier periods such traditional meaning and valuecommunication was carried mainly in the fine and folk arts. But todaythese are subsumed amongst many communicating modes. Theterm 'arts' requires expansion to include those advanced technological media which are neither fine nor folk.''16 We've seen the need for new concepts regarding the nature ofexistence; yet concepts are expanded or constricted in direct relationto the relevancy of prevailing languages. In a world where change isthe only constant, it's obvious we can't afford to rely on traditionalcinematic language. The world has changed immeasurably in theseventy years since the birth of cinema: for one thing "world" nowincludes the microcosm of the atom and the macrocosm of the universe in one spectrum. Still popular films speak a languagedeveloped by Griffith, Lumière, Méliès, derived from traditions ofvaudeville and literature.In the Agricultural Age man was totally passive, conditioned andvictimized by the environment. In the Industrial Age man's role was participatory; he became more aggressive and successful in hisattempts to control his environment. We're now moving into theCybernetic Age in which man learns that to control his environmenthe must cooperate with it; he not only participates but actuallyrecreates his environment both physical and metaphysical, and inturn is conditioned by it.To be free of the toil of old relationships we must first be free of theconditioning that instills it within us. As radical evolution gainsmomentum the need to unlearn our past becomes increasingly clear:contemporary life is a process of miseducation/uneducation/reeducation, at a cost of much precious time. McLuhan has noted thatthe true significance of Pavlov's experiments was that any controlledman-made environment is a conditioner that creates "non-perceptivesomnambulists." Since then science has discovered that "molecularmemory" is operative in single-celled and some multi-celledorganisms, and there's evidence that memory-in-the-flesh exists inhumans as well. Biochemists have proven that learned responses toenvironmental stimuli are passed on phylogenetically fromgeneration to generation, encoded in the RNA of the organism'sphysical molecular structure.17 And what could be a more powerfulconditioning force than the intermedia network, which functions toestablish meaning in life?Science has proven that there's no such thing as "human nature."Just as water takes the shape of its container, so human nature isrelative to its past and present conditioning. Optimum freedom ofbehavior and increased self-awareness are implicit in the industrialequation that is trending toward physical success for all men;Paleocybernetic man, however, has not learned to control the environment he creates. "The content of what is available for emulationon the part of the young in each society is itself culturally shapedand limited... the individual typically remains, throughout his lifetime,unaware of how his own habits, which to him appear 'only natural,' infact result from a learning process in which he never had anopportunity to attempt alternative responses.''18 This process is fortunate to have a tool that makes him awareof his own enculturation and thus he enjoys greater psychic freedomthan his ancestors. This tool is what Teilhard de Chardin has calledthe noosphere, the film of organized intelligence that encircles theplanet, superposed on the living layer of the biosphere and thelifeless layer of inorganic material, the lithosphere. The minds ofthree-and-a-half-billion humans—twenty-five percent of all humanswho ever lived—currently nourish the noosphere; distributed aroundthe globe by the intermedia network, it becomes a new "technology"that may prove to be one of the most powerful tools in man's history.John McHale: "World communications... diffuse and interpenetratelocal cultural tradition, providing commonly-shared culturalexperience in a manner unparalleled in human history. Within thisglobal network the related media share and transmit man's symbolicneeds and their expression on a world scale. Besides theenlargement of the physical world, these media virtually extend ourpsychical environment, providing a constant stream of moving,fleeting images of the world for our daily appraisal. They provide
psychic mobility for the greater mass of our citizens. Through thesedevices we can telescope time, move through history, and span theworld in a great variety of unprecedented ways.'' 19Like all energy sources the noosphere can be used for negativepurposes. Its resources can be manipulated to disguise craft ascreativity, especially in these Paleocybernetic days when we're stillimpressed by the sudden influx of information. Fuller hasdifferentiated craft from industry by demonstrating that craft isinherently local in technique and effect whereas industry is inherentlycomprehensive and universal in technique and effect. One mightmake a similar analogy between entertainment and art: entertainment is inherently "local," that is, of limited significance, whereasart is inherently universal and of unlimited significance. Too oftentoday we find that so-called artists working in the intermedia network are little more than adroit imitators, collectors of data andphenomena, which they glean from the noosphere and amalgamateinto packages that are far from whole. They're clever and glib;they've made an art of selling themselves, but they know only effect,not cause; they are merchants of mannerisms.It is precisely this confusion that clouds critical appraisal of"content" in the popular arts. All too frequently eclectic thinking isconfused with creative thinking. The distinction is subtle to be sure:integrative thinking can be the highest form of creativity. Indeed bothart and science function to reveal similarities within an a prioriuniverse of apparent dissimilarities. As with all else, however, there'san art and a craft to thinking, and the popular entertainments remainat the craft level by the very nature of their purpose.The intermedia network has made all of us artists by proxy. Adecade of television-watching is equal to a comprehensive course indramatic acting, writing, and filming. Compressed in such constantand massive dosage, we begin to see the methods and clichés moreclearly; the mystique is gone—we could almost do it ourselves.Unfortunately too many of us do just that: hence the glut of submediocre talent in the entertainment industry. Paradoxically thisphenomenon carries with it the potential of finally liberating cinemafrom its umbilical to theatre and literature, since it forces the moviesto expand into ever more complex areas of language andexperience. Evidence of television's effect on the cinema is alreadyapparent, as we shall see in our discussion of synaesthetic cinema.From another more immediate perspective, however, it is quiteunfortunate. We live in an age of hyperawareness, our sensesextended around the globe, but it's a case of aesthetic overload: ourtechnological zeal has outstripped our psychic capacity to cope withthe influx of information. We are adrift on the surface of radicalevolution unable to plumb the depths of its swift and turbulentcurrent.
The current generation is engaged in an unprecedented questioningof all that has been held essential. We question traditional conceptsof authority, ownership, justice, love, sex, freedom, politics, eventradition itself. But it's significant that we don't question ourentertainment. The disenfranchised young man who dropped out ofcollege, burned his draft card, braids his hair, smokes pot, and digsDylan is standing in line with his girl, who takes the pill, waiting tosee The Graduate or Bonnie and Clyde or Easy Rider— and they'rereacting to the same formulas of conditioned response that lulledtheir parents to sleep in the 1930's.We've seen the urgent need for an expanded cinematic language. Ihope to illustrate that profit-motivated commercial entertainment, byits very nature, cannot supply this new vision. Commercialentertainment works against art, exploits the alienation and boredomof the public, by perpetuating a system of conditioned response toformulas. Commercial entertainment not only isn't creative, it actuallydestroys the audience's ability to appreciate and participate in thecreative process. The implications become apparent when werealize that, as leisure time increases, each human will be forced tobecome a creative, self-sufficient, empirical energy laboratory.D. H. Lawrence has written: "The business of art is to reveal therelation between man and his circumambient universe at this livingmoment. As mankind is always struggling in the toil of oldrelationships, art is always ahead of its 'times,' which themselves arealways far in the rear of the living present." Jean-Jacques Lebelstated the same idea in different terms when he described art as "thecreation of a new world, never seen before, imperceptibly gaining onreality."
We've seen that man is conditioned by, and reacts to, certainstimuli in the man-made environment. The commercial entertainer isa manipulator of these stimuli. If he employs a certain triggermechanism, we're guaranteed to react accordingly, like puppets,providing he manipulates the trigger properly. I'm not saying theartist doesn't resort to audience manipulation; we know he oftendoes. The point, however, is the motivation in doing so. If the artistmust resort to trigger mechanisms to make himself clear, he will; butit's only a means to his end. In the case of the commercialentertainer, however, it's the end in itself.Plot, story, and what commonly is known as "drama" are thedevices that enable the commercial entertainer to manipulate hisaudience. The very act of this manipulation, gratifying conditionedneeds, is what the films actually are about. The viewer purchases itwith his ticket and is understandably annoyed if the film asks him tomanipulate himself, to engage in the creative process along with theartist. Our word poetry derives from the Greek root poiein meaning"to make" or "to work." The viewer of commercial entertainmentcinema does not want to work; he wants to be an object, to be actedupon, to be manipulated. The true subject of commercialentertainment is this little game it plays with its audience.By perpetuating a destructive habit of unthinking response toformulas, by forcing us to rely ever more frequently on memory, thecommercial entertainer encourages an unthinking response to dailylife, inhibiting self-awareness. Driven by the profit motive, thecommercial entertainer dares not risk alienating us by attemptingnew language even if he were capable of it. He seeks only to gratifypreconditioned needs for formula stimulus. He offers nothing wehaven't already conceived, nothing we don't already expect. Artexplains; entertainment exploits. Art is freedom from the conditionsof memory; entertainment is conditional on a present that isconditioned by the past. Entertainment gives us what we want; artgives us what we don't know we want. To confront a work of art is toconfront oneself—but aspects of oneself previously unrecognized.The extent to which blatant audience manipulation not only istolerated but extolled is alarming. Alfred Hitchcock, for example, inhis interview with François Truffaut, finds merit in his ability tomanipulate preconditioned needs for formula stimulus. Speaking of
Psycho, Hitchcock frankly admits: "It wasn't a message that stirred them, nor was it a great performance, or their enjoyment of thenovel... they were aroused by the construction of the story, and theway in which it was told caused audiences all over the world to reactand become emotional.''21 It is essential to understand that Hitchcock openly admits that hedidn't even try to expand awareness or to communicate somesignificant message, but only exploited a universal tradition ofdramatic manipulation in order to supply his audience with thegratification it paid for. The audience sees itself and its dreamsreflected in the film and reacts according to memory, whichKrishnamurti has characterized as being always conditioned."Memory," says Krishnamurti, "is always in the past and is given lifein the present by a challenge. Memory has no life in itself; it comesto life in the challenge [preconditioned formula stimulus]. And allmemory, whether dormant or active, is conditioned."22 It is thisprocess that the entertainment industry calls audience identification.To a healthy mind, anything that is primarily art is also immenselyentertaining. It seems obvious that the most important things shouldbe the most entertaining. Where there's a difference between whatwe "like" and what we know to be vital, we have a condition ofschizophrenia, an unnatural and destructive situation. I speakdeliberately of a "healthy" mind as one capable of creative thinking.Filmmaker Ken Kelman: "The old cinema removes experience,making us see things along with (or through) a protagonist withwhom we identify, and a plot in which we are caught. Such anapproach tends toward not only a lack of viewpoint, of definition of
whose experience it is, but also filters the power of sight into merehabit, dissolves insight into vicariousness. The spectator is reducedto a voyeur—which is, increasingly, the individual's role in society atlarge."23 Minimalist painter David Lee: "When people do not trust theirsenses they lack confidence in themselves. For the last fewcenturies people have lacked confidence. They have not trusted their experience to provide a standard for knowing how to act." 24 It isquite obvious that most of us not only don't know much about art, wedon't even know what we like. Krishnamurti: "One of the fundamentalcauses of the disintegration of society is copying, which is theworship of authority."25 Imitation is the result of inadequate information. Information resultsin change. Change requires energy. Energy is the result of adequateinformation Energy is directly proportional to the amount ofinformation about the structure of a system. Norbert Wiener: "Information is a name for the content of what is exchanged with theouter world as we adjust to it and make our adjustment felt upon it …to live effectively is to live with adequate information."26 From thecinema we receive conceptual information (ideas) and designinformation (experiences). In concert they become one phenomenon, which I've described as the experiential information of aestheticconceptual design. This information is either useful (additive) or redundant. Useful information accelerates change. Redundant information restricts change. If sustained long enough redundant information finally becomes misinformation, which results in negativechange.In communication theory and the laws of thermodynamics thequantity called entropy is the amount of energy reversiblyexchanged from one system in the universe to another. Entropy alsois the measure of disorder within those systems. It measures thelack of information about the structure of the system. For ourpurposes "structure of the system" should be taken to mean "thehuman condition," the universal subject of aesthetic activity. Entropyshould be understood as the degree of our ignorance about thatcondition. Ignorance always increases when a system's messagesare redundant. Ignorance is not a state of limbo in which noinformation exists, but rather a state of increasing chaos due to
misinformation about the structure of the system.The First Law of Thermodynamics states that energy is constant: itcannot be created or destroyed; its form can change, but not its quantity.
The Second Law states that the amount of energy within a localsystem is naturally entropic—it tends toward disorder, dissipation,incoherence. And since energy is defined as "a capacity to rearrange elemental order," entropy, which runs counter to thatcapacity, means less potential for change. We've learned fromphysics that the only anti-entropic force in the universe, or what iscalled negentropy (negative entropy), results from the process offeedback. Feedback exists between systems that are not closed butrather open and contingent upon other systems. In the strictestsense there are no truly "closed" systems anywhere in the universe;all processes impinge upon and are affected by other processes insome way. However, for most practical purposes, it is enough to saythat a system is "closed" when entropy dominates the feedbackprocess, that is, when the measure of energy lost is greater than themeasure of energy gained.The phenomenon of man, or of biological life on earth taken as aprocess, is negentropic because its subsystems feed energy backinto one another and thus are self-enriching, regenerative. Thusenergy is wealth, and wealth according to Buckminster Fuller is "thenumber of forward days a given system is sustainable." BiologistJohn Bleibtreu arrived at a similar conclusion when he noted that theconcept of time can best be viewed as a function of the Second Lawof Thermodynamics—that the measure of entropy in a system is ameasure of its age, or the passage of time since the systemoriginated.27 In other words the degree of a system's entropy isequal to redundancy or stasis whereas its negentropy is equal tokinesis or change. So information becomes energy when itcontributes to the self-enriching omni-regenerative wealth of thesystem. When it's not contributing (i.e., redundant) it is allowing thenatural entropy to increase."It is possible to treat sets of messages as having an entropy likesets of states of the external world... in fact, it is possible to interpretthe information carried by a message as essentially the negative ofits entropy... that is, the more probable the message the lessinformation it gives. Clichés, for example, are less illuminating thangreat poems." 28 Thus the more information concerning the human condition that the artist is able to give us, the more energy we havewith which to modify ourselves and grow in accord with theaccelerating accelerations of the living present.Commercial entertainment may be considered a closed systemsince entropy dominates the feedback process. To satisfy the profitmotive the commercial entertainer must give the audience what itexpects, which is conditional on what it has been getting, which isconditional on what it previously received, ad infinitum. Inherent inthe term "genre," which applies to all entertainment, is that it must beprobable. The content of westerns, gangster movies, romances, etc.,is probable in that it can be identified and comprehended simply byclassification. The phenomenon of drama itself usually is notconsidered a genre, but is in fact the most universal and archetypicalof all genres. Drama, by definition, means conflict, which in turnmeans suspense. Suspense is requisite on the expectation of knownalternatives. One cannot expect the unknown. Thereforeexpectation, suspense, and drama are all redundant probablequalities and thus are noninformative.Drama requires a plot that forces the viewer to move from point Ato point B to point C along predetermined lines. Plot does not mean"story" (beginning-middle-end). It simply indicates a relatively closedstructure in which free association and conscious participation arerestricted. Since the viewer remains passive and is acted upon bythe experience rather than participating in it with volition, there's nofeedback, that vital source of negentropy. Norbert Wiener:"Feedback is a method of controlling a system by reinserting into itthe results of its past performance... if the information whichproceeds backward from the performance is able to change thegeneral method and pattern of performance, we have a processwhich may well be called learning."29 Fuller: "Every time man makesa new experiment he always learns more. He cannot learn less.”30 In the cinema, feedback is possible almost exclusively in what I callthe synaesthetic mode, which we'll discuss presently. Because it isentirely personal it rests on no identifiable plot and is not probable.The viewer is forced to create along with the film, to interpret forhimself what he is experiencing. If the information (either concept or design) reveals some previously unrecognized aspect of the viewer'srelation to the circumambient universe—or provides language withwhich to conceptualize old realities more effectively— the viewerrecreates that discovery along with the artist, thus feeding back intothe environment the existence of more creative potential, which mayin turn be used by the artist for messages of still greater eloquenceand perception. If the information is redundant, as it must be incommercial entertainment, nothing is learned and change becomesunlikely. The noted authority on communication theory, J. R. Pierce,has demonstrated that an increase in entropy means a decrease inthe ability to change. 31 And we have seen that the ability to changeis the most urgent need facing twentieth-century man.The notion of experimental art, therefore, is meaningless. All art isexperimental or it isn't art. Art is research, whereas entertainment isa game or conflict. We have learned from cybernetics that inresearch one's work is governed by one's strongest points, whereasin conflicts or games one's work is governed by its weakestmoments. We have defined the difference between art and entertainment in scientific terms and have found entertainment to beinherently entropic, opposed to change, and art to be inherentlynegentropic, a catalyst to change. The artist is always an anarchist,a revolutionary, a creator of new worlds imperceptibly gaining onreality. He can do this because we live in a cosmos in which there'salways something more to be seen. When finally we erase thedifference between art and entertainment—as we must to survive—we shall find that our community is no longer a community, and weshall begin to understand radical evolution.
 The image I would offer as representative of the PaleocyberneticAge is that of the dying man whose life passes before him: a Retrospective Man who discovers the truth about himself too late to makeuse of it. The information explosion is not a window on the future somuch as a mirror of the past catching up with the present. Theintermedia network, or global communications grid, taps knowledgeresources that always have existed in discrete social enclavesaround the planet and saturates them into the collective consciousness. Suddenly the mass public "discovers" African culture,East Indian and American Indian cultures, folk music, politics.Knowledge previously the domain of scholars becomes commonknowledge, and precisely at that point when the old order is about tofade it sees itself clearly for the first time. William Burroughs hascalled it the Age of Total Confront, noting that all the heretoforeinvisible aspects of our condition have quite suddenly becomevisible.Through Duchamp, Cage, and Warhol, for example, we haverediscovered art in the ancient Platonic sense in which there's nodifference between the aesthetic and the mundane. Although thesemen certainly fulfill an avant-garde function in present society, theyin fact conform to the most universal and enduring definition of art. Ifthey've been rejected as artists by the majority of our citizens it'sbecause we've been conditioned by an economic system in whichaesthetic concerns must assume a secondary position if the systemis to survive. Thus art is separated from common experience and anelite hierarchy is established, which seems only natural to everyonecaught up in the economic struggle. John Dewey: "When art attainsclassic status it becomes isolated from the human conditions underwhich it was brought into being and from the human consequences itengenders in actual life experience... when, because of their remoteness, the objects acknowledged by the cultivated to be works offine art seem anemic to the mass of people, aesthetic hunger islikely to seek the cheap and the vulgar."32 Twentieth-century man is retrospective also because the symbolic and value content of his messages—most of which take the form ofcommercial entertainment—is predominantly redundant. NorbertWiener: "Society can only be understood through a study of themessages and the communication facilities which belong to it."33 Almost without exception, these messages tend to be concernedwith what is known as the "human condition." The history of popularentertainment, in terms of its conceptual content, can be divided intothree general categories: (1) idealization, which corresponds tostates of happiness in which life is seen as a heavenly experienceand man is characterized by his most noble deeds; (2)frustration, anexpression of the conflict between inner and outer realities, whenwhat is is not what should be; (3) demoralization, generallyexpressed as "the blues." In commercial entertainment cinema thesethree formulas are followed religiously, almost without exception, andusually comprise the nature of the message. They are the humancondition, that which is taken for granted, the given, the facts of life.Everyone has ideals, everyone is frustrated, everyone gets theblues. But this information is redundant; we must go on from there.Commercial entertainment is "popular" and not what we call artbecause it doesn't go on from there. To insure the widest possibleacceptance of his message, the commercial entertainer must speaka common language. He copies, repeats, or imitates that whichalready exists within the grasp of the so-called average man. Andthe majority of us embrace it because it offers security, a crutch, inthe knowledge that the miseries we suffer are shared by others. Butart transcends the human condition. The artist doesn't want to hearour problems and our dreams—he already knows them. Instead hewants to know what we're doing about them, and he gives us theinstruments we need for the task. The symbol is the basic instrumentof thought; those who create new symbols—artists, scientists, poets,philosophers—are those who, by giving us new instruments to thinkwith, give us new areas to explore in our thinking.A rather indignant woman once asked me how I could have thenerve to suggest that an "abstract" film like Brakhage's Dog Star
Man could be more important than an immortal classic like Renoir's  like Renoir's do not contain one single insight into the nature of thehuman condition that has not already been absorbed by the collective consciousness. Bob Dylan: "How many times must a man lookup before he can see the sky? How many ears must one man havebefore he can hear people cry?" And my own question: how manytimes must we acknowledge the human condition before it becomesredundant? How long must we tolerate the same facts of life beforewe begin seeking new facts? We intuit that the human condition hasexpanded since yesterday, but the popular arts aren't telling us. Thehuman condition does not stop with what we know about ourselves.Each genuinely new experience expands the definition of the humancondition that much more. Some are seeking those new facts, thosenew experiences, through the synaesthetic research of expandedcinema.Barbara Rose: "The new art... posits an entirely new world viewwhich shifts cultural values from a death-oriented, commemorative,past-enshrining culture to a life-oriented, present-oriented civilization... In this sense [Claes] Oldenburg's monuments represent, as hecontended, not the appearance of something, but its disappearance... the tomb, the memorial, the shrine, the monument, all belongto cultures that commemorate."34 John McHale: "The problem now is that those areas of our formaleducation which deal with the symbolic and value content of ourculture do so almost entirely in terms of the past35 ... The neweducational technologies are largely being used as twentieth-centurychannels to convey a conceptual context which is still nineteenthcentury or earlier. The most recent example was mathematics,where the Sputnik-inspired 'second look' revealed that mathematicsas generally taught was quite out of date. Science has begun to takea second look at its contents as currently taught. But the arts andhumanities remain relatively unaware of any need to revise theconceptual framework of studies little removed from the politeeducation of eighteenth-century gentry."
 The entropy of commercial entertainment is the chaos that resultsfrom its retrospective nature, forever commemorating past events,historical figures, social eras, life-styles, or the memory of the viewer,while the living present speeds farther from our grasp. Alvin Toffler:"We offer children courses in history; why not also make a course in'future' a prerequisite for every student? A course in which thepossibilities and probabilities of the future are systematically explored exactly as we now explore the social system of the Romansor the rise of the feudal manor?
Our discussion obviously has excluded many important works of artthat function completely within the genres of drama, plot, and story.
Citizen Kane, L'Avventura, Pierrot le Fou, and 8½ are dramatic, plotfilms, yet no one denies their greatness. We know also that most ofthe truly significant films such as Beauty and the Beast or Pather
Panchali operate entirely within parameters of the human conditionas generally recognized. Moreover, common sense tells us that theartist must work with what exists, with the given, the humancondition; he could produce no art at all if he relied exclusively oninformation that is totally new.Yet the undeniable aesthetic value of these works does notcontradict what I have said about art and entertainment. These filmstranscend their genres. They are not important for their plots orstories but rather for their design. Susan Sontag: "If there is any'knowledge' to be gained through art, it is the experience of the formor style of knowing the subject, rather than a knowledge of thesubject itself."38 To perceive that the artist functions as design scientist we mustfirst understand that in their broadest implications art and scienceare the same. Eddington's classic definition of science, "The earnestattempt to set in order the facts of experience," corresponds withBronowski's view of science as "The organization of knowledge insuch a way that it commands more of the hidden potential innature…all science is the search for unity in hidden likenesses."39 It'sthe same in art: to set in order the facts of experience is to reveal therelation between man and his circumambient universe with all itshidden potential.Herbert Read: "Only in so far as the artist establishes symbols forthe representation of reality can mind, as a structure of thought, takeshape. The artist establishes these symbols by becoming conscious of new aspects of reality and by representing his consciousness inplastic or poetic form... it follows that any extension ofawareness ofreality, any groping beyond the threshold of present knowledge,must first establish its sensuous imagery." 40Our word "design" is composed of "de" and "sign," indicating that itmeans "to remove the symbol of." In this context "symbol" signifiesideas distinct from experiences. As design scientist the artistdiscovers and perfects language that corresponds more directly toexperience; he develops hardware that embodies its own softwareas a conceptual tool for coping with reality. He separates the imagefrom its official symbolic meaning and reveals its hidden potential, itsprocess, its actual reality, the experience of the thing. (A. N.Whitehead: "Process and existence pre-suppose each other.") Heestablishes certain parameters that define a discrete "special case"phenomenon, principle, or concept known as the subject. The work,in effect, poses this "problem" of perception and we as viewers mustdraw from this special case all the "general case" metaphysicalrelationships that are encoded within the language of the piece.This language is the experiential information of aestheticconceptual design; it is addressed to what Wittgenstein termed the"inarticulate conscious," the domain between the subconscious andthe conscious that can't be expressed in words but of which weconstantly are aware. The artist does not point out new facts somuch as he creates a new language of conceptual design information with which we arrive at a new and more completeunderstanding of old facts, thus expanding our control over theinterior and exterior environments.The auteur theory of personal cinema indicates those instanceswhen the filmmaker's design science transcends the parameters ofhis genre; our comprehension of that genre, that human condition isthus expanded. But cybernetics has demonstrated that the structureof a system is an index of the performance which may be expectedfrom it.41 That is, the conceptual design of a movie determines thevariety and amount of information we're likely to obtain from it. Andsince we've seen that the amount of information is directly proportional to the degree of available choices we can seethat drama, story, and plot, which restrict choice, also restrict information. So the auteur is limited to developing new designs for oldinformation, which we all know can be immensely enjoyable andinstructive. There are no "new" ideas in L'Avventura, for example,but Antonioni voiced the inarticulate conscious of an entiregeneration through the conceptual and structural integrity of histranscendental design science, merging sense and symbol, form andcontent.Rudolph Arnheim: "Perceiving achieves at the sensory level whatin the realm of reasoning is known as understanding... eyesight isinsight."42 If we realize that insight means to see intuitively, weacknowledge that Arnheim's assertion is true only when ordinaryvision—conditioned and enculturated by the most vulgar of environments—is liberated through aesthetic conceptual design information. Film is a way of seeing. We see through the filmmaker's eyes.If he's an artist we become artists along with him. If he's not,information tends toward misinformation.The artist's intuitive sense of proportion corresponds to thephenomenon of absolute pitch in musicians and answers a fundamental need in comprehending what we apprehend. In the finalanalysis our aptitudes and our psychological balance are a result ofour relation to images. The image precedes the idea in the development of consciousness: an infant doesn't think "green" when itlooks at a blade of grass. It follows that the more "beautiful" theimage the more beautiful our consciousness.The design of commercial entertainment is neither a science noran art; it answers only to the common taste, the accepted vision, forfear of disturbing the viewer's reaction to the formula. The viewer'staste is conditioned by a profit-motivated architecture, which hasforgotten that a house is a machine to live in, a service environment.He leaves the theatre after three hours of redundancy and returnshome to a symbol, not a natural environment in which beauty andfunctionality are one. Little wonder that praise is heaped on filmswhose imagery is on the level of calendar art. Global man stands onthe moon casually regarding the entire spaceship earth in a glance, yet humanity still is impressed that a rich Hollywood studio can lugits Panavision cameras over the Alps and come back with prettypictures. "Surpassing visual majesty!" gasp the critics over A Man
and a Woman or Dr. Zhivago. But with today's technology andunlimited wealth who couldn't compile a picturesque movie? In factit's a disgrace when a film is not of surpassing visual majesty because there's a lot of that in our world. The new cinema, however,takes us to another world entirely. John Cage: "Where beauty endsis where the artist begins."
"The final poem will be the poem of fact in the language of fact. But it will be the poem
of fact not realized before." WALLACE STEVENSExpanded cinema has been expanding for a long time. Since it leftthe underground and became a popular avant-garde form in the late1950's the new cinema primarily has been an exercise in technique,the gradual development of a truly cinematic language with which toexpand further man's communicative powers and thus his awareness. If expanded cinema has had anything to say, the message hasbeen the medium.1 Slavko Vorkapich: "Most of the films made so farare examples not of creative use of motion-picture devices andtechniques, but of their use as recording instruments only. There areextremely few motion pictures that may be cited as instances ofcreative use of the medium, and from these only fragments and shortpassages may be compared to the best achievements in the otherarts."2 It has taken more than seventy years for global man to come toterms with the cinematic medium, to liberate it from theatre andliterature. We had to wait until our consciousness caught up with ourtechnology. But although the new cinema is the first and only truecinematic language, it still is used as a recording instrument. Therecorded subject, however, is not the objective external human condition but the filmmaker's consciousness, his perception and its process. If we've tolerated a certain absence of discipline, it has been infavor of a freedom through which new language hopefully would bedeveloped. With a fusion of aesthetic sensibilities and technologicalinnovation that language finally has been achieved. The new cinemahas emerged as the only aesthetic language to match theenvironment in which we live.Emerging with it is a major paradigm: a conception of the nature ofcinema so encompassing and persuasive that it promises todominate all image-making in much the same way as the theory ofgeneral relativity dominates all physics today. I call it synaesthetic
cinema. In relation to traditional cinema it's like the science of bionicsin relation to previous notions of biology and chemistry: that is, itmodels itself after the patterns of nature rather than attempting to"explain" or conform nature in terms of its own structure. The newartist, like the new scientist, does not "wrest order our of chaos."Both realize that supreme order lies in nature and traditionally wehave only made chaos out of it. The new artist and the new scientistrecognize that chaos is order on another level, and they set about tofind the rules of structuring by which nature has achieved it. That'swhy the scientist has abandoned absolutes and the filmmaker hasabandoned montage.Herbert Read: "Art never has been an attempt to grasp reality as awhole—that is beyond our human capacity; it was never even anattempt to represent the totality of appearances; but rather it hasbeen the piecemeal recognition and patient fixation of what is significant in human experience."3 We're beginning to understand that"what is significant in human experience” for contemporary man isthe awareness of consciousness, the recognition of the process ofperception. (I define perception both as "sensation" and "conceptualization," the process of forming concepts, usually classified as"cognition." Because we're enculturated, to perceive is to interpret.)Through synaesthetic cinema man attempts to express a total phenomenon—his own consciousness.
Synaesthetic cinema is the only aesthetic language suited to thepost-industrial, post-literate, man-made environment with its multidimensional simulsensory network of information sources. It's theonly aesthetic tool that even approaches the reality continuum ofconscious existence in the nonuniform, nonlinear, nonconnectedelectronic atmosphere of the Paleocybernetic Age. "As visual spaceis superseded," McLuhan observes, "we discover that there is nocontinuity or connectedness, let alone depth and perspective, in anyof the other senses. The modern artist—in music, in painting, inpoetry—has been patiently expounding this fact for decades."5 Themodern synaesthetic filmmaker has been patiently expounding thisfact for decades as well, and with far more success than painters orpoets.Finally, I propose to show that synaesthetic cinema transcends therestrictions of drama, story, and plot and therefore cannot be called agenre. In addition to matching McLuhan's view of contemporaryexistence, it also corresponds to Buckminster Fuller's observationson natural synergetics and consequently is negentropic. Beforediscussing specifics, however, we must first understand why synaesthetic cinema is just now being developed into a universallanguage, more than seventy years after the birth of the medium.Like most everything else, it's because of television.
Just as every fact is also metaphysical, every piece of hardwareimplies software: information about its existence. Television is thesoftware of the earth. Television is invisible. It's not an object. It's nota piece of furniture. The television set is irrelevant to the phenomenon of television. The videosphere is the noosphere transformed into a perceivable state. "Television," says video artist LesLevine, "is the most obvious realization of software in the generalenvironment. It shows the human race itself as a working model ofitself. It renders the social and psychological condition of the environment visible to the environment."A culture is dead when its myths have been exposed. Television isexposing the myths of the republic. Television reveals the observed,the observer, the process of observing. There can be no secrets inthe Paleocybernetic Age. On the macrostructural level all televisionis a closed circuit that constantly turns us back upon ourselves.Humanity extends its video Third Eye to the moon and feeds its ownimage back into its monitors. "Monitor" is the electronic manifestationof superego. Television is the earth's superego. We become awareof our individual behavior by observing the collective behavior asmanifested in the global videosphere. We identify with persons innews events as once we identified with actors or events in fictionfilms. Before television we saw little of the human condition. Now wesee and hear it daily. The world's not a stage, it's a TV documentary.Television extends global man throughout the ecological biospheretwenty-four hours a day. By moving into outer space, televisionreveals new dimensions of inner space, new aspects of man'sperception and the results of that perception.This implosive, self-revealing, consciousness-expanding process isirreversible. Global information is the natural enemy of local government, for it reveals the true context in which that government is operating. Global television is directly responsible for the politicalturmoil that is increasing around the world today. The politicalestablishments sense this and are beginning to react. But it's toolate. Television makes it impossible for governments to maintain theillusion of sovereignty and separatism which is essential for theirexistence. Television is one of the most revolutionary tools in theentire spectrum of technoanarchy.We recognize television's negative effect on the popular arts: that itinduces a kind of sedentary uniformity of expression and generates afalse sense of creativity. In its broader consequences, however,television releases cinema from the umbilical of theatre andliterature. It renders cinema obsolete as communicator of the objective human condition. It has affected cinema in much the same wayas the invention of photography affected sculpture and painting.Cubism and other means of abstracting the realistic image were bornwith the photographic plate because painting no longer provided themost realistic images. The plastic arts abandoned exterior reality forinterior reality. The same has happened to cinema as a result oftelevision: movies no longer provide the most realistic images sothey've turned inward.We're in direct contact with the human condition; there's no longerany need to represent it through art. Not only does this releasecinema; it virtually forces cinema to move beyond the objectivehuman condition into newer extra-objective territory. There aremanifold trends that indicate that virtually all cinema has felt theprofound impact of television and is moving inevitably towardsynaesthesis. The progression naturally includes intermediary stepsfirst toward greater "realism," then cinéma-vérité, before the final andtotal abandon of the notion of reality itself. The fact that we're nowapproaching the peak of the realism stage is demonstrated byWarhol, for example, whose recent work contrasts "reality" with"realism" as manifested in the spontaneous behavior of actors pretending to be acting. In addition there's virtually all of Godard's work,as well as John Cassavetes' Faces, James McBride's David
Holzman's Diary, Peter Watkins' The War Game, Gillo Pontecorvo's
The Battle of Algiers, Paul Morrissey's Flesh, and Stanton Kaye's
Georg and Brandy in the Wilderness.
Most of this work is characterized by an astute blending of scriptedand directed acting with spontaneous improvisation, in which theactor randomly fills in the parameters of a characterization predetermined and predestined by the director. Yet precisely becausethey attempt to approximate objective reality without actually beingreal, places them firmly in the tradition of conventional Hollywoodpretend movies, with the exception of camera presence or whatmight be called process-level perception.It's only natural that contemporary filmmakers should be moresuccessful at imitating reality since the intermedia network makes usmore familiar with it. But there's a curious and quite significantaspect to the nature of this new realism: by incorporating a kind ofbastardized cinéma-vérité or newsreel style of photography andbehavior, the filmmaker has not moved closer to actual unstylizedreality itself but rather a reality prestylized to approximate ourprimary mode of knowing natural events: television. We accept it asbeing more realistic because it more closely resembles the processlevel perception of TV watching, in which unstylized reality is filteredand shaped through the process of a given medium.The traditional dramatic structure of these films becomes moreeasily discernible in contrast with pure cinéma-vérité work such asJean Rouch's Chronicle of a Summer, Pennebaker's Don't Look
Back, or Chris Marker's brilliant Le Joli Mai. A comparison of Facesor David Holzman's Diary with Warhol's Nude Restaurant is evenmore revealing: the difference between prestylized and predestinedrealities on the one hand, and Warhol's totally random and onlypartially prestylized reality on the other, is brought into sharp focus.Warhol has expressed regret that a camera cannot simply beswitched on and left running for twenty-four hours, since the "important" (naturally-revealing) events seem to occur at that momentjust after it stops turning. Godard disclosed similar sentiments whenhe said: "The ideal for me is to obtain right away what will work. Ifretakes are necessary it falls short of the mark. The immediate ischance. At the same time it is definitive. What I want is the definitiveby chance."
Simultaneous Perception of Harmonic Opposites Time, said St. Augustine, is a threefold present: the present as weexperience it; the past as present memory; the future as presentexpectation. Hopi Indians, who thought of themselves as caretakersof the planet, used only the present tense in their language: past wasindicated as "present manifested," and the future was signified by"present manifesting.”6 Until approximately 800 B.C., few culturesthought in terms of past or future: all experience was synthesized inthe present. It seems that practically everyone but contemporaryman has intuitively understood the space-time continuum.Synaesthetic cinema is a space-time continuum. It is neither subjective, objective, nor nonobjective, but rather all of these combined:that is to say, extra-objective. Synaesthetic and psychedelic meanapproximately the same thing. Synaesthesis is the harmony ofdifferent or opposing impulses produced by a work of art. It meansthe simultaneous perception of harmonic opposites. Its sensorialeffect is known as synaesthesia, and it's as old as the ancientGreeks who coined the term. Under the influence of mindmanifesting hallucinogens one experiences synaesthesia in additionto what Dr. John Lilly calls "white noise," or random signals in thecontrol mechanism of the human bio-computer.7 Any dualism is composed of harmonic opposites: in/out, up/ down,off/on, yes/no, black/white, good/bad. Past aesthetic traditions,reflecting the consciousness of their period, have tended toconcentrate on one element at a time. But the Paleocyberneticexperience doesn't support that kind of logic. The emphasis of traditional logic might be expressed in terms of an either/or choice, whichin physics is known as bistable logic. But the logic of the CyberneticAge into which we're moving will be both/and, which in physics is called triadic logic. Physicists have found they can no longer describe phenomena with the binary yes/no formula but must operatewith yes/no/maybe.The accumulation of facts is no longer of top priority to humanity.The problem now is to apply existing facts to new conceptual wholes,new vistas of reality. By "reality" we mean relationships. PietMondrian: "As nature becomes more abstract, a relation is moreclearly felt. The new painting has clearly shown this. And that is whyit has come to the point of expressing nothing but relations."8 Synaesthetic cinema is an art of relations: the relations of the conceptual information and design information within the film itselfgraphically, and the relation between the film and the viewer at thatpoint where human perception (sensation and conceptualization)brings them together. As science gropes for new models to accommodate apparent inconsistencies and contradictions, the need forseeing incompatibles together is more easily discerned. For example, the phenomenon of light is conceived in both/and terms: bothcontinuous wave motions and discontinuous particles. And we havenoted our incapacity for observing both movement and position ofelectrons.This is but one of many reasons that synaestheticcinema is theonly aesthetic language suited to contemporary life. It can function asa conditioning force to unite us with the living present, not separateus from it. My use of the term synaesthetic is meant only as a way ofunderstanding the historical significance of a phenomenon withouthistorical precedent. Actually the most descriptive term for the newcinema is "personal" because it's only an extension of the filmmaker's central nervous system. The reader should not interpret"synaesthetic" as an attempt to categorize or label a phenomenonthat has no definition. There's no single film that could be calledtypical of the new cinema because it is defined anew by each individual filmmaker.I've selected about seven films that are particularly representativeof the various points I wish to make. I'm using them only to illuminatethe nature of synaesthetic cinema in general, not as specific archetypal examples. Sufficient literature exists on Brakhage's Dog Star
Man to preclude any major expository analysis here, but it is exemplary of virtually all concepts involved in the synaesthetic mode, inparticular syncretism and metamorphosis. Will Hindle's Chinese
Firedrill is an outstanding example of the evocative language ofsynaesthetic cinema as distinct from the expositional mode ofnarrative cinema. Pat O'Neill's 7362, John Schofill's XFilm, andRonald Nameth's Exploding Plastic Inevitable provide some insightinto kinaesthetics and kinetic empathy. Carolee Schneemann's
Fuses, in contrast with Warhol's Blue Movie and Paul Morrissey's
Flesh, illustrates the new polymorphous eroticism. And, finally,Michael Snow's Wavelength has been chosen for its qualities ofextra-objective constructivism.
Montage as Collage The harmonic opposites of synaesthetic cinema are apprehendedthrough syncretistic vision, which Anton Ehrenzweig has characterized as: "The child's capacity to comprehend a total structure ratherthan analyzing single elements... he does not differentiate the identityof a shape by watching its details one by one, but goes straight forthe whole."9 Syncretism is the combination of many different formsinto one whole form. Persian tapestries and tile domes aresyncretistic. Mandalas are syncretistic. Nature is syncre-tistic. Themajority of filmgoers, conditioned by a lifetime of conven-tionalnarrative cinema, make little sense of synaesthetic cinema becausetheir natural syncretistic faculty has suffered entropy and atrophy.Buckminster Fuller: "All universities have been progressivelyorganized for ever-finer specialization. Society assumes that specialization is natural, inevitable and desirable. Yet in observing a littlechild we find it is interested in everything and spontaneously apprehends, comprehends and coordinates an ever-expanding inventory of experience.''10 It has been demonstrated that all species of life on earth that havebecome extinct were doomed through overspecialization, whetheranatomical, biological, or geological. Therefore conventional narrative cinema, in which the filmmaker plays policeman guiding oureyes here and there in the picture plane, might be described as"specialized vision," which tends to decay our ability to comprehendthe more complex and diffuse visual field of living reality.The general impression that syncretism, and therefore synaestheticcinema, is empty of detail or content is an illusion: "… it is highlysensitive to the smallest of cues and proves more efficient in identifying individual objects. It impresses us as empty, vague and generalized only because the narrowly-focused surface consciousnesscannot grasp its wider more comprehensive structure. Its precise,concrete content has become inaccessible and ‘unconscious.’11 ''Synaesthetic cinema provides access to syncretistic contentthrough the inarticulate conscious. Similarly, it contradicts theteachings of Gestalt psychology, according to which we must makean either/or choice: we can choose either to see the "significant"figure or the "insignificant" ground. But when the "content" of themessage is the relationship between its parts, and when structureand content are synonymous, all elements are equally significant.Ehrenzweig has suggested that syncretism is "Gestalt-free perception," and indeed this must be the case if one expects any visual"meaning" from synaesthetic cinema.Paul Klee, whose syncretistic paintings closely resemble certainworks of synaesthetic cinema, spoke of the endotopic (inside) and
exotopic (outside) areas of a picture plane, stressing their equalimportance in the overall experience.12 Synaesthetic cinema, primarily through superimposition, fuses the endotopic and exotopic byreducing depth-of-field to a total field of nonfocused multiplicity.Moreover, it subsumes the conventional sense of time by interconnecting and interpenetrating the temporal dimension with images thatexist outside of time. The "action" of Dog Star Man, for example,could be an entire life-span or merely a split second in the inarticulate conscious of Stan Brakhage. I stress "action" as commonlyunderstood in the cinema because synaesthetic syncretism replacesmontage with collage and, as André Bazin has observed, "montageis the dramatic analysis of action." Bazin was perceptive enough torealize that "only an increased realism of the image can support theabstraction of montage.''13 Synaesthetic cinema subsumes Eisenstein's theory of montage-ascollision and Pudovkin's view of montage-as-linkage. It demonstratesthat they were certainly correct but didn't follow their own observations to their logical conclusions. They were restricted by the consciousness of their times. Synaesthetic cinema transcends the notionof reality. It doesn't "chop the world into little fragments," an effectBazin attributed to montage, because it's not concerned with theobjective world in the first place. The new filmmaker is showing ushis feelings. Montage is indeed an abstraction of objective reality;that's why, until recently, Warhol did not cut his films at all. Butsynaesthetic syncretism is the only mode in which the manifestationsof one's consciousness can be approximated without distortion.There's no conflict in harmonic opposites. Nor is there anything thatmight be called linkage. There is only a space-time continuum, amosaic simultaneity. Although composed of discrete elements it isconceived and edited as one continuous perceptual experience. Asynaesthetic film is, in effect, one image continually transforming intoother images: metamorphosis. It is the one unifying force in all ofsynaesthetic cinema. The notion of universal unity and cosmicsimultaneity is a logical result of the psychological effects of theglobal communications network.If montage is the dramatic analysis of action, a film without classicmontage thus avoids at least the structural element of dramainherent within the medium. All that remains to avoid drama entirelyis to exclude dramatic (i.e., theatrical) content by making content andstructure the same. Warhol's films are not dramatic, and neither arefilms at the extreme opposite end of the spectrum, synaesthesia. Theclassical tension of montage is dissolved through overlappingsuperimposition. For example: we have shots A, B. and C. First wesee A, then B is superimposed over it to produce AB. Then A fadesas C fades in. There's a brief transitional period in which we'reseeing ABC simultaneously, and finally we're looking only at BC. Butno sooner has this evolved than B begins to fade as D appears, andso on.This is a physical, structural equivalent of the Hopi "presentmanifested" and "present manifesting" space-time continuum. It's theonly style of cinema that directly corresponds to the theory of generalrelativity, a concept that has completely transformed all aspects ofcontemporary existence except traditional Hollywood cinema. Theeffects of metamorphosis described above become more apparent ifshots A, B. and C happen to be of the same image but from slightly different perspectives, or with varied inflections of tone and color. Itis through this process that a synaesthetic film becomes, in effect,one image constantly manifesting.And finally we're forced to admit that the pure art of cinema existsalmost exclusively in the use of superimposition. In traditionalcinema, superimposition usually gives the impression of two moviesoccurring at once in the same frame with their attendant psychological and physiological connotations coexisting separately. In synaesthetic cinema they are one total image in metamorphosis. This doesnot imply that we must relinquish what Eisenstein called "intellectualmontage." In fact, the conflict-juxtaposition of intellectual effects isincreased when they occur within the same image. Fiction, legend,parable, myth, traditionally have been employed to make comprehensible the paradoxes of that field of nonfocused multiplicity that islife. Synaesthetic cinema, whose very structure is paradox, makesparadox a language in itself, discovering the order (legend) hiddenwithin it.
Stan Brakhage: Dog Star Man
Dog Star Man is a silent, seventy-eight-minute film divided into
Prelude and Parts One through Four. It was shot in 1959-60 andedited during the next four years. Prelude is an extremely fastcollage of multiple-level superimpositions and compounded imagesthat emerge from a blurry diaphanous haze and slowly take form,only to be obscured by other images and countermotions. We beginto discern specific objects, patterns, and finally a motif or theme: theelements of Earth, Air, Fire, and Water; a childbirth; a man climbinga mountain with his dog; the moon; the sun throwing off huge solarprominences; lovemaking; photomicrography of blood vessels; abeating heart; a forest; clouds; the faces of a man and a woman; andliterally thousands of other images to appear in the rest of the film.These images exist essentially autonomously and are superimposed or compounded not for "dramatic" effect but rather as a kind ofmatrix for psychic exercise on the part of the viewers. For example,over an expanding solar prominence we might see Brakhage'sleonine face or a line of snow-covered fir trees in the mountains ofColorado. We are not asked to interpret or find "meaning" in these combinations, though vastly rich experiences arepossible. When theimages emerge from a hazy blur, for example, we are not asked tointerpret this as the creation of life or some similar dramatic notion,but rather as a perceptual experience for its own sake, in addition tothe contextual relationship of this image to the rest of the film, orwhat Eisenstein indicated by the term "intellectual montage."Whereas Prelude is a rapid barrage of multiple overlays, Part Oneis superimposed sparingly, concentrating on interface relationshipsbetween individual shots. However, every effort is made to subdueany effect that might be considered montage. The shots fade in andout very slowly, often fading into a color such as red or green. Thefragments of Prelude fall into place and an overwhelming sense ofoceanic consciousness evolves. We begin to realize that Brakhageis attempting to express the totality of consciousness, the realitycontinuum of the living present. As his solitary figure climbs thesnow-covered mountain, we see images of man's world from themicrospectrum of the bloodstream to the macrospectrum of the sun,moon, and universe. Both time and space are subsumed in thewholeness of the experience. Superimposition is not used as aneconomical substitute for "parallel montage"—indicating simultaneous but spatially separate events—for spatio-temporal dimensionsdo not exist in the consciousness. Brakhage is merely presenting uswith images orchestrated in such a way that a new reality arises outof them.When we see the sun superimposed over a lovemaking scene, it'snot an invitation to interpret a meaning such as cosmic regenerationor the smallness of man in the universe, but rather as an occasion toexperience our own involuntary and inarticulate associations. Theimages are not symbolic, as in The Seventh Seal, or artfully composed as in Last Year at Marienbad. Brakhage does not manipulateus emotionally, saying: "Now I want you to feel suspense" or "Now Iwant you to laugh" or "Now is the time to be fearful." This is the ployof the commercial entertainer: an arrogant degradation of cinema,using film as a tool for cheap sensationalism. This is not to say thatspatio-temporal experiences, or suspense, humor, or any emotioncannot be found in synaesthetic cinema. Quite the contrary: becausewe're dealing with our own personal associations, emotion is guaranteed. And it will be more genuinely profound than the formulatriggered gratification of conditioned response that we receive fromcommercial entertainment.Brakhage has spoken of "restructuring" vision through his films,and often refers to the "untutored" vision of the child before he'staught to think and see in symbols. In what he calls "closed-eyevision," Brakhage attempts to simulate, by painting and scratching onfilm, the flashes and patterns of color we perceive when our eyes areclosed. Approximately midway through Dog Star Man, otherwisemundane images take on wholly new meanings and in some casesnew appearances. We stop mentally labeling images and concentrate instead on the synaesthetic/kinaesthetic flow of color,shape, and motion.This is not to suggest a nonobjective experience. The imagesdevelop their own syntactical meaning and a "narrative" line isperceived, though the meaning of any given image may change inthe context of different sequences. This constitutes a creative use ofthe language itself, over and above any particular "content" conveyed by that language. (Wallace Stevens: "A new meaning isequivalent to a new word.") The effect of synaesthetic cinema is tobreak the hold that the medium has over us, to make us perceive itobjectively. Art is utter folly unless it frees us from the need of art asan experience separate from the ordinary.Wittgenstein has described art as a game whose rules are madeup as the game is in process. The exact meaning of words (images)becomes known only in the context of each new statement.14 E. H.Gombrich, on the other hand, demonstrates that objective realismalso is a game, but one whose schema is established prior to its useand is never altered. Artists and society thus learn to read theschema as though it were objective reality. But since the languageitself is not used creatively, the viewer is seduced beyond form intoan abstract content with an illusion of being externally objective.15 Thus the viewer is captive under the hold, or spell, of the mediumand is not free to analyze the process of experience.
14 Ludwig Wittgenstein, Philosophical Investigations (Oxford: Blackwell Press, 1963).
Brakhage expressed this concept with respect to his own work:"Imagine an eye unruled by man-made laws of perspective, an eyeunprejudiced by compositional logic, an eye which must know eachobject encountered in life through a new adventure of perception.Imagine a world alive with incomprehensible objects and shimmeringwith an endless variety of movement and gradations of color.Imagine a world before the beginning was the word.''16
16 Stan Brakhage, "Metaphors on Vision," ed. P. Adams Sitney, Film Culture (Fall, 1963).
Evocation and Exposition:
Toward Oceanic Consciousness There is an important distinction to be made betweenevocation, thelanguage of synaesthetic cinema, primarily poetic in structure andeffect, and exposition, the language of narrative cinema, whichchiefly conforms to traditional, literary narrative modes. Intermediaartist and filmmaker Carolee Schneemann has characterized evocation as the place between desire and experience, the interpenetrations and displacements which occur between various sense stimuli."Vision is not a fact," Miss Schneemann postulates, "but anaggregate of sensations. Vision creates its own efforts towardrealization; effort does not create vision.”17 Thus, by creating a new kind of vision, synaesthetic cinema createsa new kind of consciousness: oceanic consciousness. Freud spokeof oceanic consciousness as that in which we feel our individualexistence lost in mystic union with the universe. Nothing could bemore appropriate to contemporary experience, when for the first timeman has left the boundaries of this globe. The oceanic effect ofsynaesthetic cinema is similar to the mystical allure of the naturalelements: we stare in mindless wonder at the ocean or a lake orriver. We are drawn almost hypnotically to fire, gazing as thoughspellbound. We see cathedrals in clouds, not thinking anything inparticular but feeling somehow secure and content. It is similar to theconcept of no-mindedness in Zen, which also is the state of mantraand mandala consciousness, the widest range of consciousness.Miss Schneemann defines perception as eye-journey or empathydrawing. It is precisely through a kind of empathy-drawing that thecontent of synaesthetic cinema is created jointly by the film and theviewer. The very nature of evocation requires creative effort on thepart of the viewer, whereas expository modes do all the work and theviewer becomes passive. In expositional narrative, a story is being figure of Stan Brakhage in Dog Star Man actually moves through apsychic environment created by the viewer, whose deeply-hiddencreative resources and hungers have been evoked by the film.With typical poetic eloquence, Hermann Hesse has summarizedthe evocative effects of oceanic consciousness in this memorablepassage from Demian: "The surrender to nature's irrational, strangelyconfused formations produces in us a feeling of inner harmony withthe force responsible for these phenomena... the boundaries separating us from nature begin to quiver and dissolve... we are unable todecide whether the images on our retina are the result ofimpressions coming from without or from within... we discover towhat extent we are creative, to what extent oursoul partakes of theconstant creation of the world.'
Will Hindle: Chinese FiredrillThere have been essentially three generations of personal filmmakers in the United States. The first began with the invention of themedium and continued in various stages through the 1940's. Thesecond began approximately in the mid-1950's with the increasingavailability of inexpensive 8mm. and 16mm. equipment. It represented the first popular movement toward personal cinema as a wayof life. The third generation has evolved since the mid-1960's, primarily in the San Francisco area, where the latest trend is toward ablending of aesthetics and technology. One reason personal cinemais more eloquent than commercial cinema is that the filmmaker isforced into a closer interaction with his technology.Will Hindle is exemplary of this recent technological awareness, acombination of engineering and aesthetics. Trained in art, literature,and professional television filmmaking, Hindle has applied hisknowledge to personal cinema in a singularly spectacular fashion.His ability to invest a technical device with emotional or metaphysical content is truly impressive. He has, for example, developedthe technique of rear-projection rephotography to a high degree ofeloquence. He shoots original scenes with wide-angle lenses, then"crops" them by projecting and rephotographing this footage using aspecial single-frame projector. Thus extremely subtle effects are achieved that would be prohibitively expensive, if not impossible, ifdone through conventional laboratory optical printing.Although many synaesthetic films are wonderfully evocative,Hindle's recent works are especially notable for their ability to generate overwhelming emotional impact almost exclusively from cine matic technique, not thematic content. Hindle has an uncanny talentfor transforming spontaneous unstylized reality into unearthly poeticvisions, as in Billabong (1968), a wordless impressionistic "documentary" about a boy's camp in northern California, and Watersmith (1969), a spectacular visual fantasy created from footage of anOlympic swimming team at practice.
Chinese Firedrill, unique in Hindle's work, was prestylized and"performed" almost in the traditional sense of a scripted, directed,and acted movie. The difference is that Hindle used the images notfor their symbolic or theatrical content but as ingredients of an almosticonographic nature, to be compounded and manipulated through theprocess of the medium. Although there are "actors" (Hindle plays theprincipal role), there is no characterization. Although there are sets,we're not asked to suspend our disbelief.
Chinese Firedrill is a romantic, nostalgic film. Yet its nostalgia is ofthe unknown, of vague emotions, haunted dreams, unspoken words,silences between sounds. It's a nostalgia for the oceanic presentrather than a remembered past. It is total fantasy; yet like the bestfantasies—8½, Beauty and the Beast, The Children of Paradise— itseems more real than the coldest documentary. The "action" occursentirely within the mind of the protagonist, who never leaves thesmall room in which he lives. It's all rooms everywhere, all cubicleswherever we find man trapped within his dreams. Through thedoor/mirror is the beyond, the unreachable, the unattainable, thebeginning and the end. Not once in the film's twenty minutes can wepinpoint a sequence or action that might be called "dramatic" in theusual sense. Yet almost immediately an overwhelming atmosphereof pathos is generated. There are moments of excruciating emotionalimpact, not from audience manipulation but from Hindle's ability torealize metaphysical substance, stirring the inarticulate conscious.Every effort is made to distance the viewer, to keep us aware of ourperceptions, to emphasize the purely cinematic as opposed to thetheatrical.
 We find Hindle kneeling on the floor of his surrealistic room stuffingthousands of IBM cards into boxes. Over this we hear a strangemonologue of fragmented words and sentences in an odd foreignaccent. This is punctuated by fierce thunderclaps and howling windthat evolve into ethereal music and tinkling bell sounds. Periodicallythe screen is slashed across with blinding white flashes while thecentral images constantly are transformed through lap-dissolves andmultiple superimpositions. There are flash-forwards of images to beencountered later, though we don't recognize them and thereforedon't interpret them. We see nude lovers, a small boy bathing, abeautiful woman with candles, a huge eyeball, a battery of glaringlights. These are noted for their inherent psychological connotationsand not as narrative devices.The most memorable sequence of Firedrill, possibly one of thegreat scenes in the history of film, involves Hindle lying in anguish onhis floor and slowly reaching out with one hand toward theglimmering void beyond his door. Suddenly a mirror-like reflection ofhis arm and hand appears on the opposite side of the mirror. Whenhe removes his hand we see the vague shadowy figure of a nudewoman silhouetted ghostlike, her skin sparkling. In slow motion thesilhouette of a nude man enters from an opposite direction and thetwo gossamer figures embrace in a weightless ballet of gracefulmotion in some dream of bliss. In the film's final image, the hauntedman has become a child once again, splashing in his bath in a seriesof freeze-frames that grow ever fainter until they vanish.
Synaesthetics and Kinaesthetics:
The Way of All Experience The term kinetic generally indicates motion of material bodies andthe forces and energies associated with it. Thus to isolate a certaintype of film as kinetic and therefore different from other films meanswe're talking more about forces and energies than about matter. Idefine aesthetic quite simply as: the manner of experiencing something. Kinaesthetic, therefore, is the manner of experiencing a thingthrough the forces and energies associated with its motion. This iscalled kinaesthesia, the experience of sensory perception. One whois keenly aware of kinetic qualities is said to possess a kinaestheticsense.The fundamental subject of synaesthetic cinema—forces andenergies—cannot be photographed. It's not what we're seeing somuch as the process and effect of seeing: that is, the phenomenon ofexperience itself, which exists only in the viewer. Synaestheticcinema abandons traditional narrative because events in reality donot move in linear fashion. It abandons common notions of "style"because there is no style in nature. It is concerned less with factsthan with metaphysics, and there is no fact that is not also metaphysical. One cannot photograph metaphysical forces. One cannoteven “represent" them. One can, however, actuallyevoke them in theinarticulate conscious of the viewer.The dynamic interaction of formal proportions in kinaestheticcinema evokes cognition in the inarticulate conscious, which I call kinetic empathy. In perceiving kinetic activity the mind's eye makesits empathy-drawing, translating the graphics into emotionalpsychological equivalents meaningful to the viewer, albeit meaningof an inarticulate nature. "Articulation" of this experience occurs inthe perception of it and is wholly nonverbal. It makes us aware offundamental realities beneath the surface of normal perception:forces and energies.

Social media content shared today in cities, such as Instagram images, their tags and descriptions, is the key form of contemporary city life. It tells people where activities and locations that interest them are and it allows them to share their urban experiences and selfrepresentations. Therefore, any analysis of urban structures and cultures needs to consider social media activity. In our paper, we introduce the novel concept of social media inequality. This concept allows us to quantitatively compare pattern in social media activities between parts of a city, a number of cities, or any other spatial areas. We define this concept using an analogy with the concept of economic inequality. Economic inequality indicates how some economic characteristics or material resources, such as income, wealth or consumption are distributed in a city, country or between countries. Accordingly, we define social media inequality as unequal spatial distribution of social media sharing in a particular geographic area or between areas. To quantify such distributions, we can use many characteristics of social media such as number of people sharing it, the number of photos they have shared, their content, and user assigned tags. We propose that the standard inequality measures used in other disciplines, such as the Gini coefficient, can also be used to characterize social media inequality. To test our ideas, we use a dataset of 7,442,454 public geo-coded Instagram images shared in Manhattan during five months (March-July) in 2014, and also selected data for 287 Census tracts in Manhattan. We compare patterns in Instagram sharing for locals and for visitors for all tracts, and also for hours in a 24 hour cycle. We also look at relations between social media inequality and socio-economic inequality using selected indicators for Census tracts. The inequality of Instagram sharing in Manhattan turns out to be bigger than inequalities in levels of income, rent, and unemployment.

Social media content shared today in cities, such as Instagram images, their tags and descriptions, is the key form of contemporary city life. It tells people where activities and locations that interest them are and it allows them to share their urban experiences and selfrepresentations. Social media also has become one of the most important representations of city life to both its residents and the outside world. One can argue that any city today is as much media content shared in that city on social networks as its infrastructure and economic activities. For these reasons, any analysis of urban structures and cultures needs to consider social media activity and content. While the industry developed many concepts and measurement tools to analyze social media, these concepts and tools were not developed with the view for the comparative urban analysis. Therefore, we need to develop our own concepts that bridge the perspectives of urban studies and design and quantitative analysis of social networks that uses computational methods and “big data.” In the last few years, one of the most frequently discussed public issues has been the rise in income inequality (Stiglitz 2012, Piketty 2014, Atkinson 2015). But inequality does not only refer to distribution of income. It is a more general concept, and it has been used for decades in a number of academic disciplines besides economics, such as urban planning, sociology, education, engineering, and ecology. The quantitative measurements of inequality allow researchers to characterize a set of numbers or compare multiple sets, regardless of what the data represents. In addition to income inequality, we can measure inequality in wealth, education levels, social well-being, and numerous other social characteristics. In our paper, we introduce the novel concept of social media inequality. We define this concept using an analogy with the concept of economic inequality. Economic inequality refers to how some economic characteristics or material resources, such as income, wealth or consumption are distributed in a city, country or between countries (Ray 1998, Milanovic 2007, OECD 2011). Accordingly, we can define social media inequality as the measures of distribution of characteristics of social media content shared in a particular geographic area or between areas. An example of such characteristics is the number of photos shared by all users of a social network such as Instagram in a given city or city area. Another example is the number of hashtags – how many hashtags users added to the photos, and how many of these hashtags are unique. Other examples include average number of tweets shared by a user in a particular period; numbers of tweets shared per month, per week or per hour of a day; the proportions of tweets that were retweeted, and so on. Of course, we can computer and analyze features of content itself - for example, how many different subjects appear the photos, and what are their proportions. In fact, any metric of social media can be used to compare inequality in social 5 media activity between areas - for example, number of likes, length of text messages, most frequent and least frequent words, number of unique topics, number of distinct photographic styles, image compositions, styles of video editing, and so on. We propose that the standard inequality measures used in other disciplines, such as the Gini coefficient, can also be used to characterize social media inequality. We can also compare these measures between content shared on various social networks (Instagram, Twitter, etc.) in the same area or areas. We can do these comparisons for social networks where the main content is text (e.g., Twitter, VK), images (e.g., Instagram, Tumblr), video (e.g., YouTube), or combination of different media (e.g. Facebook, QZone, Sina Weibo, Line, etc.). Finally, we can also compare characteristics of shared content with various social and economic characteristics in the same areas, such as income, rent, the level of education, or ethnic mix. The paper tests some of these ideas using a large dataset of Instagram images shared in Manhattan borough of New York City. This dataset, which we created for this study, contains 7,442,454 public geo-coded Instagram images shared in Manhattan during five months (February 1, 2014 July 31, 2014). Among these images, 1,524,046 were shared by 515,608 city visitors; the remaining 5,918,408 images were shared by 375,876 city residents. Our analysis of the images shared by two types of users in this paper is inspired by the pioneering project Locals and Tourists created by Eric Fischer (Fisher, 2010.) We used the following method to divide users into “visitors” and “locals.” If a user posted all her photos within a single 12-day period out of the total five months, we consider this person a “visitor”. If a user shared a minimum of two photos within any interval larger than 12 days, we consider this person a local. Although this very simple method is expected to produce some errors, we felt that they are acceptable given the size of our dataset. (Such method is also used a number of published papers that analyze patterns in social media activity.) Comparing the locations of images shared by visitors (figure 1) and locals (figure 2) gives us an intuition for social media inequality concept. We can immediately notice that in each case these locations are not distributed evenly. Some parts of the city have many more images than other parts. These figures also suggest that the big proportion of images by city visitors are shared only in a few areas, while the locals share images in most areas of the city. Note that we use the term “shared” rather than “captured” because Instagram allows sharing of any image from user’s phone and not only the ones captured within Instagram app. So users can upload images taken previously in other locations. However, since Instagram captured the geolocation and time when an image was shared (for users who allowed Instagram access to this data), the metadata of images in our dataset tells us about people’s presence at particular place in the city at a particular time.
While the U.S. Census collects data on individuals, it only reports the data aggregated by geographic areas at different scales. We follow a similar logic in our analysis of spatial social media inequality by dividing a city into hundreds of small areas and aggregating characteristics of social media content shared in each area - as opposed to comparing individuals to each other. The way we measure social media inequality is comparable to how Milanovic defines one of the measures of global economic inequality (Milanovic 2006, Concept 1). This measure uses countries as the units of observation. Milanovic does not directly compare the income of people worldwide. Instead he compares average income across different countries to calculate global inequality. In our case, the Census tracts are our units of observation. We aggregate social media characteristics at the tract level in order to analyze social media inequality across all of Manhattan. Social media content shared in a given area may combine contributions from different kinds of users: people who reside in this area, people who live in different parts of the city or in suburbs but spend significant time in this area for work during weekdays; international or domestic tourists visiting a city; companies located in this area, and so on. Together, the content shared by all these users create a collective “voice” of a particular area of a city. A city as a whole can be compared to an orchestra of all these voices (although, of course, they are not necessary performing the same composition.) Applying the concept of inequality to a collection of these urban voices can give us new ways of understanding a city, and provide an additional metric for comparing numerous cities around the world. Social media inequality as we define it refers to the unequal distribution of social media content and its metadata and their characteristics in any type of geographic area – a city, a region, a country, or any other type of area. However, as Fischer’s maps show visually, the density of social media contributions in larger cities is much higher than in non-urban areas, which makes these cities particularly convenient areas of study. We think that our proposed measurements of social media inequality can be useful for urbanism studies, urban planning, urban design, public administration, economics, and other professional and academic fields. While researchers in the fields of social computing, spatial analytics, and “science of cities” have published many quantitative studies analyzing urban data of many kinds (Batty 2013, Goldsmith and Crawford 2014, Townsend 2014, Pucci et al. 2015, Ratti et al. 2006), a significant portion of this analysis cannot be approached without having a degree in computer science. In contrast, social media inequality measurement is a concept that is easy to understand and also easy to calculate. The locations of social media contributions reflect the presence of people in a particular part of a city at a particular time. However, in comparison to pure location data captured by mobile phones or other body sensors, social media images are much more than simple coordinates and time stamps. The content of these contributions can also tell us what people find interesting and how they are spending their time. Therefore, mapping and measuring inequality in 9 characteristics of social media can help us understand how social, economic, and urban design characteristics of cities influence life patterns and the overall “dynamism” and “vitality” of a city. Researchers have never observed perfect equality in any natural, biological or social system or population. In using the term “social media inequality,” we are not suggesting that the goal of urban planners or city administration should be to reduce differences in social media use between various areas to a minimum, or to some optimal level. If people are sharing the same amount of social media in every area of the city, it means that this city does not have any centers or attractions that stand out, or places where many people gather. In terms of modern housing, large American-type suburbs with the same density of houses and same demographics of families and income would probably generate least amount of social media inequality. Today such suburbs are common around the world, from Mexico to China. Given the wide criticism of this classical suburb type, we can assume that some level of spatial social media inequality is desirable. In this case, inequality stands for variety and differentiation while complete equality stands for sameness and lack of variety. But is extreme social media inequality a good thing? For example, do we really want all people living in a city to spend their weekends in a single place? There are certain situations where reducing extreme spatial social media inequality would be very desirable. For example, if city authorities find that most tourists’ social media activity is concentrated in just a few areas surrounding only a few landmarks (like Times Square in New York City), they can change the way the city is promoted to visitors to diversify where tourists go, what they look at, and what they experience. Being able to quantify inequality of social media would allow for better planning and evaluation of such changes. Formulated as a type of spatial analysis, our study compares the parts of the city that attract more people and generate more content shared on social media networks and thus are “social media rich” with parts of the city that are “social media poor.” What are the relationships between such social media rich and social media poor areas? Is social media inequality larger or smaller than economic or social inequality in the same areas? Does social media inequality increase worldwide, similar to how economic inequality has been growing recently? Which parts of the world have the highest social media inequality and which are the most equal? Although our analysis is focusing on one part of a single megacity (i.e., Manhattan in New York City), it can be expanded to consider hundreds of cities around the world to consider such questions.

I present a number of core concepts from data science that are relevant to digital art history and the use of quantitative methods to study any cultural artifacts or processes in general. These concepts are objects, features, data, feature space, and dimension reduction. These concepts enable computational exploration of both large and small visual cultural data. We can analyze relations between works on a single artist, many artists, all digitized production from a whole historical period, holdings in museum collections, collection metadata, or writings about art. The same concepts allow us to study contemporary vernacular visual media using massive social media content. (In our lab, we analyzed works by van Gogh, Mondrian, and Rothko, 6000 paintings by French Impressionists, 20,000 photographs from MoMA photography collection, one million manga pages from manga books, one million artworks of contemporary non-professional artists, and over 13 million Instagram images from 16 global cities.) While data science techniques do not replace other art historical methods, they allow us to see familiar art historical material in new ways, and also to study contemporary digital visual culture. In addition to their relevance to art history and digital humanities, the concepts are also important by themselves. Anybody who wants to understand how our society “thinks with data” needs to understand these concepts. They are used in tens of thousands of quantitative studies of cultural patterns in social media carried out by computer scientists in the last few years. More generally, these concepts are behind data mining, predictive analytics and machine learning, and their numerous industry applications. In fact, they are as central to our “big data society” as other older cultural techniques we use to represent and reason about the world and each other – natural languages, material technologies for preserving and accessing information (paper, printing, digital media, etc.), counting, calculus, or lens-based photo and video imaging. In short, these concepts form the data society’s “mind” – the particular ways of encountering, understanding, and acting on the world and the humans specific to our era.

Will art history fully adapt quantitative and computational techniques as part of its methodology? While the use of computational analysis in literary studies and history has been growing slowly but systematically during 2000s and first part of 2010s, this has not yet happened in the fields that deal with the visual (art history, visual culture, film, and media studies). However, looking at the history of adoption of quantitative methods in the academy suggests that these fields sooner or later will also go through their own “quantitative turns.” Writing in 2001, Adrian Raftery points out that psychology was the first to adopt quantitative statistical methods in 1920s-1930s, followed by economics in 1930s-1940s, sociology in 1960s, and political science in 1990s.2 Now, in 2015, we also know that humanities fields dealing with texts and spatial information (i.e., already mentioned literary studies and history) are going through this process in 2000s- 2010s. So I expect that “humanities of the visual” will be the next to befriend numbers.  This adaption will not, however, simply mean figuring out what be counted, and then using classical statistical methods (developed by the 1930s and still taught today to countless undergraduate and graduate students pretty much in the same way) to analyze these numbers. Instead, it will take place in the context of a fundamental social and cultural development of the early 21st century – the rise of “big data,” and a new set of methods, conventions, and skills that came to be called “data science.” Data science includes classical statistical techniques from the 19th and early 20th century, additional techniques and concepts for data analysis that were developed starting in 1960s with the help of computers, and concepts from a number fields that also develop in the second part of the 20th century around computers: pattern recognition, information retrieval, artificial intelligence, computer science, machine learning, information visualization, data mining. Although the term "data science" is quite recent, it is quite useful as it acts as an umbrella for currently most frequently used methods of computational data analysis. (Alternatively, I could have chosen machine learning or data mining as the key term for this article, but since data science includes their methods, I decided that if I am to refer to all computational data analysis using a single term, data science is best right now.)
Data science includes many ideas developed over many decades, and hundreds of algorithms. This sounds like a lot, and it is. It is much more than can be learned in one or two graduate methods classes, or summarized in a single article, or presented in a single textbook. But rather than simply picking particular algorithms and techniques from a large arsenal of data science, or borrowing whatever technique happens to be the newest and therefore is currently in fashion (for example, “topic modeling” or “deep learning”) and trying to apply them to art history, it is more essential to fist understand the most fundamental as sumption of the field as a whole. That is, we in art history (or any other humanities field) need to learn the core concepts that underlie the use of data science in contemporary societies. These concepts do not require formulas to explain, and they can be presented in one article, which is what I will attempt here. (Once we define these core concepts, a variety of terms employed in data science today can also become less confusing for the novice.)
Surprisingly, after reading thousands of articles and various textbooks over last eight years, I have not found any short text that presents these core concepts together in one place. While many data science textbooks, of course, do talk about them, their presentation often takes place in the context of mathematically sophisticated techniques or particular applications which can make it hard to understand the generality of these ideas.3 These textbooks in general can be challenging to read without computer science background. Since my article is written for humanities audience, it is on purpose biased–my examples of the application of the core concepts of data science come from humanities as opposed to economics or sociology. And along with an exposition, I also have an argument. I will suggest that some parts of data science are more relevant to humanities research than others, and therefore beginning “quantitative humanists” should focus on learning and practicing these techniques first.
If we want to use data science to “understand” some phenomenon (i.e., something outside of a computer), how do we start? Like other approaches that work on data such as classical statistics and data visualization, data science starts with representing some phenomenon or a process in a particular way. This representation may include numbers, categories, digitized texts, images, audio, spatial locations, or connections between elements (i.e., network relations). Only after such a representation is constructed, we can use computers to work on it. In most general terms, creating such a representation involves making three crucial decisions: What are the boundaries of this phenomenon? For example, if we are interested to study “contemporary societies,” how can we make this manageable? Or, if we want to study “modern art,” how we will choose what time period(s), countries, artist(s), and artworks, or other information to include? In another example, let’s say that we are interested in contemporary “amateur photography.” Shall we focus on studying particular groups on Flickr that contain contributions of people who identify themselves as amateur or semi-pro photographers, or shall we sample widely from all of Flickr, Instagram, or other media sharing service since everybody today with a mobile phone with a built-in camera automatically becomes a photographer. What are the objects we will represent? For example, in modern art example, we may include the following “objects” (in data science they can be also called data points, records, samples, measurements, etc.): individual artists, individual artworks, correspondence between artists, reviews in art journals, passages in art book, auction prices. (For example, 2012 Inventing Abstraction exhibition in MoMA in NYC featured a network visualization showing connections between artists based on the number of letters they exchanged.4 In this representation, modernist abstract art was represented by a set of connections between artists, rather than any other kind of object I listed above.) In a “society” example, we can for instance choose a large set of randomly chosen people, and study social media they share, their demographic and economic characteristics, their connections to each other, and biological daily patterns as recorded by sensors they wear. If we want to understand patterns of work in a hospital, we may use as elements people (doctors, nurses, patients, and any others), also medical procedures to be performed, tests to be made, written documentation and medical images produced, etc. What characteristics of each object we will include? (These are also referred to as metadata, features, properties, or attributes.). In humanities, we usually refer to characteristics that are already available as part of the data (because somebody already recorded them) and characteristics we have added (for example, by tagging) as metadata. In social science, the process of manually adding descriptions of data is called coding. In data science, people typically use algorithms to automatically extract additional characteristics from the objects, and they are referred as features (this process is called “feature extraction”). For example, artists’ names is an example of metadata; average brightness and saturation of their paintings, or the length of words used in all titles of their works are examples of features that can be extracted by a computer. Typically features are numerical descriptions (whole or fractional numbers) but they can also take other form. For example, a computer can analyze an image and generate a few words describing content of the image. In general, both metadata and features can use various data types: numbers, categories, free text, network relations, spatial coordinates, dates, times, and so on.  Although it is logical to think of the three questions above as three stages in the process of creating a data representation– limiting the scope, choosing objects, and choosing their characteristics – it is not necessary to proceed in such linear order. At any point in the research, we can add new objects, new types of objects and new characteristics. Or we can find that characteristics we wanted to use are not practical to obtain, so we have to abandon our plans and try to work with other characteristics. In short, the processes of generating a representation and using computer techniques to work on it can proceed in parallel and drive each other.
The author and several colleagues studied cultural differences using these computerized patterns of Instagram postings—arranged by hue and brightness—from Tokyo, New York, Bangkok, and San Francisco.

In 2002, I was in Cologne, Germany, and I went into the best bookstore in the city devoted to humanities and arts titles. Its new-media section contained hundreds of books. However, not a single title was about the key driver of the "computer age": software. I started going through indexes of book after book: No "software."

Yet in the 1990s, software-based tools were adopted in all areas of professional media production and design. In the 2000s, those developments have made their way to the hundreds of millions of people writing blogs and tweeting, uploading photos and videos, reading texts on Scribd, and using free tools that 10 years earlier would have cost tens of thousands of dollars.

Thanks to practices pioneered by Google, the world now operates on web applications that remain forever in beta stage. They can be updated anytime on remote servers without consumers having to do anything—and in fact, Google is revising its search-algorithm code as often as 600 times a year. Welcome to the world of permanent change—a world defined not by heavy industrial machines that are modified infrequently, but by software that is always in flux.

Software has become a universal language, the interface to our imagination and the world. What electricity and the combustion engine were to the early 20th century, software is to the early 21st century. I think of it as a layer that permeates contemporary societies. If we want to understand today's techniques of communication, representation, simulation, analysis, decision making, memory, vision, writing, and interaction, we must understand software.

But while scholars and media and new-media theorists have covered all aspects of the IT revolution, creating fields like cyberculture studies, Internet studies, game studies, new-media theory, and the digital humanities, they have paid comparatively little attention to software, the engine that drives almost all they study.

It's time they did.

Consider the modern "atom" of cultural creation: a "document," i.e. content stored in a physical form delivered to consumers via physical copies (books, films, audio records) or electronic transmission (television). In software culture, we no longer have "documents." Instead, we have "software performances."

If you are a scholar working inside Google or Facebook, you have a major advantage over colleagues in academe.

I use the word "performance" because what we are experiencing is constructed by software in real time. Whether we are exploring a website, playing a video game, or using an app on a mobile phone to locate nearby friends or a place to eat, we are engaging with the dynamic outputs of computation.

Although static documents may be involved, a scholar cannot simply consult a single PDF or JPEG file the way 20th-century critics examined a novel, movie, or TV program. Software often has no finite boundaries. For instance, a user of Google Earth is likely to experience a different "earth" every time he or she uses the application. Google could have updated some of the satellite photographs or added new Street Views and 3D buildings. At any time, a user of the application can also load more geospatial data created by other users and companies.

Google Earth is not just a "message." It is a platform for users to build on. And while we can find some continuity here with users' creative reworking of commercial media in the 20th century—pop art and appropriation, music, slash fiction and video, and so on—the differences are larger than the similarities.

Even when a user is working only with a single local media file stored in his or her computer, the experience is still only partly defined by the file's content and organization. The user is free to navigate the document, choosing both what information to see and the sequence in which to see it. (In Google Earth, I can zoom in and out, switching between a bird's-eye view of the area, and its details; I can also switch between different kinds of maps.)

Most important, software is not hard-wired to any document or machine: New tools can be easily added without changing the documents themselves. With a single click, I can add sharing buttons to my blog, thus enabling new ways to circulate its content. When I open a text document in Mac OS Preview media viewer, I can highlight, add comments and links, draw and add thought bubbles. Photoshop allows me to save my edits on separate "adjustment layers," without modifying the original image. And so on.

All that requires a new way to analyze media and culture. Since the early 2000s, some of us (mostly from new-media studies and digital arts) have been working to meet that challenge. As far as I know, I was the first to use the terms "software studies" and "software theory" in 2001. The field of software studies gradually took shape in the mid-2000s. In 2006, Matthew Fuller, author of the pioneering Behind the Blip: Essays on the Culture of Software(Sagebrush Education Resources, 2003), organized the first Software Studies Workshop in Rotterdam. "Software is often a blind spot in the theorization and study of computational and networked digital media," Fuller wrote in introducing the workshop. "In a sense, all intellectual work is now 'software study,' in that software provides its media and its context, but there are very few places where the specific nature, the materiality, of software is studied except as a matter of engineering."

In 2007, we started the Software Studies Initiative at the University of California at San Diego, and in 2008 we held the second software-studies workshop. The MIT Press offers a software-studies series, and a growing number of books in other fields (media theory, platform studies, digital humanities, Internet studies, game studies) also help us better understand the roles software plays in our lives. In 2011, Fuller and other researchers in Britain began Computational Culture, an open-access peer-reviewed journal.

T

here is much more to do. One question that particularly interests me is how software studies can contribute to "big data"—analyzing vast data sets—in fields like the digital humanities, computational social science, and social computing. Here are some of the key questions related to big cultural data that software studies could help answer:

What are interactive-media "data"? Software code as it executes, the records of user interactions (for example, clicks and cursor movements), the video recording of a user's screen, a user's brain activity as captured by an EEG or fMRI? All of the above, or something else?

To use terms from linguistics, rather than thinking of code as language, we may want to study it as speech.

Over the past few years, a growing number of scholars in the digital humanities have started to use computational tools to analyze large sets of static digitized cultural artifacts, such as 19th-century novels or the letters of Enlightenment thinkers. They follow traditional humanities approaches—looking at the cultural objects (rather than peoples' interaction with these objects). What has changed is the scale, not the method.

The study of software culture calls for a fundamentally different humanities methodology. We need to be able to record and analyze interactive experiences, following individual users as they navigate a website or play a video game; to study different players, as opposed to using only our own game-play as the basis for analysis; to watch visitors of an interactive installation as they explore the possibilities defined by the designer—possibilities that become actual events only when the visitors act on them.

In other words, we need to figure out how to adequately represent "software performances" as "data." Some answers can come from the field of human-computer interaction, where researchers in academe and private enterprise study how people engage with computer interfaces. The goals of that research, however, are usually practical: to identify the problems in new interfaces and to fix them. The goals of digital humanities' analysis of interactive media will be different—to understand how people construct meanings from their interactions, and how their social and cultural experiences are mediated by software. So we need to develop our own methods of transcribing, analyzing, and visualizing interactive experiences. Together with the Experimental Game Lab, directed by Sheldon Brown, for example, my lab analyzed the experiences of hundreds of users of Scalable City, a large-scale, complex virtual-world art installation created in Brown's lab. One of our goals was to help future users have more challenging interactive experiences.

Who has access to detailed records of user interactions with cultural artifacts and services on the web, and what are the implications of being able to analyze these data?

From the early days of interactive human-computer interfaces, tracking users' interaction with software was easy. Why? Because software continuously monitors inputs like key presses, mouse movements, menu selections, finger gestures over a touch surface, and voice commands.

The shift from desktop to web computing in the 1990s has turned the already existing possibility of recording and storing users' inputs into a fundamental component of a "software-media complex." Since dynamic websites and services (Amazon's online store, personal blogs that use Google's Blogger system, online games, etc.) are operated by software residing on company's servers, it is easy to log the details of user interactions. Each web server keeps detailed information on all visits to a given site. A separate category of software and services exemplified by Google Analytics has emerged to help fine-tune the design of a website or blog.

Today social-media companies make available to their users some of the recorded information about visitors' interactions with the sites, blogs, or accounts they own; the companies also provide interactive visualizations to help people figure out which published items are most popular, and where their visitors are coming from. However, usually the companies keep the really detailed records to themselves. Therefore, if you are one of the few social scientists working inside giants such as Facebook or Google, you have an amazing advantage over your colleagues in the academy. You can ask questions others can't. This could create a real divide in the future between academic and corporate researchers. While the latter will be able to analyze social and cultural patterns on both supermicro and supermacro levels, the former will have only a normal "lens," which can neither get extremely close nor zoom out to a planetary view.

Who benefits from the analysis of the cultural activities of hundreds of millions of people?Automatic targeting of ads on Google networks, Facebook, and Twitter already uses both texts of users' posts or emails and other data, but learning how hundreds of millions of people interact with billions of images and social-network videos could not only help advertisers craft more-successful visual ads but also help academics raise new questions.

Can we analyze the code of software programs? It's not as easy as you may think. The code itself is "big data."

Early software programs such as 1970s video games were relatively short. However, in any contemporary commercial web service or operating system, the program code will simply be too long and complex to allow you to read and interpret it like a short story. While Windows NT 3.1 (1993) was estimated to contain four to five million source lines of code, Windows XP (2001) had some 40 million. MAC OS turned out even bigger, with OS X 10.4 (2005) code at 86 million lines. The estimated number of lines in Adobe Creative Suite 3 (which includes Photoshop, Illustrator, and a number of other popular applications to produce media) is 80 million.

The gradual move of application software to the web also brings with it a new set of considerations. Web services, apps, and dynamic sites often use multi-tier software architecture, where a number of separate modules (for example, a web client, application server, and a database) work together. Especially in the case of large commercial sites like amazon.com, what the user experiences as a single web page may involve continuous interactions between dozens or even hundreds of separate software processes.

The complexity and distributed architecture of contemporary large-scale software poses a serious challenge to the idea of "reading the code." However, even if a program is relatively short and a cultural critic understands exactly what the program is supposed to do, this understanding of the logical structure of the program can't be translated into envisioning the actual user experience.

The attraction of "reading the code" approach for the humanities is that it creates an illusion that we have a static and definite text we can study—i.e., a program listing. But we have to accept the fundamental variability of the actual "software performances." So rather than analyzing the code as an abstract entity, we may instead trace how it is executed, or "performed," in particular user sessions. To use the terms from linguistics, rather than thinking of the code as language, we may want to study it as speech.

Some researchers, like Mark Marino and others working in "critical code studies," have been promoting nuanced, theoretically rigorous, and rich ideas about what it means to "read the code," so my critique is aimed only at a naïve version of the idea that I sometimes encounter in the humanities.

The development of methods to study contemporary software in a way that can be discussed in articles, conferences, and public debates by nonprogrammers, is a key task for software studies. However, given both the complexity of software systems and the fact that, at least at present, only a very small number of media and cultural researchers are trained in software engineering, I don't expect that we can solve this problem in a short time.

And yet, confronting it is crucial, not just for the academy but also for society at large. How can we discuss publicly the decisions made by Google Search algorithms, or Facebook's algorithms controlling what is shown on our news feeds? Even if these companies made all their software open source, its size and complexity would make public discussion very challenging.

While some of the details from popular web companies are published in academic papers written by researchers working at these companies, only people with computer-science and statistics backgrounds can understand them. Moreover, many popular software services use machine-leaning technology that often results in "black box" solutions. (While the software achieves desired results, we don't know the rules it follows.)

As more and more of our cultural experiences, social interactions, and decision making are governed by large-scale software systems, the ability of nonexperts to discuss how these systems work becomes crucial. If we reduce each complex system to a one-page description of its algorithm, will we capture enough of software behavior? Or will the nuances of particular decisions made by software in every particular case be lost?

The role of software studies is not to answer these and many other questions about our new interactive world, but rather to articulate them and offer examples of how they can be approached. And to encourage people across all disciplines to think about how software changes what they study and how they study it.

In the Phototrails project (phototrails.net), created by myself, Nadav Hochman, and Jay Chow, we visualized patterns in the use of Instagram across 2.3 million photos from 13 global cities. In the paper accompanying the project, we attempted to combine two mirror sides of software studies—thinking about software interfaces and how they influence what we do and at the same time studying large-scale behaviors of many software users. One of the key questions we raised: How much of the differences among the cities can we find, given that everybody uses the same Instagram app that comes with its own strong "message" (all photos have the same square size, and all users have access to the same set of build-in filters to make their photos more aesthetic in the same ways). While we did find small but systematic differences in the photos from each city, the use of Instagram software itself was remarkably consistent.

How does the software we use influence what we express and imagine? Shall we continue to accept the decisions made for us by algorithms if we don't know how they operate? What does it mean to be a citizen of a software society? These and many other important questions are waiting to be analyzed.

Hundreds of people stuck in a giant swimming pool passively floating to the rhythm of artificial waves. The poor resolution of the found footage muddles them into a contextless and faceless crowd. Nobody tries to escape the crowd, or go against the current. They are trapped but happy enough. It’s like Dante’s Inferno but without the drama. Just the people floating in the mud.

The final scene of Mainsqueeze captures “a contemporary atmosphere or mood” which sets the present as a time out of joint, encapsulated by the washing machine that tears itself apart over the course of the film. Rafman poses the present escape from the real towards the simulated as the result of a general feeling of turmoil that leads to flight rather than revolt. In the video, the first readable line of text is written on the forehead of a sleeping drunk man at the beginning of the film: “LOSER”. He smiles, and we are led to wonder who the loser really is.

Yet Rafman is not making a particular ethical statement: “Mainsqueeze expresses a moral condition or atmosphere without making a moral judgment. I gravitate towards communities like 4chan because I see in them a compelling mix of attraction and repulsion. This ambivalence is reflected in the current cultural moment.”

Surfing the deep web, Rafman collects, orders, observes, and makes his source material visible to us: “Mainsqueeze is entirely composed of footage found through my online explorations. The voice over text is a combination of modified quotes from literature, Tumblr, and comments on various message boards. I feel less of a need to create original material from scratch due to the sheer abundance of material out in the world to work with. The craft is found in the searching, selecting or curating, and editing together of the materials pulled from far-flung corners of the web.” Yet, he insists “it is not about fetish tourism or shocking people about what exists in the dark corners of the net, rather, I am giving the sourced material a poetic treatment.”

Rafman assumes, and in turn invites us to assume, a difficult position: he is simultaneously the drunk man, the one who paints his face and the one passerby who thinks they are both stoned. Sometimes he himself indulges in degradation, and we as viewers are equally implicated.

An original use of voice-over contributes to this sense of viewer involvement. It is used neither to generate empathy nor to signify a complete alterity (as when synthesized voices are used). Rafman explains that “This particular tone came about through experimenting with a montage of a wide range of material; moments of philosophical epiphany, pseudo-intellectual quotes from tumblr, banal confessional message boards, comments from reddit threads, etc.” InMainsqueeze, this montage is turned into a profound yet familiar voice that often addresses the viewer directly, dragging you into the world depicted on screen. Let yourself in.

Our story begins between the end of the 1950s and the beginning of the 1960s, when technological progress on one hand and developments in art on the other created the conditions for art, science and technology to intertwine once more. Such an encounter was anything but new in the history of art, having been vigorously embraced by the avant-garde movements: see Lazlo Moholy-Nagy, often invoked as one of the founding fathers of New Media Art, above all for his Licht-raum-modulator (1930), a kinetic sculpture that produces fascinating light effects. And it was the historic avant-garde movements that informed the new artistic experiences that sought to go beyond what then looked like the dead end of Abstract Expressionism: New Dada, Nouveau Réalisme, Gutai, Happening, Fluxus, Kinetic Art, Arte Programmata, Optical Art, Pop Art and Video Art. Reality, in the shape of real or represented objects, entered artworks; the pop culture conveyed by the media began to capture the attention of artists; art appropriated all media, from the human body to consumer products, from advertising to television sets to cars, and theoretical developments like cybernetics and information theory informed the lexicon of art. This is, for example, what John Brockman says about John Cage.


For the first and only time in the history of art, the implicit perspective in the most generic interpretation of the expression New Media Art became a mass strategy, common to all the avantgarde art of the period. This situation was short-lived: while a few “new media” and artistic strategies, from assemblage to photography, performance and conceptual interventions on mechanically reproduced images rapidly became the stuff of the establishment, the more radically technophile or science-based expressions, like Kinetic and Optical art, were put out of action, and video entered a splendid isolation of its own that was to last until the early 1990s. At the same time, in the States, the spectre of permanent war gave an incredible boost to scientific and technological research. In 1946 the University of Pennsylvania presented the first digital calculator, ENIAC (Electronic Numerical Integrator and Computer); 1951 saw the launch of UNIVAC, the first computer to hit the market, capable of processing both numerical data and text. These were huge machines without any kind of user interface, that accepted programs in the shape of perforated cards and could only be operated by highly skilled users. Accessibility was also very restricted: developed for military applications, they resided mostly in research centers and universities. It was in Bell Laboratories in Murray Hill (New Jersey) in particular that the first studies on the algorithmic production of text, music and images were carried out, and not by artists, but engineers and researchers who saw these experiments as more or less necessary diversions to their research work. The electronic engineer A. Michael Noll, for example, was taken on by Bell Labs in 1961 and worked there for 15 years. In the summer of 1962 he created his first works of “Computer Art”, abstract images generated by algorithms and mathematical functions that were an evident tribute to Piet Mondrian and Cubism. Around 1963 many pioneers began working in this direction, including Lillian Schwartz, Herbert Franke, Manfred Mohr, Jean-Pierre Hébert and Roman Verotsko. In April 1965 the Howard Wise Gallery in New York, the same venue that brought Gruppo Zero and Kinetic Art to America, staged the exhibition Computer-Generated Pictures by Bela Julesz and Michael Noll. Computer Art appeared in a number of group shows, including Cybernetic Serendipity (ICA, London 1968), Tendencija 4 (Zagreb 1969) and Computerkunst (Hannover 1969). [2] At the same time, potential uses of computers in literature and music were also being studied: on one hand there was the combinatory literature developed by Alison Knowles at Bell Labs and the members of the European group OuLiPo (Ouvroir de Littérature Potentielle), founded in 1960 by Raymond Queneau and François Le Lionnais; and on the other the work of the composer James Tenney at Bell Labs. [3] This initial foray into Computer Art therefore came about in an extremely restricted context, in both sociological and technological terms. From an aesthetic point of view the massive mainframes of the sixties placed great limitations on artists and were extremely difficult to use, and the result was that in this niche engineers vastly outnumbered genuine artists. In view of this, much Computer Art of the sixties is exceptionally ingenuous aesthetically speaking – in the words of Jim Pomeroy, it rolled out «flashy geometric logos tunneling through twirling wire-frames,’ graphic nudes, adolescent sci-fi fantasies, and endless variations on the Mona Lisa». [4] A. Michael Noll candidly confesses.

Yet dismissing Computer Art as merely ingenuous would be a simplistic way of looking at things. Even supposing that the only achievement of Noll and the first computer artists was to show it was possible to make art with a computer, their contribution to the evolution of the medium was crucial. For Computer Art not only paved the way for New Media Art, but the whole of computer graphics, which over the years has progressed to photorealistic videogames and 3D animation. Even considering merely this dual legacy we can appreciate the scope of its contribution to the culture of the twentieth century. And the success, however fleeting, of Computer Art also points up something else: the openness of the art world of the sixties to the most advanced, precarious fringes of cultural experimentation, its acceptance of ideas that would be hard pressed to find a welcome elsewhere. The best demonstration of this was probably the 1968 exhibition Cybernetic Serendipity curated by Jasia Reichardt at the Institute of Contemporary Art in London. This show was part of the work of the Independent Group and resulted from the 1965 encounter between Reichardt and Max Bense, the German philosopher, a key figure of the Stuttgart school, who studied the relationships between maths, language and art, and coined the term “information aesthetics”. According to Brent MacGregor, it was Bense who told Reichardt to “look into computers”. [6] In 1966 the exhibition was announced at a public conference, and fundraising began. Despite initial expectations, the only private company to invest significantly in it was IBM; the rest was covered by the Arts Council. Cybernetic Serendipity was not an exhibition of Computer Art, but a multidisciplinary event that explored the impact of information technology and cybernetic theory on life and contemporary creativity. It was divided into three sections: the first featured works – images, but also music, animations and texts – generated by computers, the second contained cybernetic robots and “painting machines”, and the third explored the social uses of computers and the history of cybernetics. Alongside the pioneers of Computer Art and cybernetic art, from Charles Csuri to Michael Noll, John Whitney to Edward Ihnatowicz to the Computer Technique Group of Tokyo, were artists who shared aesthetic, thematic or formal characteristics with the latter (Nam June Paik, Jean Tinguely and his machines, James Seawright, the Optical painter Bridget Riley, and avant-garde musicians like John Cage and Jannis Xenakis). But there were also explanatory elements and even a computer, provided by IBM, that offered a service for booking flights. According to the curator.

Cybernetic Serendipity came about in a context, the British context, which was of great interest. Catherine Mason’s research [8] has in fact shown that Britain’s distinctive education system facilitated the development of relationships between art, science and technology between the sixties and the eighties. A legacy of the Victorian education system, Britain’s design schools provided both artistic education and training in the applied arts. In the 1950s, the Independent Group addressed, among other things, the implications of science, technology and the mass media on art and society, culminating in the exhibition This Is Tomorrow (Whitechapel Art Gallery, 1956). In 1953 Richard Hamilton went to teach at King’s College in Newcastle, where, together with Victor Pasmore, he held a Basic Design course. Among their students was Roy Ascott, who was encouraged to cultivate his interest in communication, interactivity and cybernetics. In 1961, Ascott was asked by the Ealing Art School to create a two-year course based on the principles of cybernetics: his Ground Course, along with his subsequent appointments, was to play a crucial role in the education of a new generation of artists and designers. In 1967 the first polytechnics appeared, thanks to sizeable government investments in technology in the post war period, which also led to the creation of a Ministry of Technology. In the polytechnics, as Catherine Mason notes, an art student could also learn programming. In the seventies this led to a wide network of schools engaged in Computer Art, yielding interesting results above all in computer graphics for television and advertising. At the same time, these academic roots enabled students and lecturers to develop their own creative work, despite the relative lack of interest in digital art from the art world. And while British Computer Art survived in the world of academe, it soon developed systems of support and critical debate. In 1968, in connection with the British Computer Society, the Computer Arts Society (CAS) was founded. In 1969, CAS launched its own publication, Page, as a platform for debate and critical engagement. Equally early on, CAS began to look beyond the United Kingdom, setting up chapters in various European countries and coming to the States in 1971. In 1970 the association had 377 members, including libraries and institutions, in 17 countries. In this period it put together a collection that included works by pioneers like Manuel Barbadillo, Charles Csuri, Herbert W. Franke, Edward Ihnatowicz, Ken Knowlton, Manfred Mohr, Georg Nees, Frieder Nake, Lillian Schwartz and Alan Sutcliffe, and in 2007, with Mason’s involvement, this collection was bought by the Victoria and Albert Museum in London. Just how receptive the art world of the sixties was to the “art and technology” pairing is also proved by the milieu that sprung up around the distinctive figure of Billy Klüver (1927 – 2004). An electronic engineer of Swedish origin, in 1958 Klüver was hired by Bell Labs in Murray Hill. With a life-long interest in art, in the early seventies he began to work with artists. In 1960 he provided technical support to the Swiss artist Jean Tinguely (after being introduced to him by Pontus Hultén) for his spectacular Homage to New York (1960), a kinetic machine that self-destructed in the Sculpture Garden of the MoMA in New York. Robert Rauschenberg was also involved in this project. Following that, Klüver provided technical support to various artists: he worked with Rauschenberg on the installation Oracle (1962 – 1965) supplying the artist with remote controlled radios, and he helped Jasper Johns and Andy Warhol, providing the material for the latter’s famous Silver Clouds, the helium-filled pillows that accompanied his temporary break from painting, presented in a solo show at the Leo Castelli gallery in 1966. 1966 also saw Klüver’s first major production, the outcome of a collaboration with Rauschenberg. From 14 to 23 October 1966, at the 69th Regiment Armory in New York, he presented the event 9 Evenings: Theatre and Engineering, a series of multimedia performances featuring ten artists working with thirty engineers and scientists from Bell Labs. Participants included Robert Rauschenberg, John Cage, David Tudor, Yvonne Rainer, Robert Whitman and Öyvind Fahlström. During the event Klüver discussed the idea of giving this collaboration between artists and engineers more stable foundations, and this was what led to the establishment of Experiments in Arts and Technology (E.A.T.), a no-profit association launched at the start of the following year that promoted collaborations between artists and engineers with both technical and financial input, thanks to ongoing links with the technology industry. By 1969 E.A.T boasted 4,000 members and various chapters throughout the United States. [9] Klüver’s collaborative model was in fact a two-way process: while on one hand he was convinced that technicians could help artists achieve their objectives, on the other he believed that artists, as visionaries and active agents of social change, could influence the development of technology. This is Barbara Rose’s take on the matter.

If some kind of follow-up had materialized, these early experiments, and the model pursued by E.A.T. – to get acknowledged exponents of the artistic avant-garde working in close contact with engineers, while keeping their respective roles distinct – could feasibly be attributed a key role in the history of contemporary art. So how did it come to pass that the great emphasis placed in the sixties on the “art and technology” pairing by key figures like Jasia Reichardt, Roy Ascott, Billy Klüver, Robert Rauschenberg and Pontus Hultén, as well as Jack Burnham, gradually waned in subsequent years, leaving only the faintest of traces in the official historiography of art? How was it that one of the most significant components of the neo avant-garde ended up as an underground phenomenon, carving out a niche that enabled it to go unnoticed for the next thirty years? There is no single answer to this question: we must rather look to a series of circumstances that emerged during the seventies. In the first place, in this period the “art and technology” pairing found itself up against ideological and political opposition connected to the military purposes of technological research and the considerable financial interests involved. The Vietnam war, and the protests against it from artistic and intellectual quarters, fuelled opposition to the “art and technology” model. “Technology is what we do to the Black Panthers and Vietnamese”, Richard Serra asserted in 1969. [13] Beyond the political sphere, other academics have highlighted the emergence in the late sixties of “anti-computer” sentiment, bound up with enduring concepts such as the romantic vision of the artists and the fear that technology might supersede the individual and undermine the central role of the artist in the creative act. [14] It has also been observed that the critical model underpinning the acknowledgement of the importance of the “art and technology” pairing has encountered varying fortunes. In a 2007 essay, [15] in line with Jack Burnham, Edward A. Shanken asserts that the hermeneutic approach imposed by Alois Riegl and summed up in the concept of Kunstwollen, quashed the theories of Gottfried Semper, according to whom art reflects “economic, technical and social relationships”. In Shanken’s opinion, this approach still endures today, helping to keep New Media Art outside the canons of contemporary art. In the short term, these two prejudices conspired against the operative and interpretative model of the “art and technology” pairing, with a number of significant results: video retired into a niche, despite continuing to have (limited) critical success, above all in works that put formal exploration of the medium in second place, as per the “narcissistic” line plotted by Rosalind Krauss; Kinetic Art and Optical Art, also steeped in technophile rhetoric, vanished completely from the scene, after an initial period of great success, to be rediscovered only relatively recently; even a certain interpretative approach to Conceptual Art – as put forward by Jack Burnham in Software (New York, Jewish Museum 1970) and Kynaston McShine in Information (New York, MoMA 1970) – that relates conceptual work to the advent of information technologies, surrendered to other approaches with less of a technological vein. As for the nascent field of New Media Art, the collaborative model developed by Klüver was well suited to the organization of one-off events, but less to facilitating continuity in artists’ work. Lastly, Computer Art had to come to terms with its aesthetic limitations and the problems involved in actually accessing the machines, which continued to be expensive and bulky. During the seventies computers became more accessible, albeit gradually. Research into increasingly intuitive forms of manmachine interaction made enormous progress, and in 1969 the first distributed network made its appearance, in the shape of Arpanet. In 1971, thanks to the creation of a common protocol among various university and corporate networks, the internet was born. In parallel to this, alongside the cumbersome mainframes, cheaper, more manageable computers appeared: minicomputers (like the PDP-8, distributed as of 1968); microcomputers, like the famed Altair 8800, distributed as of 1975; and home computers, headed up by the equally legendary Apple II (1977), produced by the startup Apple Computer (founded by Steve Wozniak and Steve Jobs in 1976). With the arrival of home computers on the scene, computing branched out of research centers and universities and entered offices and households. A complex, variegated culture sprung up around them, with contributions not only from engineers and high level researchers, but also amateurs and enthusiasts. Many of them had radical political ideas, influenced by Californian counterculture. Much of the New Media Art of the seventies was an expression of this complex cultural milieu. In this context it is not easy to identify figures who can be described simply as “artists”: most of them worked across the disciplines, researchers and employees of the hi-tech industry with an artistic sideline. Douglas Kahn relates, for example, that the first serious attempt to make music with an Altair 8800 was undertaken between 1970 and 1975 by Ned Lagin, who was doing astronaut training at the MIT, but also studied jazz and composition. This work earned him a temporary collaboration with the Grateful Dead. In the same enclave of enthusiasts in the Bay Area there was Paul De Marinis, who worked with Jim Pomeroy and David Tudor on a number of sound installations before starting out on his own artistic career. [16] Visual experimentation received impetus from university and corporate circles. In Stanford University in 1970, the Xerox Corporation opened the Palo Alto Research Centre (PARC), devoted to the development of graphic applications; in the same year, General Electric presented Genigraphics, a graphic system designed for the business world, but used extensively by artists. In 1973, the main computing association in the United States, the ACM (Association for Computing Machinery), set up SIGGRAPH, its “Special Interest Group on GRAPHics and Interactive Techniques”, which organized its first conference in 1974. From then on SIGGRAPH became the main international showcase for developments in computer graphics. This field was to be heavily influenced by the discovery of fractals, described in 1975 by the French-American mathematician Benoît Mandelbrot, then researcher at IBM, as geometric forms that can be split into parts, each a small scale copy of the whole. [17] Throughout the decade, thanks to institutional and corporate support, research into the algorithmic generation of images thus developed, between the more aesthetically and conceptually conscious work of such artists as Charles Csuri, Manfred Mohr and Vera Molnar on the one hand, and the simple deployment of the productive and aesthetic potential of the new tools on the other. Something similar happened with robotics. In 1973 at the University of California San Diego (UCSD), Harold Cohen launched the AARON project, which consisted in developing a form of artificial intelligence capable of painting. Having trained as a painter, over the years Cohen attempted to teach AARON the basic rules of painting, developing its “aesthetic tastes” and decision-making power. The painting done by AARON naturally closely resembles that of Cohen, though the machine did gradually develop its own style over time. In Britain Edward Ihnatowicz, who in 1971 began working as a Research Assistant in the Department of Mechanical Engineering at University College in London, produced his most ambitious project, the cybernetic sculpture The Senster (1970 – 1974), thanks to a commission from Philips, which exhibited it for four years in its permanent exhibition space in Eindhoven, before dismantling it. The sculpture, a 4 meter aluminium structure controlled by a computer, responded to the voices and movements of viewers. In the late seventies and early eighties it was above all telecommunications that lent New Media Art a presence and a profile outside of the corporate/university world. While one to one communication systems (like the telephone) and one to many systems (like mail) elicited the attention of the avant-garde movements and Fluxus, before the advent of the Internet satellite broadcasting was the technology that afforded concrete opportunities to explore the field of communications. In 1973, for the first time in history, satellite technology succeeded in broadcasting a cultural event – Elvis Presley’s concert in Hawaii – to the whole world. On 29 December 1976, with the support of the Contemporary Arts Museum in Houston, the video artist Douglas Davis broadcast the closing minutes of his performance Seven Thoughts to all the IntelSat channels. The following year, thanks to funding from NASA, the Californian artists Kit Galloway and Sherrie Rabinowitz produced Satellite Arts Project ’77, which connected two NASA centers, one on the East Coast and one on the West Coast, via satellite: images of dancers performing in the two centers were filmed and edited, using a simple chroma-key, to form a single live image. In this way, performers physically 3,000 miles apart could act as if dancing together on the same stage. Dance was adopted as a traditional performing art capable of exploring the limitations and potential of technology. [18] In the same year Documenta 6, curated by Manfred Schneckenburger, was devoted to means of communication, with the aim of exploring the position of art in the media society. The exhibition presented photography, video and video installations, and opened up to television by means of satellite broadcasts of performances by Davis, Nam June Paik and Joseph Beuys.

It was above all in the 1980s that artistic work on communication gathered pace, extending to telematics too. 1980 saw two major events, the conference Artists’ Use of Telecommunications, organized by Carl Eugene Loeffler at the Museum of Modern Art in San Francisco, and Hole in Space, a public art project by Galloway and Rabinowitz. The former was an international event that connected up participants in different areas of the globe by satellite, Slow-Scan TV (video broadcast via telephone) or telematic network: from the Center for Advanced Visual Studies at the M.I.T. in Cambridge (USA) to Japan’s Tsukuba University; from the Alternative Media Center of New York to the Trinity Video and Ontario College of Art in Toronto; from the Western Front Society in Vancouver to the Museum des 20 Jahrhunderts in Vienna. Participants included Robert Adrian, Bill Bartlett, Douglas Davis, Carl Loeffler, David Ross, Aldo Tambellini, Norman White, Gene Youngblood and Peter Weibel. The event highlighted the presence of a solid network of traditional art institutions, research centers and media centers. Hole in Space, on the other hand, created a satellite bridge between public areas in two cities (New York and Los Angeles), with large screens installed at the Lincoln Center for the Performing Arts in New York City and the Broadway Department Store in Century City, Los Angeles, respectively. The screens showed live footage from a camera placed beside each one, enabling people in the street, most of whom were unaware that the event was taking place, to interact with others thousands of miles away. The result was a highly participative, spectacular event, that attracted various audiences who explored different levels of interaction and remote communication: relational aesthetics ante-litteram – but also, as it has been defined on YouTube, “the mother of all video chats”. In 1982 it was the turn of The World in 24 Hours, coordinated by Robert Adrian from the Ars Electronica Festival in Linz and featuring a wide range of communications technologies: from phone to fax, Slow-Scan TV and telematic networks, followed in 1983 by La Plissure du Texte by Roy Ascott (Paris, Musée d’Art Moderne de la Ville de Paris), a collaborative text produced by various users connected by BBS, and in 1984 by Good Morning Mr Orwell, a satellite broadcast of video pieces and live performances coordinated by Nam June Paik and produced by WNET TV in New York in collaboration with the Pompidou Center in Paris, seen by more than 10 million people. All these events reveal both the upsurge in interest from traditional art institutions and the great ferment of the field, with the involvement of both companies and specialized centers, some of which came into being in that very decade. The interest from traditional art institutions must however be seen in context. The nascent technologies were the hot topic of the day, and it was not difficult to get sponsorship from the hi-tech industry and television networks. By the early eighties the latter enjoyed an unprecedented presence in society, and critical reflections on the media and their power to manipulate were advanced by artists and intellectuals, and reached the public at large (Sidney Lumet’s film Network, on the power of television, was released in 1976). Moreover, in the decade that saw the return of painting and the explosion of the art market, the institutions took it upon themselves to support less stable, less marketable artistic genres like video, photography and performance. In other words, while conditions were favorable, the reappearance of New Media Art in the establishment art world during the 1980s was conditioned by external factors and was on the whole too fleeting to lead to lasting continuity. All of this emerges clearly if we consider two key events in this decade: the exhibition Les Immateriaux, curated by Jean Francois Lyotard and Thierry Chaput for the Pompidou Center in Paris in 1985; and the 1986 Venice Biennale, coordinated by Maurizio Calvesi and entitled “Art and Science”. The first was not actually an exhibition devoted to the New Media or art numerique, as it is known in France. It started life as a project on the “new materials of creativity”, but the involvement – at a late stage – of Lyotard transformed it into an exploration of post-modern sensibility. As Lyotard said: «It is not our intention to sum up the new technologies in this exhibition [...] or to explain how they work. All it attempts is to discover and raise a sensibility that is specific to post-modernism, and we assume that it exists already». [19] Its press release described it as a “non-exhibition”, and one of its stated aims was to challenge the modern, “prescriptive” model of the exhibition, connected to the 19th century salon and the gallery. In Les Immateriaux works were not hung on the walls: cables attached to the floor and ceiling divided up a decentralised setting, which could be explored in various ways. Visitors were given a walkman with the soundtrack of the exhibition, which played according to their position in the venue: this collage of music, sounds and texts, only some of which actually related to the exhibition, aimed to create a powerful sensation of instability. The event also featured works by conceptual and minimal artists, from Joseph Kosuth to Dan Flavin and Robert Ryman, precursors like Marcel Duchamp and MoholyNagy, and artists working with communication technologies, such as Roy Ascott and Rolf Gelhaar; yet it was the exhibition itself that was designed “as a work of art”, to the point that the actual works on show are rarely mentioned in the numerous comments that the event elicited. [20] Once again, we are faced with a singular contrast: while on one hand Les Immateriaux was of seminal importance for New Media Art, configuring the aesthetic and philosophic categories that were to be its focus in subsequent decades, on the other hand it showed the art crowd that, as Jasia Reichardt commented with regard to Cybernetic Serendipity, this area was yet to produce any definitive outcomes, comparable with those of other artistic tendencies, and was as yet mainly to be appreciated for its aspect of research and experimentation. Similar observations could be made with regard to the 1986 Venice Biennale, where the “Technology and Computing” section curated by Roy Ascott, Don Foresta, Tom Sherman and Tommaso Trini was given a deliberately “workshop” style layout. The central nucleus of this was the Planetary Network, coordinated by Roy Ascott: for three weeks in this workshop in the heart of the Corderie venue, the artists present conducted communicational exchanges of various kinds with other artists in twenty different locations, from Canada to Australia, using three communications protocols: email, fax and Slow-Scan TV. The networking aspect – artists across the globe working together – clearly prevailed over the actual material exchanged: video, images faxed with manual interventions by the artists involved, computer-generated images and texts. According to Ascott, networking and working within a telematic network – with meetings, interactions, negotiations, and visualizations in the electronic arena – was at the core of this show. [21] In the exhibition catalogue, Tom Sherman [22] also returns to the idea of interaction as a founding element of the electronic arts, in an illuminating text that also dwells on their exclusion from the art world in the 1970s and their radical “difference” that continues to make them unpalatable today: their love of machines, feared by the public at large; their propensity for collaborations, which clashes with the rampant careerism of the art world, and the notion of interaction (between artist and machine, between artists via machine, and between machine and public). The 1986 Biennale was undoubtedly a great platform for New Media Art, which in Venice found a unique opportunity to network and succeeded in exploring a large part of its potential. Around the Planetary Network the event featured the most groundbreaking work in computer graphics, as well as less technological, more amateur images; the “first interactive art videodisc” by Lynn Hershman Leeson; a fascinating installation of sounds and coloured lights by Brian Eno, and the sound environment Very Nervous System (1984) by the Canadian David Rokeby: a space controlled by a system of sensors that perceived the presence of the viewer and his or her movements in the area, translated into sounds by a computer. From the 1980s onwards this vast, variegated scene found its first, privileged point of encounter at the Ars Electronica festival in Linz, Austria. [23] Ars Electronica came about in 1979 as a renewed version of the Bruckner Festival, an event devoted to contemporary music accompanied by an academic symposium. The initial idea was to dedicate the symposium to electronic music. But the involvement of the Austrian Broadcasting Corporation (ORF), directed locally by Hannes Leopoldseder, raised the bar. Leopoldseder proposed going beyond the limits of the symposium and creating a permanent festival devoted to technology and its impact on art and society. On 18 September 1979 the first edition of the Ars Electronica festival opened with a spectacular open-air event, in front of an audience of 100,000. The success of this first edition excited the organisers, who began to think about making it a stable thing. The business model behind it had not yet firmed up, and the following editions, up to 1986, took place on a biennial basis. In the meantime the Austrian artist and curator Peter Weibel joined the artistic committee, and from 1986 the event was scheduled to take place every year, with a common theme for the festival and symposium. 1987 saw the launch of the Prix Ars Electronica, a prize – divided into different categories – that was to play a fundamental role in stimulating creativity, as well as establishing a series of critical and qualitative criteria, and developing a hierarchy of merit within the artistic community. In the early 1990s, feasibility studies were undertaken into founding a permanent center, the Ars Electronica Center in Linz, which got off the ground in 1995, accompanied by Ars Electronica Futurelab. The former was conceived as a “Museum of the Future”, gathering and hosting emerging results from the digital medium, while the latter was devoted to production and research, involving artists in courses and workshops and putting the most advanced technologies at their disposal. As emerges from this brief overview, Ars Electronica and the people involved in it were to play a decisive role in establishing New Media Art world as an independent arena. By stimulating debate, proposing categories and criteria of value, facilitating the production and circulation of works, developing a strategic network with other centers, universities and companies and contributing to the development of an economy and model of sustainability for New Media Art, Ars Electronica became its undisputed mecca. Locally, the Ars Electronica model was made possible by the fact that the post-industrial city of Linz was attempting to reinvent itself as the cultural and technological capital of Austria and central Europe. But its success was above all linked to the existence of a flourishing art scene in search of a stable platform for producing and exhibiting its work, not linked to one-off events like the aforementioned 1986 Biennale, and to the slow but ongoing development of an alternative system of festivals and centers like V2_, launched in Hertogenbosch, Holland in 1981 before moving to Rotterdam in 1994, where it stages a biennial festival called the Dutch Electronic Art Festival (DEAF). All these developments are obviously a product of the inexorable progress of technology, which was gradually seeping into everyday life. After the Apple II, various models of home computer appeared on the market: from the Atari 400 to the Commodore VIC-20, the first computer to achieve sales of over a million; from the Sinclair ZX Spectrum to the Commodore 64 and the IBM PC. In 1984, Apple Computer launched the Macintosh, a genuine revolution in the history of the personal computer: relatively cheap (at almost 2,500 dollars), the computer functioned with keyboard and mouse, and featured a graphic interface that replaced the customary green text against a black background. This graphic interface heralded the introduction of common metaphors inspired by the world of the office that the computer was destined for: desktop, wastebasket, windows, files and documents. Lastly, the computer featured a modem, a device that enabled it to connect up to a telematic network via a simple telephone line. Telematic networks also began to spread, and while Internet remained mainly linked to the American university system, some countries (like France with Minitel) created a national network, and on an amateur level BBS (Bulletin Board Systems) took off. These computer systems functioned like electronic noticeboards, with users connecting to them to share or download files and exchange messages. BBS technology first appeared in 1977 and became popular above all thanks to Fidonet, (invented by the American Tom Jennings in 1984), a network of different BBS. But computing did not make its way into households (and the everyday lives of millions) only by means of home computers and networks. In 1961 the MIT labs created Spacewar!, the first videogame in history. It did not take long for the business world to realise that this very basic interactive interface could be the start of a profitable sector of cultural entertainment. In the second half of the 1970s arcade games took off, along with the first home platforms for videogames. From Pong (1972) to Space Invaders (1978) and Pacman (1980), the videogames industry expanded exponentially, and the advent in 1983 of the NES (Nintendo Entertainment System) was to make an indelible mark on the collective consciousness. These developments had conspicuous consequences on the cultural sphere. The 1980s were the decade of hackers, cyberpunk, basic telematics, virtual reality and the start of the free software movement: phenomena which are too complex to be explored in detail here. Cyberpunk, for example, came about as a literary movement in the United States in the early 80s, thanks to the science fiction successes of William Gibson and Bruce Sterling, and the rediscovery of Philip K. Dick, but in Italy it developed as a political movement, attaching onto the substrate of punk, the ferment of the social centers and the left-wing protest movements in 1977. [24] Likewise in California, where a pivotal role was played by figures like Timothy Leary, exponent of counterculture and advocate for psychedelic drugs, who went on to develop videogames, use BBS and become a leading figure of “cyberculture”, and scholar of virtual reality. Both the hacker movement and the Free Software philosophy were rooted in this complex milieu. Artists played an active role in shaping this culture, and enriching its imagery with their works. It is often difficult, if not impossible, to separate the art from the context it is an active, integral part of. The association between New Media and New Media Art formed in the previous decades, but consolidated in the 1980s. This arose perhaps because on one hand, these artists were excluded from – or deliberately avoided – traditional artistic contexts, and on the other because there was a proliferation of hybrid, multidisciplinary figures who did not separate their art from their political activism, or their contribution to the network. In 1986, reviewing an Italian festival, Vittorio Fagone wrote about a “third culture”, distinguishing digital culture from humanistic and scientific culture: a culture in which «engineers, mathematicians, information technologists, architects, musicians and artists (or, if we wish, “visual operators”) and graphic designers live and work together, often exchange not roles but models and objectives. Electronic art occupies this space». [25] In parallel, the system of relationships, events and production centers that conveyed and supported “electronic art”, also firmed up. While in previous decades New Media Art was rooted in the universities and research centers, in the 80s New Media Art became an independent “art world” in its own right and laid the foundations for its continued existence. On the networks debate was conveyed above all on the BBS, while in the real world New Media Art was distributed at temporary events like technology and electronic art festivals, in line with the Linz model. Towards the end of the decade the first “New Media Centers” appeared, really taking off in the early 90s. The advent of these new distribution channels outside of the traditional art world gave the “third culture” fairly sound foundations in terms of visibility, critical debate and preservation. Yet in this regard Italy remained a fairly isolated case. Despite the presence of an active, vibrant art scene (with artists and groups like Tommaso Tozzi, the Giovanotti Mondani Meccanici, Correnti Magnetiche, Mario Canali, Studio Azzurro, Giacomo Verde and, later on, Piero Gilardi and Maurizio Bolognini), the lack of institutional involvement led to a proliferation of autonomous, isolated initiatives, the result of voluntary efforts by curators like Mario Costa and Maria Grazia Mattei, conducted mostly in private venues or peripheral institutional settings. Even now Italy has no Media Centers, and its few active festivals struggle to make a name for themselves internationally.

1989 is a pivotal year in terms of gaining insight into the subsequent fate of New Media Art, and could indeed be taken as the symbolic date in its process of institutionalisation. The initial setting for this was Europe, where specialized institutions (art centers, museums, workshops, archives and festivals) flourished at an unprecedented rate. It was in 1989 that the ZKM (Zentrum für Kunst und Medientechnologie) of Karlsruhe (Germany) was founded, a center that could, broadly-speaking, be seen as the leader of this process. In the same year the fall of the Berlin Wall and the Soviet empire opened an entirely new season, for art too. Russia, together with the countries of Eastern Europe, was obliged to speedily institutionalize contemporary art, which to date had been developing in unofficial situations like squats and private homes. This process was heavily influenced by the billionaire philanthropist George Soros with his Soros Centers of Contemporary Art (SCCA). As Lioudmila Voropai writes, [26] there were some interesting aspects to this process of institutionalization. In the first place, New Media Art had always stressed its “social utility” and contribution to the creative development of the New Media, thus adding to the legacy of confusion between the development of the medium and its use for artistic purposes, between “New Media” and “New Media Art”. This confusion was accompanied by the ambiguous and conflictual relationship between New Media Art and contemporary art, and was indeed one of the reasons behind the conflict: the social utility of New Media Art implicitly opposed the non-utility of contemporary art, which not coincidentally bases its economy on a luxury market.

The conflict between the two became even more pronounced when they were made to coexist in the same institution. The institution in question was the ZKM, the very notion of which speaks volumes about the nature of the relationship between contemporary art and New Media Art in the early 1990s. The two different art worlds coexist here, like a separated couple still sharing the same roof, thanks to an apparently virtuous division into a series of “institutes” and departments, coordinated since 1999 by the director Peter Weibel: the Museum of Contemporary Art, founded in 1999 and also a venue for temporary exhibitions; the Media Museum, which has a permanent, and unique, collection of “interactive media art”, accompanied in recent years by a number of “permanent exhibitions” on the latest developments in New Media Art; the Institute for Visual Media, the center’s “research and development” division (founded and directed by the artist Jeffrey Shaw until 2003); the Institute for Music and Acoustics, the Institute for Media, Education, and Economics, and the Filminstitute. In reality the ZKM only opened its premises, in a converted industrial area, in 1997, but it prepared the terrain with a series of temporary initiatives, like the Multimedia festival of 1989. Its vocation, linked to the orientation of its director (or rather the duo Weibel – Shaw) and its origins in the early 90s, made it into a temple for the interactive, immersive and technologically groundbreaking installations of the last decade of the century, so much so that in Europe the expression “ZKM art” is normally used, tongue in cheek, to refer to this kind of art. [27] Criticism aside, the ZKM has the undisputed merit of being the first in the 90s to raise the question of the “museification” of New Media Art, and issues related to how to preserve it and create a canon, in this way establishing a model for other international players, like Tokyo’s Intercommunication Center (ICC), founded in 1990 and given a permanent venue in 1997. Back in Europe, we have already seen how in the 90s various long-standing institutions like Ars Electronica and V2_ reinforced their position. In the Netherlands sizeable institutional investments in the new media led to the foundation in 1990 of the Inter-Society for the Electronic Arts, or ISEA, that organizes the International Symposium on Electronic Art. This association, which moved its headquarters to Montreal in Québec from 1996 to 2001, before returning to Holland, has an extremely international outlook, as evinced by the itinerant nature of the symposium, always staged in a different location. In Germany, the Institute for New Media (INM) in Frankfurt was set up in 1989 as an experimental workshop in the context of the Art School, before evolving into an independent research platform for post-graduate students. 1988 saw the founding in Britain of the FACT in Liverpool (then known as Moviola), which remains the country’s most important New Media Art institution. These are just a few examples on an international panorama in constant expansion. In this context it is inevitable to take a brief look at what was going on in Eastern Europe, not only for the significant contribution it gave to the development of New Media Art in the 90s, but because what went on there in the space of a decade appears to encapsulate the entire history of New Media Art. In Eastern Europe, up to the 90s, avant-garde art existed entirely outside of the institutional sphere. The Open Society Institute & Soros Foundation Network was the first to make a serious move in this direction. As of 1991 SCCAs were set up in 17 former Soviet block countries. These were relatively shortlived: in 1999, after the Soros foundations were restructured, all the SCCAs became independent non-governmental organizations. For many of them this meant tackling the crucial issue of funding, not always an easy task where public funds for culture were in relatively short supply. But some managed to survive. Supporting New Media Art was one of the key missions of the SCCAs. This came about because in an area where the personal computer was still a rarity and a status symbol, the social utility of the centers lay in their ability to guarantee the population (and the artists) access to the network and the new technologies. In postsocialist countries there was no tradition of New Media Art: information technology was linked to military uses and scientific research, and the embargo which followed the war with Afghanistan effectively prevented Western-made technologies from arriving in Russia. Yet the networking that got under way, and the widespread use of the network, enabled New Media Art to flourish. In 1993 the SCCA in Moscow set up its New Media Art Laboratory, led by Alexei Isaev and Olga Shishko. In 1994 the artist Alexei Shulgin established the Moscow-WWW-Art-Lab, and in the same year Gallery 21, a no-profit venue in the famous quarter of Pushkinaskaya 10 – a squat converted into an art center – opened its doors in St. Petersburg. Leaving Russia, Budapest saw the opening of the C3, the Center for Culture and Communication, which is still up and running, and which combined the traditional functions of an art center with teaching activities, holding courses and workshops on Internet and the new technologies, while Ljubljana opened the Ljudmila Digital Media Lab, promoting festivals and events, and supporting the artistic activities of Vuk osi, one of the pioneers of Net Art. As Voropai notes, the post-Soros era began during the golden age of New Media Art in the West. 1999 was the year of net_condition, a travelling exhibition organised by the ZKM, which opened the season of the major museum exhibitions, destined to continue – above all in the States – until 2002. In Russia the decline of the New Media institutions gave rise to a difficult situation. The affirmation of an uncertain, poorly regulated art market, buoyed up by the new rich, who saw art as a way of laying claim to elite status, did not favour New Media Art, which was held – rightly or wrongly – to be an institutional art form.

This is the situation that has come to pass, in a more recent period, and with the same dynamics, in the West. Here the development of a system of New Media Art, by means of the dynamics we have attempted to illustrate, has gone hand in hand with increasing interest from traditional artistic institutions. Yet the latter tend to be uninterested in the underground tradition of New Media Art, and focus their attention on its most recent results, connected to the mass spread of digital technologies and the advent of the web in the second half of the 90s. Indeed at the start of the decade there were as yet few artists using “domestic” technologies with some degree of awareness, to make art: figures like the Italian Maurizio Bolognini, who in the early 1990s produced installations in a highly conceptual vein by reprogramming and “sealing” personal computers in such a way that their vitality and continued functioning, perceptible as a monotonous hum, could be detected but not visualised through any output devices; [28] or like the German artist Wolfgang Staehle, who in New York in 1991 used various BBS to found The Thing, conceived as a “social sculpture” à la Beuys. And while home computing remained the main arena for the formation of the digital cultures of the 90s, at the start of the decade New Media Art focused above all on immersive systems and virtual reality, telepresence and interactivity (with figures like Jeffrey Shaw, David Rokeby, Paul Sermon and, back in Italy, Mario Canali, Piero Gilardi and Studio Azzurro), technological prostheses and robotics (Eduardo Kac, Stelarc), and 3D graphics and generative algorithms (Karl Sims). But this work involved the use of cutting edge technologies, and was too focused on the latest developments in technology and too detached from the developments in contemporary art in that period to be properly interesting in this context. With the advent of the World Wide Web (Mosaic, the first commercial browser, appeared in 1994), and the mass distribution of the personal computer (1995), this situation changed radically. The computers of the 90s were cheap and featured an intuitive interface; anyone, with a minimum of instruction (which was often undertaken in universities, in the workplace or, for the young generations, by means of videogames) could use them. Processing text, modifying images, and creating sound and video files were relatively simple matters. At the same time the web gave the internet network a multimedia, hypertext interface based on a programming language (html), the basics of which can be picked up in a few days. Making art with a computer no longer required technological training, access to research labs, collaborations with engineers and professionals. Anyone could do it, and not necessarily to make art that was accessible only via computer. So while on one hand computers could be used by any artist, they could also be employed by anyone wishing to exploit the extraordinary communicative, aesthetic and narrative potential of the web. Net Art came about in this very way. It was no longer a question of creating the finest image possible with a given tool, or generating an immersive interface, but about exploring and subverting an elementary language, creating a short circuit in communication, infiltrating a global communications medium. The first net artists did not come from the New Media Art of previous years, but from photography (Alexei Shulgin), post-conceptual art (Vuk osi), film (Olia Lialina), street art (Heath Bunting), painting (Mark Napier) and video (Jodi); they had an artistic, rather than a technological training; some turned to the web out of frustration with the contemporary art world, others were fresh out of art school, and others had links with political activism, which in that very period was beginning to realise the web’s unprecedented potential for media impact (Ricardo Dominguez). Net Art was ironic, subversive and played with the limits of meaning; it looked to the avant-garde and neo avant-garde movements; it practiced pastiche, collage and linguistic games, and it was the output of an era of cultural production that eliminated the difference between original and copy. Net Art originated between 1995 and 1997. In 1997 Documenta, one of the most important dates in the contemporary art calendar, had a section devoted to Net Art. The year before, the Swiss collective etoy won a Golden Nica at the Prix Ars Electronica, in the “World Wide Web” category, for the work Digital Hijack, a spectacular operation of search engine manipulation that diverted hundreds of thousands of internet users onto their site. [29] In the “Computer Animation” category, the first prize went to Pixar, for the animated movie Toy Story (1995), the first movie produced entirely using computer graphics. In the photograph that commemorates the event, an etoy agent with a shaved head and mirror sunglasses, in an orange jacket and black trousers, shares the stage with Japanese interactive artist Masaki Fujihata, Canadian electroacoustic music composer Robert Normandeau, and writer and film director Pete Docter from Pixar: they are all smiling, but they seem to be wondering what they are doing on the same stage. And the question is by no means irrelevant: while 1989 was the key year for consolidating the New Media Art world, 1997 was the annus horribilis of the split between the art and its world: the moment when so called “new media artists” started wondering what they had in common, besides the medium and their under-recognition by mainstream art worlds. The events that we have described, from the eighties onwards, appear to be entirely concentrated in Europe. So what was going on with the States, the homeland of the new technologies and the first artistic experiments in this direction? Lev Manovich accounts for [30] the American delay on this front with two simple considerations. In the first place, the rapidity with which the new technologies were assimilated in the States made them invisible in a very short space of time. In other words, in the US there was no hiatus between the arrival of a new technology and its normalization, the hiatus that enables artists to develop a critical distance from the medium. Secondly, Manovich blames the lack of institutional support, at least compared to areas like Western Europe, Australia and Japan, where the New Media Art world leaned heavily on public funding in the 80s and 90s. In the States the art world is market-driven, and in that context an artistic practice that had always professed its unsaleability had trouble getting by for many years. This, at least, was the case until the late 90s, when the situation changed completely. Universities and art schools set up courses and programs of New Media Art and New Media Design; prestigious academic publishers like the MIT Press began producing books on the subject; renowned institutions like the Princeton Institute for Advanced Studies, the Rockefeller Foundation and the Social Science Research Council set about organizing conferences, prizes and funding, and the major contemporary art museums, from the Whitney Museum of American Art in New York to MoMA, from the San Francisco Museum of Modern Art to the Walker Art Center in Minneapolis to the Guggenheim in New York, together with numerous university museums, got involved with exhibitions, programs and curatorial positions. Even some private galleries, like the Postmasters Gallery in New York, staged solo and group shows of New Media Art. Various no-profit organizations (often led by artists) also appeared, along with specialized institutions like Eyebeam in New York, while existing structures like the Electronic Arts Intermix (EAI) founded by Howard Wise in 1971 and mainly focussed on video, opened up more substantially to the digital media. In other words, interest in New Media Art exploded in the States at a period in which the New Media sector was gaining financial thrust, and New Media Art was becoming financially and technically sustainable for any artist. This phenomenon, however, was fairly short-lived: after the collapse of the New Economy, and the consequent disappearance of the funding that had boosted interest in it, the enthusiasm of American museum system cooled off considerably. At this point the American New Media Art scene was faced with two alternatives, both of which it explored. On one hand it attempted to tackle the arduous task of integrating into the contemporary art system and its market. On the other it looked to Europe with interest, attempting to come up with an alternative model for survival that would enable it to preserve its specific characteristics.

As Arthur Danto wrote, [5] from the sixties onwards (namely from the acceptance of the new “paradigm” introduced by Marcel Duchamp in the 1910s with his first readymades) anything and everything could be art, as long as there was an internal reason for which a given thing should be considered art. Identifying this reason, however, is not always easy. Francesco Bonami, in a book that sets out to explain to the man in the street “why contemporary art really is art”, spectacularly fails in this mission by adopting oblique strategies that constantly avoid the question. In the introduction, Bonami explains that to understand a work of art «all you need is an open-minded approach», curiosity and courage, and that the important thing in art is not the technique, but the idea, which has to be “new” and “right”: «The important thing, in any case and if possible before others get there before you, is to think the right thing at the right time». [6] Yet Bonami does not explain the concept of “new”. In this complete absence of rules, the only one that appears to withstand scrutiny, and that Bonomi returns to frequently, is the central role of the idea. The “right idea”, “good contents”, is the only thing that links Duchamp, who «learned how to generate hot air better than others», and the “reactionary” art of Lucian Freud, who paints «as if Duchamp and Warhol had never existed». I have mentioned Bonami’s dumbed-down aesthetics, rather than more structured theories, because I think it reveals something significant about the arena we are analyzing. One of the most renowned international critics and curators, Bonami does not seem to base his work on a specific “idea of art”. He seems to operate more like a water-diviner, who can see art where others cannot – and is almost always in the right place. Obviously this is possible because when Bonami makes his choice, he has the authority and the means to impose it as the “right” choice to other members of the art world: a consideration that implies a contextual definition of art, according to which art is art because there is a surrounding context that says it is. As Blais and Ippolito explain, [7] this idea is nothing more than intellectual provocation (that of Duchamp) turned intellectual inertia (that of today’s art world). If a work of art is defined by its aura, and if in the age of its technical reproducibility that aura is no longer an integral part of it, the process of “conferring” that aura – namely the work of critics, museums, gallerists and dealers – does not follow but actually precedes the recognition of an object as a work of art. Art is art because critics write about it, museums exhibit it and collectors collect it, not vice versa; the aura is the consequence of this intellectual attention, the interest of the museums, the investments made by collectors, and so on, rather than the cause. [8] This theory, which crops up not infrequently among both those within the art world, and those criticizing it from the outside, is undoubtedly an enthralling one. Also because, once embraced, it is very easy to find evidence to back it up, and very difficult to find arguments against it. By way of example, it is all too easy to look at Damien Hirst, one of the stars of today’s art world, and see the results of canny investments made by an advertising mogul (Charles Saatchi), an extremely solid art world (the English establishment), an unprecedented eye for business (that of the artist) and the concerted efforts of museums, collectors, galleries, critics and curators. It is more difficult to explain why his colored dots mesmerize us, why his butterfly wings fascinate us and why his pharmacies and animals in formaldehyde embody our angst more than many other present day works of art. In other words, it is more difficult to understand whether we would have recognised these pieces as works of art before the art world lent them an aura, variously boosted by the torrents of words used to describe them, the floods of money spent on buying them and the sacral ambiance of the white cube. This problem obviously arises from the weak nature of the few attempts that have been made to come up with a definition of art that transcends the contextual theory. Bonami’s “theory of the right idea” encapsulates this weakness fairly well. Even a vastly more sophisticated theory, like that of the philosopher Mario Perniola (2000) does not seem to yield the results hoped for. Today «we consider it “natural” that some objects are works of art and that some people are artists; any other question seems superfluous», [9] Perniola writes. But just what is it, aside from economic worth and communicative value, that makes art art? According to the philosopher, the answer to this question lies in art’s shadow, «a shady form which contains the most unsettling and enigmatic elements that belong to it». Yet Perniola refuses to define this shadow, conscious that by nature it «disappears when exposed to the light». We can at best identify only a few components of that shadow – the “splendour of the real”, the “sex appeal of the inorganic”, the “logic of dissent”. But shedding light on it necessarily means making it vanish. What seems to emerge from all these “weak” theories is the need for strong contents, art’s ability to home in on an issue, objectivize it and present it for our analysis. This also gives rise to prejudice against media specificity, and art that is not “just art”. This prejudice is linked on one hand to the “damnatio memoriae” that struck Clement Greenberg in the States, and on the other to the fact that art appears to have entered a “postmedia” phase that best manifests itself in multimedia installations, and the nomadic shifting between different media that characterizes the work of many artists. In particular, according to Rosalind Krauss, medium specificity was overcome around the 1970s, on one hand by Marcel Broodthaers with his “eagle principle”, that «simultaneously implodes the idea of an aesthetic medium and turns everything equally into a readymade that collapses the difference between the aesthetic and the commodified»; [10] and on the other by video that, sharing the «television’s “constitutive heterogeneity”», proclaimed the end of medium specificity. «In the age of television, so it broadcast – Krauss writes – we inhabit a post-medium condition». [11] Which does not mean that staying with one medium is inappropriate, or that exploring the specific characteristics of that medium is a cardinal sin. Krauss tries to explain this in another essay, significantly entitled “Reinventing the medium”. According to Krauss, a medium can be rediscovered and reinvented by artists in the post-medium phase when it has fallen into obsolescence: not to explore its creative and aesthetic potential, but to examine it as a “theoretical object” of art.
In Remainder, the first novel by the English artist and writer Tom McCarthy, the main character has survived an accident, followed by a grueling rehabilitation process, that has left him with partial memory loss, but compensation of several million pounds. With this money the character attempts relentlessly to regain the authenticity of some brief episodes of his past and present life by faithfully reconstructing and reenacting them. His first project involves reproducing the atmosphere of a house he believes he has lived in. The setting is reconstructed in great detail (down to the cracks in the walls, the black cats on the roof in front, the sounds and the smells), and various “reenactors” are hired full-time to enable him to relive these moments whenever he feels like it. This is followed by other “projects”, staged with the involvement of hundreds of professionals and “reenactors”: the obsessive reconstruction of a minor accident he once had in a gas station, a murder, a bank robbery. All of this is done to enable him to relive the tingling feeling he experiences when authenticity is achieved. At one point someone asks him: «Does he, perhaps, […] consider himself to be some kind of artist?» To which he replies: «No. I wasn’t any good at art. In school». [13] These lines are telling. They reveal that today’s art is not something you learn at school, and is not necessarily associated with traditional artistic techniques. They also say that art is something visionary and gratuitous; it is not to do with objects, but projects, and it does not produce anything of use, but requires total dedication, generous funds and the involvement of many different kinds of professionals. The artist figure that emerges from this picture is still firmly anchored to the romantic vision of the genius, obviously updated to today’s standards. Figures like Olafur Eliasson, who created waterfalls cascading down the struts of New York’s bridges, and Matthew Barney, who spent five years of his life producing an unprecedented cycle of films, conceived in its entirety as a sophisticated allegory of male genitalia, embody this idea to perfection. The romantic genius acquires celebrity status, and is required to be an excellent entrepreneur of him or herself: think of figures like Damien Hirst, Maurizio Cattelan and Francesco Vezzoli, and further back Jeff Koons and Andy Warhol. If we descend gradually from art’s lofty pinnacles into the complex, variegated fauna of artists, many of these aspects fade away, but the one constant, the one thing we always expect from an artist, is absolute devotion to a project, an idea. With this one lodestar established, everything else is up for discussion, renegotiation. The mythos of complete freedom also admits the option of choosing an entirely reactionary path – that of manual skill, technical prowess, obsessively nurturing a single language. Artists can hide their identities behind a pseudonym or a collective: in this way an academic painter like John Currin can rub shoulders with the likes of Jeff Koons, who has skilled craftsman producing his marble busts. And while the latter, who places himself at the center of many of his works, explores – and reinforces – the cult of the personality of the artist, in contemporary art it is not difficult to come across collaborative platforms, in which individual contributions merge into collective output: the existence of collectives like the Indian RAQS Media Collective – a platform that operates on an artistic, critical and curatorial level – comes as no surprise.
In Mercanti d’aura, Alessandro Dal Lago and Serena Giordano assert that the notion of “purpose” represents an insurmountable barrier to an object being a work of art. If an object has a purpose, it cannot be art, because art serves no purpose; it exists unto itself. And the writers go one further, maintaining that objects created to serve a purpose (therefore the products of worlds such as that of fashion, design and the entire cultural industry) possess disturbing properties that make opposition to them particularly vehement. These objects disturb us because they are artworks in all respects, but also «services marked by the stigma of subordinate work». [14] This theory is undoubtedly a fairly convincing one. Conceived by the aesthetes of the late nineteenth century, the idea of art for art’s sake has stayed with us, in various different forms, in the art and criticism of the twentieth century. Yet continuing to envisage the world of contemporary art as an ivory tower under constant threat from base, secondary practices, is frankly anachronistic. All of the arts have their own “art world”, and most of the artifacts they generate can only be appreciated according to the canons of those worlds. Yet each of these worlds can produce – has produced and continues to produce – a series of artifacts (usually a fairly limited series) able to fulfil the conditions of another world, for example that of contemporary art. This happens for various reasons: because the historic schism between some of these “art worlds” is actually a fairly recent thing, and because certain phenomena that are part of the mythology of contemporary art, like modernism, envisaged a reconciliation that continues to crop up at regular intervals – and, lastly – because the contemporary art world, intended as an arena of free experimentation, unfettered by ulterior motives, has always been particularly receptive to approaches and figures viewed as anomalous by the other art worlds. In other words, the skin of the contemporary art world is much more porous and permeable than that of other worlds, and while it may have proved slightly less porous at some periods in its history, the period that began in 1989, with the fall of the Berlin wall and the recovery of the art market after the recession at the end of the 80s, was undoubtedly particularly open to contamination. In his critical and curatorial work Germano Celant has often highlighted this.
This situation has given rise to two movements: one of appropriation, which encourages artists to engage with other media, be it importing them into the contemporary art world or shifting towards those others worlds, and one of convergence, which sees many hybrid, borderline figures (filmmakers, designers, musicians, etc.) bringing their works into the arena of contemporary art. This does not happen, as might be expected, only on the “borders of the empire”, but at its summit, involving figures of prime importance. Think of Matthew Barney and Shirin Neshat, who have taken works to the Venice Film Festival; think of the numerous artists who have directed Hollywood movies (from Robert Longo to Kathryn Bigelow to Julian Schnabel); or Pierre Bismuth, who won an Oscar for his screenplay for the film Eternal Sunshine Of The Spotless Mind (2004), written with the director Michael Gondry. And think, too, of Takashi Murakami’s collaboration with Vuitton, the double identity of Carsten Nicolai (who also works as a musician, going by the name of Alva Noto), and Peter Greenaway’s nomadism. All of this is also facilitated by internal developments in the contemporary art world, which is increasingly forging a presence as one of the sectors of the cultural industry and show business. And museums and institutions, traditionally more conservative, are facilitating this process, hosting exhibitions devoted to fashion and design, in ways that can be debatable and are indeed debated, but are undeniably forging a trend
The question of how all this is to be reconciled with the traditional conception of the visual work of art, intended as an artifact that is unique (or reproduced in limited editions), collectible, and therefore financially valuable, is constantly being renegotiated, and obviously entails some interesting compromises. In the contemporary art world value is attributed by means of a complex system that includes criticism, museums and other institutions, prizes, exhibitions and the market. Not being able to deal with each of these players singly, I will consider above all the market, which, in my analysis, represents the missing link in the world of New Media Art. The art market has played a key role in the world of visual arts since the nineteenth century, when the arts began gradually severing their ties with the nobility and institutional powers, becoming a private activity mainly destined for the cultured bourgeoisie in search of the social prestige that only a productive relationship with the world of culture can confer. Particularly after the Second World War, art became increasingly bound up with the market: in this way, while the “dematerialization of art” became possible in a period when the market was relatively weak, when the market recovered in the 1980s, and there was a resurgence in demand for marketable artifacts, traditional practices like painting and sculpture rose to the fore once more. The collapse of the stock market in 1989, together with other crucial factors – the new geopolitical situation, and AIDS wiping out an entire generation of artists – played a key role in changing the lie of the land in the early nineties. The phase which followed this, and which is still under way, is a complex one for various reasons. Globalization is bringing forth new art scenarios, new exhibiting platforms and new markets; major temporary art events, like the biennales, are springing up, creating new destinations for cultural tourism; contemporary art museums are being revamped, testing the terrain of the global museum, and becoming artistic objects in their own right, with containers that are often more appealing than their contents, boosting the number of services on offer and becoming focal points of a society in which the services sector, media and culture play a key role; and lastly, the advent of the information society has generated an exponential increase in platforms for criticism, with the launch of dozens of new magazines. The art market spearheads this transformation. Private galleries stage events; by means of contemporary art fairs they increasingly condition the construction of museum collections; by paying for advertising space in art magazines they finance art criticism, and even if the relationship forged between the two is not, at least in the most virtuous cases, a genuine exchange, they inevitably end up conditioning the choices made. Art fairs have grown exponentially in the last decade and some of them (like Art Basel, Frieze or New York’s Armory Show) have established themselves as primary cultural events, key destinations for global tourism, on a par with museum exhibitions and biennales. Lastly, auctions, the main arena for the so-called “secondary market”, have gradually opened up to contemporary art and the so-called “primary market”, their fluctuations influencing the careers of artists. In The Art Fair Age (2008), the Spanish critic Paco Barragán defines art fairs as «Urban Entertainment Centers», [16] and contemporary collecting as a pyramid: on the bottom layer, art is sought after as “social capital”, a source of prestige and affirmation; on the next level art is collected as “financial capital”, namely for its investment value; on the third level of the pyramid we find companies who view art as a “brand” of sure-fire appeal, and include it in their market strategies, while at the top we come to private collectors who seek intellectual fulfillment from art. And the latter are increasingly putting their collections into the public domain, by means of donations to museums (like Giuseppe Panza di Biumo), taking over established institutions (like the new Palazzo Grassi owned by the French entrepreneur François Pinault) or setting up their own foundations (like the Fondazione Sandretto Re Rebaudengo in Turin), thus boosting their influence over the process of institutionalization. The close bond between the contemporary art world and its economy was incisively analysed by the English critic Julian Stallabrass in his book Art Incorporated (2004), which explicitly focuses on the «regulation and incorporation of art in the new world order». [17] According to Stallabrass, art’s micro-economy, governed by a handful of dealers, critics and collectors, is precisely what ensures its freedom from the rules of global capitalism and mass culture. Yet at the same time contemporary art can be seen as a giant metaphor for the capitalist system, with which it has more than one affinity. After demonstrating that the salient characteristics of the art of the 90s – multiculturalism, the success of the installation and the emphasis on youth – are closely linked to its economy, Stallabrass dwells on the way in which the economy of the art world conditions production. The author explains that, while most other art worlds are based on an economy of usage, the core business of contemporary art consists in the «production of rare or unique objects that can only be owned by the very wealthy, whether they are states, businesses or individuals» (p. 102). In recent decades this idiosyncratic economy has had to come to terms with the existence of technically reproducible languages, giving rise to some bizarre compromises: while on one hand photographic works and video exist on the market in very limited series, highly-priced and accompanied by an authentication, on the other, artists like Jeff Koons and Takashi Murakami create digital images which they then get professionals to paint, transforming an infinitely reproducible file into a unique artwork, using a practice (painting) that is manual and entirely traditional. And the ups and downs of the market also obviously influence the type of art that is produced. In the eternal struggle between traditional (and easily marketable) languages, and more difficult forms, the former experience a predictable revival at every economic boom, while the latter emerge more forcefully in every recession, in a «predictable and mechanical process» (p. 107). As for the artists, the idiosyncrasies of the system almost always relegate them to poverty. While there are a few big names who manage to make a killing, most artists are at the lower end of the earning scale. Poverty is at once a side-effect of the particular workings of the system, a contradiction and an ideal: poverty suits art. The artist’s is a high level profession, usually practised by people of high social extraction but low income, who often fund their art with other activities. As Stallabrass concludes: «As a whole, the art market is an archaic, protected enclave, so far immune from the gales of neoliberal modernization that have swept aside so many other less commercial practices. Its status grants it social distinction and a degree of autonomy, even sometimes from the odd market that is at its basis» (p. 114). We might object by asserting that Stallabrass’ vision is a bit too prosaic, that art is something else altogether, something not so exclusively tied to the fortunes of the market. We could object that the present period as it will be reconstructed in two hundred years’ time will have little to do with auction prices, corporate investments and collectors. This is true up to a point, given that the fluctuations of the art economy influence critical debate and the construction of museum collections, as Stallabrass warned us right from the beginning: «the art world is layered vertically and heterogeneous horizontally, comprising many overlapping spheres of association and commerce» (p. 25). This can also be said of the other art worlds, and it is exactly what makes it difficult to reason systematically. At the same time, it is on this horizontal plane that the various worlds intersect, mutually influencing their respective fates.
As we have seen, the world of New Media Art came about to offer artists wishing to experiment with technologies of all kinds the opportunity to do so, removed from the constrictions and limits of a world, the contemporary art world, which is strongly conditioned by its economy and a critical predilection for contents above the exploration of a medium. Far from challenging this configuration, New Media Art criticism merely takes it for granted, and replicates it ad infinitum, to the point of asserting, as Edward Shanken does in Media Art Histories, that contemporary art has never accepted New Media Art because it has always rejected the interpretative model based on the relationship between art, science and technology. [18] Which would imply that it can only be interpreted in this way. In 2006 Gerfried Stocker, director of the Ars Electronica Center and the yearly festival connected to it, returned to discuss this idea of art. The text, rhetorically entitled “The Art of Tomorrow”, [19] is significant from various points of view. Indeed Stocker acknowledges that the current developments in new technologies call for a rethink of the structure and functioning of a festival like Ars Electronica, but does so basically without challenging the idea of art it is based on, namely that art is «a test-drive of the future» (p. 7); that Media Art is «an experiment […] that often brings the creators and proponents of this “new art” into an association with engineers and researchers» (p. 11); and that its basic characteristic is its ability to go beyond an instrumental use of the media as a «medium of representation», making the media not only its tool and medium, but also its subject matter, triumphantly concluding.
In Art of the Digital Age, Bruce Wands [21] depicts the digital artist as someone equipped with technological skills and a good dose of «technological curiosity»; often a programmer, used to working in collaboration with other programmers and IT engineers; attracted to new technologies and viewing art in terms of research and experimentation; a risk-taker who readily veers off the beaten track of established languages and forms to venture into new terrain. Though this definition does not add anything new to what we have said so far, it is an interesting one from various points of view. In the first place, New Media Art appears to have entirely overcome the romantic conception of the artist as genius, and seems to be more interested in returning to the Renaissance models of artist as artisan and artist as scientist. Familiarity with programming also takes the New Media artist into another sociologically interesting terrain: that of hacking (used here in its original sense, freed from the negative connotations attributed by the mass media). It goes without saying that many New Media artists are, and consider themselves to be hackers, to all intents and purposes, and have much in common with hacker ethics: great enthusiasm for their work, limited interest in making a profit, a propensity for knowledge sharing and a belief in the free circulation of information. [22] In 2003, the Net Art group [epidemiC] engaged with this, activating a curious social short circuit. Invited to take part in the Ars Electronica festival, [epidemiC] created Doubleblind Invitation: a program that, if visualized in code form, looked like a beautiful piece of “obfuscated code”, namely formatted like a calligram – a technical feat which holds great kudos in the hacking world, where there are competitions devoted to this particular art form. Yet if executed, [epidemiC]’s code sent out emails – seemingly on behalf of the curator Christiane Paul – to dozens of hackers, fans of obfuscated code, inviting them to take part in the festival. The responses from the invitees, some embarrassed, some enthusiastic, show both the proximity of these two similar cultural niches, and the basic divergence between their two different approaches to programming. This portrait of the New Media artist, albeit an abstract one, appears so far removed from the type of artist cultivated by the contemporary art world that we might be tempted to think that the difference between the two worlds is a question of anthropology rather than history. And while, as we have seen, the contemporary art world is permeable enough to occasionally accept anomalous figures entirely unconnected to the notion of the “career” artist, the appeal of an art world basically without any kind of market economy, devoted to developing knowledge and exploring the arena of digital media, remains strong. Casey Reas is a case in point. Reas is an American artist whose work consists in defining processes and translating them into images. In other words, Reas writes programs that, when executed by a computer, generate animated images that can, if desired, be translated into videos or prints. Unsatisfied with the existing tools, in 2001 Reas, working with the artist and designer Benjamin Fry, created Processing, an open source programming language and freely downloadable program for the creation of images, animations and interactive installations. [23] Processing is now used by a slew of artists, designers and researchers, and obviously Reas himself, who utilizes it in his work. Although Reas works with galleries, he considers himself above all a programmer, designer and researcher: he writes books, holds conferences and coordinates the department of Design and Media Arts at UCLA; and while the resulting products (prints, videos and installations) are produced in limited series, his programs are released with an open source license. He earns his living mainly through teaching and holding workshops on Processing around the world. It is not difficult to come across stories like these in the New Media Art world, just as it is not difficult to meet artists who put their own talent and efforts at the service of temporary collaborative experiments, voluntarily sacrificing their own authorship.

I want to start with the proposition that in a place like New York City, we live in the over-developed world. Somehow we overshot some point of transformation. A transformation that didn’t happen, perhaps couldn’t happen. But in having failed to take that exit, we end up in some state of over-development. In the over-developed world, the commodity economy is feeding on itself, cannibalizing itself.
There is of course an under-developed world, sometimes in intimate proximity to the over-developed one. You can find it even here in New York City. One can critique the orientalism of the fact that Willets Point, Queens is known among New Yorkers as ‘little Calcutta’, but it really is a place without paved roads, running water, and with mostly off the books, illegal or precarious jobs.
But you can forget that under-developed world exists if you live in the bubble of the over-developed world. Some of us don’t have to do the manual version of precarious labor, at least. But there is a sense in which some characteristics of that labor have actually found their way into the over-developed world as well.
Viewed from inside the bubble of New York, the paradox of digital labor these days is the way that tech enables the over-development of under-development. Technologies are shaped by the struggle over their form. It was not given from an essence that the digital would end up as control over labor rather than control by labor. But in the current stage of conflict and negotiation, the over-development of under-development seems to me to describe a tendency for labor.
In any case, labor isn’t the only class struggling in and against the digital. I still think there is a difference between being a worker and being a hacker. I think of hacker as a class category: there is a hacker class. Hackers are those whose efforts are commodifed in the form of intellectual property. What they make can be turned into copyrights, patents or trademarks.
The hacker class is distinguished by a few qualities. It usually means working with information, but not in a routine way. It is different from white-collar labor. It is about producing new arrangements of information rather than ‘filling in the forms.’
As such, it can be a bit hard to make routine. New things just don’t appear on time. Not if they are really new. There’s a kind of ‘innovation’ that is actually quite close to routine, and the hacker class does that too. It’s the new ad campaign, the new wrinkle on the old technical process, the new song or app or screenplay. But the big qualitative leaps are much harder to subordinate to the reified, routinized forms of labor.
The ruling class of our time, what I call the vectoral class, needs both these kinds of hack. The vectoral class needs the almost-routine innovation. The existing commodity cycles demand it. As our attention fades and boredom looms, there has to be some just slightly new iteration of the old properties: some new show, new app, new drug, new device.
What is interesting at the moment are the strategies being deployed to spread the cost and lower the risk of this routine innovation. This is what I think start-up culture is all about. It spreads and privatizes the risk while providing privileged access to innovation that is starting to prove its value to the vectoral class, whose ‘business model’ is to own, control, flip, litigate, and – if absolutely necessary – even build out new kinds of intellectual property.
The other kind of hack, the really transformative ones, are another matter. To some extent the vectoral class does not really want these, no matter what the ruling ideology says about disruption. Having your life disrupted is for little people. The vectoral class doesn’t like surprises. Its goal is to come as close enough to a monopoly in something to extract rent from it.
The kind of mode of production we appear to be entering is one that I don’t think is quite capitalism as classically described. This is not capitalism, this is something worse. I see the vectoral class as the emerging ruling class of our time, whose power rests on attempting to command the whole production process by owning and controlling information. In the over-developed world, an information infrastructure, a kind of third nature, now commands the old manufacturing and distribution infrastructure, or second nature, which in turn commands the resources of this planet, which is how nature now appears to us.
The command strategies of the vectoral class rely on the accumulation of stocks, flows and vectors of information. The vectoral class turns information as raw material into property, and as property into asymmetry, inequality and control. It extracts a rent from privatized information, held as monopoly, while minimizing or displacing risk.
One strategy is to socialize the risk of the real hack. This is probably why public universities and publicly funded research still exist. The tax-payer can take the risk on the really basic research. The university research park model is now set up to carefully modulate access to information about anything that might make a valuable property.
Another strategy is what one might call auto-disruption. Learning from the mistakes of the old capitalist firms of the industrial eras, this model takes the hacker practice in-house. Firms with existing rent-extraction revenue flows become hoarders of potentially monetizable intellectual property, or the people who look like they could produce it. This is to be deployed only when it disrupts somebody else’s business more than one’s own.
So that’s the vectoral class. The problem with belonging to the hacker class in a world the vectoral class rules are these: firstly, certain modest forms of the hack now fall into an outsourced, ‘casualized’, even amateur kind of economy. Certain competences became widespread that there is no way to extract value from them as skills. Certain models of distributed or algorithm-based path-seeking turn out to work as well as hiring the top talent to pick a path for you.
Secondly, more higher-order hack abilities might still command their own price in the market, and one might even end up owning a piece of whatever one produces. But it becomes less and less likely that you get to own it all. One becomes at best a minor share-holder in one’s own brain.
Of course the situation with the worker is even worse than the hacker. The commodification of the life-world eats into the old cultures of solidarity and equality. Everything becomes game-like, a win-lose proposition. The world of third nature, that Borgesian data map that exactly covers its territory, is quite literally programmed to be anti-social.
In daily life there can be a continuum of experience between being a worker and a hacker. They are not absolute categories from the point of view of experience. One can pass from one to the other. Both can be precarious ways to make a living. The white male ‘bro-grammer’ is not the only kind of hacker, just as the blue collar hard-hat is not the only kind of worker.
For worker and hacker alike, the dominant affects are those of envy and jealousy, and covetousness. One is supposed to hate those with just a bit more than you, while at the same time loving those with much, much more. Those with a bit more must be undeserving; those who own everything apparently do so with unquestionable right.
For worker and hacker alike, there is a struggle to achieve some kind of class consciousness, and a social consciousness even beyond that, against the atomizing affect of the time. I just don’t think it is quite the same class consciousness.
For labor, it is always a matter of solidarity and equality. For the hacker, class consciousness is always modulated by the desire for difference, for distinction, for recognition by one’s real peers. It is a sensibility that can be captured by the bourgeois individualism propagated by the vectoral class, but it is not the same thing. Winning the stock-option lottery is not the same thing as the respect of one’s peers. Nor does it translate into any agency in giving form to the world.
It may not come as any surprise that the world this work and these hacks are building is one that cannot last. One might as well say already that this is already a civilization that does not exist. The material conditions that afford it are eroding already. Whether we are adding to the world some quantity of labor or some quality of hack, it is as if we were just building more sandcastles while the tide comes in.
This is the meaning of the Anthropocene: that the futures of the human and material worlds are now totally entwined. Just as Nietzsche declared that God is dead, now we know that ecology is dead. There is no longer a homeostatic cycle that can be put right just by withdrawing. There is no environment that forms a neutral background to working and hacking.
Just as the category of ‘man’ collapses once there is no God, so too the category of the social collapses when there is no environment. The material world is laced with traces of the human, and the human turns out to be made of nothing much besides displaced flows of this or that element or molecule.
The dogma that ‘reality is socially constructed’ turns out not so much to be wrong as to be meaningless. What all the workers and hackers of the world are building is more and more of the same impossible, nonexistent world. We are building third nature as the hyperreal.
Two tasks present themselves, then. The first is to think the worker and hacker as distinct classes but which have a common project. The second is to think that common project as building a different world. Can this infrastructure we keep building out, this second and third nature, actually be the platform for building another one? Can it be hacked?
It is a dizzying prospect. This is why I turn to the work of Alexander Bogdanov, because he thought it could be done. Sometimes it is good to have ancestors, even if they are funny uncles and queer aunts rather than the patriarchs. Bogdanov was Lenin’s rival for the leadership of the Bolshevik party. Shunted aside by about 1910, he turned to two projects, which went by the names of tektology and proletkult.
I think of Bogdanov’s tektology as a project of worker and hacker self-organization that would use the qualitative medium of language rather than the quantitative one of exchange as the means for conveying forms, ideas, diagrams, from one design problem to another. Could there be an art of sharing what works? Could a hack that derives from one design problem be floated speculatively as a possible form or guide for another? Bogdanov’s tektology is like a philosophical Github.
I think of Bogdanov’s proletkult as a project of autonomous worker and hacker cultural production. Bogdanov had a positive, rather than a negative theory of ideology. We all need an affect, a story, a structure of feeling that is really motivating and connecting. Can we be moved and joined by something other than envy, greed, spite, rage or the other click-bait of the game-ifed, commodifed, hyperreal world? Can there be other worldviews and worldviews of the others?
In a way tektology is the work and proletkult the play aspects of building an actual world, in the gaps and fissures of this unreal one that surrounds us. The keynote for Bogdanov was that this had to be a cooperative and collaborative project, based on the worldview of the hacker and worker. This would be a different worldview to both those of authoritarianism and exchange.
We have to think how things work without assuming there is someone or something in charge, a final God-like arbiter, even if it is the hyper-chaos God of the speculative realists. And we also have to think how things work without imagining there’s just a bunch of atomized monads, competing with each other, where the ideal order is magically self-organizing and emergent.
We need another worldview, one drawn out of what is left of the actually collaborative and collective and common practices via which the world is actually built and run, a worldview of solidarity and the gift. A worldview that works as a low theory extracted from worker and hacker practices, rather than a high theory trying to legislate about them from above.
It is not hard to see here what infuriated Lenin about Bogdanov. For Bogdanov, both proletkult and tektology are experimental practices, of prototyping ideas and things, trying them out, modifying them. There’s no correct and controlling über-theory, as there is in different ways in Lenin or Lukacs. There is more of a hacker ethos here, rather than that of the authoritarian worldview one still finds in a Lenin or a Lukacs or in parody form in Zizek, where those in command of the correct dialectical materialist worldview are beyond question.
In Bogdanov’s worldview, there is no master-discourse that controls all the others. There is a continuum of practices, from the natural sciences, through engineering and design, to culture and art. The science and design part is mostly covered by the idea of a tektology; the culture and art part by proletkult. But they overlap, and both matter.
Bogdanov’s openness to the natural sciences, engineering and design are, I think, very contemporary. We only know about things like climate change — and other signs of the Anthropocene — because of the natural sciences. Without the very extensive global knowledge hack that is climate science, we would literally not know what the hell is going on around us. Why these droughts? These floods? These weird changes in the ranges of species, or their sudden extinctions or population booms? None of it would make sense.
Neither Heidegger nor Adorno have anything to say about any of this. But curiously, Bogdanov almost figured out global climate change for himself, between 1908 and 1920. He understood something about the carbon cycle. He understood the need to think social labor as acting on and in and with and against nature, producing a second nature and even a third. He understood the need to build an infrastructure that could adapt to changes in its interactions with its conditions of existence.
Lenin conducted a vigorous campaign to excommunicate Bogdanov, one which the Marxist tradition has strikingly never really revisited or attempted to reverse. This is among other things a great injustice. Bogdanov’s kind of experimental, open-ended Marxism, which neither tries to dominate, ignore, or subordinate itself to the natural sciences, became something of a rarity. His closest contemporary analog is, I think, Donna Haraway. Or so I argue in Molecular Red.
The Anthropocene calls not so much for new ways of thinking as for new ways of practicing knowledge. When the going gets weird, the weird turn pro. And it is likely to get weird — in this lifetime, or the next. That’s why I think we could start working now, not on theory of the Anthropocene, but theory for the Anthropocene. One could do worse, I think, than imagine and practice again something like a tektology and a proletkult – a tektology for hackers, a proletkult for cyborgs. Let’s build a world, and live in it.

The fate of cultural studies in the United States appears to be twofold. On the one hand, it still generates moral panic. Right-wing nut-jobbers think that “cultural Marxism” is some insidious, decadent creed, probably created by Jews and Blacks to destroy America. On the other hand, it has finally become seamlessly commodified. Dick Hebdige, once known as the author of a famous book about subcultures, is now a character in a novel by Chris Kraus that has been optioned for a TV pilot by the makers of the popular show Transparent. These two modes of recuperation were, incidentally, what Hebdige thought was the fate of all subcultures.

Hebdige broke new ground by rescuing subcultures such as the British mods, rockers and punks from the clutches of criminologists who could only think of them under the heading of deviance, and who at their most open-minded wanted to send the social workers after them rather than the cops. For Hebdige, subculture was rather a matter of culture and aesthetics, a form of “resistance through rituals.” This became a rather influential approach, not least in the art world, which is always on the lookout for new sources of aesthetic value, even if it means slumming it.

Angela McRobbie thought this was fine as far as it went, but that Hebdige tended to see subculture as something that mostly young working class men get up to. What happens instead if one looks at the self-making of young working class women as subculture? That too was fine as far as it went, but one has to ask whether the noise and resistance of late twentieth century British subcultures was a generalizable phenomenon. Maybe it was of its time and place. Maybe it was an artifact of a declining industrial working class crossing with the rise of broadcast-era consumer culture in the space of the city.

In her book Be Creative (Polity 2016), McRobbie updates the subculture story, tracing the fall-out from the clash of subculture with the culture industry through to a more recent obsession with the precariat and the creative industries. Is there more than a mere change in terminology here? And what can be learned by tracing the paths of young working-class women through this more contemporary urban landscape?

Drawing on Stuart Hall, and others in the cultural studies field including Dick Hebdige, Paul Gilroy and Andrew Ross, McRobbie takes a close interest in the utopian possibilities of everyday culture, but subjects it to a critical scrutiny, attendant to how popular aspirations are coopted by the commodity form or channeled elsewhere by disciplinary power. The people make culture, but not in a context of their own choosing.

Firstly, McRobbie has to bring the story up to date. Hebdige was writing about a time when subcultures appeared as noise, interrupting the orderly repetitions of the mass culture industry. Mass industrial work at least afforded mass industrial leisure. Subculture was, among other things, a displacement of the aspirations of the working class, shifted from struggles at the sites of mass production to the sites of mass consumption.

That was the case in the sixties and seventies, and the pattern was at least partially recognizable through to the club culture of the eighties and beyond. But from the rise of New Labour in 1997 onwards, all that fell away. Elements of youth subculture got imported into the creative industries. The night time economy of club culture translated into endless work days. A rapid capitalization of the cultural field led to a celebrity media sphere of a more individualistic kind, one that encouraged self promotion and self exploitation. With it came a more extensive detachment from community and class culture. The solution to social problems lay in getting out and getting on.

And what is the role of university in managing this? There is a certain irony here, as “the unexpected outcome of cultural studies is to have found itself canonized as a curriculum for a new creative economy.” (9) Now McRobbie, who teaches at London’s Goldsmith’s College, encounters students who aspire to work in the creative industries, who are often young and childless, but who juggle endless part time jobs while trying to get their degree. The university exists both as a form of credentialing, but also as place for networking.

Those part-time jobs are a means to an end, to an idea of a creative life. Subculture used the space of leisure as one in which a creativity suppressed at and by work could flourish. Now the idea is that work itself can be the site of that expression. Work becomes a kind of romantic relationship. “Work has been reinvented to satisfy the needs and demands of a generation who, ‘dis-embedded’ from traditional attachments to family, kinship, community or region, now find that work must become a fulfilling mark of the self.” (22)

Of course, all this independent, creative work ends up dependent on centrally owned and controlled infrastructure, from which a new kind of ruling class extracts the rent. Around it buzzes the old kind of petit-bourgeois ‘ducking and diving’, of trying one’s hand at this and then that, rather than specializing in a trade or profession. Young people function as the crash-test dummies for new styles of living this old kind of work, as passionate and involving. The older kind of petit-bourgeois could not dream of a million instagram followers.

McRobbie is sensitive to the ambivalence and ambiguity of all this. “What starts as an inner desire for rewarding work is retranslated into a set of techniques for conducting oneself in the uncertain world of creative labor.” (37) From the point of view of the young worker, its about autonomy; from the point of view of the state, “it is a matter of managing a key sector of the youthful population by turning culture into an instrument of both competition and labor discipline.” (38)

Marx had imagined that the petit-bourgeois strata would become progressively proletarianized as big capital moved in and colonized its various market niches. McRobbie describes something like the opposite phenomena. Various strata of what was once a working class is made petit-bourgeois. Capital non longer owes them even a factory or an office to work in. The absence of security is presented entirely as a good thing, as a lack of routine. The generalized urban economy, no longer of culture industry (singular) but the creative industries (plural) gives the young, particularly young women, a feeling of going places. It presents the endless possibility of personal success.

While I am skeptical as to how useful a concept neoliberalism might actually be, it does help account for some aspects of how class is subjectively lived today. Following Foucault, McRobbie traces neoliberalism to the ordo-liberals, German state functionaries and intellectuals who had kept their heads down during the Nazi years, and came up with a more palatable right wing philosophy after it. One that, in an irony of history McRobbie doesn’t mention, displaced the state-socialism of those intellectuals who had rallied to the British state in the cause of defeating the Nazis. The ordo-liberals redefined the human not in terms of labor but as an entrepreneur of its own life-force. It is a kind of market-vitalism, which proscribes a narrow set of rules for human conduct, the sole objective of which is, in every sense, self-appreciation.

While political theorists may dream of such a neoliberal subject, what actual subjects end up thinking and feeling and doing may be a bit more complicated and interesting, and that is McRobbie’s bailiwick. “I see passionate attachment to creative work as comprising ‘lines of flight’, embedded family histories of previously blocked hopes and frustrations.” (46) The class politics of the parent culture that is submerged in commodification used to reappear as subculture, but now (post) subculture is no longer an injection of noise against the hegemonic order but the seeding of new information for it to commodify. Meanwhile the industriousness sustaining the creative industries is provided by a ‘risk class’ without permanent jobs. Creativity promises the reward of realized self; insecurity appears as part of the adventure.

This all seems to confirm the work Eve Chiapello and Luc Boltanski did on how the ruling class responded to the challenge to its hegemony in the sixties by resisting one line of attack yet incorporating the other. The line resisted was the labor critique, in the form of wildcat strikes and factory occupations. The line that was incorporated was the artistic critique, which spoke not of labor but of alienation. It turns out that extracting value out of labor could function just fine without rigid, externally imposed discipline and uniformity. McRobbie: “While the prevailing value system celebrates the growth of the creative economy and the rise of talent, the talented themselves are working long hours under the shadow of unemployment in a domain of intensive under-employment, and self-activated work.” (153) McRobbie works this observation through a study of the work of Richard Florida, Ricard Sennett and the Italian ‘workerist’ school and its descendants, such as Paolo Virno, Franco Berardi and Maurizio Lazzarato.

McRobbie offers a less rosy view than that of Richard Florida, with his celebrations of thecreative class that populates prosperous cities, for whom the old working-class districts become gentrified playgrounds. McRobbie points out that Florida’s sunny vision is the flip side of what for Loic Waquant is a decline of sociological explanation about how urban space actually functions. The occupation of the city as a space for creative class play takes place against a background of mass incarceration which criminalizes a whole other urban population. On the one hand, part of what was subculture can become the creative industries; but on the other, a part of it no longer gets the social-worker treatment but goes straight from school to prison.

Where Richard Florida celebrates the hipster version of the creative industry, Richard Sennett prefers a more traditional version of of the value of ordinary work and craft labor. As McRobbie notes, there’s something patriarchal and conservative about some of Sennett’s attraction to old guys working steadily with their tools, but there might yet be something to draw out of this counter-model to the embrace of the creative industries.

Sennett sees work as life-enhancing and not mere drudgery. Here he thinks about labor quite differently to Hannah Arendt. However, for Seennett, changes in work may have led to a corrosion on character. Perhaps this could be reversed by returning to older habits of cooperative labor with their ethic of the job well done. McRobbie thinks there might be value in putting creative work alongside supposedly ‘uncreative’ craft work to counter the romance of creation, although one has to wonder if this is just another romance.

There are also tricky issues here of how any kind of labor might give rise to some form of intellectual property separable from the thing itself, which might at one and the same time yield an ‘author’ — and owner — and on the other a means of controlling the market in a particular line. Unfortunately, McRobbie does not pursue this, perhaps because her example is fashion, where intellectual property intervenes mostly at the level of the brand and the trademark rather than the individual designs.

All the same, Sennett offer a way of thinking about craft labor as one of the rhythms of the city, which has a certain value in its impersonality. It is a less grandiose way of thinking work; less about genius, talent, inspiration and competition, to which one might add — less about intellectual property. “A craft approach means being able to work all the time with failure…” (156) Craft skills are within the reach of most people. Its not an elite sensibility. Its reach is local. All well and good. “But the patient labor of craft is likely to remain a distant ideal for freelancers working on a piece-rate system and having to cut corners.” (158)

Craft may have little place in the contemporary cities of the over-developed world, with their sundering of local ties, temporary social relations, and relentless corporate culture of ‘team work.’ There might be some capacity for resistance (or — dare we hope — political innovation) embodied through memory and family history, but it may no longer take a subcultural in form. Perhaps it is in the residues of a craft sensibility which show up in the creative industries. Sennett provides a ‘parent culture’ view. Crafters and artists could do well to look it up, as it is their story.

McRobbie wonders too what would happen if the kinds of labor traditionally thought of as women’s work get the same treatment in Sennett as his craftsmen. “Where it may be fruitful to downgrade the dizzy expectations of artists and creative people so that they can sit alongside others, and benefit from the time-slow pace of a mode of working that gratifies on the basis of a job being done for its own sake, it proves more difficult to upgrade some stubbornly unrewarding jobs such as domestic cleaning.” (160)

One might pause here to consider the loss or invisibility of familial or community connection to such an ethos of craft labor. Thus the neo-bohemians populating Chicago’s Wicker Park in Richard Lloyd’s study are not able to see who is no longer living among them. Meanwhile thedigital artisans Andrew Ross studied at New York’s Razorfish advertising agency could be self-ironizing about their cool sweatshop and the cult-like commitment it extracted from its associates, but they have not much appreciation of how their laptop creations could end up as the decorations on merchandise made in actual sweatshops. In any case, things have moved on. Wicker Park and Razorfish are names from a forgotten era, even if it was only a decade ago.

Rather than try to keep up with the ever-changing fascination with cool neighborhoods and cool employers, McRobbie returns to a study Jacques Rancière did in the seventies, about nineteenth century workers. In Proletarian Nights, Rancière looked to workers whose aspirations were not limited to forming unions or cooperatives or political parties, or even to demanding the abolition of work. These deserters from the class struggle wanted a different kind of work. They wanted independence, and they expressed that desire in things like poetry — of an often quite formal and traditional kind.

McRobbie connects this to the British cultural studies tradition, which had shifted attention from the sphere of production to that of consumption in order to understand how the desires and ambitions of labor had sought expression there. Taken together, these parallel French and English approaches took an interest in non-traditional kinds of ‘politics’, if that is still the word for it. The British approach, more cultural than capital-P political, took the disco or the kitchen table as significant sites. McRobbie: “These communal, familial, collective or indeed institutional spaces permitted alternative working lives to be imagined. Cultural studies therefore anticipated a neo-Marxism open to difference and diversity, open to the equal stature of the family and the community alongside that of the workplace and the sphere of formal politics.” (58)

But this popular culture of labor’s aspirations and capacities could in turn be instrumentalized. In the British context, the significant changes happened under so-called New Labour. Creativity became a kind of labor reform, in which the artist would stand as a model for a new kind of human capital. McRobbie: “These were the Damian Hirst times.” (42) Art and culture were put to work. This was a kind of transitional model, replaced in more recent times by the idea of tech-centric innovation. The basic formula is not that different, however. The new-model worker is to aspire to apply creativity to achieving individual success and celebrity. In both its creative and innovative flavors, this is a model hostile to traditional or ‘elitist’ version of either culture or the social. It can sometimes have a vaguely inclusive rhetoric. It is meritocratic, but does not pause for too long to ask if winners really started from the same starting-blocks as the losers. And of course, it never presents all this from the worker’s perspective. Workers are supposed to go away. The labor movement is replaced by networks of demassified, autonomous free agents.

Ironically, cultural studies itself became a kind of textual material that could be reworked into this image. For example, Paul Willis on working class youth creativity got repurposed in the language of ‘New Times’ post-labor politics, initially sponsored by the Gramscian wing of whatever was left of the Communist Party. This in turn became language for New Labour. It would be churlish to hold against cultural studies what others did with it. McRobbie defends Stuart Hall as as trying to cope with rise of post-Fordism and its effects on consumer culture via a new popular politics. “Hall’s expansive ideas for how the left could forge a new popular politics were taken up and deflected in unexpected right wing directions…” (68)

A characteristic of British culture that sets it part a bit from the United States is that for a long time had maintained a public education system that opened a pathway into the arts for talented and often disaffected working class kids. For McRobbie and others in cultural studies, this was the site of displaced anatagonisms from the factory floor. Where that tradition ended however is probably with the artist-celebrity as champion of so-called neoliberal ideas of self-making, of which Tracey Emin and Damien Hirst would be avatars.

What the ordo-liberals probably did not anticipate was that the artist would in some ways become the ideal type of the neo-liberal subject, who would engage in a knowing self-exploitation in pursuit of the dream job. McRobbie thinks there are three subtypes of the artist, although I don’t find the categories too convincing. They are the socially engaged artist, the global artist and the artist-precariat, with the latter forming a kind of critical refusal from within in the ideal neo-liberal subjectivity of the artist. McRobbie: “The rhizomatic tactics and strategies of such creative activities are totally incommensurate with the vocabularies of the toolkits and business studies modules and thus can be seen as a direct challenge to the ‘entrepreneurial university.'” (84)

Where the modernist artist was the exception to the culture industry; the contemporary artist is the exemplar of the creative industries. The category of the creative industries is of interest not least because it blurs the line between fine art and applied art, or craft. Postmodern art’s stylistic complications of the boundaries between the aesthetic and kitsch seem tame compared to the extended commodification of the production of information of all kinds.

McRobbie is more interested in the more ‘vulgar’ kinds of creative industry and the young women drawn to them. There she finds enthusiastic career girls, performing elaborate body rituals that are coded by a kind of post-feminist masquerade. They perform so-called immaterial labor and emotional labor, or what McRobbie calls “passionate work.” (89) They don’t entirely disavow class or ethnicity or community. They just see a narrow path to a more passionate life that involves some compromises. Normative femininity is a way to cover over traditional working class traits that may be disabling in the workplace. Feminism opened up a path of opportunity but one now reclaimed by a more traditional-seeming code of femininity.

“Capitalism makes a seductive offer to young women with the promise of pleasure in work, while at the same time this work is nowadays bound to be precarious…” (105) These women tried to refuse work as a way to escape from monotonous jobs in favor of self-directed activity, but this then has become recuperated too.  McRobbie: “… the idea of ‘romance’ has been deflected away from the sphere of love and intimacy and instead projected into the idea of a fulfilling career.” (91)

Compared it to Italian workerist thinkers, where a rather masculinist approach to politics remained standard, McRobbie’s cultural studies approach opens up some interesting questions were labor and gender combine. By contrast to the ‘Bologna school’, the Birmingham school moved from the factory floor to everyday life and uncoupled different kinds of struggle. McRobbie: “without a concept of ‘culture,’ the idea of ‘the street’ can only connote a weaker space which is not the shop-floor and hence not primarily an expected location for class politics. In this thinking the idea of the factory floor still takes precedence even when the workforce is in flight from it.” (95) Where the workerists spoke of the social factory; cultural studies might speak of the social kitchen. A change of metaphor here might alter how we think of what became of both labor and culture in the era of the so-called creative industries.

The workerists still treat the classic class antagonism of capital and labor as central, whereas the culturalists treated the political and cultural levels of the social formation as equally substantive. In the spirit of EP Thompson and Raymond Williams, cultural studies saw culture as a popular landscape of resistance and protest. The workerists thought there was a new kind of subjectless class politics to which post-Fordist production processes as the response. “Yet lacking a strong concept of working-class culture these authors can only rest their case on the refusal of work…” (97) Even if they were never quite clear what refusal meant. Meanwhile, the culturalists expanded Gramsci’s conception of a popular culture (although after Gilroy no longer necessarily a national one) as a common resource.

McRobbie retrieves from the workerists the idea of the line of flight, the desire to escape, and mobility as response to labor. Of course not all workerists (and post-workerists) are enthusiasts for these lines of flight. Lazzarato and Berardi, for example, becomes quite pessimistic. McRobbie’s question is whether young women get the same chance at the ‘immaterial’ as young men. Is today’s labor market just as (or more) gender segregated? Is there a return to a kind of neo-traditional sexism? Or, more broadly: “How then can we talk about the gender of post-Fordism?” (101)

McRobbie: “… refusal is more of a desire and a yearning for rewarding work, something that is within sight and perhaps within reach through access to further and higher education. This ‘flight’ also acquires gendered characteristics. The impact of 1970s feminism made the idea of a career for young women completely acceptable. Unlike the autonomist Marxists, I do not make such fulsome claims for a new radical politics emerging from the ‘social factory,’ instead I see a field of ambivalence and tension, where lines of flight connect past parental struggles with the day-to-day experiences of their children in the modern work economy.” (93)

Despite the utopian promise of passionate work as escape from traditional labor, it ends up being a desire that can in turn be exploited. Passion becomes a means of production, complete with precarity, long hours and low pay. “I pose the idea of passionate work being a distinctive mode of gender re-traditionalization… whereby the conservatism of post-feminism re-instates young women’s aspirations for success within designated zones of activity such as creative labor.” (110) Passionate work becomes self-exploitation, complete with its own codes of affect — a permanent appearance of enjoyment — and a bodily style of exuberant enthusiasm.

Creative work has become separated from ordinary labor, but does it follow as McRobbie thinks that it is thereby depoliticized? Maybe there’s another kind of politics for something that is not exactly labor. It is the case that a wedge was driven between creative labor and other kinds, thereby weakening social democratic politics. But perhaps the strategy is not then to bring the former back into the latter. McRobbie sometimes sounds as nostalgic as Sennett, if not exactly for the same image of the past. Rather than a neoliberal vocabulary of the entrepreneur, or the old social democratic one of industrial labor, perhaps its time to think of another one that might more accurately map onto the class formations of our time. Rather than reverse the neoliberal turn, lets take a new turn.

McRobbie encourages us to look to less masculine-coded practices for signs of possibility. She is interested in women crafters who make public women’s traditional skills, such as the yarn-bombers weaving public artworks that knit bicycles to lampposts. There’s an ambivalence to these scenes. In part, they look back to a rather traditional and idyllic culture of femininity. On the other, they sometimes draw from those pasts to create more self-consciously feminist practices in the present. As always with culture, there’s tensions and ambiguities which can be fruitful and interesting. This scene, as an example, is a sort of return to the craft critique of production of William Morris, without the paternalism.

Perhaps it is not quite the same to be making the old things and to be making new information. Perhaps the latter seems so lacking in the history and culture of labor because it isn’t exactly labor. The temporality of its production may not have much in common with the patient persistence of craft-work. Its relation to the commodity form is rather different. It does not produce the thing to be sold as a piece of property, but rather creates an arrangement of information that is novel enough to count as intellectual property. It is so easily copied that other strategies have to come into play to extract value from its production, which is where creating the aura of special skill around particular creators comes into play. The networks within which information is made are partly local, and as yet nothing beats the city as a way of organizing it. But its networks also extend beyond far urban space. The infrastructure of information makes the physical and informational aspects appear quite separate, although there is nothing immaterial about it.

In short, maybe this relatively new kind of producer is as unlike craft labor as it is unlike industrial labor. It comes into being only at a time when information can be private property, and yet information can be rapidly and fully copied ands shared. Its a relatively new set of forces of production that make it possible — information technology. It is both shaped by, and exceeds, the relations of production extruded out of the property form to embrace it — intellectual property. Maybe it even produces quite distinct class relations, between producers and owners of information.

These aspects of the creative industries I find neglected both in British cultural studies and Franco-Italian workerist theory. One might however draw from McRobbie an attention to gender in how creative industries have evolved. If one looks at fashion as the archetypal form of creativity and tech as the archetypal form of innovation, one finds very strong imposition of very conservative ideas about what is men’s work and what is women’s work — even if neither is exactly work any more.

One thing that the left and right now seem to agree on is that the society in which we live is called capitalism. And strangely enough, both now seem to agree that it is eternal. Even the left seems to think there is an eternal essence to capitalism, and only its appearances change. The parade of changing appearances yields a series of modifiers: this could be late capitalism or communicate capitalism or cognitive capitalism or neoliberal capitalism. But short of an increasingly allegorical or messianic leap into something other — it is as if this self-same thing just went on forever.

Maybe its because I have a taste for old-fashioned modernism, but whenever I come across a piece of language about which there is such wide consensus I want to trouble it, somehow. This capitalism that we have all agreed that we live in: has it not become too familiar, too comfortable an idea? The reality the term tried to describe is of course far from comfortable. Capitalism, if this is what this is, appears to be smashing not only the social but the natural conditions of its existence to pieces. But then maybe this is the thing to ask about. Why have we become so comfortable with a way of describing an uncomfortable reality? Do we want a certainty in language that can’t be had anywhere else?

That the world we live in is capitalism has become a familiar way of describing something that destroys what is familiar. It atomizes and alienates. It renders everything precarious except its own grasp on the imagination. If the greatest trick of the devil was to persuade us that the devil does not exist; then maybe the greatest trick of capitalism is to gull us into imaging that there is nothing but capitalism.

It is hard to describe things that change imperceptibly.  This may well be the level of language on which the problem rests. It has to do with using the combinations of language, which have something of a binary quality, to describe changes that might be gradual or might be swift, but which aren’t neat digital divides between one term and another. It is as hard to describe transitions between modes of production as it is to describe changes in mood.

There was once a language about transitions between modes of production. It is striking how the left and the right alike ended up working within the same language about this. Marx really was one of the great modern poets. Of course he worked with the materials of the languages he had to hand, but he wrought something lasting: a combinatory of terms for describing history. Like any great poetic corpus, his work contains multitudes. But there are a few standard permutations than came to stick in the mind, like great pop songs.

Here I think is his greatest hit, one that has become something of an earworm. It goes something like this: this is capitalism. It has an essence and it has appearances. Its essence is defined by these things: the commodity form, with its doublet of use value and exchange value; by labor’s double form, as concrete and abstract labor; by the extraction of surplus value in the production process, by the wage relation, by the rising organic composition of capital, by the crisis of the tendency of the rate of profit to fall. And finally, by negation.

There are actually two variants of the poem here, about negation. Either capitalism negates itself, brought to ruin by its own contradictions. Or: it is negated by a force it produces as its own negation, the working class. In either variant, one thing is key: capitalism can change its appearances, but never its essence. Its essence can only be negated, by contradiction or struggle. Various variant tunes spill out of this rhetorical frame, like mutating like genres of techno music.

There are other variations. One can swap out the abstract verb negation and replace it with acceleration. This is currently popular again, as it was in the twenties. Here the idea is that there’s nothing that can negate capital, either in its own contradictions or in the force it produces in and against itself. Rather, the best one can do is accelerate it to its end, towards a Promethean leap into another historical figure. But note that this is not as much of a change in tune as its advocate like to imagine. It leaves intact the rhetorical form of capital as an essence.

The essence of capital is eternal. This is the striking feature of how it is now imagined. Those who love it of course embrace this thought. It needs merely to be perfected by our love. This is sometimes called, with a stunning lack of imagination — neoliberalism. But what is even stranger is that those who do not love it seem to agree. The essence of capital is eternal. It goes on forever, and everything is an expression of its essence. Capital is the essence expressed everywhere and its expression is tending to become ever more total.

The other side of the eternal essence of capital is its ever changing appearances. Change is accounted for via the use of modifiers. Its appearances can even be periodized. There was merchant capitalism then industrial capitalism, then monopoly capitalism then neoliberal capitalism. There’s some ambiguity as to what to call the current stage, however. It could be multinational, cognitive, semio, late, neoliberal, or postfordist capitalism, to name just a few. Note that the last two of these are temporal modifications to a modifier: neoliberal, postfordist. Could there be any better tribute to the complete enervation of the imagination by capitalism, or whatever it is, that this is the best our poets can do? Modify the modifier? Capitalism must be very disappointed in our linguistic competence.

Of course there’s the opposite rhetorical tack as well, which is to go a bit overboard with the binary difference between two terms, although its partisans have not been so bold as to break too much with the essence of capitalism. Rather, it worked like this: there used to be material labor; now there is immaterial labor. Its a different kind of labor. Its the opposite! But its still only a modified capitalism, a cognitive capitalism. Its not material any more. Capitalism itself is about ideas. Its striking how much one can get carried away with the play of language, and forget to look at the world. Somehow, I don’t think the hundred million industrial workers of China perceive their work as immaterial.

The task of this talk is thus a provocation: to think the possibility that capitalism has already been rendered history, but that the period that replaces it is worse. That it could be worse gets us away from the happy narratives in which capitalism gave way to a postindustrial society or some other magic kingdom, free from contradiction and class struggle. Rather, in this thought experiment, I propose to think the present as a new kind of class conflict, including new kinds of class arising out of recent mutations in the forces and relations of production. But putting this pressure on our received ideas and legacy language, perhaps we can begin to see the outlines of the present afresh, estranged from our habits of thought.

There was once an attempt to have done with at least part of this great rhetorical-historical edifice. It started with questioning the idea of capital as having an essence and an appearance. What if appearances were as equally real as the essence? There were actually two version of the essence-appearance structure. One took economic to be the essence, but in the sense of being the base, and everything else was dependent on it. This version is called economism. In the other version, its not the economic, but the commodity form that is the essence, one that has come into being in history and then become the essence of history, which records its forms of appearance as a false totality, as spectacle.

Against this, some took the view that the economic only determined everything else in thelast instance, that things like politics and culture were not mere appearances but had their own material form, but one whose function was to reproduce the essential economic form of capitalism.

If things like politics or culture are relatively autonomous, if they have their own material form, maybe they even have their own essence! It did not take long for culture to have its own essential categories: the signifier and the signified were just like exchange value and use value! An abstract essence! But a different one! So one could just specialize in singing the song of this (relatively) autonomous world of essences and appearances, while still gesturing to the master-narrative, that this is indeed and will remain, capitalism.

If the economy has an essence and appearances, and culture too has an essence and appearances, then maybe politics does too! The wonderful thing about language is that if you seek it you can find it. Yes, politics has an essence, the great fundamental drama of friend versus enemy, or maybe its dissensus, or something. The main thing is we can sing the song of the essence and appearances of politics, while still gesturing to the master-narrative, that this is indeed and will remain, capitalism.

I have to say that my inner modernist finds this all rather banal. Is this the best we can do to speak the sublime language of our century? Why does it all seem the same? Like pop music? Variations on themes, all leading back to the same old note, that capital is eternal? That one day (that will never come) there will be a messianic leap into something else, but until then, let’s just go to the movies. It seems to me that our poetry of capitalism, or whatever it is, shows all the signs of being a culture industry. Nowhere in these tunes is there that striking note of non-equivalence, or that moment of de-familiarization when the roof falls in.

Perhaps one has to ask: what the emotional attachment that we have to the idea that this is capitalism, and that it is eternal? It has to be said that the most vigorous attempts to tell a different story, to strike a different tune, where made in bad faith. Still are, perhaps. There was a time when it was a popular art form. Once the narrative of capitalism and its coming negation got out, you could make a good living coming up with a different story. Not surprisingly, it was former Marxists and socialists who came up with most of those alternative stories.

Thus we had the story of the managerial revolution, of the postindustrial society, of the conditions for take-off and growth. What these stories all had in common was that they accepted the basic premise of the Marxist story. They conceded its power, its poetry. But they changed the ending. Rather than negation, the story ended in a resolution of contradictions. These were extorted reconciliations. But they had some currency nonetheless. But with the collapse of the supposedly socialist world, which at least pretended to live up to the great Marxist story, these counter-narratives lost their force.

One counter-story from that era survives. It was not written by a socialist, although he briefly worked for a socialist government. In this story, capitalism negates itself, and in a good way. It can pivot and disrupt itself. Indeed, its essence becomes its self-disruption. And it is our sacred duty never to get in its way. This is the rhetorical art-form of the ‘California ideology.’ Into it can be folded certain other variations, about the fourth industrial revolution, for example.

The conceit of all these post-capitalist stories was that this is not capitalism, it’s better! When people hear the beginnings of a story about this no longer being capitalism, their resistance generally rises at this point. Unless you happen to be worth several million dollars, the chances are you do not perceive this as something better than capitalism.

But maybe it would be interesting, politically and aesthetically, to take the other fork of the binary here. Instead of the idea that this is not capitalism, ts better, what if we explored the idea that this is not capitalism, but worse? This also meets a lot of resistance. This I can tell you from experience, having tried to write variations on this text for fifteen years. Nobody wants to leave the certainty of the devil they know, or think they know, for something that promises to be worse.

Interestingly, few people will even attempt it as a thought experiment. There really is something fundamental to the belief that this is capitalism. It may even be the defining feature of ideology today. Ideology today is not the acceptance of a neoliberal structure of feeling or habits of thought and action. Ideology today is clinging to the belief that this is capitalism.

Another way to tackle this would be impute some meaning to Marx’s famous remark to the effect that he was not a Marxist. What if what he meant by that is that he was not one of those who simply took a language and a rhetorical form extracted from his texts as a given? He was, to the contrary, the one who had constructed that language with a quite particular purpose in mind: to understand the situation of his times from the labor point of view. So: what if we kept the commitment to understanding, not his times, but ours, from the labor point of view, whatever that might mean now — and bracketed off the rest?

That makes a certain sense to me. I really am puzzled by why we should use blocks of linguistic material from his time again to understand our time. Why use the fashionable philosophy, the popular science, the political tracts, or the technological metaphors of the mid-nineteenth century? When poets or novelists do that, we immediately think its dated and quaint. But somehow we want our great narrative to be about capitalism, even if it is dated and quaint.

Of course different genres of text have a different relationship to tradition and innovation, and at different moments in their development. They aren’t always in synch. And of course there’s generally a culture industry in which the texts get pulped into sameness, and an avant-garde trying to do something else. If you are trying to write an interesting, rather than merely successful, novel or poem, you want to change things at the formal level, rather than ship your wine in the same old bottles. The thing is, where readings and rewritings of Marx are concerned, they seem to me to belong to the culture industry. Its a commonplace now to read Capital as a work of philosophy or an epic novel, but to do so very conservatively. And indeed could there be anything more conservative now that the tradition of continental philosophy?

I have not named names in this text, partly to avoid embarrassing its characters. But mainly because I take it as given that texts writes their author, rather than their author writing them. Authors are never good guides to their own writings, as the writings exceed conscious intention — although I would not take that insight as far as the psychoanalytically inclined, who maybe create too big an interpretive playground for themselves out of it. So in describing my own attempt to write within the space, all these caveats also apply to me.

It has not always been the case that Marx was read conservatively, as a great text for explication, interpretation and imitation. Where the Marxocological savant became a master simply by producing a variation on the theme. There are those who read Marx the same way they read Rimbaud and Lautréamont. I’ll mention just three: Aimé Cesaire, JBS Haldane, and Guy Debord. From the latter I’ll also take a few clues about method. Could there be a way to write after Marx that isn’t based on conservative habits of mastery and interpretation, but which are based instead on experimentation and détournement?

Of course, being a very minor poet, I did not get very far. But I gave it a shot. I wrote a way of describing the current situation that is not capitalism, but worse. Here’s how: what if, rather than start at the beginning, one started at the end? The capitalism story always starts in the past, with the birth of capitalism, and imagines a destiny, a teleology, wherein the present must be some continuum from that past. This must be some modification of the essence of the thing. Let’s do it the other way around. Let’s first describe the present, then secondarily figure out where it came from. This may even, in the end, involve modifying our understanding of capitalisms past. In short, let’s start where Marx started, describing a present — not from his results.

Let’s start by being very ‘orthodox’ — (I use the term ironically). Let’s start with the forces of production, with the relations of production that correspond to them, the class conflict generated out of those relations of production, and the political and culture superstructures that correspond to that base. And let us also, just as Marx did, try to describe what may be emerging, rather than what is established. If one starts with what is established, it is easy to interpret any new aspect of the situation as simply variations on the same essence. Starting with what is emerging provides a suitable derangement of the senses, a giddy hint that all that was solid is melting into air.

The thought experiment that might result is quite simple. What if it was like this: There really is something qualitatively distinct about the forces of production that produce and instrumentalize and control information. This is because information really does turn out to have strange ontological properties. Making information a force of production produces something of a conundrum within the commodity form. Information wants to be free but is everywhere in chains. It isn’t scarce, and the whole premise of the commodity is its scarcity.

Information as a force of production called into being particular relations of production. In classic Marxist style, one can look here at the evolution of legal forms. What we see there is the emergence of intellectual property as close to an absolute private property right. One that makes the once separate and local property forms of patent, copyright and trademark equivalent forms of private property. Forms which, as the negotiations on the Trans-Pacific Partnership make clear, need transnational forms of legal enforcement, precisely because information is such a slippery and abstract thing.

And so, like the enclosures or the joint-stock company before it, intellectual property law becomes the form of a new kind of relation of production, more abstract than its predecessors, and one which makes not land or physical plant a form of private property, but information itself. Like those preceding forms of private property, this one gives rise to a class relation. As an absolute form of private property, it creates classes of owners and non-owners of the means of realizing its value. Land as private property gave rise to the two great classes of farmer and landlord. Capital as private property gave rise to the two great classes of worker and capitalist. Is there a new class relation that emerges out of the commodification of information?

For argument’s sake, let’s it’s say it does. I call those classes the hacker class and the vectoralist class. The hacker class produces new information. What is ‘new’ information? Whatever intellectual property law recognizes as new. Its a strange kind of production. Where the farmer grows crops and the worker stamps out units of some thing, the hacker has to make the same old stuff, information, appear in new configurations. Getting this done is not like the seasonal repetitions of farming or the clocking-on of the worker. It happens when it happens, including time spent napping or pulling all-nighters. Hackers can’t be managed like farmers or workers. They are not the same as either class.

Like the farmer and the worker, the hacker does not usually end up owning the product of her efforts. Unless you own a drug company or a tech company or whatever, you have to sell the rights to what you produce. It is not always the same as selling labor-power. You might still own the intellectual property, for example. But the hacker rarely captures the value of what they invent. not everyone gets to be Bill Gates — precisely because there is a Bill Gates, who is not the avatar of the hacker class, but of its opposite — the vectoralist class.

The vectoralist class owns and controls the vector, a term I use to describe in the abstract the infrastructure on which information is routed, whether through time or space. You can own stocks or flows of information, but far better to own the vector, the legal and technical protocols for keeping information scarce.

If one takes a look at the top Fortune 500 companies, it is surprising how many of them are really in the information business. I don’t just mean the tech and telco companies like Apple or Google or Verizon or Cisco, or the drug companies like Pfizer. One could think of the big banks as a subset of the vectoralist class rather than as ‘finance capital.’ They are in the information asymmetry business. And as we learned in the 2008 crash, even the car companies are in the information business — they made more money from car loans than cars. The military-industrial sector also also in the information business. Even the companies that make things like Nike are really in the brand business. Walmart and Amazon compete with different models of the information logistics business. The oil companies are in the prospecting business. The actual oil drilling is contracted out. Perhaps the vectoralist class is no longer emerging. Maybe it is the new dominant class.

That might only be the case in the overdeveloped world where we live. Many of the world’s peoples are still peasants who are being turned into farmers by the theft of their land by a landlord class. Much of the world is a giant sweatshop. The resistance of labor to capital is alive and well in China or India. The older class antagonisms have not gone away. Its just there’s a new layer on top, trying to control them. Just as the capitalist class sought to dominate and subordinate the landlord class as a subordinate ruling class, so too the vectoralist class tries to subordinate both landlords and capitalists, by controlling the patents, the brands, the trademarks, the copyrights, but more importantly the logistics of the information vector.

A side note here: In Capital, Marx really only deals with an ideal-type political economy with two classes. But in his political writings it is clear that he understands social formations as hybrids of combined and overlapping modes of production. Landlords and farmers loom large in his writings on France, for example. So here I’m simply taking my cue from the political writings, and thinking a matrix of six classes, three ruling and three subordinate. The dominant classes are thus: landlords, capitalists, vectoralists. The subordinate classes are: farmers, workers, hackers.

Now imagine all the possibilities of class alliance and conflict that this generates. It turns out that politics is much less about the relation between the friend and the enemy, and much more crucially about relations among non-friends and non-enemies. As anyone who has actually done politics, or knows some semiotics, could figure out.

So how is it worse that capitalism? The vectoral infrastructure throws all of the world into the engine of commodification. There is nothing that can’t be tagged and captured via information about it and considered a variable in the simulations that drive resource extraction and processing. Quite simply, we have run out of world to commodify. And now commodification can only cannibalize its own means of existence, both natural and social. Its like that silent film where the train runs out of firewood, so the carriages themselves have to be hacked to pieces and fed to the fire to keep it moving, until nothing but the bare bogies are left.

It is worse also in that rather than some vague multitude, there’s complex class alliances at play in the political space. The trickiest part of it is the politics of the hacker class. Which after all is the class most of us here belong to. Yes, it sometimes appears as a privileged class. But it is a class that has a very hard time thinking its common interests. Largely because the kinds of new information its various sub-fractions produce are all so different. We have a hard time thinking what the poet and the scientist and the engineer have in common. Well, the vectoral class does not have that problem, what all of us make is intellectual property, which from its point of view is all equivalent and tradable as a commodity.

Also, the hacker class experiences extremes of a winner-take-all outcome of its efforts. On the one hand, fantastic careers and the spoils of some simulation of the old bourgeois lifestyle, on the other hand, precarious and part time work, start-ups that go bust, and the making routine of our jobs by new algorithms designed by others of our very own class. Of course it is always a tough argument to propose common interests among subordinate classes. Counter-hegemony is hard. Hackers, like workers or farmers, are distracted by particular and local interests. Class consciousness is rare among hackers. Most of us are rather reactionary — even in the nontechnical trades. But then class consciousness is always a rare and difficult thing. Unlike other identities, it has to be argued contrary to appearances.

I could add more to the picture, but perhaps that will do for now. Treat it as a thought experiment. Maybe like a science fiction story where you have to suspend disbelief. Or an avant-garde prose poem. That was secretly how I thought about A Hacker Manifesto when I wrote it, although of course I did not tell Harvard University Press that, as everyone knows prose poems don’t sell. I can say that I got that prose poem to sell quite well. And be reprinted, and translated into eight or nine languages. But I think now I can safely reveal that my first crack at this way of experimenting with Marx was an also a stab at a avant-garde prose poem.

It was written, incidentally, in a non-existent language. I wrote it in European. That’s a language, which, if it existed would be equal parts church Latin, Marxism and business English. Maybe that’s why I suspect it reads better in French, German, Italian or Spanish, as those translations are better than my translation of it into English.

So to sum up: what if we took a more daring, modernist, de-familiarizing approach to writing theory? What if we asked of theory as a genre that it be as interesting, as strange, as poetically or narratively as rich as we ask our poetry or fiction to be? What if we treated it not as high theory, with pretentions to legislate or interpret other genres; but as low theory, as having no greater or lesser claim to speak of the world than any other. It might be more fun to read. It might tell us something about the world. It might, just might, enable us to act in the world otherwise.

Some might think it a new low, when a candidate for high office starts talking on television about the size of his penis. As if the regular, non-penile spectacle within which we all live and breathe was somehow some lofty public sphere. But perhaps its more a question of the current stage of spectacular live exposing itself in its ruined perfection. The spectacle has a history. Its current stage is what I have called, in a book of the same name, the spectacle ofdisintegration. I wrote it three years ago, but really to talk about some people who say it coming thirty years ago. Here is how I explain what I think the spectacle of disintegration is and what it means. The book from which it forms the introduction is here. The spectacle of disintegration is this big — a totality, actually.

When the storm hit the Hansa Carrier, twenty-one shipping containers fell from its decks into the Pacific Ocean, taking some 80,000 Nike sneakers with them. Seattle-based Oceanographer Curtis Ebbsmeyer used the serial numbers from the sneakers that washed up on the rain coast of North America to plot the widening gyre of ocean-going garbage that usually lies between California and Hawaii. Bigger than the state of Texas, it is called the North Pacific Subtropical Gyre, and sailors have known for a long time to steer clear of this area from the equator to 50 degrees north.

It’s an often windless desert where not much lives. Flotsam gathers and circles, biodegrading into the sea. Unless it is plastic, which merely photo-degrades in the sun, disintegrating into smaller and smaller bits of sameness. Now the sea here has more particles of plastic than plankton. The Gyre is a disowned country of furniture, fridges, cigarette lighters, televisions, bobbing in the sea and slowly falling apart, but refusing to go away.

New Hawaii is the name some humorists prefer for the North Pacific Subtropical Gyre now that it has the convenience of contemporary consumer goods. Or one might call it a spectacle of disintegration. It is as good an emblem as any of the passing show of contemporary life, with its jetsam of jostling plastic artifacts, all twisting higgledy-piggledy on and below the surface of the ocean. Plastic and ocean remain separate, even as the plastic breaks up and permeates the water, insinuating itself into it but always alien to it.

The poet Lautréamont once wrote: “Old Ocean, you are the symbol of identity: always equal to yourself… and if somewhere your waves are enraged, further off in some other zone they are in the most complete calm. But this no longer describes the ocean, which now appears as far from equilibrium. It describes instead the spectacle, the Sargasso Sea of images, a perpetual calm surrounded by turbulence, at the center always the same.

When Guy Debord published The Society of the Spectacle (1967), he thought there were two kinds: the concentrated and the diffuse spectacle. The concentrated spectacle was limited to fascist and Stalinist states, where the spectacle cohered around a cult of personality. These are rare now, if not entirely extinct. The diffuse spectacle emerged as the dominant form. It did not require a Stalin or Mao as its central image. Big Brother is no longer watching you. In His place is little sister and her friends: endless pictures of models and other pretty things. The diffuse spectacle murmured to its sleeping peoples: “what appears is good; what is good appears.”

The victory of the diffused spectacle over its concentrated cousin did not lead to the diffusion of the victor over the surface of the world. In Comments on the Society of the Spectacle (1988), Debord thought instead that an integrated spectacle had subsumed elements of both into a new spectacular universe. While on the surface it looked like the diffused spectacle, which molds desire in the form of the commodity, it bore within it aspects of concentration, notably an occulted state, where power tends to become less and less transparent.

That the state is a mystery to its subjects is to be expected; that it could become occult even to its rulers is something else. The integrated spectacle not only extended the spectacle outwards, but also inwards; the falsification of the world had reached by this point even those in charge of it. Debord wrote in 1978 that “it has become ungovernable, this wasteland, where new sufferings are disguised with the names of former pleasures; and where the people are so afraid…. Rumor has it that those who were expropriating it have, to crown it all, mislaid it. Here is a civilization which is on fire, capsizing and sinking completely. Ah! Fine torpedoeing!”

Since he died in 1994, Debord did not live to see the most fecund and feculent form of this marvel, this spectacular power that integrates both diffusion and concentration. In memory of Debord, let’s call the endpoint reached by the integrated spectacle the disintegrating spectacle, in which the spectator gets to watch the withering away of the old order, ground down to near nothingness by its own steady divergence from any apprehension of itself.

And yet the spectacle remains, circling itself, bewildering itself. Everything is impregnated with tiny bits of its issue, yet the new world remains stillborn. The spectacle atomizes and diffuses itself throughout not only the social body but its sustaining landscape as well. As Debord’s former comrade T. J. Clark writes, this world is “not ‘capital accumulated to the point where it becomes image’ to quote the famous phrase from Guy Debord, but images dispersed and accelerated until they become the true and sufficient commodities.”

The spectacle speaks the language of command. The command of the concentrated spectacle was: OBEY! The command of the diffuse spectacle was: BUY! In the integrated spectacle the commands to OBEY! and BUY! became interchangeable. Now the command of the disintegrating spectacle is: RECYCLE! Like oceanic amoeba choking on granulated shopping bags, the spectacle can now only go forward by evolving the ability to eat its own shit.

The disintegrating spectacle can countenance the end of everything except the end of itself. It can contemplate with equanimity melting ice sheets, seas of junk, peak oil, but the spectacle itself lives on. It is immune to particular criticisms. Mustapha Khayati: “Fourier long ago exposed the methodological myopia of treating fundamental questions without relating them to modern society as a whole. The fetishism of facts masks the essential category, the mass of details obscures the totality.”

Even when it speaks of disintegration, the spectacle is all about particulars. The plastic Pacific, even if it is as big as Texas, is presented as a particular event. Particular criticisms hold the spectacle to account for falsifying this image or that story, but in the process thereby merely add legitimacy to the spectacle’s claim that it can in general be a vehicle for the true. A genuinely critical approach to the spectacle starts from the opposite premise: that it may present from time to time a true fragment, but it is necessarily false as a whole. Debord: “In a world that really has been turned on its head, the true is a moment of falsehood.”

This then is our task: a critique of the spectacle as a whole, a task that critical thought has for the most part abandoned. Stupefied by its own powerlessness, critical thought turned into that drunk who, having lost the car keys, searches for them under the street lamp. The drunk knows that the keys disappeared in that murky puddle, where it is dark, but finds it is easier to search for them under the lamp, where there is light – if not enlightenment.

And then critical theory gave up even that search and fell asleep at the side of the road. Just as well. It was in no condition to drive. In its stupor, critical thought makes a fetish of particular aspects of the spectacular organization of life. As Todd Gitlin says, the critique of content became a contented critique. It wants to talk only of the political, or of culture, or of subjectivity, as if these things still existed, as if they had not been colonized by the spectacle and rendered mere excrescences of its general movement. Critical thought contented itself with arriving late on the scene and picking through the fragments. Or, critical thought retreated into the totality of philosophy. It had a bone to pick with metaphysics. It shrank from the spectacle, which is philosophy made concrete. In short: critical thought has itself become spectacular. Critical theory becomes hypocritical theory. It needs to be renewed not only in content but in form.

When the American Food and Drug Administration announced that certain widely prescribed sleeping pills would come with strong warnings about strange behavior, they were not only responding to reports of groggy people driving their cars and making phone calls, but also purchasing items over the internet. The declension of the spectacle into every last droplet of everyday life means that the life it prescribes can be lived even in one’s sleep. This creates a certain difficulty for prizing open some other possibility for life, even in thought.

Debord’s sometime comrade Raoul Vaneigem famously wrote that those who speak of class conflict without referring to everyday life, “without understanding what is subversive about love and what is positive in the refusal of constraints, such people have a corpse in their mouth.” Today this formula surely needs to be inverted. To talk the talk of critical thought, ofbiopolitics and biopower, of the state of exception, bare life, precarity, of whatever being, orobject oriented ontology without reference to class conflict is to speak, if not with a corpse in one’s mouth, then at least a sleeper.

Must we speak the hideous language of our century? The spectacle appears at first as just a maelstrom of images swirling about the suck hole of their own nothingness. Here is a political leader. Here is one with better hair. Here is an earthquake in China. Here is a new kind of phone. Here are the crowds for the new movie. Here are the crowds for the food riot. Here is a cute cat. Here is a cheeseburger. If that were all there was to it, one could just load one’s screen with better fare. But the spectacle is not just images. It is not just another name for the media. Debord: “The spectacle is a social relationship between people mediated by images.” The trick is not to be distracted by the images, but to inquire into the nature of this social relationship.

Emmalee Bauer of Elkhart worked for the Sheraton Hotel company in Des Moines until she was fired for using her employer’s computer to keep a journal which recorded all of her efforts to avoid work. “This typing thing seems to be doing the trick,” she wrote. “It just looks like I am hard at work on something very important.” And indeed she was. Her book-lengthwork hits on something fundamental about wage labor and the spectacle, namely the separation of labor from desire. One works not because one particularly wants to, but for the wages, with which to then purchase commodities to fulfill desires.

In the separation between labor and desire is the origins of the spectacle, which appears as the world of all that can be desired, or rather, of all the appropriate modes of desiring. “Thus the spectacle, though it turns reality on its head, is itself a product of real activity.” The activity of making commodities makes in turn the need for the spectacle as the image of those commodities turned into objects of desire. The spectacle turns the goods into The Good.

The ruling images of any age service the ruling power. The spectacle is no different, although the ruling power is not so much a ruling monarch or even a power elite any more, but the rule of the commodity itself. The celebrities that populate the spectacle are not its sovereigns, but rather model a range of acceptable modes of desire from the noble to the risqué. The true celebrities of the spectacle are not its subjects but its objects.

Billionaire Brit retailer Sir Philip Green spent six million pounds flying some two hundred of his closest friends to a luxury spa resort in the Maldives. The resort offers water sports and a private beach for each guest. Much of the décor is made from recycled products and there is an organic vegetable garden where residents can pick ingredients for their own meals. ‘Sustainability’ is the Viagra of old world speculative investment.Sir Philip is no fool, and neither is his publicist. This retailer of dreams has the good sense to appear in public by giving away to a lucky few what the unlucky many should hence forth consider good fortune. And yet while this story highlights the fantastic agency of the billionaire, the moral of the story is something else: even billionaires obey the logic of the spectacle if they want to appear in it.

The spectacle has always been an uninterrupted monologue of self-praise. But things have changed a bit. The integrated spectacle still relied on centralized means of organizing and distributing the spectacle, run by a culture industry in command of the means of producing its images. The disintegrating spectacle chips away at centralized means of producing images and distributes this responsibility among the spectators themselves. While the production of goods is out-sourced to various cheap labor countries, the production of images is in-sourced to unpaid labor, offered up in what was once leisure time. The culture industries are now the vulture industries, which act less as producers of images for consumption than as algorithms which manage databases of images that consumers swap between each other – while still paying for the privilege. Where once the spectacle entertained us; now we must entertain each other, while the vulture industries collect the rent. The disintegrating spectacle replaces the monologue of appearances with the appearance of dialogue. Spectators are now obliged to make images and stories for each other that do not unite those spectators in anything other than their separateness.

The proliferation of means of communication, with their tiny keyboards and tiny screens, merely breaks the spectacle down into bits and distributes it in suspension throughout everyday life. Debord: “The spectacle has spread itself to the point where it now permeates all reality. It was easy to predict in theory what has been quickly and universally demonstrated by practical experience of economic reason’s relentless accomplishments: that the globalization of the false was also the falsification of the globe.” Ever finer fragments of the time of everyday life become moments into which the spectacle insinuates its logic, demanding the incessant production and consumption of images and stories which, even though they take place in the sweaty pores of the everyday, are powerless to effect it.

It is comforting to imagine that it is always someone else who is duped by the spectacle. Former movie star turned tabloid sensation Lindsay Lohan allegedly spent over one million dollars on clothes in a single year, and $100,000 in a single day, before consulting a hypnotist to try to end her shopping addiction. Lohan’s publicist denied the story: “There is no hypnotist, and Lindsay loves clothes, but the idea that she spent that much last year is completely stupid.” The alleged excess of an other makes the reader’s own relation to the spectacle of commodities seem just right. Its all about having the right distance. For Debord, “no one really believes the spectacle.” Belief, like much else these days, is optional. The spectacle is what it is: irrefutable images, eternal present, the endless yes. The spectacle does not require gestures of belief, only of deference. No particular image need detain us any longer than this season’s shoes.

They call themselves the Bus Buddies. The women who travel the Adirondack Trailways Red Line spend five and even six hours commuting to high paid jobs in Manhattan, earning much more money than they could locally in upstate New York. They are outlier examples of what are now called extreme commuters, who rarely see their homes in daylight and spend around a month per year of their lives in transit. It is not an easy life. “Studies show that commuters are much less satisfied with their lives than non-commuters.” Symptoms may include “raised blood pressure, musculoskeletal disorders, increased hostility, lateness, absenteeism, and adverse effects on cognitive performance.” Even with a blow-up neck pillow and a blankie, commuting has few charms.

For many workers the commute results from a simple equation between their income in the city and the real estate they can afford in the suburbs, an equation known well by the real estate development companies. “Poring over elaborate market research, these corporations divine what young families want, addressing things like carpet texture and kitchen placement and determining how many streetlights and cul-de-sacs will evoke a soothing sense of safety. They know almost to the dollar how much buyers are willing to pay to exchange a longer commute for more space, a sense of higher status and the feeling of security.” By moving away from the city, the commuter gets the space for which to no longer have the time. Time, or space? This is the tension envelope of middle class desire. Home buyers are to property developers what soldiers are to generals. Their actions are calculable, so long as they don’t panic.

There are ways to beat the commute. Rush hour in Sao Paulo, Brazil features the same gridlocked streets as many big cities, but the skies afford a brilliant display of winking lights from the helicopters ferrying the city’s upper class home for the evening. Helipads dot the tops of high-rise buildings and are standard features of Sao Paulo’s guarded residential compounds. The helicopter speeds the commute, bypasses car-jackings, kidnappings – and it ornaments the sky. “My favorite time to fly is at night, because the sensation is equaled only in movies or in dreams,” says Moacir da Silva, the president of the Sao Paulo Helicopter Pilots Association. “The lights are everywhere, as if I were flying within a Christmas tree.”

Many Paulistanos lack not only a helicopter, but shelter and clean water. But even when it comes with abundance, everyday life can seem strangely impoverished. Debord: “the reality that must be taken as a point of departure is dissatisfaction.” Even on a good day, when the sun is shining and one doesn’t have to board that bus, everyday life seems oddly lacking.

Sure, there is still an under-developedworld that lacks modern conveniences such as extreme commuting and the gated community. Pointing to this lack too easily becomes an alibi for not examining what it is the developing world is developing towards. And rather than a developed world, perhaps the result is more like what the Situationists called an over-developed world, which somehow overshot the mark. This world kept accumulating riches of the same steroidal kind, pumping up past the point where a qualitative change might have transformed it and set it on a different path. This is the world, then, which lacks for nothing except its own critique.

The critique of everyday life – or something like it – happens all the time in the disintegrating spectacle, but this critique falls short of any project of transforming it. The spectacle points constantly to the more extreme examples of the ills of this world – its longest commutes, its most absurd disparities of wealth between slum dwellers and the helicopter class, as if these curios legitimated what remains as some kind of norm. How can the critique of everyday life be expressed in acts? Acts which might take a step beyond Emmalee Bauer’s magnum opusand become collaborations in new forms of life? Forms of life which are at once both aesthetic and political and yet reducible to the given forms of neither art nor action? These are questions that will draw us back over several centuries of critical practice.

Once upon a time there was a small band of ingrates – the Situationist International – who aspired to something more than this. Their project was to advance beyond the fulfillment of needs to the creation of new desires. But in these chastened times the project is different. Having failed our desires, this world merely renames the necessities it imposes as if they were desires. Debord: “It should be known that servitude henceforth truly wants to be loved for itself, and no longer because it would bring any extrinsic advantage.”

What if we tried a thought experiment? Just for shits and giggles? The thought experiment runs as follows: What if this was no longer capitalism, but something worse? Could we start by describing relations of exploitation and domination in the present, starting with the newest features, and work back and out and up from that?

This might draw our attention to two things. Firstly, to some features of the forces of production. It is still the case that extracting useful organic and inorganic matter from the earth is the basis of social existence. And it is still the case that applying vast amounts of energy in the form of fossil fuels and labor to that base matter is still how the endless array of commodities around us come into existence.

But both those processes seem these days to be subordinated to a third form of relation. At the smallest and largest scales, so much of primary production and secondary manufacturing seems to be controlled by rapid flows and extensive archives and complex algorithms whose concrete existence is in a tertiary form – that of information.

The forces of production that seem most characteristic of the present run on information. They extend all the way into the production process, whether in the form of robots or the detailed and constant surveillance of living labor. They extend all the way out to global networks of measurement, command and control that work in real-time. These networks of information subsume not only inorganic and organic matter and energy in their web but also the human, as we become producers of information even when we are not working. The value of information can be extracted even from nonlabor.

Secondly, the relations of production seem to have evolved to enclose these forces in rather novel extensions of the private property form. Wittgenstein had a rather robust proof of the proposition that there is no private language, but in our time, privatized languages are everywhere, and not just languages. Images, languages, codes, even genes can become private property, produced in quite novel kinds of productive process.

New forms of information, now recognizable as private property, are extracted from a class whose efforts are hardly described by the category of labor, for the simple reason that while labor repeats an action whose form is given in advance, the whole point of these novel processes is to produce unique instances of such forms in the first place. Alongside theworker is the figure of the hacker, producer not of content but of form, and which more often that not ends up being someone else’s property.

One has to ask whether the ruling class presiding over this mode of production is still adequately described as capitalist. It seems no longer necessary to directly own the means of production. A remarkable amount of the valuation of the leading companies of our time consists not of tangible assets, but information. A company is its brands, its patents, its trademarks, its reputation, its logistics, and perhaps above all its distinctive practices of evaluating information itself.

Strangely enough, despite the posthuman turn, in which not just labor but all forms of human attention are subordinated to what Lazzarato calls machinic enslavement, a company is also its personality. Companies really are ‘people’ now, and in more than a legal sense. And they have to be embodied in an actual human, someone the financial markets can believe in.

Some like to talk as if one could just add an adjective or two to capitalism and describe all this. Let’s call it communicative capitalism, semio-capitalism, cognitive capitalism, neoliberal capitalism, or financial capitalism. That sounds comforting at least, as then we know what we are dealing with. But perhaps that’s not quite adequate. Maybe its not the same old familiar endless essence of capitalism cloaked in new appearances.

Maybe the rise of finance is really just a symptom. Yann Moulier Boutang invites us to see finance as something more than speculative excess. It has to do with the whole problem of exchange value in an age where the forces of production are extensively and intensively controlled by information, which is that nobody knows what anything is worth. Financialization is a perverse socializing of the problem of value.

So just for fun, let’s think of it as a post-capitalist mode of production, with a ruling class of a different kind. I call them the vectoralist class. Their power does not lie in directly owning the means of production, as the capitalist class does. Nor does it lie in owning agricultural land, as the capitalists’ old enemy, the landlord class did. And just as there was conflict between capitalist and landlord, so now there is conflict between capitalist and vectoralist.

It was with new forces of production that capital defeated labor in the late twentieth century. But capital in turn finds itself struggling against those who provided the very means of that victory. If one can use an information infrastructure to route around labor’s power to block the production process, one can use the same means to make capitalist producers compete with each other on a global scale.

Remember, this is only a thought experiment. It may have one small merit, and one rather recalcitrant problem attached to it. The small merit is that it enables one to tell a fairly coherent story about what happened between the 1970s and now. For comparison, let’s look at some of the more characteristic language about that period. My examples here are from Erik Olin Wright, Understanding Class (Verso, 2015).

Wright: “The combination of globalization and financialization meant that from the early 1980s the interests of the wealthiest and most powerful segments of the capitalist class in many developed capitalist countries, perhaps especially in the United States, became increasingly anchored in global financial transactions and speculation and less connected to the economic conditions and rhythms of their national bases or any other specific geographical location.” (237) A sentence which, stripped of its decoration, basically says: the cause of financialization is financialization, and the cause of globalization is globalization.

Wright speaks of a period when “global competition intensified” where there was the “integration of commodity chains and production chains” and the “emergence of a global labor force” and even the “dramatic financialization of capitalist economies.” (236-237) With what means? By whom? These are phrases from sentences that don’t consistently link subjects to objects, and which are fond of passive verbs. This is a theory of history that can be summed up as: shit happens.

Of course, these are statements Wright adopts from a consensus language. We have all agreed to talk about financialization as if that just happened, without requiring actual material practices and techniques. We have all agreed to talk about neoliberalism as if that described an actual agency at work that causes things to happen. We have decided not to be Marxists, in other words. We have decided not to subject the language of the times to its own critical pressure. Marx certainly did not take the abstract nouns of his era as a given.

But this brings us to the recalcitrant problem. Its one thing to play with the language of Marxism. It will at least admit modifiers. You can call this neoliberal capitalism, if you want. The essence – capitalism – takes on a particular historical appearance. But one is not supposed to question the metaphysical construct, wherein capitalism is an essence with appearances, which can end only when its productive capacities are exhausted, when the proletariat break through the mere transitory appearance and transforms its essence into socialism, and prehistory has come to an end with the abolition of the last form of class exploitation.

Here let’s look at Wright again, whose work is in many other respects a salutary example of how to bring analytic rigor to the Marxist tradition. He writes: “At the very heart of Marxism as a social theory is the idea of emancipatory alternatives to capitalism.” (121) And “Unless one retains some coherent idea of there being an alternative to capitalism, a Marxist class analysis loses its central anchor.” (167) Hence even in this social-scientific version of the Marxist tradition we’re not far from the metaphysic, in which history can only be understood through an ahistorical concept – of capitalism. Emancipation is though negatively, as emancipation from capitalism. Therefore, this must be capitalism, the negative of emancipation.

Of course, there’s plenty of evidence for this still being capitalism, or mostly capitalism. The question would be whether something else is emerging, and whether it is qualitatively different enough to call it something else. The problem with an inherited concept, like inherited money, is that we didn’t make it ourselves and come to take it for granted. Particularly if it is part of a whole metaphysical conceptual structure. Maybe we need a bit of good old Brechtian alienation-effect even from heirloom concepts like ‘capitalism.’

Now, all I’m talking about here is a thought experiment: what if we thought about a mode of production emerging after capitalism that was worse? Could that at least minimally explain observable features of the world that might be genuinely strange, qualitatively novel, observable tendencies in recent history? What light would this perspective shed on our habits of thought, our received ideas?

As an example of how one might conduct thought experiments, I turn again to Erik Olin Wright. He is the author of a substantial body of sociological work, both conceptual and empirical, on class. The papers collected in Understanding Class are their own kind of thought experiment. They ask: what can a Marxist concept of class bring to theoretical and empirical work that thinks class as stratification, or which uses class concepts drawn more from the work of Max Weber or Emile Durkheim? Wright deftly shows what a Marxist concept of class can do, where endless capitalism is a given. The addition question I want to ask is what happens if we take away the assumption that this is still the same old capitalism.

Wright has mercifully given up the “paradigm aspiration” (17) wherein Marxism is superior to all the social sciences because it has a superior problematic or method. Those whose memory stretches back away will recall that this led to endless attempts to prove by purely theoretical means that Marxism had rendered all of bourgeois philosophy and social science obsolete. But it hardly ever delivered on the promise of a new kind of knowledge. The “grand narrative” (122) fell apart. And in any case it ended up being produced in the margins of the very same institutions as that bourgeois philosophy and social science.

Instead, Wright makes two sorts of claims. The modest claim is that one can connect Marxist work to other kinds of sociology. Each has its perspective and they illuminate each other. The stronger claim is that the Marxist perspective is a bigger picture, which shows something about the world and history that is beyond the reach of other approaches. Here he does for social theory what Fred Jameson does for literary theory or Perry Anderson for historical thought: make the claim that Marxism offered the point of view from which to interpret and synthesize other bodies of work. If Jameson’s famous watchwords are “always historicize!” then Wright’s might be “always socialize!” where that means to adopt the point of view of a social formation riven by relations of class exploitation and domination as the outer limits of the macroscopic perspective.

In one of his brilliant summaries, Wright argues that Durkheimian, Weberian and Marxist approaches operate on different levels of the social gamespace, the situational, the institutional and the systemic. The Durkheimian approach is situational, and is about moveswithin the game. The Weberian approach is institutional, and is about rules of the game. The Marxist approach is systemic, and is about changing the game.

All of these approaches involve class structure that generate class actors who have at least partially conscious intentions, whether it is to make moves that advance them, or contest rules of the game that might advantage some class or other, or to change the whole game to another game more in one’s class interest. My question would be about the class unconscious. Perhaps the game changed of its own accord, as the forces of production push forward into new relations of production, with which our superstructural languages for describing class structure have yet to catch up.

Wright thinks that the opportunity for game-changing, for overthrowing capitalism in favor of a more equal and free society, is not present. “One way of interpreting the history of the past half-century is that there has been a gradual shift in the levels of the game at which, for many analysts, class analysis seems most relevant.” (123) Hence it makes sense to reach out to the Weberians (whose scholarly interests are at the level of contesting the rules of the game rather than changing it) and even to the Durkheimians, whose focus is on the moves actors get to make within given rules of the game. But his overall aim is to concatenate these three approaches as appropriate to different scales, with Marx speaking to the larger and more visionary scale.

Wright resists the death of class counter-narrative, a discourse whose most prominent member is probably Ulrick Beck. Wright offers a supple class analysis and backs it up with actual results. His concept of class has three dimensions: property, authority, expertise. His view of class structure offers class locations at three levels, which do not always neatly overlap. Relations of property generates the class locations of employers, petit-bourgeois, employees. Relations of authority generates the locations of managers, supervisors, the supervised and managed. Relations of expertise generate the locations of professionals, the skilled, the nonskilled.

He is interested, for example, in the “permeability” (146) of class boundaries, so he looks at three kinds of class connection: intergenerational mobility, cross-class friendship, cross-class households. He finds the property boundary the least permeable – a result that won’t surprise Marxists. Class connections between workers and employers is limited. The employee / petit bourgeois boundary is more permeable. Wright frankly acknowledges that in the United States, racial boundaries may be even less permeable, but that does not negate the usefulness of the category of class. Class is only a modest predictor, however “class often performs as well or better than many other social structural variables in predicting a variety of aspects of attitudes.” (154)

David Grusky and Kim Weedon offer what Wright classifies as a Durkheimian analysis of class in which occupation are the unit of analysis. They see class homogeneity only at the micro, occupational level, not in ‘big’ concepts of class. Its more about actual labor markets and how they define occupations. Such occupations act on behalf of members, extract rents if they can, and shape life chances. For them, even academic sociologists and economists count as different ‘classes.’ Which might be the beginnings of an approach to how academics, at a time when, under the threat of vectoral power, their life chances seem diminishing and their means of opportunity hoarding to be failing, cannot quite come together and act on shared interests.

The Durkheimian approach focuses on selection and self-selection into closed groups who interact more with each other than with other groups. Licensing and the formal definition of occupations play a role here. This works well for explaining individual-level outcomes. Wright claims that except in the study of education, income and wealth, this micro approach works better than macro ones of a more Weberian or Marxist kind. The Durkheimians are good on lifestyles, tastes, political and social attitudes.

The key to Weberian theories of class for Wright is opportunity hoarding or social closure, by such means as credentialing, licensing, the color bar, gender exclusions. One could even see labor unions as a form of opportunity hoarding from the point of view of precarious workers, but we’ll come back to that.

As Wright points out, perhaps the most important mechanism of opportunity hoarding is private property itself. “The core class division within both Weberian and Marxian traditions of sociology between capitalists and workers can therefore be understood as reflecting a specific form of opportunity hoarding enforced by the legal rules of property rights.” (7)

Wright brings into the Weberian theoretical frame the Marxist concept of antagonistic classes. Advantages are causally linked to disadvantages “The rich are rich because the poor are poor.” (8) The Marxist concept of exploitation and domination are about control over the lives of others. “Exploitation refers to the acquisition of economic benefits from the laboring activity of those who are dominated.” (9)

It could be argued that class is peripheral to Weber’s work, but his writings on ancient slavery seem close to Marx. Unlike concepts such as status and party, class need not generate identity or collective action. And hence does not fit well with Weber’s habitual explanatory modes.

Both Marx and Weber see property as fundamental to a relational concept of class. Both grasp distinction between objectively defined class and subjectively lived class. Both thought humans followed material interest in the long run. Weber was much less inclined to think classes would polarize and become the key social dynamic. Marx shared Weber’s view that status groups impeded the effects of the market and constitute an alternative basis of collective action. Both thought the rationalization of market relations would abolish status groups in time.

Marx thought this simplified class whereas Weber did not. Weber thought class determined life chances within rationalized society. He was less interested in deprivation than in instrumental rationality. Marx was more interested in class exploitation in production; Weber in class as factor in determining life chances in the market. Wright: “Marxist class analysis includes the Weberian causal processes, but adds to them a causal structure within production itself.” (46)

For Wright, class in Weber is closely connected to the theme of rationalization. Of Weber’s three sources of power, he conceives of both non-rational and rational forms. Thus the power-source that is honor can appear as ranks and titles, or as a meritocracy. Authority can appear as patriarchal or in a rational-legal form. And material sources of power can appear as consumption groups or as class.

Class is thus part of rationalization, part of the abolition of traditional peasantry, part of the transition from landed aristocracy to agricultural entrepreneurs. Class is part of the rise of the calculation of material interest. The peasant, who owes a duty to the baron, becomes the farmer who pays rent to the landlord.  “While class per se may be a relatively secondary theme in Weber’s sociology, it is, nevertheless, intimately linked to one of his most pervasive theoretical preoccupations – rationalization.” (31)

My question here would be to ask: why one would think, if this has given rise to two kinds of rationalizing class antagonism that overlapped and interfered with each other, why might it not give rise to a third? The farmer-landlord antagonism arose out of the ruins of feudalism. The efficiencies in agricultural production that came with this rationalization threw off a surplus population what would become urban workers, in an antagonistic relation to capitalists. And yet landlords and capitalists also had interests that contradicted each other. This is the central theme of David Ricardo’s political economy: the opposition between landlord and capitalist. But did rationalization stop, with the creation of classes of farmer and worker? What happens when the production, not of food or products, but of new information itself becomes rationalized?

Weber did not have a lot to say about labor, but where he did, it was in terms of work discipline. Employers are free to hire and fire. Workers lack ownership, but workers are responsible for their own social reproduction. These are the conditions under which indirect compulsion operates. But it raises the problem of how to get maximum labor effort. Wright: “running throughout Weber’s work is the view that rationalization has perverse effects that systematically threaten human dignity and welfare.” (52) One sees this clearly in the latest impositions on the labor process, such as Uber or Amazon’s Mechanical Turk, which make labor both highly autonomous and yet very closely monitored at the same time.

But Weber does not integrate interest in labor discipline and domination into the category of class. Here we need a bit of Marx, for whom, as Wright says, “exploitation infuses class analysis with a specific kind of normative concern.” (53) Exploitation steers research to questions of class as relational in both exchange and production. “Weber’s treatment of work effort as primarily a problem of economic rationality directs class analysis towards a set of normative concerns centered above all on the interests of capitalists: efficiency and rationalization.” (55) Georg Lukacs and Theodor Adorno excepted, one might add.

The Wrightian synthesis of Marx and Weber makes exploitation fundamental, but makes particular use of the idea of opportunity hoarding as that which defines the middle class.From there one could build up a picture of the United States as highly polarized by exploitation, and where middle class opportunity hoarding is being eroded by what he calls neoliberalism and deindustrialization, but which I think can be understood more clearly in terms of new forces of production that instrumentalize and rationalize information, giving rise to new property forms and hence new class relations, including an antagonistic relation between a hacker class tasked with making novelty out of information (the condition of it becoming property) and a vectoralist class that owns or controls the vector of information control and domination itself. Here the Marxist perspective of exploitation and the Weberian one of rationalization can be fruitfully combined.

Beside reaching out to those indebted to more classical approaches of Durkheim and Weber, Wright addresses prominent contemporary social theorists who try to offer original perspectives. Here I’ll treat only his papers on those known outside their disciplines, such as Thomas Piketty, Michael Mann, Guy Standing and former New School professor, the late Charles Tilly.

Wright thinks Charles Tilly’s approach to durable inequalities is closer to Marx than Tilly wants to acknowledge. Tilly was against individualist approaches. Explanations of inequalities have to be relational. He offers a structuralism of types of social relations and types of mechanism. The types of social relation are chain, hierarchy, triad, organization, categorical pair –  of which organization is the most durable kind. The types of mechanism are exploitation, opportunity hoarding, emulation and adaption.

Structural relations are solutions to problems generated within social systems. For example, Problem 1: how to secure stable access to resources?

Solution 1: opportunity hoarding and exploitation. Problem 2: how to sustain and even deepen exploitation and opportunity hoarding, and sustain trust and cooperation among those who benefit? Solution 2: categorical inequality. Problem 3: how to stabilize and reproduce inequality? Solution 3 emulation and adaptation lock distincions in place.

Wright sees Tilly as importing Weberian ideas into a Marxist framework. Culture is not an autonomous superstructure, as in certain post-Marxist theories. Nevertheless, Tilly goes beyond Marx in attempting to subsume gender, race, nationalism under a unitary framework. For Tilly, forms of categorical inequality make exploitation more efficient. One could perhaps usefully extend this to think about how what one might shorthand as algorithmic mechanisms of discrimination, which work so subtly with databases rather than categories, might reinforce exploitation in our time.

Michael Mann’s work is approached a bit differently by Wright. Here his interest is in the disjunction between how his theory treats class and some of his more specific findings. In his theory, Mann sees class only in terms of collective actors, not structural locations. But in particular studies, class location does seem to shape individual interests.

Crucial here is the distinction Wright makes between class structure, class formation and class actors. This might correspond very loosely to three scales of analysis: the Marxist, Weberian and Durkheimian perspectives, respectively. Mann, like Bourdieu, thinks that class structure that produces no class actor is just ‘class on paper,’ an academic exercise. Class has competing forces against it: ethnic, racial, linguistic, national, religious, gender.

Mann’s social theory works off two clusers of concepts: sources of social power and forms of organization that deploy those powers. There are four sources of power: social, ideological, economic, military. The kinds of organization are expressed as dichotomies: collective / distributional; extensive / intensive; authoritative / diffuse.

In Mann’s world, the pursuit of goals requires entering into power organizations that determine the structure of society. This is an agency rather than a structure centered approach. The creation, reproduction or transformation of social structures is the result of goal directed actions. Rather like rational choice theory, Mann starts with actors and their goals, only his approach is not individualistic.

Class in Mann is a kind of collective actor among many that comes together in organizations to deploy economic power resources. He pairs the concept of class with that of segment, which cuts across class and groups actors of different classes in particular industries. Class, for Mann, is of little sociological interest.

In working through the formation of the middle class, Mann actually becomes rather more interested in class structure and class formation. For Mann, the middle class is composed of three separate categories: The petit-bourgeois, professionals and careerists. The economic situation of all three tie their life chances to capital accumulation, and their relation to the state forces them together. They have similar consumpti0on patterns and may all be investors of small amounts of capital. They are held together by ideological and political citizenship.

Mann rejects the classic Marxist distinction between class in itself / class for itself as teleology, but so too does Wright and many more sophisticated Marxists. Wright: “One can believe that class relations and class structures are real and generate real effects without also believing in any one-to-one mapping between the complex structure of class relations and the formation of collective actors.” (108)

Here Wright’s signature concept of contradictory class locations proves very useful. Actual jobs are a mix of property relations and authority relations, and might be located in class terms differently along those axes. Class changes over time and is mediated by family and community. One might be born into the petit-bourgeoisie but end up a waged-worker. One might be working class and marry a shop-keeper, and so on. Class can be a bit messy. One is reminded here of Jean Baudrillard wry remarks in America on Marxist academics managing their stock portfolios.

Thomas Piketty deserves credit for putting inequality back on the agenda as more than a mere problem of unequal opportunity. His empirical work shows that the sharp rise in income of the top 10%, is really that of the 1% or even the .1%. A fair bit of this came from the rise of super salaries rather than income on capital. The CEO ‘class’ are setting their own pay. Here I would want to inquire as to how, in a political economy running on information, the capacity to control (but not entirely own) the means of production accrues to a class that presents itself as the celebrities of information control itself.

The technicalities of Piketty’s work centers on the capital / income ratio as a way of measuring value of capital relative to total income of economy. Wright: “Piketty’s basic argument is that this ratio is the structural basis for the distribution of income between owners of capital and labor: all other things being equal, for a given return on capital, the higherthis ratio, the higher the proportion of national income going to wealth holder.” (133) As  growth declines, the capital/income ratio rises. There’s a rise in the weight of inherited wealth, while concentrations of income also rise. It’s the worst of both worlds: a rentier class plus a meritocracy of appearance-peddlers carving up the world between them at the expense of everyone else.

Pickety starts out with a class analysis, but loses it once he gets into the empirical work, where he treats CEO income as return on labor, as most income tables do. Wright: “In the modern corporation many of the powers-of-capital are held by top executives…. They occupy what I have called contradictory locations within class relations… They exercise their capitalist-derived power within the class relations of the firm to appropriate part of the corporation’s profits for their personal accounts.” (136)

But is their power really capitalist-derived, or is it now something else? Something like a joint managing of appearances between those who represent a firm to the market, and the market that is supposed to value it. But how to value it when so much of its asset-base takes the form of information? A firm is among other things a brand, a slew of patents, a logistical process, a corral of expert hackers turning out new intellectual property. How can information be turned into value, and an opportunity to be hoarded, when there aren’t really private languages, and information is in principle a non-rivalrous good?

Wright points out that Piketty does not separate out real estate from capital. There might be good reasons to do so. Elsewhere I wrote about Matteo Pasquinelli’s arguments about how landlords now increase their rents by extracting the information-value that the presence of either the hacker class, or of those parts of the middle class that manage rather than create information. One could think further here about Ricardo’s ancient tension between ground-rent and profit, but with the focus shifted from the rural to the urban, and the monopoly rents to be extracted from urban locations.

Guy Standing is the name most associated with the now widely-discussed idea of theprecariat as a class rather than just a bad life chance. He offers a three-dimensional definition of class, as structured by relations of production, relations of distribution and (interestingly) relations to the state. He identifies seven classes: plutocracy, salariat, proficians (professional + technician, working class, prevcariat, unemployed, lumpen-precariat. The precariat have insecure insecure jobs. Their sources of income other than wages disappearing. They become less citizens of the state and more mere denizens. Not only are their jobs precarious, they are vulnerable within relations of distribution and marginal to the state.

The precariat includes people bumped out of working class communities and families. They experience a relative deprivation in relation to a real or imagined past. It also includes migrants and asylum seekers for whom it’s the present that is absent. They have no home. The precariat increasingly includes people falling out of an educated middle class – think academic adjunct labor – who lack a future. For Standing this makes a potentially dangerous class.

Marxists might think of the precariat as workers who (in Weberian terms) experienced poor life chances. Standing thinks there are antagonisms between the precariat and the working class. But do the precariat and workers have distinct interests? Wright thinks not. He thinks they share an interest in changing the game (although one might want to say more here about how workers and the precariat might have different interests about the rules and moves of the game). Unionization, for example, can secure some sort of steady work for the workers in the union at the expense of those without – a side-effect of unionizing academic adjunct labor that is rarely discussed.

Certainly the most controversial of Wright’s propositions is one that picks up on the work ofWolfgang Streeck and others on class struggle and class compromise. For Streek, arguing in a Durkheimian vein, capitalism works better when there are constraints on rational, self-interested action. Capitalism works better when there’s non-capitalist social forms present, based in trust, legitimacy, responsibility.

The wrinkle Wright introduces is to argue that the level of constraint on elf-interest that is optimal for capitalists is below that which is optimal for workers. Capital seeks to remove constraints to augment its power even past the point where these are economically inefficient. Wright: “… the zeal to dismantle the regulatory machinery of capitalism since the early 1980s was driven by a desire to undermine the conditions for empowerment of interests opposed to those of capitalists – even if doing so meant under-regulating capitalism from the point of view of long-term needs of capital accumulation.” (183)

Although perhaps one could see this a bit differently by separating out the interests of the capitalist and vectoralist class. The regulatory regime emerging in the last quarter century favors the mobility of information – and not just finance – as a means of coordinating economic activity transnationally, at the expense not just of workers but of those forms of capitalist enterprise tied to physical plant and infrastructure, and thus with an interest in local, regional or national relations of trust, legitimacy and responsibility.

Hence we can read Wright’s conclusion against the grain: “Enlightenment of the capitalist class to their long-term interests in a strong civic culture of obligation and trust is not enough; the balance of power also needs to be changed. And since this shift in balance of power will be costly to those in privileged positions, it will only occur through a process of mobilization and struggle.” (184) But what if those capitalists tied to actually producing things in a particular place already know this, but they have lost power to a quite different kind of ruling class, which operates at a higher level of abstraction, or in Weberian terms, at a new stage of rationalization? They own or control the information about things, rather than the things themselves.

Hence to imagine new kinds of class compromise might require a rethink about which classes could compromise. First, one has to have some perspective on the impulse to think all class compromise as illusion or stalemate. Could there be a non-zero-sum game between otherwise antagonistic classes? Working class organization may actually have positive effects for capital accumulation, as it enables problem solving, negotiation, skill development, tech change.

But perhaps that only worked when particular capitalist employers were able to exercise something like a monopoly on the production of a given class of product or in the context of a mercantilist strategy of restricted consumption at home while expanding exports abroad and sustaining rising wages as a return on rising productivity. Such as would describe Japanese manufacturing in its heyday, for example.

Since there’s no way to change the game, Wright looks to those who wanted to change the rules within the game, such as those Scandinavian social democratic inheritors of Ernst Wigforss, such as Walter Korpi and Gøsta Esping-Anderson. But one has to ask if its possible to revive social democratic strategies from the era of the great national manufacturing industries in an era where the information vector greatly lowers the cost of geographic dispersal, and puts manufacturing regions in direct competition with each other.

Wright advocates for some salutary counter-hegemonic strategies, based in geographic rootedness, local public goods and worker’s cooperatives. But one has to wonder if, in an era where the forces of production drive increasingly abstract processes of rationalization, which appear then as transnational legal and treaty forms protecting information as private property, such things are all that viable.

Wright: “Changes in technology may make the anchoring of capitalist production in locally rooted, high productivity small and medium size enterprises more feasible.” (143) One might call this the Brooklyn-effect, after the boom in small business, even manufacturing, there. But while the actual products have some connection to locality, the information infrastructure such localism has to rely on belongs to the vectoralist class. Amazon, Paypal and so forth all get their cut.

Thus, where Wright says, “I assume that an exit from capitalism is not an option in the present historical period” (239) – I think we have to question that assumption, but not in a good way. Maybe this is already not capitalism, but something worse. Its not just a rentier bubble of speculation spooling out of the “real economy.” (244) One could no longer know in advance which part of it is real at all – and perhaps one never could. This is an era not just of so-called neoliberalism’s “aggressive affirmation and enforcement of private property rights” (237) but of the creation of new forms of private property, and new antagonistic relations over it, particularly in the form of intellectual property.

There’s a lot to be said for the way Wright subsumes rival social theories as collaborators within the larger frame of a fairly traditional Marxist sociology. But perhaps that in turn has to be put back in contact with the historical study of the transformation of the forces of production, and in particular how information emerges as both a technical and social force. One could then, as a further step, bring this perspective together with the study of themetabolic rift, wherein the instrumentalizing of information mobilizes the whole planet as a rationalized sphere of resource extraction under the sign of exchange value. To the point where this rationalization becomes completely irrational, threatening to take the whole planet down with it.

Here it might be helpful, in Bogdanovite fashion, to press on some other metaphors at work in Wright, viz: “A society is not a system in the same way that an organism is a system. It is more like the loosely coupled system of an ecosystem in which a variety of processes interact in relatively contingent ways.” (121) In the Anthropocene, it may turn out that ecosystems are the ones that are tightly coupled. But that would be a whole other thought experiment.

Maybe we need an asocial science that rethinks whether one can even conceive of the social as a separate domain of analysis at all. On the one side, the social meshes seamlessly with information technology; on the other, it depends on planetary scale resource mobilization causing catastrophic metabolic rifts. One might be in need of an even ‘bigger’ conceptual framework within which to rest Wright’s partial synthesis as a component part.

RetroDada begins with disgust. Once again the world gets its war on. While some cities are attacked by bombers, others are strafed by art fairs. This time there’s no Switzerland of neutrality where refugees might cool their heels, as now the whole globe itself overheats. The insomnia of reason breeds monsters.

But before we can take two steps forward, let’s take one leap back. Back a whole century. Back to the first of the world wars to be numbered; back to the birth of Dada disgust. Back to that great refusal of what the century was to become. Why shouldn’t a .gif run backwards as well as forwards? Its RetroDada time! In principle RetroDada is against manifestoes, but it is also also against principles. So here goes nothing.

The world is full of mistakes, but the worst is the art that got made. Art gives us Dante’s Inferno as styled by interior decorators. RetroDada aims to please neither at art nor anti-art, as nobody should serve masters. We will put an end to spectacle and replace it with convulsive laughter from continent to continent. It’s shit after all, but from now on we mean to shit in different colors.

Psychoanalysis is itself the disease. It makes the bourgeois self seem interesting. Ethics produces atrophy like every plague produced by intelligence. Theory merely guides us in a round-about way to the prejudice we had in the first place. What we need are works that are tender and precise and forever beyond understanding. There is a lot of negative work to be done.

RetroDada is our intensity. RetroDada is for and against unity. RetroDada is the abolition of their reasons. RetroDada is the refusal of an inheritance. RetroDada is a convulsive effusion, like a cellphone cooked in a microwave. RetroDada is a divinity of the lowest order. RetroDada has no theory. There is enough of that in art school. We love the old things for their freshness.

How to make a RetroDada manifesto: Take a Dada manifesto or two. Copy the good bits. Toss them in a file. Move the bits around. Improve on them. The manifesto will be like you. As for intelligence, it will be found in the streets.

RetroDada is working with all its might to introduce the idiot everywhere. RetroDada is a venture capital form for the exploitation of other people’s ideas. God can afford to be unsuccessful. So can RetroDada. It is luxury without value or price. RetroDada gives itself to nothing, neither to work nor play. RetroDada seduces you with your own idea.

RetroDada is at once stand-up comedy and a requiem mass. RetroDada trusts only in the sincerity of situations. RetroDada fights against the thanaticism of the times. RetroDada develops the plasticity of the digital. We should make all art and literature and cinema free. The medium is as unimportant as we are. Essential only is the forming. Take any material at all.

RetroDada is a theoretical virus. RetroDada means letting oneself be thrown by events. Say yes to a life that strives downward toward negation. RetroDada refuses to be contemporary to any of this shit. RetroDada reintroduces art and everyday life so they can have queer sex in back alleys. RetroDada is the statelessness of the mind. RetroDada rejects both the stylish order and the stylized disorder of contemporary aesthetics. We are convinced of the arbitrariness and falsity of our poor creation, the world. We look unencumbered into the heights and depths.

Let’s only steal from the best, and from their actions, not their styles. The resonant Sophie Taeuber, the drum-banger Richard Huelsenbeck, the tubular Hugo Ball, the mystic Emmy Hennings, the hypeman Tristan Tzara, the ironic Jean Arp, the dadasoph Raoul Hausmann, the runfast Hannah Höch, the aviarist Baroness Elsa. Let’s take their leavings not their droppings. Music, dances, theories, manifestos, poems, paintings, costumes, masks. To be begun again, from the beginning.

So many fates of the west befell Dada. It tried everything already, so we don’t have to. Arthur Cravan became a legend. Mina Loy became a poet. Marcel Duchamp became the enabler of contemporary art. Hugo Ball became a devout Catholic. Emmy Hennings became a Protestant mystic. Hannah Höch wrote a trans-species children’s book. Richard Huelsenbeck became a therapist. Tristan Tzara became a communist. Marcel Janco returned to the Orient. Baroness Elsa died in poverty and obscurity. Jacques Vaché killed himself before it even all began.

To be for this manifesto is to be RetroDada! To be against this manifesto is to be RetroDada! To be for and against this manifesto is to be RetroDada! To be neither for nor against this manifesto is to be RetroDada! There is no escape from the history yet to be unmade!

I am a recovering Althusserian. For decades now I have been Althusser-free, for the most part, but we all have our lapses. The first step to becoming a recovering Althusserian is to recognize that you have no control and are unconsciously always a little bit Althusserian whether you want to be or not.

Louis Althusser is however not so much a poison as what Derrida and Stiegler and Stengerscall a pharmakon. That is, something that is undecidable, both poison and cure. It may well be that there are good reasons, in the twenty-first century, to be an Althusserian. I am not objectively in a position to say, and in any case: by their results shall we judge them. When there is a useful Althusserian response to the Anthropocene (or whatever you want to call the current ‘conjuncture’) consider the matter settled. As of yet there is no such response, perhaps for reasons to be elucidated later.

Perhaps this is to judge too harshly. In what follows I want to read some essays by the ‘young Althusser’. I leave it to others to account for the mature works. I want to think about what is living and dead in the Althusserian ‘problematic’, through a series of antimonies that he had to face. This first part deals with his essay ‘On the Young Marx.’ This essay is instructive, as it both sets up a method of reading Marx that we can also apply reflexively to Althusser, and also provides a useful answer to one of the key problems in Marxological thought: the relation of the young to the old (or mature) Marx.

Althusser neatly characterizes the two extant approaches of his time. In one, there areelements in the young Marx that anticipate the older one, and his thought can be read as a teleology, as always having tended towards this goal, this truth. Or: one can read the young Marx as announcing a broad, ethical program which is then either narrowed or even betrayed by the more economistic and social-scientific work of his later years. The former is a typical reading for orthodox communists of the time; the latter the characteristic program of Western Marxism and even of the New Left more broadly. Hence as Althusser wryly notes, all discussions of young and old Marx are political discussions.

Both these readings tend to focus on elements of the text, finding for example characteristically Hegelian or Feuerbachian elements in the young Marx, or themes submerged in the young Marx later brought out more fully in later writing. Althusser wants to dispense with ‘Hegelian’, or rather bog-Hegelian readings in particular, such as the proposition that the materialist core of Marx was present in the early works but in a still idealist form.

Althusser says that “this method which is constantly judging cannot make the slightest judgment of any totality unlike itself.” There is a totalizing sameness to those readings of Marx that annex him onto Hegel. If there’s just a little bit of Hegel to be detected in Marx somewhere, then it becomes Hegel all the way down.

In place of all this, another method: Ideologies have to be considered structurally, as having an underlying problematic of different terms and their combinations. Any particular ideology also has to be thought in the ideological field in which it partakes. That field has determinants outside itself, in specific historical situations. Althusser wants to claim that this is the beginning of a scientific method for treating the ideological, rather than for merely extending ideology. This is more asserted than demonstrated, but for those inclined to the formal methods of literary analysis, this is progress. Reading is to have its method.

The young Marx, Althusser candidly says, writes ideology. He writes it well, but it is just an extension or permutation of the ideological field of his time. Even devout Marxocologicalists should not be embarrassed by this. Althusser: “Early Works are as inevitable and as impossible as the singular object displayed by Jarry: the skull of the child Voltaire.”

One advantage of Althusser’s reference of young Marx to the ideological field is that it rules out another method, more common in our time: the Great Books of the Apostolic Succession. One reads Hegel, one then reads Feuerbach (extra-credit only, he is not quite canonic), then one reads Marx — and then one reads Althusser. But as Althusser rightly insists, the Hegel that Marx read was “not the library Hegel we can meditate on in the solitude of 1960,” – or 2016. The Hegel of Marx was the Hegel of the neo-Hegelian social movement.

In short, Marx came into a very particular ideological field, and his thought as a young writer was within a problematic determined by that field, particularly that of the left Hegelians, and even more particularly that of Feuerbach. A problematic, for Althusser is a kind of structural system through which other material can be processed. Hence Marx applied the Feuerbachian problematic to religion, as Feuerbach did, but also to political and economic ideologies, as he did not.

Interestingly, the concept of problematic becomes a way not to think the Hegelian totality for Althusser. A problematic is a systematic structure with rules of composition, not a unity whose essence is expressed in all its particulars. A problematic, moreover is something that thinks through you, rather than being what you think. It is in a sense unconscious. It calls for special methods for determining how the problematic is at work in the text. Note how the path is open already for a kind of specialized labor of textual exegesis here. Althusser: “a problematic cannot generally be read like an open book, it must be dragged from the depths.” Henceforth we are with Hermes, running a gimlet eye of suspicion over the text, as if if were a symptom of what it hides.

So Marx unconsciously plays out certain permutations of a problematic. It’s a theory which neatly inverts Sartre’s notion of a writer’s necessary freedom to commit to a project. But this presents then a special problem for accounting for how Marx broke with the ideological field of his time. In backward Germany in Marx’s day, intellectuals put a special effort into thinking what was to be done but could not happen. They looked to the political revolutions of France and the industrial revolutions of England. Unable to actually produce either revolution – they theorized them. Most fully, in Althusser’s account, not so much in Hegel but the Hegelianism of the 1830s and 1840s.

Cunningly, Althusser says that Marx retreated from this ideological field, rather than overcoming or surmounting it. He went back to the original problematics of the political economists and political theorists who Hegel had claimed to synthesize into his own philosophy. This is coupled with two discoveries that are extra-philosophical. It was Marx’s experience of political radicalism in Paris, and Engels’ first-hand psychogeography of Manchester capitalism, that were the key to moving forward after this retreat from Hegel. Althusser: “In France, Marx discovered the organized working class; in England, Engels discovered developed capitalism and a class struggle obeying its own laws and ignoring philosophy and philosophers.”

The failure of German liberalism pushed Marx out of Germany. The bourgeois backers of his radical journalism melted away. And with that failure came the retreat from the ideological field to which they belonged. Marx’s training in German idealism was not wasted, however. It provided the ability to think abstractly, which was only awaiting actual concrete things in the world that really needed to be thought.

Thus, Althusser understands Marx’s thought as breaking with the ideological field of his formation, and founding a science. One might remain skeptical about the second part of this claim — the founding of a science — and still find useful the first part — the concept of the ideological field. Perhaps they are rather harder than that to exit. And what if this method were applied in turn to Althusser? What was the ideological field to which his work belonged? What was its underlying problematic? What historical situation gave rise to it? And closer to my own dabblings with it, what historical situation led to the uptake of Althusser in the Anglophone world in the 1970s and 80s?

To tackle the last first: the defeat of the New Left in the 70s led, among other things, to a kind of embedding in the cultural and educational apparatus of those who had dreamed of larger things. This was past the era of the Chinese, Cuban and Algerian revolutions, still the time of the Vietnam war; the time of the rise and fall of New Left activism in the west. This was perhaps not unlike the situation of the left-Hegelians in Germany in the 1840s. And perhaps with not so different results. That which could no longer be enacted– was to be thought as a theoretical revolution instead.

If Sartre had appealed to a more committed, activist time; Althusser appealed to one of quietism, at least as far as he was read in the Anglophone world in the 70s and 80s. (The Althusserians of 1960s France were a different story). What was to be taken up was something already apparent in this brief essay on young Marx: specialized method. Althusser legitimated the scientific study of the ideological field, the search for the unconscious problematic.

This had certain benefits. It meant an insistence on certain standards for accounting for how the ideological field is structured. It also implies a certain relative autonomy and consistency of the ideological level. It led in practice, however, to a deepening of an academic division of labor, via which Marxist thought could accommodate itself to the disciplines. The economic, political and ideological could then be studied as separate objects, each in their own field, in increasingly diminishing contact with each other.

Let’s look at a famous Althusser essay from the early sixties. ‘Contradiction and Overdetermination’ builds on Althusser’s ‘On the Young Marx’ essay, in deciding against the various Hegelian readings of Marx. Althusser rejects the metaphors of ‘turning Hegel right side-up’, or ‘restoring the rational kernel of dialectic without the mystical shell.’ Rather, he thinks of Marxism as replacing Hegel’s dialectic with a different problematic.

How is one to take an argument of this kind? One way would be to subject it to philological proof, a kind of scrutiny it may not actually withstand. But perhaps one can take the argument in a different vein: that one could replace the Hegelian dialectic with a different one, even that one ought to replace it. Althusser is very nervous about opportunistic or merely ideological dilutions of Marxism, and so he insists that his is a rectification, or a drawing out of a dialectic that Engels rather misconstrued, and that neither Marx nor Lenin had the time to write. As a party member, he could hardly appear to be reading Marx at all creatively.

Perhaps now one can see it as creative. Althusser took a cutting of Marx, taken from its German-idealist root-stock, and grafted it to a quite different problematic. One could mention here at least four coordinates of the ideological field in which that problematic resided. One would be the French social thought from Durkheim via Mauss to Levi-Straus, stretching from anthropology to linguistics. A second might be Spinoza, a third would be the distinctive philosophy of science in France centered on Gaston Bachelard. A fourth would be a version of Marxist ‘orthodoxy’ uninterested in the post-56 ‘thaw’, and loyal to Lenin, Stalin – and Mao.

Of these, Spinoza is probably decisive, although neatly dove-tailing with French social thought, in the way Althusser thinks a totality that produces, among other things, subjects, rather than thinking a totality that subjects produce through their encounter with, and recognition of themselves in, an objective world. Althusser begins the new dialectic with the category of over-determination (borrowed in this case from psychoanalytic readings of structural linguistics). Rather than one dialectical totality, unfolding in all its complexity around a central contradiction, Althusser posits a totality with at least three kinds of contradiction that can over-determine the central one – the class struggle between labor and capital.

Of most use to me is his passing recognition that other classes can over-determine the contradiction between labor and capital. As is clear from Marx’s political writings, from Gramsci, from Kautsky on the peasant question, the simplification of class dynamics down to two classes of Marx’s Capital is not always helpful. The economic dynamics of capitalism might hinge on the class relation, but politics is more complicated. That class contradiction may be over-determined that of other classes. (I pushed this thesis to extreme in my A Hacker Manifesto).

A second over-determination comes from the relative autonomy of the superstructures. It may well be that forces at work in the political or ideological levels may either retard or accelerate the development of the principal economic contradiction. In the case of the Russian revolution, Althusser thinks there is an element of ideological over-determination. The working class was intensely class conscious, thanks to a militant and organized intellectual movement. This idea of the relative autonomy of the superstructures will become a crucial legitimating move for political theory and cultural studies, as we shall see.

A third over-determination take us outside the national-cultural frame so dear to Gramsci, into the space of the relations between imperial states. Taking up Lenin’s thesis that the imperial system broke at its weakest link – the Russian empire, Althusser reads this as a third kind of over-determination. History advances ‘bad side first’, as Marx and Engels put it in theHoly Family. It was not where the capitalist infrastructure was most developed that the revolution broke out – as ‘vulgar’ determinist Marxists might have expected.

Thus the world-historical situation is not the product of the ‘beautiful’ contradiction between labor and capital alone. Strikingly, this implies a root-and-branch rethinking of Marxism itself, both of its theory, but also of its history. “One day it will be necessary to do what Marx and Engels did for utopian socialism, but this time for those still schematic-utopian forms of mass consciousness unfluenced by Marxism… a true historical study of the conditions and forms of that consciousness.” And, one might add, this root-and-branch critical history is now required for the Althusserian turn as well.

The (admittedly simplified) Hegelian theory against which this is launched saw world-historical movement as a dialectic between the sphere of needs, of civil society, versus political society, or the state and its governing Idea. In this Hegel, material life, civil society, the economy – is merely the means through which reason, embodied in the state, works itself out in history. No matter whether this was the Hegel of the Hegelians, it was the Hegel of the Marxists for whom Marx was Hegel put right-side-up. In that version, it is the other way around. The sphere of the social production of men’s needs – economy – is the hidden truth of its political and economic forms. Economy is essence and the superstructures mere appearance. Althusser: “The logical destination of this temptation is the exact mirror image of the Hegelian dialectic. The only difference being that it is no longer a question of deriving the successive moments from the Idea, but from the Economy.”

Even as a recovering Althusserian, I am thankful for this break Althusser makes from the metaphysics of essence and appearance. That metaphysic remains the ideological field of theories of eternal capitalism, in which the essence of its economy never changes, and any new feature is ‘just circulation’ or some other such non-thought. Althusser is the beginning of a way to think historically again, outside of the mythic grand narrative of the ‘beautiful contradiction’, as he calls it, which is the hidden God governing all appearances.

For Althusser, Marx’s whole project is a break with exactly this dialectic. Althusser: “his concern was rather the ‘anatomy’ of this world and the dialectic of the mutations of this ‘anatomy.’ Therefore the concept of ‘civil society’ – the world of individual economic behavior and its ideological origin – disappears from Marx’s work.” In its place, a retreat from Hegel to his sources in classical political economy, such as Smith, and forward to Ricardo and others who follow Smith, and the development of a critique of the very categories through which the sphere of needs is imagined in bourgeois thought.

One might pause here to note that this set Althusserians on a course of seeing the relations of production as the crucial and determinate component of the economic ‘instance’, not theforces of production. This had a certain utility when expanded out into a concept of relations of production and reproduction — a pathway opened by Althusser and his students inReading Capital, which paid attention to Marx’s rather neglected Capital vol. 2. This later enabled a co-joining of Marxist and feminist concepts of how a capitalist social formation might be reproduced.

But there was a relative neglect of the forces of production, the study of which can’t be performed on a purely philosophical level but requires some detailed inquiry into the technologies of the day. Althusser does not ask after Marx’s interest in Charles Babbage’sfield studies in industrial technique, or his readings in German scientific materialism, where the science and engineering and their impact on the forces of production were a lively concern. This was unfortunate, given how rapidly the forces of production changed in the late twentieth century, changes those under the Althusserian spell rather neglected. And one might note here that this made the forces of reproduction even harder to fathom, and no connection was possible to Marxist-feminists such as Donna Haraway whose work was surely centrally connected to the question of the forces of reproduction.

Note that Althusser’s metaphor is the anatomy of the economic, not its metabolism, a term Marx uses in Capital vol. 3 that has proven very useful for green Marxism in thinking the Anthropocene. There’s a sense in which whatever the merits of Althusser’s influence in rescuing Marxism from economic-determinist vulgar thought, it prevented it on the other hand from not being vulgar enough, and really trying to grasp the historical development of the forces of production.

In other respects, with Althusser there was progress. The state, in this new dialectic, is not the embodiment of an Idea, but the instrument of the ruling classes. In place of the essence-appearance metaphysic, is a relation between separate and equally ‘real’ instances: economic, political, ideological, which relate through their structural differences rather than as expressive components of a whole. What was civil society, of the sphere of needs, becomes properly the mode of production, an historically specific form in which needs are socially met. It remains, in Marxist fashion, the determinate factor, but “in the last instance.” Its effectivity may be over-determined by, among other things, the political or ideological superstructures. Indeed, Althusser asserts, “the last, lonely hour of the ‘last instance’ never comes.” In this ‘dialectic,’ relations are separate and external to the terms they permutate. In this case the instances (economy, polity, ideology) are each separate levels with their own internal ‘contradictions’ between terms, each of which is then at a meta-level (over-determination) in a relation of externality and effectivity to each other. Goodbye Hegelian dialectic – negative or not.

This might be a grand and rather ironic example of what Guy Debord and the Situationistscalled détournement: the copying and correcting of past ideas, texts, materials, from past to present, with no regard for property or propriety. But détournement is a topic for another time. Where Debord advocated it a means of cultural and ideological production that abolished all claims to property and propriety, Althusser did the opposite — he established the property claims of those who held the philosophical keys to correct method.

The reason this appeared so urgent at the time takes us out of the those coordinates of the ideological field governed by academic intellectual life, and further into those governed at the time by the intellectual life of the communist party. What was at stake was a double question: who would have authority over Marxist discourse for the party? To which revolution would the party — and its intellectuals — owe allegiance?

The first footnote in Althusser’s text ‘On the Materialist Dialectic’ (1963) is not to Marx or Hegel or Spinoza, it is to Roger Garaudy. Who the fuck was Roger Garaudy? Trust me: you don’t want to know. Garaudy was the kind of hack who passed for a ‘thinker’ within the French Communist Party of the time (and whose later career is to ignominious to even mention). As is often the case, particularly with Marxist thinkers, the ideological field for Althusser was shaped by institutional figures and forces who do not even appear if one studies ‘library Marxism’ in graduate school.

Althusser’s celebrated early works all happen between two world-historical events: Khrushchev’s ‘secret speech’ of 1956, in which he revealed a tiny portion of the crimes of Stalin, and set about a partial de-Stalinization of the Communist movement. The other key event is the Sino-Soviet split, which starts to unfold from 1960, and led to break of the Chinese Communists from the Soviet ‘camp by 1965.

Khrushchev’s speech led to an ideological ‘thaw’, but also to a profound crisis for the western communist parties. A rather vacuous ‘socialist humanism’ became the prevailing ideology, partly inspired by a turn to the young Marx. This current saw Marxism as a continuation of the bourgeois enlightenment project. In some respects, this was a return to the popular front style of thinking of the inter-war years.

The Sino-Soviet split was over many things, of which ideology was probably the least important. Still, Mao did not follow the de-Stalinization line. I remember, when visiting China in 1987, that one could still find portraits of the “four beards” on the walls of official party buildings: the four being Marx, Engels, Lenin and Stalin. Their profiles, one in front of the other, would usually face a portrait of Mao on the opposite wall. In short: Mao was the true successor in the party’s Apostolic Succession, Stalin included.

The rupture between the Soviet and Chinese parties had its impact within the western communist parties as well. The Chinese revolution had appeared as a vindication of at least one idea of Lenin’s: that imperialism would break at its ‘weakest links’ — a form of over-determination in the Althusserian dialectic. The Chinese appeared to be trying to avoid the bureaucratization of their revolution. They seemed to want to do something different to the building of a massive heavy industry that simply reproduced under socialist conditions the same alienated mass labor as happened under capitalism.

As with enthusiasm for the Russian revolution, western enthusiasm for the Chinese revolution was based on very limited information. Since taking power in 1949 Mao appeared to have reformed agriculture, combatted illiteracy, embarked on a huge, labor-intensive program of national reconstruction, all with an aura of egalitarianism and purpose. The human costs of all of which were apparent to almost nobody in the west, whether on the right or the left — the Situationist René Viénet and his comrades excepted.

When the split opened up between the Soviets and China, not a few western communists opted to support China, either within the mainstream communist party, or by leaving it. In France, the party made the mistake of expelling the ‘Maoists’ en bloc, enabling them to swiftly set up a rival party of not negligible size. While there would be splits and factions, Maoism would be a strong current on the French left – much more so than in many other western countries.

Althusser did not leave the pro-Soviet Communist Party of France. His relation to the party and to China question is a rather subtle one. Certainly, ‘young Althusser’ texts can be read as formulating, at a very high level of sophistication a Maoist ‘line’ of sorts, or at the very least one opposed to the politics and culture of the Soviet thaw. Certainly several key students of his were active in one or other Maoist formation.

It is also possible to read Althusser, strangely enough, through what is usually thought of as the ‘voluntarism’ inherent in Lenin, Stalin and Mao’s thought, best expressed in the latter’s slogan “put politics in command.” This would be the idea, even the practice, of considering either ideological propagandizing or political mobilization as the lever via which the whole social formation would be transformed, as in Stalin’s own Cultural Revolution or later as Mao’s ‘Great Leap Forward’. The economic, as the realm of needs, needs a force from without to transform it.

Hence: “a revolution in the structure does not ipso facto modify the existing superstructures and particularly the ideologies at one blow.” Here Althusser appears to complement the Maoist critique of what had gone wrong in the Soviet Union: That the revolution had not been ideologically and politically vigilant enough. Moreover “the new society produced by the revolution may itself ensure the survival, that is the reactivation, of older elements through… the forms of its new superstructures….”

Is it too much to see here an echo of Stalin’s darkest thesis, that of the ‘sharpening of contradictions’ after the revolution? Not to mention Mao’s extension of it to constant mobilizations which, depending on your point of view, were aimed either at preventing the formation of a counter-revolutionary superstructure – or were meant merely to keep Mao the old tyrant in power.

Ideas travel in strange ways. However much Althusser may (or may not) have meant his position to be a Maoist one (a ‘superstructuralism’ but hardly a voluntarism) it ended up being something quite different, particularly in the Anglophone world: a legitimation for the ‘long march’ through the superstructures of a generation of intellectuals, fighting the good fight in the academy, or the media, or the arts, in imaginative but rarely any actual contact with, organized labor.

While in this essay I am critical of the legacy of Althusser today, I want to pay tribute nevertheless to those for whom his texts were one source of inspiration for a life of militancy, in France and elsewhere, often of considerable personal sacrifice. These are people whose names are only known to a few, who gave up lives that were in some cases of high privilege, to work sometimes under assumed names in factories or industrial towns. They could be rather dour and prickly – the basis in fact for Deleuze and Guattari’s portrait of the ‘sad militant’. But particularly in the ‘red decade’ in France (1966-1976) they did their best. For me that is always to be remembered with honor.

This might then be a thumbnail of the ideological field into which Althusser made his most influential interventions. It need only be added that his institutional location was not an insignificant one: the École Normale Supérieure (ÉNS). He taught at the absolute apex of a rather rigidly hierarchical educational system. The Grandes écoles in France produce the elite in each of there respective fields, in the case of the ÉNS – intellectuals. Sartre had been a normalien before him, as were Derrida and Foucault, in whose training Althusser had a hand.

A striking number of western Marxists were ‘outsiders’ of one kind or another, marked by difference. Even the archetypal ‘French intellectual’ Sartre was actually from Alsace. Several were German-speaking Jews. Althusser was a pied noir – a person of white French background born in Algeria. But more significant to our story is Althusser’s cognitive difference. He suffered periodic episodes of depression (and according to Eric Hobesbawm, quite extreme mania). That he murdered his wife while in an irrational state ought not to go unmentioned. I note this also because of the irony that Althusser is one of the sources for a kind of universalist and rationalist stand in continental philosophy, and yet could not have been further outsie the personae of the ‘universal rational man.’

And yet Althusser was also a ‘insider’, a Marxist and at the lofty ENS, teaching philosophy, in a country where philosophy actually matters. Unlike in the Anglophone world, philosophy is embedded within the French school curriculum. It informs a wide range of ideological processes. While Althusser would reject some of Gramsci’s ways of formulating the problem, he would surely have understood the minor but not-insignificant role of philosophy in sustaining what for Gramsci would be called hegemony.

Althusser contributed to a kind of counter-hegemonic base-building which produced in France for a time a quite interesting anti-capitalist cultural sphere. Althusser created an intellectual base for a Marxism that did not need the Communist Party to authorize it, which was one of the conditions of possibility for a non-communist intellectual left which could almost endure what Felix Guattari called the ‘winter years’ of the 1980s.

The real significance of Althusser is in the transition from a Marxism of the party to a Marxism of the academy. The means via which he got Marx from one to the other are now moot. It is rather like the fable of Captain Cook’s axe: first the handle was lost and replaced, then the head was lost and replaced, and yet it remains Captain Cook’s axe. Curiously, this severing of Marx from the actual party was in very different fashions also the goal of the Lukacs of History and Class Consciousness and the Sartre of Critique of Dialectical Reason. In the first case, the party was strong enough to shut this rival down, in the latter case to ignore it. But Althusser of the ÉNS managed to establish a parallel kind of authority over Marxism, independent of the party, and which would perhaps even outlive it.

Some elements of the text ‘On the Materialist Dialectic: On the Unevenness of Origins’ might help explain this move. It is among other things an ur-text for the notion of a capitalized ‘Theory’. In Althusser, this Theory was supposed to be the guarantee of the scientific character of Marxism, of its break with ideology, and a defense against ideological back-sliding. It was not to be. It never became an infallible debugger of method. Rather, we have had to learn to live with what Stuart Hall famously called a Marxism without guarantees.

Althusser stressed the break between Hegel and Marx. He also – rather fatefully – offered a pluralist rather than a mono-causal philosophy of history. (Remember here the three kinds of over-determination). Having defended this against the Hegelians, in this text he shored up the other flank, and defended this limited pluralism against what he calls a “hyper-empiricism.” Once one has more than one historical dialectic – why not lots and lots and lots?

The answer is to advance a theoretical practice. Here Althusser claims to follow Lenin, in the insistence that without correct revolutionary theory, there can be no correct revolutionary practice. What he adds is that the production of that theory is itself its own kind of practice. If the Leninists had professionalized political practice, making it a specialized form of labor, Althusser makes theoretical practice a specialized practice.

Althusser: “ideology is not always taken seriously as an existing practice. But to recognize this is the indispensable prior condition for any theory of ideology.” Oddly enough, one of Althusser’s precursors here is Bogdanov, for whom ideologies were products of real, material practices whose function was to motivate and coordinate labor. Althusser (and his student Dominique Lecourt) are orthodox, even vigilant Leninists in their hostility to Bogdanov, but it is curious that a selection of Bogdanov did appear in the series Althusser edited. Certainly Bogdanov, not Lenin, was Althusser’s precursor in breaking with Hegelian interpretations of Marx!

Althusser: “The theoretical practice of a science is always completely distinct from the ideological theoretical practice of its prehistory: this distinction takes the form of a ‘qualitative’ theoretical and historical discontinuity which I shall follow Bachelard in calling an ‘epistemological break.” There is both the beginnings of something here but a lack of follow through. A more practical study of how sciences constitute themselves in and against an ideological field would seem here to be an excellent suggestion.

But Althusser will not be a return towards a genuine study of science, in the manner of JD Bernal or Joseph Needham. Nor is he a precursor to science studies as it will later flower. Rather, he gets stuck at the level of asserting a merely formal break between a science and the ideology which precedes and surrounds it. There will be those in science studies who will pass over their debts to the Marxism of Bernal and Needham by loudly declaiming their distance from this Marxism of Althusser.

What decides for the science of Marxism is Theory, “none other than dialectical materialism,” although a rather different one to the diamat constructed out of random bits of Marx and Engels by Althusser’s Soviet counterparts. It is to decide in advance what the dialectic is, before the encounter in a given investigation of a new situation or problem. Before beginning any investigation, or any practice, researchers “need Theory, that is, the materialist dialectic, as the sole method that can anticipate their theoretical practice by drawing up its formal conditions.” One could juxtapose this not just to Needham, but also to Bogdanov, at least as I read them. In their hands, theory is a matter of extracting concepts (the ‘dialectic’) from particular practices of the production of empirical and scientific knowledge, and then the speculative testing and adaptation of them to other fields.

In other words, theirs is a genuine pluralism, within a general speculative method, but also with specific empirical tests of the validity of that method in each domain of knowledge production. In short, Althusser wants democratic centralism – the party of Theory’s decision is final – whereas Bogdanov and Needham are more ‘syndicalist’ in their approach to the comradely cooperation of knowledge.

Althusser’s is not quite as ambitious a view as that which Plekhanov took over from Engels, in which a dialectic can be applied even to the natural sciences. Althusser’s ambitions do not extend that far. But he does appear to want a Theory that can legislate outside the bounds of the natural sciences. He is particularly on his guard (as Leninist always are) againstspontaneity, particularly among new-fangled practices of the production of knowledge.  The new social sciences and humanities fields in particular are not to think they are self-legislating – autonomous.

The most ambitious claim of this text is to ground the general method of a theoretical practice. This comes, curiously, out of a gentle but thorough critique of Mao’s text ‘On Contradiction’. What follows strikingly is not so much a theory as a metaphor. Theoretical practice is to be understood on the metaphor of production in general, but in a rather peculiar way. The production of knowledge starts with Generality I: with general concepts, the existing ones of ideology, as raw material. They are transformed by the labor of Generality II. These are the means of production, more or less contradictory, of the production of knowledge of a given moment. The work of Generality II on Generality I produces knowledge as specified concepts, a concrete generality. In short: “theoretical practice produces Generalities III by the work of Generality II on Generality I.

It should be apparent at once that this is metaphorical. Nothing concrete about the labor of the production of knowledge appears here at all. Althusser even says “if we abstract from men in these means of production for the time being….” But the abstraction never ends. What follows is then the dogmatic assertion that there is an epistemological break between Generality I and Generalities III, guaranteed by the vigilance of Theory over the transformative work of Generalitiy II. And what results is not the concrete-as-such, but the concrete in thought. The criteria of valid knowledge are all internal to the theoretical procedure, understood metaphorically to be a labor procedure. True knowledge is that which Theory guarantees, and no other. It is the theoretical concrete which is knowledge.

One could mount an internal critique of this version of what Marxist Theory ought to be doing. But I think it more useful to put it into the ideological field, and ask: what ‘work’ was it doing at large? To my mind, Althusser is trying to set up a procedure for the coordination and validation of correct Marxist ‘practice’ within the division of labor of the university, or even the ideological apparatuses writ large. One that appears to parallel and supplement that of the party, but which actually replaces it.

This has two aspects. One is the replacement of the authority of the Party with that of Theory. To some extent the controversy that this aroused is moot, given the decline of that very Party, and the marginal status of those that would try to reproduce it. Interestingly, by the time we get to the later work of Althusser’s student Alain Badiou, there are four kinds of event which produce the subject and its truth, of which politics is only one, not dominant one.

The other aspect is the question of the coordination of different kinds of knowedge that might claim to be part of a larger Marxist project within the university. This is rather more interesting. It is if anything an even more significant problem today. Althusser’s solution is a ‘democratic centralist’ one, a para-Party called Theory, which has both legislative, judicial (and policing) power as to what constitutes knowledge, at least outside the domain of the natural sciences. It is as opposed to ‘spontaneous’ theories and their lateral, transversal flow between sites of work as the Leninist party was to all forms of spontaneity, whether it be in the style of Rosa Luxemburg, or Antoine Pannekoek, or Alexander Bogdanov.

Not surprisingly, given the institutional context of the ENS and French hegemonic culture more generally, this para-Party is essentially that of philosophy. Marxist philosophy will legislate, judge and police all the other forms of knowledge in a remarkably similar way to how non-Marxist philosophy has always considered itself to have such powers in the French context. The counter-hegemonic (in Gransci’s terms) is a mirror of the hegemonic. It does not have its own form. In this regard, and to make an extreme provocation, we have to conclude that Althusserianism was a kind of reformism in the domain of knowledge and culture. Unlike, for example, Bogdanov and the Situationists, in their rather different ways, there is no imagining here of a different form for a counter-production of knowledge.

Althusser’s metaphoric approach to the question of knowledge production led in at least two different directions. One was an even more hyperbolic rationalism, but more on that later. The other was to make the metaphor more ‘real’, in the sense of examining actual, material processes of the production of knowledge, and in particular of those kinds of knowledge which seem to have direct power-effects. This is the path of Michel Foucault. Granted, this approach leant as much on Nietzsche’s diagnoses of the will to power in forms of knowledge or ideology. And granted, this in many ways became a new kind of doxa in the humanities, where ‘power’ could be found everywhere and nowhere.

Still: there seems to me something positive in this general procedure, and one with more affinity for Marx than might be at first apparent. A Marxist approach to knowledge, and in fact even of natural scientific knowledge, should enquire into the material practices of its production, and moreover, should see itself within the limits of those means of production and not at some Archimedian point outside of them. Much of science studies actually took this kind of trajectory.

Here we have to mention Althusser’s very curt dismissal of any approach (such as that of Bogdanov) which starts from the reality of that which the apparatus and labor of knowledge produces. Althusser simply asserts that if one starts from sensation, even in this historically grounded way, one has no way of filtering in advance what is ideological from it, and thus of producing a science. The reply to this is obvious: Althusser’s rationalism has no such procedure either. It is simply asserted that the vigilance of Theory will perform this miracle, and do so in a universal way.

But why not a method for all the sciences? Particularly those that impinge most heavily on us. Here what might be worth developing, out of Bogdanov and Needham, or out of Donna Haraway and Karen Barad, might be a concrete, historical, specific approach to the actual production of knowledge in both natural and human sciences. What we need is not something like Mao’s ‘On Contradition’, to legislate as Theory for all of knowledge. What we need is many versions of Capital, actual critical accounts of other kinds of knowledge,particularly of the forces of their production, besides political economy. Althusser would of course consider this a “hyper-empiricism.”

If Althusser has merit still today, it is in his sly way of always asking: what is at stake in the politics of knowledge at any given world-historical moment? Let’s quote here how he defined his own moment: “what will later be called by a name which does not exist as yet… when in the struggle for peaceful co-existence the first revolutionary forms are appearing in certain so-called under-developed countries out of their struggles for national independence.” These are not those times. The capitalist west no longer confronts twosocialist camps, one sprung from the colonized world. Rather, I take the defining feature of the conjuncture to be a now-globally victorious regime of commodified production to be confronting the limits imposed by its own destabilizing of the metabolic processes of the planet itself.

It turns out there are resources for thinking such a moment in the Marxist tradition, from Marx’s own concept of metabolic rift, to Bogdanov’s tektology, Bataille’s general economy, Asger Jorn’s ornamentation, or Sartre’s practico-inert. But one might yet retain from Althusser the break from the Hegelian ideological field, to the extent that it saw labor as in a dialectic of spiritualizing nature, imbuing it with rational form, and subsuming nature into teleological project. Where Adorno reversed the Hegelian dialectic and ended up with nothing but the consolation of aesthetics, Althusser replaced it with one that at least gestured in a formal way to the problem of how to organize labor and knowledge.

The thing about being a recovering Althusserian is that one can’t help remembering the good times. Being on Althusser really does feel great. It makes certain problems disappear. For example, one is no longer trapped in the oppressive totality of Hegelian Marxism, and yet nor does one have to return to the world of ‘economistic’ Marxism. One can fly free from all that! (Ah, but as in any addiction narrative, there’s a price to pay…)

So on the plus side, the problematic is no longer constrained by those readings of Marx that see him as essentially putting the Hegelian dialectic back on its feet, or retrieving its rational kernel from its mystical shell. Althusser’s essay on the young Marx already opens up this dimension. But he is an alternative also to those western Marxists who, one way or another, tempered their Hegelianism with a dose of Kierkegaard.

This came in a lot of favors. Lukacs centered his Hegelian totality-in-process on the proletariat as universal subject-object, which frees itself from reification and acts on and as the totality. But even here there is something of a Kierkegaardian irrationality about the proletariat in action, a kind of revolutionary leap of faith. It is a figure that will recur in various ways in Sartre, Badiou and Zizek. Adorno and Sartre, in rather different ways, cut their Hegel with even more Kierkegaard to prevent Lukacs’ totality from self-closure. In Sartre, individuals only ever temporarily subsume themselves into the movement of the totality. In Adorno the dialectic itself is to attend to the unrecoverable fragment. It twists away from the extorted reconciliation of exchange value.

Walter Benjamin’s relation to Kierkegaard is complicated, but let’s just say that Althusser was most certainly an alternative to taking too many hits of Benjamin, wherein history is only ever allegorically present, in the form of fragments that are shot through with a messianic time. All of these Marxisms had a tendency to reduce everything to commodification and its attendant effects: reification, extorted reconciliation, inter-passivity. Either history had become the bad totality of exchange, as in Adorno, or the good totality perpetually postponed where the rational meets the real, as in Sartre.

The Althusserian decision against Hegel and Kierkegaard with Marx comes at a price, however. One is done with totalities and fragments, but has to contend instead with the straying apart from one another of the three (or four) levels of the social formation. This shows up in three post-Althusserian tendencies. One is that version of theory which I think has to be called ‘Jacobin Marxism’. This is particularly clear in some of Althusser’s students, such as Nicos Poulantzas, Etienne Balibar, Jacques Ranciere, and on into other work such as Chantal Mouffe and Ernesto Laclau. The feature of this tendency is to isolate Politics with a capital-P as the decisive level or instance of the social formation, at the expense of any larger sense of a political economy. Politics becomes absolutely autonomous and even ontologically prior.

A second tendency was cultural studies. In rather different ways, cultural theorists from Stuart Hall to Judith Butler took off from Althusser’s famous essay on ‘Ideology and the Ideological State Apparatuses’ and used it to legitimate the study of the ideological (or cultural) as an autonomous sphere with its own materiality and formal laws — to be understood using the tools of semiotics and rhetoric. It was even imagined that the cultural could in some sense lead revolutionary change while economic struggle stagnated.

A third tendency one could call Hyper-Rationalism. There are three kinds of regional knowledge in Althusser, corresponding to the three relatively autonomous levels within the social formation of the economic, the political and the ideological. But philosophy stands apart, legislating for what constitutes scientific knowledge in all domains — indeed for any science. What marks the difference between an ideology and a science is that the former produces subjects who misrecognize themselves in it, while sciences do not. Althusserian rationalist epistemology becomes mathematics as ontology in his student, Alain Badiou, which becomes the speculative realism of his student, Quentin Meillassoux.

All of these tendencies seem to me to still be dependent on Althusser and to point away from the two crucial encounters in our own times. Encounters for which neither Hegelian or Kierkegaardian nor economistic not even Althusserian Marxism are all that adept. One is the technical transformation of the forces of production and reproduction. The other is the metabolic rift opened up by the application of those forces, via the private property form, on a planetary scale. For that we have to look elsewhere, once we have weaned ourselves off Althusser, and certain other habits of thought. Habits which, like all addictions, reproduce themselves within our thought and within institutionalized discourse in pursuit of their own necessity, regardless of what takes place in the world.
