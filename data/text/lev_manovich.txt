Social media content shared today in cities, such as Instagram images, their tags and descriptions, is the key form of contemporary city life. It tells people where activities and locations that interest them are and it allows them to share their urban experiences and selfrepresentations. Therefore, any analysis of urban structures and cultures needs to consider social media activity. In our paper, we introduce the novel concept of social media inequality. This concept allows us to quantitatively compare pattern in social media activities between parts of a city, a number of cities, or any other spatial areas. We define this concept using an analogy with the concept of economic inequality. Economic inequality indicates how some economic characteristics or material resources, such as income, wealth or consumption are distributed in a city, country or between countries. Accordingly, we define social media inequality as unequal spatial distribution of social media sharing in a particular geographic area or between areas. To quantify such distributions, we can use many characteristics of social media such as number of people sharing it, the number of photos they have shared, their content, and user assigned tags. We propose that the standard inequality measures used in other disciplines, such as the Gini coefficient, can also be used to characterize social media inequality. To test our ideas, we use a dataset of 7,442,454 public geo-coded Instagram images shared in Manhattan during five months (March-July) in 2014, and also selected data for 287 Census tracts in Manhattan. We compare patterns in Instagram sharing for locals and for visitors for all tracts, and also for hours in a 24 hour cycle. We also look at relations between social media inequality and socio-economic inequality using selected indicators for Census tracts. The inequality of Instagram sharing in Manhattan turns out to be bigger than inequalities in levels of income, rent, and unemployment.

Social media content shared today in cities, such as Instagram images, their tags and descriptions, is the key form of contemporary city life. It tells people where activities and locations that interest them are and it allows them to share their urban experiences and selfrepresentations. Social media also has become one of the most important representations of city life to both its residents and the outside world. One can argue that any city today is as much media content shared in that city on social networks as its infrastructure and economic activities. For these reasons, any analysis of urban structures and cultures needs to consider social media activity and content. While the industry developed many concepts and measurement tools to analyze social media, these concepts and tools were not developed with the view for the comparative urban analysis. Therefore, we need to develop our own concepts that bridge the perspectives of urban studies and design and quantitative analysis of social networks that uses computational methods and “big data.” In the last few years, one of the most frequently discussed public issues has been the rise in income inequality (Stiglitz 2012, Piketty 2014, Atkinson 2015). But inequality does not only refer to distribution of income. It is a more general concept, and it has been used for decades in a number of academic disciplines besides economics, such as urban planning, sociology, education, engineering, and ecology. The quantitative measurements of inequality allow researchers to characterize a set of numbers or compare multiple sets, regardless of what the data represents. In addition to income inequality, we can measure inequality in wealth, education levels, social well-being, and numerous other social characteristics. In our paper, we introduce the novel concept of social media inequality. We define this concept using an analogy with the concept of economic inequality. Economic inequality refers to how some economic characteristics or material resources, such as income, wealth or consumption are distributed in a city, country or between countries (Ray 1998, Milanovic 2007, OECD 2011). Accordingly, we can define social media inequality as the measures of distribution of characteristics of social media content shared in a particular geographic area or between areas. An example of such characteristics is the number of photos shared by all users of a social network such as Instagram in a given city or city area. Another example is the number of hashtags – how many hashtags users added to the photos, and how many of these hashtags are unique. Other examples include average number of tweets shared by a user in a particular period; numbers of tweets shared per month, per week or per hour of a day; the proportions of tweets that were retweeted, and so on. Of course, we can computer and analyze features of content itself - for example, how many different subjects appear the photos, and what are their proportions. In fact, any metric of social media can be used to compare inequality in social 5 media activity between areas - for example, number of likes, length of text messages, most frequent and least frequent words, number of unique topics, number of distinct photographic styles, image compositions, styles of video editing, and so on. We propose that the standard inequality measures used in other disciplines, such as the Gini coefficient, can also be used to characterize social media inequality. We can also compare these measures between content shared on various social networks (Instagram, Twitter, etc.) in the same area or areas. We can do these comparisons for social networks where the main content is text (e.g., Twitter, VK), images (e.g., Instagram, Tumblr), video (e.g., YouTube), or combination of different media (e.g. Facebook, QZone, Sina Weibo, Line, etc.). Finally, we can also compare characteristics of shared content with various social and economic characteristics in the same areas, such as income, rent, the level of education, or ethnic mix. The paper tests some of these ideas using a large dataset of Instagram images shared in Manhattan borough of New York City. This dataset, which we created for this study, contains 7,442,454 public geo-coded Instagram images shared in Manhattan during five months (February 1, 2014 July 31, 2014). Among these images, 1,524,046 were shared by 515,608 city visitors; the remaining 5,918,408 images were shared by 375,876 city residents. Our analysis of the images shared by two types of users in this paper is inspired by the pioneering project Locals and Tourists created by Eric Fischer (Fisher, 2010.) We used the following method to divide users into “visitors” and “locals.” If a user posted all her photos within a single 12-day period out of the total five months, we consider this person a “visitor”. If a user shared a minimum of two photos within any interval larger than 12 days, we consider this person a local. Although this very simple method is expected to produce some errors, we felt that they are acceptable given the size of our dataset. (Such method is also used a number of published papers that analyze patterns in social media activity.) Comparing the locations of images shared by visitors (figure 1) and locals (figure 2) gives us an intuition for social media inequality concept. We can immediately notice that in each case these locations are not distributed evenly. Some parts of the city have many more images than other parts. These figures also suggest that the big proportion of images by city visitors are shared only in a few areas, while the locals share images in most areas of the city. Note that we use the term “shared” rather than “captured” because Instagram allows sharing of any image from user’s phone and not only the ones captured within Instagram app. So users can upload images taken previously in other locations. However, since Instagram captured the geolocation and time when an image was shared (for users who allowed Instagram access to this data), the metadata of images in our dataset tells us about people’s presence at particular place in the city at a particular time.
While the U.S. Census collects data on individuals, it only reports the data aggregated by geographic areas at different scales. We follow a similar logic in our analysis of spatial social media inequality by dividing a city into hundreds of small areas and aggregating characteristics of social media content shared in each area - as opposed to comparing individuals to each other. The way we measure social media inequality is comparable to how Milanovic defines one of the measures of global economic inequality (Milanovic 2006, Concept 1). This measure uses countries as the units of observation. Milanovic does not directly compare the income of people worldwide. Instead he compares average income across different countries to calculate global inequality. In our case, the Census tracts are our units of observation. We aggregate social media characteristics at the tract level in order to analyze social media inequality across all of Manhattan. Social media content shared in a given area may combine contributions from different kinds of users: people who reside in this area, people who live in different parts of the city or in suburbs but spend significant time in this area for work during weekdays; international or domestic tourists visiting a city; companies located in this area, and so on. Together, the content shared by all these users create a collective “voice” of a particular area of a city. A city as a whole can be compared to an orchestra of all these voices (although, of course, they are not necessary performing the same composition.) Applying the concept of inequality to a collection of these urban voices can give us new ways of understanding a city, and provide an additional metric for comparing numerous cities around the world. Social media inequality as we define it refers to the unequal distribution of social media content and its metadata and their characteristics in any type of geographic area – a city, a region, a country, or any other type of area. However, as Fischer’s maps show visually, the density of social media contributions in larger cities is much higher than in non-urban areas, which makes these cities particularly convenient areas of study. We think that our proposed measurements of social media inequality can be useful for urbanism studies, urban planning, urban design, public administration, economics, and other professional and academic fields. While researchers in the fields of social computing, spatial analytics, and “science of cities” have published many quantitative studies analyzing urban data of many kinds (Batty 2013, Goldsmith and Crawford 2014, Townsend 2014, Pucci et al. 2015, Ratti et al. 2006), a significant portion of this analysis cannot be approached without having a degree in computer science. In contrast, social media inequality measurement is a concept that is easy to understand and also easy to calculate. The locations of social media contributions reflect the presence of people in a particular part of a city at a particular time. However, in comparison to pure location data captured by mobile phones or other body sensors, social media images are much more than simple coordinates and time stamps. The content of these contributions can also tell us what people find interesting and how they are spending their time. Therefore, mapping and measuring inequality in 9 characteristics of social media can help us understand how social, economic, and urban design characteristics of cities influence life patterns and the overall “dynamism” and “vitality” of a city. Researchers have never observed perfect equality in any natural, biological or social system or population. In using the term “social media inequality,” we are not suggesting that the goal of urban planners or city administration should be to reduce differences in social media use between various areas to a minimum, or to some optimal level. If people are sharing the same amount of social media in every area of the city, it means that this city does not have any centers or attractions that stand out, or places where many people gather. In terms of modern housing, large American-type suburbs with the same density of houses and same demographics of families and income would probably generate least amount of social media inequality. Today such suburbs are common around the world, from Mexico to China. Given the wide criticism of this classical suburb type, we can assume that some level of spatial social media inequality is desirable. In this case, inequality stands for variety and differentiation while complete equality stands for sameness and lack of variety. But is extreme social media inequality a good thing? For example, do we really want all people living in a city to spend their weekends in a single place? There are certain situations where reducing extreme spatial social media inequality would be very desirable. For example, if city authorities find that most tourists’ social media activity is concentrated in just a few areas surrounding only a few landmarks (like Times Square in New York City), they can change the way the city is promoted to visitors to diversify where tourists go, what they look at, and what they experience. Being able to quantify inequality of social media would allow for better planning and evaluation of such changes. Formulated as a type of spatial analysis, our study compares the parts of the city that attract more people and generate more content shared on social media networks and thus are “social media rich” with parts of the city that are “social media poor.” What are the relationships between such social media rich and social media poor areas? Is social media inequality larger or smaller than economic or social inequality in the same areas? Does social media inequality increase worldwide, similar to how economic inequality has been growing recently? Which parts of the world have the highest social media inequality and which are the most equal? Although our analysis is focusing on one part of a single megacity (i.e., Manhattan in New York City), it can be expanded to consider hundreds of cities around the world to consider such questions.

I present a number of core concepts from data science that are relevant to digital art history and the use of quantitative methods to study any cultural artifacts or processes in general. These concepts are objects, features, data, feature space, and dimension reduction. These concepts enable computational exploration of both large and small visual cultural data. We can analyze relations between works on a single artist, many artists, all digitized production from a whole historical period, holdings in museum collections, collection metadata, or writings about art. The same concepts allow us to study contemporary vernacular visual media using massive social media content. (In our lab, we analyzed works by van Gogh, Mondrian, and Rothko, 6000 paintings by French Impressionists, 20,000 photographs from MoMA photography collection, one million manga pages from manga books, one million artworks of contemporary non-professional artists, and over 13 million Instagram images from 16 global cities.) While data science techniques do not replace other art historical methods, they allow us to see familiar art historical material in new ways, and also to study contemporary digital visual culture. In addition to their relevance to art history and digital humanities, the concepts are also important by themselves. Anybody who wants to understand how our society “thinks with data” needs to understand these concepts. They are used in tens of thousands of quantitative studies of cultural patterns in social media carried out by computer scientists in the last few years. More generally, these concepts are behind data mining, predictive analytics and machine learning, and their numerous industry applications. In fact, they are as central to our “big data society” as other older cultural techniques we use to represent and reason about the world and each other – natural languages, material technologies for preserving and accessing information (paper, printing, digital media, etc.), counting, calculus, or lens-based photo and video imaging. In short, these concepts form the data society’s “mind” – the particular ways of encountering, understanding, and acting on the world and the humans specific to our era.

Will art history fully adapt quantitative and computational techniques as part of its methodology? While the use of computational analysis in literary studies and history has been growing slowly but systematically during 2000s and first part of 2010s, this has not yet happened in the fields that deal with the visual (art history, visual culture, film, and media studies). However, looking at the history of adoption of quantitative methods in the academy suggests that these fields sooner or later will also go through their own “quantitative turns.” Writing in 2001, Adrian Raftery points out that psychology was the first to adopt quantitative statistical methods in 1920s-1930s, followed by economics in 1930s-1940s, sociology in 1960s, and political science in 1990s.2 Now, in 2015, we also know that humanities fields dealing with texts and spatial information (i.e., already mentioned literary studies and history) are going through this process in 2000s- 2010s. So I expect that “humanities of the visual” will be the next to befriend numbers.  This adaption will not, however, simply mean figuring out what be counted, and then using classical statistical methods (developed by the 1930s and still taught today to countless undergraduate and graduate students pretty much in the same way) to analyze these numbers. Instead, it will take place in the context of a fundamental social and cultural development of the early 21st century – the rise of “big data,” and a new set of methods, conventions, and skills that came to be called “data science.” Data science includes classical statistical techniques from the 19th and early 20th century, additional techniques and concepts for data analysis that were developed starting in 1960s with the help of computers, and concepts from a number fields that also develop in the second part of the 20th century around computers: pattern recognition, information retrieval, artificial intelligence, computer science, machine learning, information visualization, data mining. Although the term "data science" is quite recent, it is quite useful as it acts as an umbrella for currently most frequently used methods of computational data analysis. (Alternatively, I could have chosen machine learning or data mining as the key term for this article, but since data science includes their methods, I decided that if I am to refer to all computational data analysis using a single term, data science is best right now.)
Data science includes many ideas developed over many decades, and hundreds of algorithms. This sounds like a lot, and it is. It is much more than can be learned in one or two graduate methods classes, or summarized in a single article, or presented in a single textbook. But rather than simply picking particular algorithms and techniques from a large arsenal of data science, or borrowing whatever technique happens to be the newest and therefore is currently in fashion (for example, “topic modeling” or “deep learning”) and trying to apply them to art history, it is more essential to fist understand the most fundamental as sumption of the field as a whole. That is, we in art history (or any other humanities field) need to learn the core concepts that underlie the use of data science in contemporary societies. These concepts do not require formulas to explain, and they can be presented in one article, which is what I will attempt here. (Once we define these core concepts, a variety of terms employed in data science today can also become less confusing for the novice.)
Surprisingly, after reading thousands of articles and various textbooks over last eight years, I have not found any short text that presents these core concepts together in one place. While many data science textbooks, of course, do talk about them, their presentation often takes place in the context of mathematically sophisticated techniques or particular applications which can make it hard to understand the generality of these ideas.3 These textbooks in general can be challenging to read without computer science background. Since my article is written for humanities audience, it is on purpose biased–my examples of the application of the core concepts of data science come from humanities as opposed to economics or sociology. And along with an exposition, I also have an argument. I will suggest that some parts of data science are more relevant to humanities research than others, and therefore beginning “quantitative humanists” should focus on learning and practicing these techniques first.
If we want to use data science to “understand” some phenomenon (i.e., something outside of a computer), how do we start? Like other approaches that work on data such as classical statistics and data visualization, data science starts with representing some phenomenon or a process in a particular way. This representation may include numbers, categories, digitized texts, images, audio, spatial locations, or connections between elements (i.e., network relations). Only after such a representation is constructed, we can use computers to work on it. In most general terms, creating such a representation involves making three crucial decisions: What are the boundaries of this phenomenon? For example, if we are interested to study “contemporary societies,” how can we make this manageable? Or, if we want to study “modern art,” how we will choose what time period(s), countries, artist(s), and artworks, or other information to include? In another example, let’s say that we are interested in contemporary “amateur photography.” Shall we focus on studying particular groups on Flickr that contain contributions of people who identify themselves as amateur or semi-pro photographers, or shall we sample widely from all of Flickr, Instagram, or other media sharing service since everybody today with a mobile phone with a built-in camera automatically becomes a photographer. What are the objects we will represent? For example, in modern art example, we may include the following “objects” (in data science they can be also called data points, records, samples, measurements, etc.): individual artists, individual artworks, correspondence between artists, reviews in art journals, passages in art book, auction prices. (For example, 2012 Inventing Abstraction exhibition in MoMA in NYC featured a network visualization showing connections between artists based on the number of letters they exchanged.4 In this representation, modernist abstract art was represented by a set of connections between artists, rather than any other kind of object I listed above.) In a “society” example, we can for instance choose a large set of randomly chosen people, and study social media they share, their demographic and economic characteristics, their connections to each other, and biological daily patterns as recorded by sensors they wear. If we want to understand patterns of work in a hospital, we may use as elements people (doctors, nurses, patients, and any others), also medical procedures to be performed, tests to be made, written documentation and medical images produced, etc. What characteristics of each object we will include? (These are also referred to as metadata, features, properties, or attributes.). In humanities, we usually refer to characteristics that are already available as part of the data (because somebody already recorded them) and characteristics we have added (for example, by tagging) as metadata. In social science, the process of manually adding descriptions of data is called coding. In data science, people typically use algorithms to automatically extract additional characteristics from the objects, and they are referred as features (this process is called “feature extraction”). For example, artists’ names is an example of metadata; average brightness and saturation of their paintings, or the length of words used in all titles of their works are examples of features that can be extracted by a computer. Typically features are numerical descriptions (whole or fractional numbers) but they can also take other form. For example, a computer can analyze an image and generate a few words describing content of the image. In general, both metadata and features can use various data types: numbers, categories, free text, network relations, spatial coordinates, dates, times, and so on.  Although it is logical to think of the three questions above as three stages in the process of creating a data representation– limiting the scope, choosing objects, and choosing their characteristics – it is not necessary to proceed in such linear order. At any point in the research, we can add new objects, new types of objects and new characteristics. Or we can find that characteristics we wanted to use are not practical to obtain, so we have to abandon our plans and try to work with other characteristics. In short, the processes of generating a representation and using computer techniques to work on it can proceed in parallel and drive each other.
The author and several colleagues studied cultural differences using these computerized patterns of Instagram postings—arranged by hue and brightness—from Tokyo, New York, Bangkok, and San Francisco.

In 2002, I was in Cologne, Germany, and I went into the best bookstore in the city devoted to humanities and arts titles. Its new-media section contained hundreds of books. However, not a single title was about the key driver of the "computer age": software. I started going through indexes of book after book: No "software."

Yet in the 1990s, software-based tools were adopted in all areas of professional media production and design. In the 2000s, those developments have made their way to the hundreds of millions of people writing blogs and tweeting, uploading photos and videos, reading texts on Scribd, and using free tools that 10 years earlier would have cost tens of thousands of dollars.

Thanks to practices pioneered by Google, the world now operates on web applications that remain forever in beta stage. They can be updated anytime on remote servers without consumers having to do anything—and in fact, Google is revising its search-algorithm code as often as 600 times a year. Welcome to the world of permanent change—a world defined not by heavy industrial machines that are modified infrequently, but by software that is always in flux.

Software has become a universal language, the interface to our imagination and the world. What electricity and the combustion engine were to the early 20th century, software is to the early 21st century. I think of it as a layer that permeates contemporary societies. If we want to understand today's techniques of communication, representation, simulation, analysis, decision making, memory, vision, writing, and interaction, we must understand software.

But while scholars and media and new-media theorists have covered all aspects of the IT revolution, creating fields like cyberculture studies, Internet studies, game studies, new-media theory, and the digital humanities, they have paid comparatively little attention to software, the engine that drives almost all they study.

It's time they did.

Consider the modern "atom" of cultural creation: a "document," i.e. content stored in a physical form delivered to consumers via physical copies (books, films, audio records) or electronic transmission (television). In software culture, we no longer have "documents." Instead, we have "software performances."

If you are a scholar working inside Google or Facebook, you have a major advantage over colleagues in academe.

I use the word "performance" because what we are experiencing is constructed by software in real time. Whether we are exploring a website, playing a video game, or using an app on a mobile phone to locate nearby friends or a place to eat, we are engaging with the dynamic outputs of computation.

Although static documents may be involved, a scholar cannot simply consult a single PDF or JPEG file the way 20th-century critics examined a novel, movie, or TV program. Software often has no finite boundaries. For instance, a user of Google Earth is likely to experience a different "earth" every time he or she uses the application. Google could have updated some of the satellite photographs or added new Street Views and 3D buildings. At any time, a user of the application can also load more geospatial data created by other users and companies.

Google Earth is not just a "message." It is a platform for users to build on. And while we can find some continuity here with users' creative reworking of commercial media in the 20th century—pop art and appropriation, music, slash fiction and video, and so on—the differences are larger than the similarities.

Even when a user is working only with a single local media file stored in his or her computer, the experience is still only partly defined by the file's content and organization. The user is free to navigate the document, choosing both what information to see and the sequence in which to see it. (In Google Earth, I can zoom in and out, switching between a bird's-eye view of the area, and its details; I can also switch between different kinds of maps.)

Most important, software is not hard-wired to any document or machine: New tools can be easily added without changing the documents themselves. With a single click, I can add sharing buttons to my blog, thus enabling new ways to circulate its content. When I open a text document in Mac OS Preview media viewer, I can highlight, add comments and links, draw and add thought bubbles. Photoshop allows me to save my edits on separate "adjustment layers," without modifying the original image. And so on.

All that requires a new way to analyze media and culture. Since the early 2000s, some of us (mostly from new-media studies and digital arts) have been working to meet that challenge. As far as I know, I was the first to use the terms "software studies" and "software theory" in 2001. The field of software studies gradually took shape in the mid-2000s. In 2006, Matthew Fuller, author of the pioneering Behind the Blip: Essays on the Culture of Software(Sagebrush Education Resources, 2003), organized the first Software Studies Workshop in Rotterdam. "Software is often a blind spot in the theorization and study of computational and networked digital media," Fuller wrote in introducing the workshop. "In a sense, all intellectual work is now 'software study,' in that software provides its media and its context, but there are very few places where the specific nature, the materiality, of software is studied except as a matter of engineering."

In 2007, we started the Software Studies Initiative at the University of California at San Diego, and in 2008 we held the second software-studies workshop. The MIT Press offers a software-studies series, and a growing number of books in other fields (media theory, platform studies, digital humanities, Internet studies, game studies) also help us better understand the roles software plays in our lives. In 2011, Fuller and other researchers in Britain began Computational Culture, an open-access peer-reviewed journal.

T

here is much more to do. One question that particularly interests me is how software studies can contribute to "big data"—analyzing vast data sets—in fields like the digital humanities, computational social science, and social computing. Here are some of the key questions related to big cultural data that software studies could help answer:

What are interactive-media "data"? Software code as it executes, the records of user interactions (for example, clicks and cursor movements), the video recording of a user's screen, a user's brain activity as captured by an EEG or fMRI? All of the above, or something else?

To use terms from linguistics, rather than thinking of code as language, we may want to study it as speech.

Over the past few years, a growing number of scholars in the digital humanities have started to use computational tools to analyze large sets of static digitized cultural artifacts, such as 19th-century novels or the letters of Enlightenment thinkers. They follow traditional humanities approaches—looking at the cultural objects (rather than peoples' interaction with these objects). What has changed is the scale, not the method.

The study of software culture calls for a fundamentally different humanities methodology. We need to be able to record and analyze interactive experiences, following individual users as they navigate a website or play a video game; to study different players, as opposed to using only our own game-play as the basis for analysis; to watch visitors of an interactive installation as they explore the possibilities defined by the designer—possibilities that become actual events only when the visitors act on them.

In other words, we need to figure out how to adequately represent "software performances" as "data." Some answers can come from the field of human-computer interaction, where researchers in academe and private enterprise study how people engage with computer interfaces. The goals of that research, however, are usually practical: to identify the problems in new interfaces and to fix them. The goals of digital humanities' analysis of interactive media will be different—to understand how people construct meanings from their interactions, and how their social and cultural experiences are mediated by software. So we need to develop our own methods of transcribing, analyzing, and visualizing interactive experiences. Together with the Experimental Game Lab, directed by Sheldon Brown, for example, my lab analyzed the experiences of hundreds of users of Scalable City, a large-scale, complex virtual-world art installation created in Brown's lab. One of our goals was to help future users have more challenging interactive experiences.

Who has access to detailed records of user interactions with cultural artifacts and services on the web, and what are the implications of being able to analyze these data?

From the early days of interactive human-computer interfaces, tracking users' interaction with software was easy. Why? Because software continuously monitors inputs like key presses, mouse movements, menu selections, finger gestures over a touch surface, and voice commands.

The shift from desktop to web computing in the 1990s has turned the already existing possibility of recording and storing users' inputs into a fundamental component of a "software-media complex." Since dynamic websites and services (Amazon's online store, personal blogs that use Google's Blogger system, online games, etc.) are operated by software residing on company's servers, it is easy to log the details of user interactions. Each web server keeps detailed information on all visits to a given site. A separate category of software and services exemplified by Google Analytics has emerged to help fine-tune the design of a website or blog.

Today social-media companies make available to their users some of the recorded information about visitors' interactions with the sites, blogs, or accounts they own; the companies also provide interactive visualizations to help people figure out which published items are most popular, and where their visitors are coming from. However, usually the companies keep the really detailed records to themselves. Therefore, if you are one of the few social scientists working inside giants such as Facebook or Google, you have an amazing advantage over your colleagues in the academy. You can ask questions others can't. This could create a real divide in the future between academic and corporate researchers. While the latter will be able to analyze social and cultural patterns on both supermicro and supermacro levels, the former will have only a normal "lens," which can neither get extremely close nor zoom out to a planetary view.

Who benefits from the analysis of the cultural activities of hundreds of millions of people?Automatic targeting of ads on Google networks, Facebook, and Twitter already uses both texts of users' posts or emails and other data, but learning how hundreds of millions of people interact with billions of images and social-network videos could not only help advertisers craft more-successful visual ads but also help academics raise new questions.

Can we analyze the code of software programs? It's not as easy as you may think. The code itself is "big data."

Early software programs such as 1970s video games were relatively short. However, in any contemporary commercial web service or operating system, the program code will simply be too long and complex to allow you to read and interpret it like a short story. While Windows NT 3.1 (1993) was estimated to contain four to five million source lines of code, Windows XP (2001) had some 40 million. MAC OS turned out even bigger, with OS X 10.4 (2005) code at 86 million lines. The estimated number of lines in Adobe Creative Suite 3 (which includes Photoshop, Illustrator, and a number of other popular applications to produce media) is 80 million.

The gradual move of application software to the web also brings with it a new set of considerations. Web services, apps, and dynamic sites often use multi-tier software architecture, where a number of separate modules (for example, a web client, application server, and a database) work together. Especially in the case of large commercial sites like amazon.com, what the user experiences as a single web page may involve continuous interactions between dozens or even hundreds of separate software processes.

The complexity and distributed architecture of contemporary large-scale software poses a serious challenge to the idea of "reading the code." However, even if a program is relatively short and a cultural critic understands exactly what the program is supposed to do, this understanding of the logical structure of the program can't be translated into envisioning the actual user experience.

The attraction of "reading the code" approach for the humanities is that it creates an illusion that we have a static and definite text we can study—i.e., a program listing. But we have to accept the fundamental variability of the actual "software performances." So rather than analyzing the code as an abstract entity, we may instead trace how it is executed, or "performed," in particular user sessions. To use the terms from linguistics, rather than thinking of the code as language, we may want to study it as speech.

Some researchers, like Mark Marino and others working in "critical code studies," have been promoting nuanced, theoretically rigorous, and rich ideas about what it means to "read the code," so my critique is aimed only at a naïve version of the idea that I sometimes encounter in the humanities.

The development of methods to study contemporary software in a way that can be discussed in articles, conferences, and public debates by nonprogrammers, is a key task for software studies. However, given both the complexity of software systems and the fact that, at least at present, only a very small number of media and cultural researchers are trained in software engineering, I don't expect that we can solve this problem in a short time.

And yet, confronting it is crucial, not just for the academy but also for society at large. How can we discuss publicly the decisions made by Google Search algorithms, or Facebook's algorithms controlling what is shown on our news feeds? Even if these companies made all their software open source, its size and complexity would make public discussion very challenging.

While some of the details from popular web companies are published in academic papers written by researchers working at these companies, only people with computer-science and statistics backgrounds can understand them. Moreover, many popular software services use machine-leaning technology that often results in "black box" solutions. (While the software achieves desired results, we don't know the rules it follows.)

As more and more of our cultural experiences, social interactions, and decision making are governed by large-scale software systems, the ability of nonexperts to discuss how these systems work becomes crucial. If we reduce each complex system to a one-page description of its algorithm, will we capture enough of software behavior? Or will the nuances of particular decisions made by software in every particular case be lost?

The role of software studies is not to answer these and many other questions about our new interactive world, but rather to articulate them and offer examples of how they can be approached. And to encourage people across all disciplines to think about how software changes what they study and how they study it.

In the Phototrails project (phototrails.net), created by myself, Nadav Hochman, and Jay Chow, we visualized patterns in the use of Instagram across 2.3 million photos from 13 global cities. In the paper accompanying the project, we attempted to combine two mirror sides of software studies—thinking about software interfaces and how they influence what we do and at the same time studying large-scale behaviors of many software users. One of the key questions we raised: How much of the differences among the cities can we find, given that everybody uses the same Instagram app that comes with its own strong "message" (all photos have the same square size, and all users have access to the same set of build-in filters to make their photos more aesthetic in the same ways). While we did find small but systematic differences in the photos from each city, the use of Instagram software itself was remarkably consistent.

How does the software we use influence what we express and imagine? Shall we continue to accept the decisions made for us by algorithms if we don't know how they operate? What does it mean to be a citizen of a software society? These and many other important questions are waiting to be analyzed.
