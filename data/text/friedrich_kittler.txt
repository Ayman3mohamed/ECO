Codes—by name and by matter—are what determine us today, and what we must articulate if only to avoid disappearing under them completely. They are the language of our time precisely because the word and the matter code are much older, as I will demonstrate with a brief historical regression. And have no fear: I promise to arrive back at the present. Imperium Romanum Codes materialize in processes of encryption, which is, according to Wolfgang Coy’s elegant defi nition, “from a mathematical perspective a mapping of a fi - nite set of symbols of an alphabet onto a suitable signal sequence.”1 This defi nition clarifi es two facts. Contrary to current opinion, codes are not a peculiarity of computer technology or genetic engineering; as sequences of signals over time they are part of every communications technology, every transmission medium. On the other hand, much evidence suggests that codes became conceivable and feasible only after true alphabets, as opposed to mere ideograms or logograms, had become available for the codifi cation of natural languages. Those alphabets are systems of identically recurring signs of a countable quantity, which map speech sounds onto letters more or less one- to- one and, hopefully, completely. A vocalic alphabet of a type such as Greek,2 justly praised for being the “fi rst total analysis of a language,”3 does appear to be a prerequisite for the emergence of codes, and yet, not a suffi cient one. For what the Greeks lacked (leaving out of consideration sporadic allusions in the work of Aischylos, Aenas, Tacticus, and Plutarch to the use of secret writing4 was that second prerequisite of all coding, namely, developed communications technology. It is anything but coincidental that our reports of the fi rst secret message systems coincide with the rise of the Roman Empire. In his Lives of the Caesars, Suetonius—who himself served as secret scribe to a great emperor—recounts discovering encrypted letters among the personal fi les left behind by both the divine Caesar and the divine Augustus. Caesar contented himself with moving all the letters of the Latin alphabet by four places, thus writing D instead of A, E instead of B, and so forth. His adoptive son Augustus, by contrast, is reported to have merely skipped one letter, but a lack of mathematical discernment led him to replace the letter X, the last in his alphabet, by a double A.5 The purpose was obvious: When read aloud by those not called upon to do so (and Romans were hardly the most literate of people), a stodgy jumble of consonants resulted. And as if such innovations in matters of encryption were not suffi cient, Suetonius attributes to Caesar another invention immediately beforehand—that of having written in several columns, or even separate pages, reports to the Roman Senate on the Gallic campaign. Augustus is credited with the illustrious deed of creating, with riders and relay posts, Europe’s fi rst strictly military express- mail system.6 In other words, the basis on which command, code, and communications technology coincided was the Empire, as opposed to merely the Roman Republic or shorthand writers like Cicero. Imperium is the name of both the command and its effect: the world empire. “Command, control, communications, intelligence” was also the Pentagon’s imperial motto until very recently, when, due to the coincidence of communication technologies and Turing machines it was swapped for C4 —“command, control, communication, computers”—from Orontes to the Scottish headland, from Baghdad to Kabul. It was the case, however, that imperia, the orders of the Emperor, were also known as codicilla, the word referring to the small tablets of stripped wood coated with wax in which letters could be inscribed. The etymon codex for its part—caudex in Old Latin and related to the German verb hauen (to hew)—in the early days of the Empire assumed the meaning of “book,” whose pages could, unlike papyrus scrolls, for the fi rst time be leafed through. And that was how the word that interests us here embarked on its winding journey to the French and English languages. From Imperator Theodosius to Empereur Napoleon, “code” was simply the name of the bound book of law, and codi- fi cation became the word for the judicial- bureaucratic act needed to arrest in a single collection of laws the torrents of imperial dispatches or commands that for centuries had rushed along the express routes of the Empire. Message transmission turned into data storage,7 pure events into serial order. And even today the Codex Theodosius and Codex Iustinianus continue to bear a code of ancient European rights and obligations in those countries where Anglo- American common law does not happen to be sweeping the board. In the Corpus Iuris, after all, copyrsights and trademarks are simply meaningless, regardless of whether they protect a codex or a code. Nation- States The question that remains is why the technical meaning of the word “code” was able to obscure the legal meaning to such a degree. As we know, contemporary legal systems regularly fail to grasp codes in the fi rst place and, in consequence, to protect them, be it from robbers and purchasers or, conversely, from their discoverers and writers. The answer seems to be simple. What we have been calling a code since the secret writings of Roman emperors to the arcana imperii of the modern age was known as a “cipher” from the late Middle Ages onward. For a long time the term code was understood to refer to very different cryptographic methods whereby words could still be pronounced, but obscure or innocuous words simply replaced the secret ones. Cipher, by contrast, was another name for the zero, which at that time reached Europe from India via Baghdad and put sifr (Arabic: “emptiness”) into mathematical- technical power. Since that time, completely different sets of characters have been devised (in sharp contrast to the invention of Greek for speech sounds and numbers: on one side of language the alphabet of the people, on the other the numbers of the bearers of secrets—the name of which spelled the Arabic sifr once again. Separate character sets, however, are productive. Together they brew wondrous creatures that would never have occurred to the Greeks or Romans. Without modern algebra there would be no encoding; without Gutenberg’s printing press, no modern cryptology. In 1462 or 1463, Battista Leone Alberti, the inventor of linear perspective, was struck by two plain facts. First, that the frequency of occurrence of phonemes or letters varies from language to language, a fact which is proved, according to Alberti, by Gutenberg’s letter case. From the frequency of shifted letters as they were written by Caesar and Augustus, cryptanalysis can heuristically derive the clear text of the encrypted message. Second, it is therefore insuffi cient to encrypt a message by shifting all the letters by the same number of places. Alberti’s proposal that every new letter in the clear text be accompanied by an additional place- shift in the secret alphabet was followed up until World War II.8 One century after Alberti, François Viète, the founder of modern algebra, and also a cryptologist in the service of Henry IV, intertwined number and letter more closely still. Only since Viète have there been equations containing unknowns and universal coef- fi cients written with numbers encoded as letters.9 This is still the work method of anybody who writes in a high- level programming language that likewise allocates variables (in a mathematically more or less correct manner) to alpha numeric signs, as in equations. On this basis—Alberti’s polyalphabetic code, Viète’s algebra, and Leibniz’ differential calculus—the nation- states of the modern age were able to technically approach modernity. Global Message Traffi c Modernity began, however, with Napoleon. As of 1794, messengers on horseback were replaced by an optical telegraph which remote- controlled France’s armies with secret codes. In 1806, the laws and privileges surviving from the old days were replaced by the cohesive Code Napoléon. In 1838, Samuel Morse is said to have inspected a printing plant in New York in order—taking a leaf from Alberti’s book—to learn from the letter case which letters occurred most frequently and therefore required the shortest Morse signals.10 For the fi rst time a system of writing had been optimized according to technical criteria—that is, with no regard to semantics—but the product was not yet known as Morse code. The name was bestowed subsequently in books known as Universal Code Condensers, which offered lists of words that could be abbreviated for global cable communications, thus reducing the length, and cost, of telegrams, and thereby encrypting the sender’s clear text for a second time. What used to be called deciphering and enciphering has since then been referred to as decoding and encoding. All code processed by computers nowadays is therefore subject to Kolmogorov’s test: Input is bad if it is longer than its output; both are equally long in the case of white noise; and a code is called elegant if its output is much longer than itself. The twentieth century thus turned a thoroughly capitalist money- saving device called “code condenser” into highest mathematical stringency. The Present Day—Turing All that remains to ask is how the status quo came about or, in other words, how mathematics and encryption entered that inseparable union that rules our lives. That the answer is Alan Turing should be well known today. The Turing machine of 1936, as the principle controller of any computer, solved a basic problem of the modern age: how to note with fi nitely long and ultimately whole numbers the real, and therefore typically infi nitely long, numbers on which technology and engineering have been based since Viète’s time. Turing’s machine proved that although this task could not be accomplished for all real numbers, it was achievable for a crucial subset, which he dubbed computable numbers.11 Since then a fi nite quantity of signs belonging to a numbered alphabet which can, as we know, be reduced to zero and one, has banished the infi nity of numbers. No sooner had Turing found his solution than war demanded its cryptanalytical application. As of spring 1941 in Britannia’s Code and Cipher School, Turing’s proto- computers almost decided the outcome of the war by successfully cracking the secret codes of the German Wehrmacht, which, to its own detriment, had remained faithful to Alberti. Today, at a time when computers are not far short of unravelling the secrets of the weather or the genome—physical secrets, that is to say, and increasingly often biological ones, too—we all too often forget that their primary task is something different. Turing himself raised the question of the purpose for which computers were actually created, and initially stated as the primary goal the decoding of plain human language: Of the above possible fi elds the learning of languages would be the most impressive, since it is the most human of these activities. This fi eld seems, however, to depend rather too much on sense organs and locomotion to be feasible. The fi eld of cryptography will perhaps be the most rewarding. There is a remarkably close parallel between the problems of the physicist and those of the cryptographer. The system on which a message is enciphered corresponds to the laws of the universe, the intercepted messages to the evidence available, the keys for a day or a message to important constants which have to be determined. The correspondence is very close, but the subject matter of cryptography is very easily dealt with by discrete machinery, physics not so easily.12 Conclusions Condensed into telegraphic style, Turing’s statement thus reads: Whether everything in the world can be encoded is written in the stars. The fact that computers, since they too run on codes, can decipher alien codes is seemingly guaranteed from the outset. For the past three- and- a- half millennia, alphabets have been the prototype of everything that is discrete. But it has by no means been proven that physics, despite its quantum theory, is to be computed solely as a quantity of particles and not as a layering of waves. And the question remains whether it is possible to model as codes, down to syntax and semantics, all the languages that make us human and from which our alphabet once emerged in the land of the Greeks. This means that the notion of code is as overused as it is questionable. If every historical epoch is governed by a leading philosophy, then the philosophy of code is what governs our own, and so code—harking back to its root, “codex”—lays down the law for one and all, thus aspiring to a function that was, according to the leading philosophy of the Greeks, exercised exclusively by Aphrodite.13 But perhaps code means nothing more than codex did at one time: the law of precisely that empire which holds us in subjection and forbids us even to articulate this sentence. At all events, the major research institutions that stand to profi t most from such announcements proclaim with triumphant certainty that there is nothing in the universe, from the virus to the Big Bang, which is not code. One should therefore be wary of metaphors that dilute the legitimate concept of code, such as when, for instance, in the case of DNS, it was not possible to fi nd a one- to- one correspondence between material elements and information units as Lily Ray discovered in the case of bioengineering. As a word that in its early history meant “displacement” or “transferral”—from letter to letter, from digit to letters, or vice versa—code is the most susceptible of all to faulty communication. Shining in the aura of the word code one now fi nds sciences that do not even master their basic arithmetic or alphabet, let alone cause something to turn into something different as opposed to merely, as in the case of metaphors, go by a different name. Therefore, only alphabets in the literal sense of modern mathematics should be known as codes, namely one- to- one, fi nite sequences of symbols, kept as short as possible but gifted, thanks to a grammar, with the incredible ability to infi nitely reproduce themselves: Semi- Thue groups, Markov chains,14 Backus- Naur forms, and so forth. That, and that alone, distinguishes such modern alphabets from the familiar one that admittedly spelled out our languages and gave us Homer’s poetry15 but cannot get the technological world up and running the way computer code now does. For while Turing’s machine was able to generate real numbers from whole numbers as required, its successors have—in line with Turing’s daring prediction—taken command.16 Today, technology puts code into the practice of realities, that is to say: it encodes the world. I cannot say whether this means that language has already been vacated as the House of Existence. Turing himself, when he explored the technical feasibility of machines learning to speak, assumed that this highest art, speech, would be learned not by mere computers but by robots equipped with sensors, effectors, that is to say, with some knowledge of the environment. However, this new and adaptable environmental knowledge in robots would remain This means that the notion of code is as overused as it is questionable. If every historical epoch is governed by a leading philosophy, then the philosophy of code is what governs our own, and so code—harking back to its root, “codex”—lays down the law for one and all, thus aspiring to a function that was, according to the leading philosophy of the Greeks, exercised exclusively by Aphrodite.13 But perhaps code means nothing more than codex did at one time: the law of precisely that empire which holds us in subjection and forbids us even to articulate this sentence. At all events, the major research institutions that stand to profi t most from such announcements proclaim with triumphant certainty that there is nothing in the universe, from the virus to the Big Bang, which is not code. One should therefore be wary of metaphors that dilute the legitimate concept of code, such as when, for instance, in the case of DNS, it was not possible to fi nd a one- to- one correspondence between material elements and information units as Lily Ray discovered in the case of bioengineering. As a word that in its early history meant “displacement” or “transferral”—from letter to letter, from digit to letters, or vice versa—code is the most susceptible of all to faulty communication. Shining in the aura of the word code one now fi nds sciences that do not even master their basic arithmetic or alphabet, let alone cause something to turn into something different as opposed to merely, as in the case of metaphors, go by a different name. Therefore, only alphabets in the literal sense of modern mathematics should be known as codes, namely one- to- one, fi nite sequences of symbols, kept as short as possible but gifted, thanks to a grammar, with the incredible ability to infi nitely reproduce themselves: Semi- Thue groups, Markov chains,14 Backus- Naur forms, and so forth. That, and that alone, distinguishes such modern alphabets from the familiar one that admittedly spelled out our languages and gave us Homer’s poetry15 but cannot get the technological world up and running the way computer code now does. For while Turing’s machine was able to generate real numbers from whole numbers as required, its successors have—in line with Turing’s daring prediction—taken command.16 Today, technology puts code into the practice of realities, that is to say: it encodes the world. I cannot say whether this means that language has already been vacated as the House of Existence. Turing himself, when he explored the technical feasibility of machines learning to speak, assumed that this highest art, speech, would be learned not by mere computers but by robots equipped with sensors, effectors, that is to say, with some knowledge of the environment. However, this new and adaptable environmental knowledge in robots would remain obscure and hidden to the programmers who started them up with initial codes. The so- called “hidden layers” in today’s neuronal networks present a good, if still trifl ing, example of how far computing procedures can stray from their design engineers, even if everything works out well in the end. Thus, either we write code that in the manner of natural constants reveals the determinations of the matter itself, but at the same time pay the price of millions of lines of code and billions of dollars for digital hardware; or else we leave the task up to machines that derive code from their own environment, although we then cannot read—that is to say: articulate—this code. Ultimately, the dilemma between code and language seems insoluble. And anybody who has written code even only once, be it in a high- level programming language or assembly, knows two very simple things from personal experience. For one, all words from which the program was by necessity produced and developed only lead to copious errors and bugs; for another, the program will suddenly run properly when the programmer’s head is emptied of words. And in regard to interpersonal communications, that can only mean that self- written code can scarcely be passed on with spoken words. May myself and my audience have been spared such a fate in the course of this essay.


My semi-technical introduction to computer graphics will, however, provide only a half-answer, one that, in particular, cannot address the necessary comparison between paintings and computer images or between subtractive and additive color mixing. Simplified accordingly, a computer image is a two-dimensional additive mixture of three base colors shown in the frame, or parergon, of the monitor housing. Sometimes the computer image as such is less apparent, as in the graphic interface of the newfangled operating systems, sometimes rather more, as in "images" in the literal sense of the word. At any rate, the generation of 2000 likely subscribes to the fallacy-backed by billions of dollars-that computers and computer graphics are one and the same. Only aging hackers harbor the trace of a memory that it wasn't always so. There was a time when the computer screen's display consisted of white dots on an amber or green background, as if to remind us that the techno-historical roots of computers lie not in television, but in radar, a medium of war.
The computer image derives precisely this addressability from early-warning systems, even if it has replaced the polar coordinates _ I of the radar screen with Cartesian coordinates. In contrast to the semi-analog medium of television, not only the horizontal lines but also the vertical columns are resolved | lI . . into basic units. The mass of these so-called- 1 I "pixels" forms a two-dimensional matrix - that assigns each individual point of the " i* ] image a certain mixture of the three base colors: red, green, and blue. The discrete, or digital, nature of both the geometric coordinates and their chromatic values makes possible the magical artifice that separates computer graphics from film and television. Now, for the first time in the history of optical media, it is possible to address a single pixel in the 849th row and 720th column directly without having to run through everything before and after it. The computer image is thus prone to falsification to a degree that already gives television producers and ethics watchdogs the shivers; indeed, it is forgery incarnate. It deceives the eye, which is meant to be unable to differentiate between individual pixels, with the illusion or image of an image, while in truth the mass of pixels, because of its thorough addressability, proves to be structured more like a text composed entirely of individual letters. For this reason-and for this reason only-it is no problem for a computer monitor to switch between text and graphics modes. The twofold digitality of coordinates and color value, however, creates certain problem areas, of which at least three should be mentioned.
Third, the digitality of computer graphics creates a problem unknown to computer music. In an essay on time axis manipulation, I have previously tried to show the leeway produced by the fact that the digital sampling of any given musical sequence falls into three elements (a triad is familiar to us through Giuseppe Peano's theory of natural numbers): an event or state of a millisecond's duration, its predecessor, and its successor.2 These three can be integrated or differentiated, exchanged or scrambled until the limits of modern academic and popular music are truly explored. In principle-and that means, unfortunately, given an exponentially higher processing time-these tricks could be adapted from digital music's single dimension to the two dimensions of digital images. The result, however, tends to be so chaotic that it is as if perception were regressing to pure sensation a la David Hume or Kaspar Hauser. The reason for this is as fundamental as it is non-trivial. Every image (in the sense of art, not of mathematics) has a top and a bottom, a left and a right. Pixels, insofar as they are constructed algebraically as two-dimensional matrices and geometrically as orthogonal grids, necessarily have more than one neighbor. In the heroic beginnings of computer science, great mathematicians had to begin by formulating truisms, whence arose W. Ross Ashby's and John von Neumann's concepts of neighboring elements. In the former, a given element is considered to be surrounded only by a cross of neighbors: above, below, left, and right; in the latter, it is surrounded by a square of the above-mentioned orthogonal elements plus four additional diagonal neighbors. A difference that could perfectly describe, if you like, the difference between the urban fabrics of Manhattan and Tokyo, respectively.
Heidegger posed the riddle of perception thus: "in the appearing of things, never do we, either preliminarily or essentially, perceive an onrush of sensations."3 For beings that dwell in language, anything seen or heard shows itself always already as something. For computer-supported image analysis, however, this something-assomething remains a distant theoretical goal, the achievement of which is not even assured. Therefore I would postpone the question of automatic image analysis for symposia on perception to take place not sooner than a decade from now, and limit myself in the following to the problem of automatic image synthesis. I am not concerned, then, with how computers simulate optical perception, but rather only with how they deceive us. For it seems to be precisely this exorbitant capacity that elevates the medium of the computer above all optical media in Western history.
Computer graphics are to these optical media what the optical media are to the eye. Just as the camera lens, literally as hardware, simulates the eye, which is literally wetware. so does software, as computer graphics, simulate hardware. The optical laws of reflection and refraction remain in effect for output devices such as monitors or LCD screens, but the program whose data directs these devices transposes such optical laws as it obeys into algebraically pure logic. These laws are generally, it should be noted from the outset, by no means all the optical laws valid for fields of vision and surfaces, shadows and effects of light; what is played out are these selected laws themselves and not, as in the optical media, just the effects they produce. It's no wonder, then, that art historian Michael Baxandall can go so far as to suggest that computer graphics provide the logical space of which any given perspective painting forms a more or less rich subset.
Conversely, computer graphics, because it is software, consists of algorithms and only of algorithms. The optimal algorithm for automatic image synthesis can be determined just as easily as non-algorithmic image synthesis. It would merely have to calculate all optical, i.e. electromagnetic, equivalencies that quantum electrodynamics recognizes for measurable spaces, for virtual spaces as well; or, to put it more simply, it would have to convert Richard Feynman's three-volume Lectures on Physics into software. Then a cat's fur, because it creates anisotropic surfaces, would shimmer like cat's fur; then streaks in a wine glass, because they change their refraction index at each point, would turn the lights and things behind them into complete color spectra. Theoretically, nothing stands in the way of such miracles. Universal discrete machines, which is to say, computers, can do anything so long as it is programmable. But it is not just in Rilke's Malte Laurids Brigge but also in quantum electrodynamics that "realities are slow and indescribably detailed."7 The perfect optics could be programmed just barely within a finite time, but, because of infinite monitor waiting times, would have to put off rendering the perfect image. Computer graphics are differentiated from the cheap real-time effects of the visual entertainment media by a capacity to waste time that would rival that of good old painters if its users were just more patient. It is only in the name of impatience that all existing computer graphics are based on idealizations-a term that functions here, unlike in philosophy, as a pejorative.

In all historical accuracy I shall begin with raytracing, if only because it, for the best or worst reasons in the world, is much older than the radiosity algorithm. As Axel Roch will soon make public, the concept of raytracing derives not at all from computer graphics, but rather from its military predecessor: the tracking of enemy airplanes with radar. And as the computer graphics expert Alan Watt has recently shown, raytracing is in fact even more venerable. The first light ray whose refraction and reflections generated a virtual image was constructed in the year of our Lord 1637 by a certain Rene Descartes.9 Eighteen years earlier, in the wartime of November 1619, Descartes had received one illumination and three dreams. The illumination was about a wondrous science-perhaps the analytic geometry he would go on to develop later. The dreams, however, began with a storm that spun Descartes, who was lame on his right side, around his own left leg three or four times. I suspect, however, that the dream and the science are one and the same. In the dream the subject becomes an unextendable point or, better, midpoint, around which one's own body, as a three-dimensional res extensa, describes the geometric figure of a circle. Cartesian philosophy, as is well known, deals with the res cogitans and the res extensa; as is far less well known, analytic geometry deals with algebraically describable movements or surface areas. Descartes made it possible, for the first time in the history of mathematics, not to produce figures like the circle as the drawn likeness of a celestial-geometrical given but rather to construct them as functions of an algebraic variable. The subject as res cogitans took a wild ride, so to speak, through all the functional values of an equation, until in Descartes's initial dream of 1619 the circle (or, in Miinchhausen's ride on the cannonball, the parabola) was described.

To be sure, Heron of Alexandria had already formulated the law of reflection, Willibrord Snell the law of refraction. It remained to Descartes, however, to piece together the path of a single ray of light through the repeated application of both laws. The Cartesian subject comes about through self-application, or, to put it in the terms of computer science, through recursion. Precisely for this reason, Cartesian raytracing never inspired any painter, let alone any optical analog medium. Only computers and, more precisely, computer languages that allow for recursive functions have the processing power to even trace the countless alternative cases or fates of a single light ray in a virtual space full of virtual surfaces. Raytracing programs begin, in the most elementary case, by defining the computer screen as a two-dimensional window onto a virtual three-dimensionality. Then, two iteration loops follow all the lines and columns of this screen until the ray of vision of a virtual eye situated in front of the screen has reached all the pixels. These virtual rays, though, keep wandering behind the pixels in order to explore the various different outcomes. Most of these have the fortune not to collide with a surface, and thus can quickly execute their task of rendering a mere background color such as that of the sky. Other rays, however, find themselves trapped in a transparent glass globe like Descartes's, where they would be subject to an endless series of refractions and reflections if the impatience of computer graphics programs did not limit the maximum allowable recursions. This is necessary if only because a light ray, should it play between two parallel and perfect mirrors, would never stop, while algorithms are all but defined by a finite use of time.

Radiosity is consequently, in contrast to raytracing, an algorithm born of necessity. Only when seen in its formal elegance can integration be defined as the reverse function of differentiation, for the bitter empirical and numerical truth is that it consumes dramatically higher processing time. Radiosity programs have only become feasible since they have stopped promising to solve their linear equation system in a single run-through.1 In more prosaic terms: one starts up the algorithm, contemplates the as yet completely black screen, takes one of the coffee breaks so famous among computer programmers, then returns after one or two hours to have a look at the first passable results of the global light energy distribution. What so-called nature can accomplish in nanoseconds with its parallel calculation drives its alleged digital equivalent to overload.

Had I promised mere recipes instead of a semi-technical introduction to computer graphics, this short text could end here. Fans of interiors would download some radiosity programs, while fans of the open horizon would surf the Net for some raytracing programs. And now that, at least with LINUX, we have the Blue Moon Rendering Tools, the very decision has become moot. This software, no less wondrous than a blue moon, calculates virtual image worlds in the first run-through following global dependencies in the sense of radiosity, but in the second runthrough follows local singularities in the sense of raytracing. It thus promises a coincidentia oppositorum, which cannot be a matter of simple addition given all that has been said above. It would be going too far afield if I were to try to explain why, in the case of such two-step processes, not only the second step must orient itself to the first but, what is nearly impossible, the first must already orient itself to the second. Otherwise, the four possible cases of optical energy transmission couldn't possibly all be taken into consideration.  
