First we had media art. In the early days of electronic and digital culture media art was an important way of considering relationships between society and technology, suggesting new practices and cultural techniques. It served as an outlet for the critique of the dark side of computer culture's roots in the military-industrial complex; and it suggested numerous utopian and beautiful ways of engagement with technology, new types of interactivity, sensuous interfaces, participative media practices, for instance. However, the more critical, egalitarian and participative branches of media art tended to be overshadowed by the advocacy of a high-tech and high-art version of it. This high-media art conceptually merged postmodern media theories with the techno-imaginary from computersciences and new wave cybernetics. Uncritical towards capitalisms embrace of technology as provider of economic growth and a weirdly paradoxical notion of progress, high-media art was successful in institutionalizing itself and finding the support of the elites but drew a lot of criticism from other quarters of society. It stuck to the notion of the artist as a solitary genius who creates works of art which exist in an economy of scarcity and for which intellectual ownership rights are declared.

In the course of the 1990ies media art was superseded by what I call The Next Layer or, for help of better words, Open Source Culture. I am not claiming that the hackers who are the key protagonists of Open Source Culture are the new media artists. Such a claim would be rubbish as their work, their ways of working and how it is referenced is distinct from media art. I simply say that media art has become much less relevant through the emergence of The Next Layer. In the Next Layer many more protagonists come together than in the more narrowly defined field of media art. It is much less elitist and it is not based on exclusivity but on inclusion and collaboration. Instead of relying on ownership of ideas and control of intellectual property Open Source Culture is testing the limits if a new egalitarian and collaborative culture.

In the following paragraphs I would like to map out some of the key components of Open Source Culture. It has been made possible by the rise of Free, Libre and Open Source Software. Yet Open Source Culture is about much more than just writing software. Like any real culture it is based on shared values and a community of people.

Open Source Culture is about creating new things, be they software, artefacts or social platforms. It therefore embraces the values inherent to any craft and it cherishes the understanding and mastery of the materials and the production processes involved. Going beyond craftmanship and being 'open source', it advocates free access to the means of production (instead of just "ownership" of them). Creativity is not just about work but about playfulness, experimentation and the joy of sharing. In Open Source Culture everybody has the chance to create immaterial and material things, express themselves, learn, teach, hear and be heard.

Open Source Culture is not a tired version of enforced collectivism and old fashioned speculations about the 'death of authorship'. It is not a culture where the individual vanishes but where the individual remains visible and is credited as a contributor to a production process which can encompass one, a few or literally thousands of contributors.

Fundamental to Open Source Culture's value system is the belief that knowledge should be in the public domain. What is generally known by humans should be available to all humans so that society as a whole can prosper. For most parts and whereever possible, this culture is based on a gift economy. Each one gets richer by donating their work to a growing pool of publicly available things. This is not a misguided form of altruism but more like a beneficial selfishness. Engaged in a sort of friendly competition everyone is pushing the whole thing forward a bit by trying to do something that is better, faster, more beuatiful or imaginative. Open Source Culture is a culture of conversation and as such based on multiple dialogues on different layers of language, code and artefacts. But the key point is that the organisation of labour is based on the self-motivated activity of many individuals and not on managerial hierarchies and 'shareholder value'.

Open Source Culture got a big push forward with the emergence of Linux and the Internet but we shouldn't forget that it has much deeper roots. History didn't start with Richard Stallmans problems with a printer driver. The historic roots could be seen as going back to the free and independent minded revolutionary artists and artisans in 19th century. More recently, it is based on post-World-War-II grassroots anti-imperialist liberation movements, on bottom-up self-organised culture of the new political movements of the 1960ies and 1970ies such as the African American civil rights movements, feminisim, lesbian, gay, queer and transgender movements, on the first and second wave of hacker culture, punk and the DIY culture, squatter movements, and the left-wing of critical art and media art practices.

In terms of the political economy, Open Source Culture could mark an important point of departure, by liberating the development of new technologies from being dictated by capital. The decision of what should be developed for which social goals is taken by the developers themselves. Technological development is not driven by greed but by deep intrinsic motivations to create things and to be recognized for ones contribution. Despite that, Open Source Culture is not an anti-capitalist ideology per se but has the potential to change capitalism from within and is already doing so.

Open Source Culture needs to be constantly aware of capitalisms propensity to adapt, adopt, co-opt and subjugate progressive movements and ideas to its own goals. The 'digital revolution' was already stolen once by the right-wing libertarians from Wired and their republican allies such as Newt Gingrich and the posse of American cyber-gurus from George Gilder to Nicholas Negroponte. More recently adept Open Source Capitalists have used terms such as Web 2.0 and social software to disguise the fact that what those terms are said to describe has emerged from open source culture and the net culture of the 1990ies and the early 2000s. Once more the creativity of the digital masses is exploited by alliances between new and old tycoons. The Next Layer emerges at a time when capitalism is stronger than ever before and it emerges at the very heart of it. This is the beauty of it. It cannot be described in a language of mainstream and underground. Open Source Culture is the new mainstream which is what capitalist media are doing their best to hide, scared by the spectre of communism as well as commonism. We don't need to ressort to the language of the Cold War and its dichotomies, howver.

The Next Layer contains not only a promise but also a threat. It emerges at a time when the means of suppression and control have been increased by rightwing leaders who try to scare us into believing we were engaged in an endless 'war on terror'. With their tactics they have managed to speed up the creation of a technological infrastructure for a society of control. The general thrust of technological development is coming from inside a paranoiac mindset. 25 years of neo-liberalism in the American lead empire have degraded civil liberties and human values. The education system has been turned into a sausage factory where engineers are turned out who construct their own digital panopticons. Scary new nano- and bio-technologies are created in secret laboratories by Big Science. And the bourgeioise intelligentsia meanwhile has stood still and does not recognize the world any more but still controls theatres, publishing and universities. In this situation it is better if Open Source Culture is not recognized as a political movement. The Next Layer will find ways of growing and expanding stealthily by filling the niches, nooks and crannies of a structurally militant and imperialist repressive regime from which, given time, it will emerge like a clear spring at the bottom of a murky glacier.



Technological determinism is the belief that science and technology are autonomous and the main force for change in society. It is neither new nor particularly original but has become an immensely powerful and largely orthodox view of the nature of social change in highly industrialised societies. In this paper I analyse the presence of technological determinism in general discourses about the relationship between social change and science and technology. I show that techno-determinist assumptions underlie developments in what is called technoscience, a term describing new practices in science and technology with particular relevancy for the related fields of genetic engineering and computer science. Those areas create a specific set of narratives, images and myths, which is called the techno-imaginary. The thesis of my paper is that the discourse on media art uncritically relies on many elements of the techno-imaginary. A specific type of media art, which is identifiable with people, institutions and a period in time, is particularly engaged with the tropes of the techno-imaginary. This strand, which I call high media art, successfully engaged in institution building by using techno-determinist language. It achieved its goals but was short lived, because it was built on false theoretical premises. It made wrong predictions about the future of a 'telematic society' and a 'telematic consciousness'; and it missed the chance to build the foundations of a theory of media art because it was and is contaminated by the false assumptions behind technological determinism.
Science and technology are widely understood to be the major, if not the only forces which cause social change. This opinion is called technological determinism. According to this view science and technology are autonomous, which means that they develop according to their own internal logic only. Once new technologies have been invented and are released into the world they have an irresistible impact on the social world. This implies that history is largely a result of the impact of new technologies. By denying the importance of other social forces such as politics and the economy human agency is effectively cancelled as a factor in the shaping of history. In the field of art a new domain has been developed which is variously called media art, digital art or just new media. This field has deeper historical roots but has gained major significance only over the past 25 years. Within this area, which is very diverse and comprises a variety of practices and approaches, a particular discourse has become dominant. I call it 'high media art'. Its ascendancy started in the 1980s and peaked by the mid 1990s. Its proponents used specific narrative strategies which were highly successful in drawing attention to the field and building institutions devoted exclusively to high media art. That discourse on high media art claimed a radical break with the past and a transgression of all other art forms. It took the material basis of its practice, the use of new media technologies and in particular the computer, as major justification for its claims. It presented itself as an avant-garde, not unlike the classical avant-garde of the 1920s, which employed high-technology to create a new aesthetics. This new aesthetics was tied into postmodern theories as well as the idea of a three-dimensional cyberspace, and it borrowed freely from the myths of computer science. Artists produced works which uncritically repeated the narrative strategies of artifical intelligence and artificial life. The techno-imaginary of the 'closed world' (Edwards 1996), developed at a time when America fought ideoligical battles with its nuclear enemy, the Soviet Union, still provides the principles of our own imaginary futures (Barbrook 2005). The media theory of McLuhan, hardened into an ideology, McLuhanism, provides the intellectual framework for high media art in the mid-1990s combined with the fashionable thesises of postmodernism about the immateriality of the world. The discourse of high media art was successful in institution building but compromised by technological determinism. I will show that technological determinism in high media art isn't just a question of interpretation or opinion but foundational for the field, as a major influence on the creation of works and the theories which came with it. Instead of taking a critical position high media art only illustrates science and technology and glorifies the aesthetics and ideology of technoscience.
The thesis, which I present in this paper, is based on a literature review which includes relevant theoretical areas, histories of media art, catalogues, articles, web sites and discussions on mailing lists. It is also based on my own experience of 20 years of working in the field as an artist, curator, critique and theorist. My own close involvement in the field over a long period of time is my main motivation for this work with which I hope to explore and analyse some major theoretic deficiencies. As a practitioner I have acquired knowledge of the practice, of the actual making and doing, which is rarely reflected in theoretic texts which are only based on the analysis of other texts. I hope to be able to bring the theory and the practice more closely together. Although the focus of this paper is primarily a critique my aim is to open up, through this critique, possibilities for further work. My analysis of the field is influenced methodologically by The Field of Cultural Production by Pierre Bourdieu (1993). He presents his approach as an alternative to two positions which were dominant at the time of writing, structuralism and post structuralism on one hand, and Marxist inspired critical theory on the other. According to Bourdieu structuralism’s and critical theory’s ways of reading works mutually exclude each other (Bourdieu 1993, 177). (Post)Structuralism favours an internal reading of works, critical theory an external reading. Bourdieu describes structuralism as "more powerful" (ibid., 178), yet criticises it for stripping the reading of the work off any "references to the social or economic conditions of its production (ibid., 178)." External analysis, in contrast, "directly links these works to the social characteristics (the social origins) of their authors or of the groups for whom they were really or potentially destined and whose expectations they are intended to meet (ibid., 180)." The weakness of this approach is, according to Bourdieu, that "understanding the work means understanding the world view of the social group that is supposed to have expressed itself through the artist acting as a sort of medium." In other words, the author is seen as a ventriloquist for his own social background and supposed 'class interest'. But this approach fails to provide means of understanding the structure of the work, its subtleties and poetic motions which are, "the prerequisite for the fulfilling of its function? (ibid., 181)" Bourdieu claims he can overcome the deficiencies of both poststructuralism and critical theory by applying the theory of the field, a "relational or structural mode of thought to the social space of the producers (ibid., 181)." Different fields are characterised by positions and position taking, by writings and writers, art works and artists who are involved in a struggle to carve out their own niche within a specific area.

A key concept in Bourdieu’s theory is contained in the term 'symbolic capital'. Paradoxically, in avant-garde movements of literature or art, those who show the least interest in outward signs of success such as awards, titles and money, accumulate the highest amount of 'symbolic capital'. They receive strong support from a closely-knit group of followers, often other artists or professional insiders (curators, critics). This results in the 'non-economy' of autonomous art. The economic and the symbolic hierarchies cannot be directly mapped onto each other. The poorest, most obscure artists are the most famous ones. If they get successful too quickly, they run danger of loosing their reputation as being relevant, cutting-edge, fresh, and innovative. Bourdieu loosely groups artists according to this perception. There are successful artists who cater to the tastes of the dominant social group. They have money, wealth, but no symbolic capital. There is the consecrated avant-garde, an avant-garde which is already partly absorbed by the system, which has its critics, its recognised names. They are in danger of being seen as selling out. New artists will come and attack their perceived dominance. Only this latest group, by being seen as staying outside heteronomic power structures, is attributed the highest symbolic capital. It acts in a field of 'restricted' cultural production which has hardly any audience and very little quantifiably measurable impact, yet this group is seen as the true avant-garde.1 Bourdieu's description of the 'game' of cultural production clearly has some limitations insofar as it may perfectly describe the French literary and artistic avant-garde of the 19th and 20th century but might not be universally applicable. For instance, the notion of popular culture with its own subcultures and avant-garde is not reflected properly in Bourdieu's theory. Bourdieu's approach is useful but cannot be adopted blindly. Therefore I use other theoretical frameworks in addition to Bourdieu, in particular science studies and critical theory.
Technological determinism is hardly ever formulated as a clearly stated theoretical position but has nevertheless become "an immensely powerful and now largely orthodox view of the nature of social change (Williams 1974, 13)." According to this opinion science and technology are autonomous, their development follows an inherent logic and is independent of influences from society. Science and technology are the main forces that shape social change, therefore history is determined by technological development. Paul N. Edwards calls it the "billiard ball theory of scientifically induced change" (Edwards 1996). According to this metaphor technology impacts on society like a billiard ball and whirls everything around. Social change is conceptualized in a very particular way, namely as a causal relationship between technology, as the origin of the force for change, and society, as its target. Society is the passive receiver of an 'impact' and has no agency in the process. The 'impact of science' is presented as something completely unavoidable, like a force of nature. In this model, science and society are completely separated. Scientists, locked away in citadels of knowledge, conduct research entirely uninfluenced by society. Scientific research is a disinterested pursuit of truth which follows its own internal dynamics only. Scientific progress is based on the strict application of the scientific method alone. New technologies are applications of scientific knowledge - applied science - put to work in the world. The effects of technology are seen as the primary mechanism that shapes history. These are the core believes behind what is called the strong version of technological determinism. It is the content of statements such as that the computer created the information society; or that the steam engine brought about industrial society. In social struggles about new technologies often the opponents also adhere to the belief of techno-determinism, when they vent their anger at a particular technology because they think it is intrinsically bad. There are a number of weaker versions of technological determinism. In those versions, technologies are seen as symptoms of society, as effects rather than as causes. The development of technologies is still seen as largely autonomous, but the impact is less deterministic. Technologies are perceived as being 'instruments' only, they are neither intrinsically good nor bad, they are only neutral tools. Any ethical questions would arise depending on the way of use or abuse of those instruments. As we cannot know which use society will make of a particular technology, unintended consequences might occur, and we cannot predict in which way exactly technology will shape society.
Technological determinism is behind assumptions such as that technological progress is the key to greater prosperity, wealth and security. Technology will solve a wide range of human and social problems. For instance, government administrations believe that the implementation of CCTV surveillance systems will help to prevent crime and contribute to the upkeep of public order. In TV advertisements the ability of gadgets such as the mobile phones is praised to win new friends or find a lover. Yoghurts are advertised as containing a 'scientifically proven formula' to make you slimmer, healthier and more attractive. This emphasis on technology as the harbinger of hope to solve all kinds of social problems is reflected in the way governments have created technology impact assessment centres since the 1970s. The direction of this type of research ignores the possibility that the assumptions behind the basic formula, technology as cause and effect, might be wrong. The real nature of the relationship between technology and society poses some of the most difficult and most unresolved historical and philosophical questions.
The concept of determinism in science has different meanings. It does not relate to the question if science determines society but to another complex of questions. Is matter organised in such a way that deterministic processes can be observed? And can science formulate descriptions or models of those processes which form objective laws of nature? In this sense, science must believe in determinism to a degree, otherwise it could not conduct its activity. "Determinism came down from the skies to earth", wrote Gaston Bachelard (Bachelard 1934/1988, 101). As a psychologist, he reflected on how the scientific spirit formed, and came to the conclusion that the observation of planets and stars was essential in the historic shaping of a scientific mindset. Whereas life on earth is messy and unpredictable, the observation of regular bodies moving in predictable ways enables to shape the expectation that objective laws of nature exist which can be understood and formulated with the help of mathematics and geometry (Bachelard 1934/1988). In the long run, this enabled the development of an exact science by Descartes and Newton. Until recently histories of science presented the development of modern science from there on as an unbroken continuity to more clarity, preciseness and abstraction. But it can also be argued that regarding the being or ontology of the world and the epistemology, the theory of knowledge that we have about it, at the beginning of the scientific project some crucial design decisions have been made. The gap between subject and object, which the ancient Greeks had already thought about, started to be conceptualized in a much more polarized way than ever before. Descartes distinguished between res extensa and res cogitans.
Philosophical interest turns to the subject, to consciousness, to the possibility of cognition and human rationality (Weber 2003, 27). Nature is turned into an object of cognition, in other words, science 'invents' nature as its object. It is incredibly successful in doing so and science gains ever more knowledge about it. But at the same time the divide between human cognition and the world gets bigger. The more we know about it, nature gets ever stranger to us. Nature is the non-self, the outside, the 'other'. Nature becomes conceptualized as lifeless, dead and abstract matter (ibid., 31). As science uses ever more abstract tools and methods it becomes 'constructivist'. This particular way of conceptualizing nature in science which arguably started with Renaissance opened the door to all kinds of ways of intrumentalising and operationalising it. In a movement which should become more fully understood only recently, science emancipates itself from nature and starts inventing or constructing it. But this process of the emancipation of science is slow and takes hundreds of years. The philosophical debates surrounding this process culminate in logical positivism. In the 1920s and 1930s members of the Vienna Circle tried to achieve two main things. They wanted to purge theories of knowledge from meta-physics and make philosophy a scientific way of speaking about the world. This in turn should help to guide science to become more rigidly defined and therefore more objective. Those theoretical goals led to an increased focus on formalized theories of language, logic and mathematic. Philosophical questioning of logical operators should help to find the universal logic of the world. Logical positivism had many important results and is a complex philosophical school but appearantly makes one major false inference. The logic of the operation of the human mind is projected on nature (Bachelard 1934/1988, Weber 2003). This false inference, also called the 'cultural fallacy', continues as mainstream model of understanding to-day and is where the scientific meaning and the social meaning of determinism meet. By saying that science is the only source of objective knowledge it becomes transcendent to society. This is not religious transcendentalism but means that scientific forms of knowledge transcend the historicity of creating knowledge and theories about knowledge. What is once objectively true must always - and everywhere - remain so. There is a philosophical tension between the objectivity of scientific discovery and personal and political freedom within human societies. Early 'natural philosophy', as science was called in Renaissance, freed humans from the dogmatic truth of the church. But this freedom would hundreds of years later found to be threatened by science becoming a dogma itself, a repressive ideology. According to critical theory and science studies the invention of a new concept of nature by science opens the door for its instrumentalisation. The scientific project of gaining knowledge about the 'laws of nature' means to put nature at our disposal, to operationalize and functionalize it. And rational mastery of the forces of nature implies social mastery, the dominance of one social group over another one (Marcuse 1964/1994). The absolute character of scientific knowledge weighs down from sky on the life of people on earth.
Logical positivism gained a defining influence on the philosophy of science in Britain and the USA after WWII. Moreover, the positivists deep engagement with logic and formal thinking contributed to the newly emerging disciplines of computer science and cybernetics. At the same time science had to open up to the possibility of increased indeterminacy, after Heisenberg's discovery of an objective indeterminacy on the level of matter. Ideas of a mechanistic universe have been put aside with quantum theory. Since then, the main questions in the epistemology of science concern relationships between determinacy and indeterminacy.
Critical Theory is inspired by the analytical method which Karl Marx developed when writing Das Kapital, but went further than Marx and could even turn against him (cf. Cox et all 2004, 8). Marx has shown that technologies are embodiments of social relationships (Marx 1957). In capitalist societies technologies, far from being neutral, are developed with specific social relationships in mind. Critical theory, inspired by Marx, sees the technology that we have as a specific type of technology developed under a capitalist economy (Marcuse 1964/1994). According to Herbert Marcuse an ideology of dominance was intrinsic to the development of the scientific worldview from the beginning. Each techno-social system introduced over the last 150 years, the railway, electricity, cars and highways, created "ideology embodied in the production process (Marcuse 1964/1994, 114)." It reorganised the strata of society according to the original vision contained in the design. Marcuse believed that political represssion is not so much a function of ideology but a function of an apparatus which uses people without them being able to see behind the machinery and overcome its heteronomic tendency. Heteronomy, as opposed to autonomy, means that people's lives are determined by outside factors beyond their control. In capitalism, technological progress is specifically set against the negotiating powers of workers. New inventions are designed to rationalize production and to increase worker's productivity in order to maximize profit. By investing into better machines, workers are submitted to a dialectical process of deskilling and reskilling. Marx analysed this tendency correctly even though he observed industrial capitalism in its very early phase. Since the days of Marx, the rationalization of labour continued, culminating in the Fordist factory, and ultimately in fully automated factories. Rationalization is not only carried out by investment into better machines but also by scientific management, also known as Taylorism. In the late 20th century with the help of the computer also other areas of human labour, not just physical work, can be replaced by machines. Ever more sophisticated forms of technological and organisational dominance are developed.
A second insight by Marx, which was also made productive by critical theory, concerns the fetishisation of commodities. By basing the exchange value solely on money the human labour that goes into the production of goods is hidden. The labour, equals human life-times, is not visible in the product anymore. Hiding the origins of commodities enables them being fetishized. The world appears as a world of shiny things, of decontextualized consumer products which nowadays appear all dancing and singing in TV adverts. Marx's insights about commodity fetishism and technologies as embodiments of social relationships has been of defining influence on both critical theory and a branch of science studies called the social shaping of technologies (SST). The social shaping of technology is a line of inquiry that asks why and how nature is made operational in specific ways serving particular interests. SST forces us to rethink what we mean when we speak about technology. Technology is never just technical but combines what is possible in terms of the engineering techniques of a time and what is desireable in a certain socio-historic context. Technologies do not just exist as technical artefacts but imply certain forms of social organisation which they help to create and maintain and on which they also depend. Therefore we should better think of technologies as techno-social artefacts. Those artefacts are not merely things - dead objects - but results of and constituitive for social relationships. The development of technology and capitalism in a mutually dependend interplay has gone on over a considerably long period of time. Techno-social artefacts have been created layer upon layer. Because we have become accustomed to live with and inside techno-social systems created by capitalism, we tend to forget that they are man-made and contingent. Because they have shaped our habitat for such a long time, we see them as a sort of second nature; it is SST's task to unentangle the social content of technologies (MacKenzkie and Wajcman 1985). Because in capitalism the work of people is hidden behind fetishised commodities this task has become so hard. The ideology of technological determinism masks the social content of technology and naturalises both technological progress and the capitalist economy.
This might be due to the influence of Marshal McLuhan who is generally credited as being one of the most, if not the most influential thinker on the influence of (new) media on society in the 20th century. The theory about media and social change which he developed, influenced by Harold Innis, is epitomised by the slogan "the medium is the message". According to McLuhan the way we think is determined by the proportionate relationship of the senses - the sense ratio. He believed that all technologies were extensions of us. As tools such as the knife or the axe were extensions of our body and limbs, media technologies were extensions of our senses and central nervous system.
The introduction of a new medium which favours a particular sense was of profound influence on the patterns of perception and the way we thought. Each new medium signifies a break boundary in human history and history can be presented as a sequence of a few large chapters - from oral culture to script, to print, to electronic culture. When we moved from an oral culture to a culture based on script we exchanged an 'ear for an eye'. With writing we left behind magic and the tribal world. But only with the printing press literacy could fully develop. Modern western society is a direct result of the influence of the printing press which favours the visual sense: "Civilisation is based on literacy because literacy is a uniform processing of a culture by a visual sense extended in space and time by the alphabet (McLuhan 1964/1965, 86)." Literacy is made responsible for the homogenisation of western societies; it created the preconditions for getting people used to the clock; and it automatically led to the violent birth of nation states competing for military and industrial hegemony. From the printing press it was a logical step (a logic inherent to the technology itself) to the Fordist factory, the defining technology of modern society. But then in McLuhan’s history of civilisation, at first unnoticed, with the advent of the telegraph and electric light, then more visibly with the invention of wireless telegraphy, radio and television, the 'electric age' began. The sense ratio once more changed. We exchanged an 'eye for an ear' because electronic media foster an oral culture, and accordingly we moved forward into the past of a tribal society living in a global village
Williams criticises that in McLuhan’s theory "all media operations are in effect de-socialised; they are simply physical events in an abstract sensorium (ibid., 127)." The apparent sophistication in McLuhan's approach is that he pays tribute to the specificity of media and their characteristics. But he does so on the basis of excluding all other factors such as social, cultural, political or ethical decisions made by people who by their very nature would be open to scrutiny and questioning. Whereas the initial formulation that the medium is the message is a simple formalism "the subsequent formulation - 'the medium is the massage' - is a direct and functioning ideology (Williams 1974, 127)." Williams, in 1974, said that McLuhan's particular rhetoric was unlikely to last long. But because this particular ideological representation of technology was coming from the most powerful nation state of the world, it would have its successors (ibid. 128). Richard Barbrook (2005) set out to explore that path. In Imaginary Futures (2005) he shows how McLuhan inspired a discourse which has still a lasting influence. According to Barbrook, in 1964 the 'Commission on the year 2000', also known as Bell commission, tried to formulate a plausible alternative to 'cybernetic communism'. America was still reeling from the Sputnik shock, when the Soviet Union was first capable of sending a communication satellite into orbit. As the Cold War logic locked the nuclear enemies into an arms race, any hot war was not winnable. Therefore the only way of winning the war was by showing that America had the better ideology, that it 'owned' the future. The Bell commission took McLuhan's ideas and re-rendered them in a more rationally sounding way. It created an ideology of McLuhanism which was purged from the more eccentric aspects of its originator.
At the time when the Bell Commission formulated its thesis, The US military was pouring huge resources into artificial intelligence (AI) research. J.C.R Licklider initiated a concerted effort of academic research into computer science funded, largely, by the military. One of the many research programmes funded by Licklider led to the invention of the internet. Other research areas included interaction with a computer via a graphical user interface using first a light-pen, then a mouse, video conferencing and early forms of virtual reality. As Barbrook argues, the military origins of the net and many advancements in computer science are well known, but usually brushed aside as insignificant, thereby obscuring the fact that the imaginary future of the 1960s was still the imaginary future of today. McLuhanism, a theory which was fetishised because it had de-linked itself from its origins, promised the glorious future of a post-industrial information society.
Contemporary technosciences have abandoned the 'correspondence theory' of science which demands a truthful representation of nature. Technoscienes instead construct their objects of study, they produce artefacts and hybrids in the laboratory (or on the computer) and then examine them. This method of constructivism is constituitive for the methodology of technoscience as well as for its understanding of nature (Weber 2003, 132). Over the course of the 20th century technoscience develops a radically new understanding of nature, of mind and of what is life. Natural systems of order and architectures which had been seen as unchangeable become historicised and open for modification. The dynamisation and historisation of the concept of nature implies that nature is becoming dynamic and self-organising. As cognition is increasingly recognized as being constructed, nature itself is also understood as an organising and constructing entity (Weber 2003). This shift is not marked by a break with the modernistic past, but by a radicalisation of some of its tendencies (Weber 2003, 136). It keeps some of modernism's epistemological foundations, which were used by science but not made explicit (tinkering, purposeful manipulation), but it takes them to the extreme and makes them more visible. Technoscience continues the logic of modern science by keeping a distanced relationship to nature, which is founded on a deep distrust of the possibility of gaining direct knowledge of nature and world. As Jutta Weber (2003) puts it, science can only explain how things work, not why and what for. Science does not answer ontological questions, because it is based on a deep ontological split in its very foundation (the cognitive subject vs. lifeless matter). This would not be a problem if the experimental and constructivist character of science was generally acknowledged, as an activity of humans under given socio-historic circumstances. Under such premises science cannot be expected to give answers which are eternally and universally true. This should be seen as a liberation of science from political demands, not as a weakening of its epistemological foundations (Latour 1999). But unfortunately science carries the historic baggage of objectivity and therefore technoscience turns into a battle ground over social power. There is a strong tension between the changed ontological foundation of technoscience and its continued naturalising rhetoric about nature. According to the representational strategy of technoscience nature has become a generalised formal system for processing algorithms and information. Nature has turned into a giant universal computer which transforms information, which is immaterial and free of context (Weber 2003, 220).
Instead of making it clear that this marks a fundamental ontological shift, technoscience sticks to naive concepts of realism and hides behind veils of mystification. Even though nature is no longer its object, strictly speaking, just its material, it nevertheless still uses nature as important legitimising and ideological entity. To the outside world technoscience presents itself as the science of old, involved in a disinterested pursuit of truth. Technoscience does only what nature always has done, the apologists of technoscience say. Technoscience, by creating new disciplines such as artificial intelligence (AI) and artificial life (AL) does nothing else but applying the 'principles of life' in artefacts. This is possible because 'in principle' organisms function like that flexible chameleon computer. The organising, saving, modifying and re-disseminating of information are being declared to be life's indispensable characteristics, characteristics which are fortunately shared by computers. In a tricky mimetic movement the specific qualities of the universal calculating machine and its software applications become essential characteristics of life (Weber 2003, 176). So, a reversal of principles happens, nature gets naturalised, reified, nailed down by technoscience. Value free and objectively science has to say what is the nature of nature, the nature of man, the nature of woman, and by doing so, our place in the world gets objectively determined. The narrative strategies of naturalism, biologism and positivism can be seen as 'manuals' how to declare nature to become the only foundation for norms and values (Weber 2003, p.40) - but of course this is nature as analysed, segmented, augmented, sliced and stitched together by reductionist and male dominated science. As many feminist studies of science have shown, biologism has served to legitimise the hierarchical structuring of gender relations (cf. Haraway 1985, 1996, Weber 2003). Naturalising strategies turn social relationships into matters of objective truth.
The re-interpretation of the concepts of nature, mind and life was made possible by the development and convergence of the paradigms of cybernetics, information theory and computer science. Alan Turing formulated the theory of the universal symbolic processing machine - the theoretic principles behind the computer. Of special significance is the separation of hardware and software. One and the same apparatus (mechanical and electronical) can be used to process any kind of algorithm. This introduces the new category of trans-classic machines. Earlier, machines could essentially only perform tasks they were specifically made for. The trans-classic machine can perform any operation that can be formulated as an algorithm. Claude Shannon, aided by Warren Weaver, formulated a mathematical theory of information which separated the content of communication from its carrier medium. Shannon was explicitely only concerned with the optimisation of the transmission of data via electronic networks, independently of the content of the data. Nevertheless Shannon's model was extended into a general model of communication.
Shannon's model contained an element of feedback which allowed for error correction. Aspects of two-way communication involving feedback mechanisms within machines, animals and humans also were of central concern to Norbert Wiener's cybernetic theory. At around the same time biology turned its focus to the molecule, following the reductionist strategy of science, yet also recognising properties of living things as systems understood according to the cybernetic paradigm (Weber 2003). The cross-fertilisation of those theories led to a new understanding of life as patterns of information (code) independent of the carrier medium (matter, hardware, the body). Life was no longer thought to be a property of matter but one of structure, a pattern of information, represented in the genetic code. During the second half of the 20th century technoscience rewrote body as text, used the metaphor of the immune-system and re-invented the self as (genetic) code (Weber 2003, 196-202). A key concept in the construction of this new paradigm are 'cellular automata', an idea of John von Neumann, inspired by an earlier text by Turing. Those are 'finite state machines' based on an on-off logic. Von Neumann took also inspiration from work by Warren McCulloch and Walter Pitts (Weber 2003, 160-196). They tried to develop a mathematical model for nerve functions and interpreted neurons in the brain according to an on-off logic. In the 1940s von Neumann tried to develop a computer model (on paper) which could simulate a biological neural network. Decades later, with progess in computing power and new programming techniques his concept could finally be realized. In the 1980s von Neumann's cellular automata, advances in neuroscience and computing (parallel processing) inspired 'connectionism' - a brain-computer analogy based on an assumed analogy between the network of neurons in the brain and the interaction of cellular automata in parallel processing computers - so called neural networks (Turkle 1995).
Another key development was the attempt to create a computer based artificial intelligence (AI). Since the late 1950s AI tried to construct machines which were intelligent, whereby a limited notion of intelligence was applied which prioritized symbolic operations and logical thought (Edwards 1996). Major funding for this project came from the US military. Following a 'closed worlds' logic of containment during the climactic years of the Cold War, the military tried to eliminate the slow and error prone human factor from decision making in fully automated and closely corresponding weapon systems involving early detection radar systems (SAGE) and nuclear retaliation capabilities by intercontinental ballistic missiles (Edwards 1996). The project of AI had the not so insignificant side effect of channeling massive financial resources into nascent computer sciences and build ever faster supercomputers (ibid.).
The project of AI was of significant influence on areas it came in touch with. Psychology, which until the 1950s had been dominated by behaviourism, now dared to turn its attention to internal states of mind. A new type of psychology was invented, which described inner states in terms of rules and logic - cognitive science. According to Sherry Turkle computer science was its 'sustaining myth' (Turkle 1996, 128). A sustaining myth is not an explicit part of a theory but an unacknowledged assumption which is called upon in representative strategies. To serve the specific needs of AI, the discipline of linguistics was reshaped as computer linguistics (Edwards 1996). As the objectives of scientific disciplines were redefined and new sciences were created the understanding of the human mind was fundamentally reshaped. Thinking became an act of information processing. The act of creating an 'intelligent' computer implied that intelligence was a function of computation. Joseph Weizenbaum, a professor in the Department of Engineering and Computer Science at the MIT, became the first prominent critic of the mechanistic approach to concepts such as mind, intelligence and consciousness. As a rather lone voice in the 1960s within the computer science community his criticism of the trivialisation of life was of no big effect.
AL and the closely related field of emergent artificial intelligence (AI) were developed in the 1980s through combined theoretical and practical efforts in computer science, cybernetics and biology rebranded as life science. The cybernetic paradigm had made possible the parallelisation of nonorganic and organic systems as open and changeable systems. Both, organic and non-organic systems can be conceptualized as consisting of variable components whose properties can be formulated according to communication- and information-theoretical models. "This makes not only possible the technologisation of the living but also the making seem alife of technology [trans.A.M.] (Weber 2003, 139)." When life basically can be described as a pattern of information - the genetic 'code' -, then information can also be seen as alife (Yoxen 1986, Weber 2003). Under this basic premise the new disciplines of emergent AI and artificial life (AL) were developed. Using new programming techniques such as genetic algorithms,8 life-like phenomena were simulated inside computers. Some scientists such as Richard A Langton and Tom Ray stand for a 'strong' approach in AL (Turkle 1995, Reichle 2005). They do not interprete replicating pieces of code as reasonable simulations of life but as living beings in the literal sense. A similar trend is observable in so called bottom-up robotics (Steels 1999, Brooks 2002) whereby robots are programmed to develop forms of emergent behaviour. Emergence is a key concept in AL and the related area of emergent AI. It means that systems are capable of arriving at a higher level of organisation spontaneously. That qualitative leap can not be programmed into systems from the top-down but can only emerge from the interaction of individual pieces of code known as 'agents' in a bottom-up way. Those software agents - strings of code that represent relatively simple actions and behaviours - are called autonomous agents. They have been designed by a programmer initually but equipped with ways of 'learning' their future interactions are not predictable. Emergent behaviour can be simulated in digital systems and also in robot systems conceptualized as embodied AL.
Technoscience is more than the activity of researchers doing their work. It is also projecting an image of itself to key audiences and the public at large. This discourse, inspired by science, but going beyond it, uses narrative strategies aimed at persuading the world that its actions are not only justified but necessary. It uses scientific findings, popular science, visual means (computer graphics, animations) and sensational announcements to shape the image of technoscience. All elements of this discourse together, and the sort of images and intellectual representations it creates, are referred to as the techno-imaginary.
Technoscience claims to be doing nothing but its job, but is actually massively involved in representational politics, not only with its practices and interventions but also its promises. According to Haraway, the promises of technoscience make its main social importance. "It does not matter if they ever get realized, what matters is that those ideas always remain alive in the timezone of unbelievable promises (Weber 2003, 144)." Actually, it is better if those promises never get realized, so that the expectation can be kept alive. The prophecies of technoscience about the future already shape the present. And, as Richard Barbrook points out, current activies in technoscience and related discourses in academic writing and the press are shaped by the technoimaginary of the past (Barbrook 2005). Some of the more extreme threats and promises of technoscience's need to be seen in the light of this strategy of the unfulfilled prophecy. The techno-imaginary projects futures in a grey zone between science and science fiction. Biologists in search of the 'secret of life' promise to slow down the ageing process so that the life-span could extend to hundreds of years and potentially, immortality. The robotics scientist Hans Moravec has predicted that robot intelligence would soon overtake human intelligence and render human life meaningless unless we decide to become robots, or cyborgs, too (Moravec 1998). The same author also wishes to upload himself to a main frame computer and continue a life freed from the fetters of bodily existence, not unsimilar to his collegue Marvin Minsky, who, like other cyber-Platonists, suggests that the body is only a burdon without which we would do better. Tom Ray's Tierra project was already mentioned. Here, small bits of code forming an 'information ecology' competing for resources inside a computer's RAM (rapid access memory), are considered to be new forms of life. Those proposals are easily dismissible as fantasmagories, yet they serve an important function within the discourse of the techno-imaginary by diverting the attention from the mainstream discourse of technoscience, which, on closer looks, tries to appear more rational yet is based on similar fundamental shifts in the understanding of the being of nature and humans.
When I reviewed the literature on media art, it became apparent that there is a problem with finding systems of classification, of categorisation and even a clear definition of the art form. Despite a 25 year history of media art, and some would say it's much longer, this work is only just beginning. "The terminology for technological art forms has always been extremely fluid" says Christiane Paul (2003). According to her, 'digital art' has first been called computer art, then multimedia art and is now subsumed under the umbrella term 'new media art' (Paul 2003, 7). Other words which have been used to refer to the field as a whole or to sub-genres of it are: electronic art, art & technology, video art, software art, net art, generative art, information art, virtual reality art, game art, tele/robotics art, hypermedia, hypertext, interactive installation. Potentially this list could be much longer. The choice of different terms for more or less the same thing often betrays a preference for a certain flavour: someone is speaking historically situated and from a specific theoretic or artistic perspective - Bourdieu's position taking. For instance, while some artists are happy being labelled as net artists, others prefer to talk about telematic art, whereby the latter appears to give the field more gravity.
It is not a diffuse 'essence' of media art which justifies it to speak of it as a separate field but the existence of a system of institutions which are more or less exclusively concerned with it. Institutionally media art is characterised by the existence of two types of institutions. On one hand there are large festivals, such as Ars Electronica, since 1979 held in Linz, Austria, and large brickand-mortar institutions such as the ZKM in Karlsruhe, which attract major funding, organise big exhibitions and produce heavy catalogues. On the other hand there are many small institutions, sometimes called 'self-institutions'11 - so called media labs or hack labs - which have been thriving over the last 10 years, forming an alternative or 'unstable' field (Druckrey 2005) with increasingly world-wide connections and a more decentralised and networked approach. Whereas the large institutions face typical pressures for legitimisation such as demands to be instrumental in regional development, the world-wide network of small institutions often lives on shoe-string budgets mostly provided by state funding agencies. Some activities are not funded at all or are rather selffunded - made possible by the energy and work of participants. According to Bourdieu this area could be called a field of restricted production. Economically it is insignificant but discursively it is important. I am not trying to construct a binary opposition between two types of institutions and acknowledge the existence of many medium sized institutions and a lively transfer between the fields. However, it is important to state that there is an institutionalised field and that it is not homogenous but heterogeneous.
Histories of media art are put into a trajectory of the genealogy of media technologies rather than art history. In The Automation of Sight: From Photography to Computer Vision, Lev Manovich (1996), draws a direct line from the invention of perspective to computer generated images. He also places this trajectory within a history of automation. "By automating perspectival imaging, digital computers completed the process which began in the Renaissance (Manovich 1996, 231)." But, as Manovich points out, the inventor of the algorithm which makes perspectival rendering on computers possible, Lawrence G.Roberts, had a 'more daring goal' in mind than creating a tool for art. The computer should not only be able to render but also to 'understand' 3-D images (through pattern recognition). Thus, the project of 3-D computer generated images was a part of the project of AI in the context of the Cold War. Yet Manovich portrays this in an euphemistic language, presenting computer vision as "the culmination of at least two histories, each a century long" (ibid., 233), the history of mechanical devices designed to aid human perception, and the history of automata. Manovich does mention that the history of automation is situated in the context of rationalisation in the industrial process and that the Czech word Robot means forced labour, yet he does not spell out what this means. Instead he celebrates 3-D imaging as technology's inevitable progress. Siegfried Zielinski comments on this frequently encountered narrative strategy.
Zielinski demands that we should not continue to find the old confirmed in the new (Zielinski 2002, 11). In those readings, history turns into a promise of continuity, a celebration of progression. He thinks that this is boring as well as paralysing for the work of the mediaarchaeologist. He demands instead to find the new in the old, to let ourselves be surprised and not just look for confirmation of what we already know. As a counter-strategy Zielinski proposes the concept of a 'deep time of media' in the form of a un-archaeology (ibid., 13) which opens up spaces for the imagination. Too quickly we tend to orient ourselves toward a new 'master medium' after which all symbolic systems have to be re-arranged, until the next master medium arrives (ibid., 17).
Christiane Paul presents a slightly different trajectory. She claims that "the notion of interactivity and 'virtuality' in art were explored early on by artists such as Marcel Duchamp and László Moholy-Nagy in relation to objects and their optical effects (Paul 2003, 13)." According to Paul, Duchamp's work was "extremely influential in the realm of digital art" because of the "shift from object to concept" (ibid., 13). Paul formulates a genealogy of digital art slightly different from Weibel's or Manovich's, emphasising the influence of Duchamp via OULIPO, a French literary group, to Fluxus and conceptual art. The conceptual 'link' here is that Dadaists, the OULIPO writers and Fluxus artists frequently created art works which were based on the execution of a set of instructions and/or rules, which can be compared to computer algorithms, which are, conceptually speaking, nothing else but sequences of instructions carried out in loops (ibid., 13). This view is supported by Peter Suchin who argues that the art of the 1960s, "institutionalised under the collective heading of 'Conceptual Art' and its legacies [...] is a key determinant of today's new media art practices." (Suchin 2004, 67) Other conceptual links between contemporary media art and art movements in the past focus on the exhibition Cybernetic Serendipity, at the ICA, London 1968, (Paul 2003, 18), as well as on the exhibition 'Software Art', curated by Jack Burnham in 1970. Younger artists who are now using the term 'software art' for their own work are claiming Burnham's show as a conceptual predecessor (Goriunova and Shulgin 2003). However, there is no continuity between the surge in cybernetic art in the late 1960s and the reappearance of the 'cyber' paradigm in the 1980s. There is even less continuity between Burnham's Software Art show which remains an early and isolated example from which there can be traced no continuous line toward software art in 2005. Thus, when such long jumps are being made, it is reasonable to assume that a desire for historic legitimisation is at work. Such moves can also be seen as following the logic that Bourdieu describes in Principles for a Sociology of Cultural Works.
Despite that there are obviously problems with history writing, I want to present a short overview of themes and positions which have been taken in a 'deep' history of media art. The most obvious theme is the notion of techno-utopianism. Hereby I differentiate between a totalitarian technoutopianism and a more participatory or democratic form of utopianism. Futurism, Suprematism and Constructivism formulated a programme of techno-utopianism which demanded that artists should use science and technology to help create the utopian society populated by the new man. Humanity reinvents itself based on the powers of science and technology. Art is carrying the banner of an utopianism which is totalitarian. The poetic writing of Khlebnikov about radio which becomes 'the heart and brain of the future society' (Khlebnikov 1921/2005) is characterised by a one-size-fits-all solution. Similar sentiments and totalitarian leanings are contained in the radio manifesto by Marinetti and Masnata (1933/1992). The high point of modern art was also the high point of modernity. Connecting new communications media with a utopian vision of society was not exclusive to art but prevalent in societies both sides of the Atlantic. Between 1900 and 1939 techno-utopianism was a common strand independently of the political ideology. While the Russians supported communism and Marinetti supported Mussolini, other techno-utopians such as Marconi and Edison built monopolistic business empires. In Not Just Another Wireless Utopia I have compared the different utopian visions competing at around 1900 and how they relate to the utopianism surrounding the internet and mobile telephony (Medosch 2004). Linking media with utopian social ideas is not unique to the 20th century. Richard Barbrook traces the political roots of media totalitarianism in the name of media freedom back to the French Revolution. The Jacobean's idea of media freedom was that the revolutionary elite needs to tightly control all media (in those days the press) because unfortunately the masses are not yet able to act in their own best interest.
In the 1980s a particular type of media art started to gather momentum. The media art scene, for which the Ars Electronica Festival in Austria provided a platform, became increasingly international, with contributions from Japan, Brazil, Australia, Canada, the USA and Europe. Changes in the thematic orientation of the festival allow charting the rise of this new type of media art. In the early 1980s the festival presented itself as an odd mix with some pioneers of cybernetic art such as Herbert W. Franke and Otto Piene attending, but also spectacles aimed at winning over large audiences, musical concerts and even operatic productions in public space involving workers from a local steel factory. As Peter Weibel recalls, in those early years Ars Electronica had been almost everything it possibly could have been, "an Ars Metallica, an Ars Pneumatica, an Ars Pyrotechnica, only not an Ars Electronica (cf. Weibel 1999, 72)." This changed, according to Weibel, when he and Gottfried Hattinger gained more influence on the festival's direction from 1986 onwards. In 1987 Ars Electronica for the first time had a theme, The Free Sound. In 1989 networks became the festivals theme for the first time with In the Network of Systems. In 1992 Peter Weibel took sole responsibility for the festival's artistic direction and presented the scientific disciplines endophysics and nanotechnology as the festival's themes. In the following year the theme was Artificial Life - Genetic Art (cf. Weibel 1999, 72-74). By that time a certain type of media art, which I call high media art, was established as the leading paradigm. In the decade between 1985 and 1995, high media art developed its forms, its milestone works and its narrative strategies, which altogether were successfully deployed in institution building. I call this form high media art for two main reasons, because it resonates with high-tech as well as with high-brow or high-cultural values. For the realisation of those works expensive and complex technology was used - which implies issues of accessibility, inclusiveness and structural dependencies. The works themselves usually presented themselves as clean, large scale productions in a sterile technological atmosphere. The dirt of the streets, so present in Gibson's Neuromancer (Gibson 1984), was rarely to be felt at high media art exhibitions. The digital aesthetics of high media art was compatible with the black cube inside the white cube of the museum: as viewers enter a darkened room with multiple projection screens they are made aware that they are entering the holy inner sanctum of a shrine to the digital; the same aesthetics is also compatible with corporate lobbies or boardrooms, with steel, glass and transparency. As the yet to be built institutions will cost a lot of money, the institutional projects have to be pitched at the highest level in industry and politics, and those circles need to get assured that they are getting value for money - high-tech, high-end art.
Shown at Ars Electronic in 1993, which had the theme Artificial Life - Genetic Art, Sommerer and Mignonneau's Interactive Plant Growing was considered one of the most advaned works of art in this genre. They went on to create Trans Plant (1995-1996) and Life Spacies (1997), works which are variations on the theme of AL. Trans Plant was realized at the ATR - Advanced Telecommunications Research Laboratory - in Kyoto, where the artists hold a research residency, and was commissioned for the permanent collection of NTTICC, Tokyo (Reichle 2005). In this work the artists used a 3-D video-key technology which they had developed themselves and which they patented. As soon as the user enters the exhibition space an image of her within a threedimensional jungle of virtual plants is created. Movements of the user influence the plant growth. The works of Sommerer and Mignonneau are shown around the world and considered to be realising many demands of the paradigm of high media art. They do not create static objects but a framework for a processual exchange between viewer and art work. In most of their works, if there is no viewer, nothing happens at all. The result of the interaction appears to be indeterministic to a certain degree. AL algorithms formulate local rules of interaction so that the exact result cannot be predicted. However, whereas the growth of plants appears to be spontaneous, the extent to which the viewer can actually 'interact' with those virtual worlds is very limited. The framework has been entirely defined by the artists and the viewer/user is left with a few simple forms of physical interaction such as moving hands over a plant or moving the body in front of a screen. The 'message' of the work appears to be very similar to the message of the discipline of AL as a whole: the principles of life can be replicated in digital code. The major difference then is that the artists have developed more beautiful visuals than the scientists. Instead of challenging the naturalistic assumptions behind AL, artists lend their aesthetic skills to the illustration of science and thereby help to advertise the achievements of technoscientific progress.
The claim that 'the world itself' was 'digitally organised' belongs to the class of strong ontological statements. Weibel echoes Flusser's theory of 'digital apparition' (Flusser 1996) where he writes that there was no ontological difference between reality and technological images such as those created by virtual reality techniques. If the latter seem 'less real' then this was only a function of their lower resolution. Turning the attention to digital apparition has the benefit of making us recognise that reality is an apparition too. "What remains is that everything is digital, i.e. that everything has to be looked at as a more or less dense distribution of point elements, of bits (ibid., 244)." Therefore, what we call real, "are those areas, those curvatures and convexities, in which the particles are distributed more densely and in which potentialities realise themselves (ibid., 244)." As Flusser suggests, this parallelism of digital and real is "the digital world picture as it is being suggested to us by science (ibid., 244)." High media art falls in line with an explanation of the ontological status of the world provided by technoscience.
Noticable is the difference between the Ascott of 1984 (Grundmann ed. 1984) and of 1996 who is increasingly working with esoteric concepts. There are many religious motives in high tech spiritualism, not all Christian. A deeper study of religious motives in the discourses of the technoimaginary is not the focus of this paper. However, ideas of a ghost in the machine or the idolatry of machinery are too many to be ignored. High-tech new age spiritualism is pervasive in discourses of technoscience such as AI and AL and is replicated in the works and words of some high media artists such as Roy Ascott.
The discourse on high media art ignores the conditions of its own creations. Nobody seems to be uncomfortable with the degree to which the artists are dependent on the computer industry and technology corporations such as Deutsche Telekom, NTT or Canon. Artists producing work at the highest technical and aesthetic level need to get supported by institutions. They need to accept high levels of separation of labour and bureaucratic management. To a degree, which would need to be established specifically for each work, the works are determined by the black box character of proprietary hard- and software, by the products of commercial software applications and by the skills of the actors involved. In a very direct way the art work is determined by the technologies used. Since in media art works form cannot be clearly separated from function, this is of major significance. It would be naive to assume that the institutionalised context does not have a bearing on the form and content of the art work (Mitchell 2003). The discourses on high media art and postmodernism share a strong focus on information and immateriality. This is something they have in common with technosciene which conducts its own strategy of dematerialisation. High media art provides high class illustration to both post-modern ideas and concepts of computer science, and unconditionally accepts the premises on which those are built. It fails to take a critical position against ideologies of dominance embedded in those theories. As Jutta Weber (2003) argues, although most post-modern theories are civilization critical, they are unable to challenge the ideology of technological determinism inherent to technoscience. As Barbrook (2005) points out, information becomes fetishised in those theories. The two grand narratives of modernism, Adam Smith's liberalism and Marx's analysis of the capitalist political economy agree at least insofar as they see humans at the driving seat of history. Post-modern theories, by dismissing those grand narratives, are actually saying that history was a process without a human subject (Barbrook 2005). High media art takes a similar position by insisting on the ontological status of the work of art as being digital. The seemingly progressive digital aesthetic is socially and politically conservative. The discourse of high media art endorses a version of McLuhanism which has technological determinism at its core.
The narrative strategies of Weibel, Ascott et al succeeded in institution building. But ironically, after high-media art climaxed at around 1995, it soon lost its discoursive relevance. A new paradigm unfolded with the mass popularisation of the internet. The 'really existent' internet showed to be rather different from what the gurus of cybernetic art had imagined it to be. People learned to do their email, download a video or a song, but failed to encounter the ghost in the machine or telematic consciousness. The ordinaryness of life on the net took over. Actually, nobody ever really lived 'on' the net. We learned to step back and understand that our bodies are still real. The ZKM and Ars Electronica remain powerful institutions, but the new aesthetical strategies are now developed elsewhere. Socially and politically aware artists shape the discoursive agenda outside the institutional context provided by high-media art. Weibel, just like the software giant Microsoft, had misinterpreted the relevancy of the internet. Instead of glorification of the products of multinational corporations net artists high-light the participatory culture of the internet. Microsoft quickly developed its own browser software Internet Explorer. Weibel got the guest curator Benjamin Weill to curate the large scale exhibition net_condition in 1998 (Greene 2004). But the discoursive bandwaggon had already left the station. Digital artists joined new alliances with hackers developing free and open source software and a new discourse on art, activism and free or copyleft culture florished. High media art, by winning institutional power, lost its symbolic capital. The discourse of high media art, which had all chances to do so, had not generated foundations on which it was safe to build. As I hope to have shown, the ideology of technological determinism has prevented it from doing so. High media art with its high-tech visions has won a phyrric victory. At the same time the technologisation of society continues and a strong critical art movement dealing with issues surrounding technology and society is as urgently needed as ever.
