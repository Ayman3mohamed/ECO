Planetary-scale computation takes different forms at different scales: energy grids and mineral sourcing; cloud infrastructure; urban software and public service privatization; massive universal addressing systems; interfaces drawn by the augmentation of the hand, of the eye, or dissolved into objects; users both overdetermined by self-quantification and exploded by the arrival of legions of nonhuman users (sensors, cars, robots). Instead of seeing the various species of contemporary computational technologies as so many different genres of machines, spinning out on their own, we should instead see them as forming the body of an accidental megastructure. Perhaps these parts align, layer by layer, into something not unlike a vast (if also incomplete), pervasive (if also irregular) software and hardware Stack. This model is of a Stack that both does and does not exist as such: it is a machine that serves as a schema, as much as it is a schema of machines.  As such, perhaps the image of a totality that this conception provides would—as theories of totality have before—make the composition of new governmentalities and new sovereignties both more legible and more effective.
My interest in the geopolitics of planetary-scale computation focuses less on issues of personal privacy and state surveillance than on how it distorts and deforms traditional Westphalian modes of political geography, jurisdiction, and sovereignty, and produces new territories in its image. It draws from (and against) Carl Schmitt’s later work on The Nomos of the Earth, and from his (albeit) flawed history of the geometries of geopolitical architectures.2“Nomos” refers to the dominant and essential logic to the political subdivisions of the earth (of land, seas, and/or air, and now also of the domain that the US military simply calls “cyber”) and to the geopolitical order that stabilizes these subdivisions accordingly. Today, as thenomos that was defined by the horizontal loop geometry of the modern state system creaks and groans, and as “Seeing like a State” takes leave of that initial territorial nest—both with and against the demands of planetary-scale computation3—we wrestle with the irregular abstractions of information, time, and territory, and the chaotic de-lamination of (practical) sovereignty from the occupation of place. For this, a nomos of the Cloud would, for example, draw jurisdiction not only according to the horizontal subdivision of physical sites by and for states, but also according to the vertical stacking of interdependent layers on top of one another: two geometries sometimes in cahoots, sometimes completely diagonal and unrecognizable to one another.4

The Stack, in short, is that new nomos rendered now as vertically thickened political geography. In my analysis, there are six layers to this Stack: Earth, Cloud, City, Address, Interface, and User. Rather than demonstrating each layer of the Stack as a whole, I’ll focus specifically on the Cloud and the User layers, and articulate some alternative designs for these layers and for the totality (or even better, for the next totality, the nomos to come). The Black Stack, then, is to the Stack what the shadow of the future is to the form of the present. The Black Stack is less the anarchist stack, or the death-metal stack, or the utterly opaque stack, than the computational totality-to-come, defined at this moment by what it is not, by the empty content fields of its framework, and by its dire inevitability. It is not the platform we have, but the platform that might be. That platform would be defined by the productivity of its accidents, and by the strategy for which whatever may appear at first as the worst option (even evil) may ultimately be where to look for the best way out. It is less a “possible future” than an escape from the present.

The platforms of the Cloud layer of the Stack are structured by dense, plural, and noncontiguous geographies, a hybrid of US super-jurisdiction and Charter Cities, which have carved new partially privatized polities from the whole cloth of de-sovereigned lands. But perhaps there is more there.

The immediate geographical drama of the Cloud layer is seen most directly in the ongoing Sino-Google conflicts of 2008 to the present: China hacking Google, Google pulling out of China, the NSA hacking China, the NSA hacking Google, Google ghostwriting books for the State Department, and Google wordlessly circumventing the last instances of state oversight altogether, not by transgressing them but by absorbing them into its service offering. Meanwhile, Chinese router firmware bides its time.

The geographies at work are often weird. For example, Google filed a series of patents on offshore data centers, to be built in international waters on towers using tidal currents and available water to keep the servers cool. The complexities of jurisdiction suggested by a global Cloud piped in from non-state space are fantastic, but they are now less exceptional than exemplary of a new normal. Between the “hackers” of the People’s Liberation Army and Google there exists more than a standoff between the proxies of two state apparatuses. There is rather a fundamental conflict over the geometry of political geography itself, with one side bound by the territorial integrity of the state, and the other by the gossamer threads of the world’s information demanding to be “organized and made useful.” This is a clash between two logics of governance, two geometries of territory: one a subdivision of the horizontal, the other a stacking of vertical layers; one a state, the other a para-state; one superimposed on top of the other at any point on the map, and never resolving into some consensual cosmopolitanism, but rather continuing to grind against the grain of one another’s planes. This characterizes the geopolitics of our moment (this, plus the gravity of generalized succession, but the two are interrelated).

From here we see that contemporary Cloud platforms are displacing, if not also replacing, traditional core functions of states, and demonstrating, for both good and ill, new spatial and temporal models of politics and publics. Archaic states drew their authority from the regular provision of food. Over the course of modernization, more was added to the intricate bargains of Leviathan: energy, infrastructure, legal identity and standing, objective and comprehensive maps, credible currencies, and flag-brand loyalties. Bit by bit, each of these and more are now provided by Cloud platforms, not necessarily as formal replacements for the state versions but, like Google ID, simply more useful and effective for daily life. For these platforms, the terms of participation are not mandatory, and because of this, their social contracts are more extractive than constitutional. The Cloud Polis draws revenue from the cognitive capital of its Users, who trade attention and microeconomic compliance in exchange for global infrastructural services, and in turn, it provides each of them with an active discrete online identity and the license to use this infrastructure.

Looking toward the Black Stack, we observe that new forms of governmentality arise through new capacities to tax flows (at ports, at gates, on property, on income, on attention, on clicks, on movement, on electrons, on carbon, and so forth). It is not at all clear whether, in the long run, Cloud platforms will overwhelm state control on such flows, or whether states will continue to evolve into Cloud platforms, absorbing the displaced functions back into themselves, or whether both will split or rotate diagonally to one another, or how deeply what we may now recognize as the surveillance state (US, China, and so forth) will become a universal solvent of compulsory transparency and/or a cosmically opaque megastructure of absolute paranoia, or all of the above, or none of the above.

Between the state, the market, and the platform, which is better designed to tax the interfaces of everyday life and draw sovereignty thereby? It is a false choice to be sure, but one that raises the question of where to locate the proper site of governance as such. What would we mean by “the public” if not that which is constituted by such interfaces, and where else should “governance”—meant here as the necessary, deliberate, and enforceable composition of durable political subjects and their mediations—live if not there? Not in some obtuse chain of parliamentary representation, nor in some delusional monadic individual unit, nor in some sad little community consensus powered by moral hectoring, but instead in the immanent, immediate, and exactly present interfaces that cleave and bind us. Where should sovereignty reside if not in what is in-between us—derived not from each of us individually but from what draws the world through us?

For this, it’s critical to underscore that Cloud platforms (including sometimes state apparatuses) are exactly that: platforms. It is important as well to recognize that “platforms” are not only a technical architecture; they are also an institutional form. They centralize (like states), scaffolding the terms of participation according to rigid but universal protocols, even as they decentralize (like markets), coordinating economies not through the superimposition of fixed plans but through interoperable and emergent interaction. Next to states and markets, platforms are a third form, coordinating through fixed protocols while scattering free-range Users watched over in loving, if also disconcertingly omniscient, grace. In the platform-as-totality, drawing the interfaces of everyday life into one another, the maximal state and the minimal state, Red Plenty and Google Gosplan, start to look weirdly similar.

Our own subjective enrollment in this is less as citizens of a polis or as homo economicuswithin a market, but rather as Users of a platform. As I see it, the work of geopolitical theory is to develop a proper history, typology, and program for such platforms. These would not be a shorthand for Cloud Feudalism (nor for the network politics of the “multitude”) but models for the organization of durable alter-totalities which command the force of law, if not necessarily its forms and formality. Our understanding of the political economy of platforms demands its own Hobbes, Marx, Hayek, and Keynes.

One of the useful paradoxes of the User’s position as a political subject is the contradictory impulse directed simultaneously toward his artificial over-individuation and his ultimatepluralization, with both participating differently in the geopolitics of transparency. For example, the Quantified Self movement (a true medical theology in California) is haunted by this contradiction. At first, the intensity and granularity of a new informational mirror image convinces the User of his individuated coherency and stability as a subject. He is flattered by the singular beauty of his reflection, and this is why QSelf is so popular with those inspired by an X-Men reading of Atlas Shrugged. But as more data is added to the diagram that quantifies the outside world’s impact on his person—the health of the microbial biome in his gut, immediate and long-term environmental conditions, his various epidemiological contexts, and so on—the quality of everything that is “not him” comes to overcode and overwhelm any notion of himself as a withdrawn and self-contained agent. Like Theseus’s Paradox—where after every component of a thing has been replaced, nothing original remains but a metaphysical husk—the User is confronted with the existential lesson that at any point he is only the intersection of many streams. At first, the subject position of the User overproduces individual identity, but in the continuance of the same mechanisms, it then succeeds in exploding it.

The geopolitics of the User we have now is inadequate, including its oppositional modes. The Oedipal discourse of privacy and transparency in relation to the Evil Eye of the uninvited stepfather is a necessary process toward an alterglobalism, but it has real limits worth spelling out. A geopolitics of computation predicated at its core upon the biopolitics of privacy, of self-immunization from any compulsory appearance in front of publics, of platforms, of states, of Others, can sometimes also serve a psychological internalization of a now-ascendant general economy of succession, castration anxiety—whatever. The result is the pre-paranoia of withdrawal into an atomic and anomic dream of self-mastery that elsewhere we call the “neoliberal subject.”

The space in which the discursive formation of the subject meets the technical constitution of the User enjoys a much larger horizon than the one defined by these kinds of individuation. Consider, for example, proxy users. uProxy, a project supported by Google Ideas, is a browser modification that lets users easily pair up across distances to allow someone in one location (trapped in the Bad Internets) to send information unencumbered through the virtual position of another User in another location (enjoying the Good Internets). Recalling the proxy servers set up during the Arab Spring, one can see how Google Ideas (Jared Cohen’s group) might take special interest in baking this into Chrome. For Sino-Google geopolitics, the platform could theoretically be available at a billion-user scale to those who live in China, even if Google is not technically “in China,” because those Users, acting through and as foreign proxies, are themselves, as far as internet geography is concerned, both in and not in China. Developers of uProxy believe that it would take two simultaneous and synchronized man-in-the-middle attacks to hack the link, and at a population scale that would prove difficult even for the best state actors, for now. More disconcerting perhaps is that such a framework could just as easily be used to withdraw data from a paired site—a paired “user”—which for good reasons should be left alone.

Some plural User subject that is conjoined by a proxy link or other means could be composed of different types of addressable subjects: two humans in different countries, or a human and a sensor, a sensor and a bot, a human and a robot and a sensor, a whatever and a whatever. In principle, any one of these subcomponents could not only be part of multiple conjoined positions, but might not even know or need to know which meta-User they contribute to, any more than the microbial biome in your gut needs to know your name. Spoofing with honeypot identities, between humans and nonhumans, is measured against the theoretical address space of IPv6 (roughly 1023 addresses per person) or some other massive universal addressing scheme. The abyssal quantity and range of “things” that could, in principle, participate in these vast pluralities includes real and fictional addressable persons, objects, and locations, and even addressable mass-less relations between things, any of which could be a sub-User in this Internet of Haeccities.

So while the Stack (and the Black Stack) stage the death of the User in one sense—the eclipse of a certain resolute humanism—they do so because they also bring the multiplication and proliferation of other kinds of nonhuman Users (including sensors, financial algorithms, and robots from nanometric to landscape scale), any combination of which one might enter into a relationship with as part of a composite User. This is where the recent shift by major Cloud platforms into robotics may prove especially vital, because—like Darwin’s tortoises finding their way to different Galapagos islands—the Cambrian explosion in robotics sees speciation occur in the wild, not just in the lab, and with “us” on “their” inside, not on the outside. As robotics and Cloud hardware of all scales blend into a common category of machine, it will be unclear in general human-robotic interaction whether one is encountering a fully autonomous, partially autonomous, or completely human-piloted synthetic intelligence. Everyday interactions replay the Turing Test over and over. Is there a person behind this machine, and if so, how much? In time, the answer will matter less, and the postulation of human (or even carbon-based life) as the threshold measure of intelligence and as the qualifying gauge of a political ethics may seem like tasteless vestigial racism, replaced by less anthropocentric frames of reference.

The position of the User then maps only very incompletely onto any one individual body. From the perspective of the platform, what looks like one is really many, and what looks like many may only be one. Elaborate schizophrenias already take hold in our early negotiation of these composite User positions. The neoliberal subject position makes absurd demands on people as Users, as Quantified Selves, as SysAdmins of their own psyche, and from this, paranoia and narcissism are two symptoms of the same disposition, two functions of the same mask. For one, the mask works to pluralize identity according to the subjective demands of the User position as composite alloy; and for another, it defends against those same demands on behalf of the illusory integrity of a self-identity fracturing around its existential core. Ask yourself: Is that User “Anonymous” because he is dissolved into a vital machinic plurality, or because public identification threatens individual self-mastery, sense of autonomy, social unaccountability, and so forth? The former and the latter are two very different politics, yet they use the same masks and the same software suite. Given the schizophrenic economy of the User—first over-individuated and then multiplied and de-differentiated—this really isn’t an unexpected or neurotic reaction at all. It is, however, fragile and inadequate.

In the construction of the User as an aggregate profile that both is and is not specific to any one entity, there is no identity to deduce other than the pattern of interaction between partial actors. We may find, perhaps ironically, that the User position of the Stack actually has far less in common with the neoliberal form of the subject than some of today’s oppositionalist formats for political subjectivity that hope (quite rightly) to challenge, reform, and resist the State Stack as it is currently configuring itself. However, something like a Digital Bill of Rights for Users, despite its cosmopolitan optimism, becomes a much more complicated, fragile, and limited solution when the discrete identification of a User is both so heterogeneous and so fluid. Are all proxy composite users one User? Is anything with an IP address a User? If not, why not? If this throne is reserved for one species—humans—when is any one animal of that species being a User, and when is it not? Is it a User anytime that it is generating information? If so, that policy would in practice crisscross and trespass some of our most basic concepts of the political, and for that reason alone it may be a good place to start.

In addition to the fortification of the User as a geopolitical subject, we also require a redefinition of the political subject in relation to the real operations of the User, one that is based not on homo economicus, nor on parliamentary liberalism, nor on post-structuralist linguistic reduction, nor on the will to secede into the moral safety of individual privacy and withdraw from coercion. Instead, this definition should focus on composing and elevating sites of governance from the immediate, suturing, interfacial material between subjects, in the stitches and the traces and the folds of interaction between bodies and things at a distance, congealing into different networks demanding very different kinds of platform sovereignty.

I will conclude with some thoughts on the Stack-we-have and on the Black Stack, the generic figure for its alternative totalities: the Stack-to-come. The Stack-we-have is defined not only by its form, its layers, its platforms, and their interrelations, but also by its content. As leak after leak has made painfully clear, its content is also the content of our daily communications, now weaponized against us. If the panopticon effect is when you don’t know if you are being watched or not, and so you behave as if you are, then the inverse panopticon effect is when you know you are being watched but act as if you aren’t. This is today’s surveillance culture: exhibitionism in bad faith. The emergence of Stack platforms doesn’t promise any solution, or even any distinctions between friend and enemy within this optical geopolitics. At some dark day in the future, when considered versus the Google Caliphate, the NSA may even come to be seen by some as the “public option.” “At least it is accountable in principle to some parliamentary limits,” they will say, “rather than merely stockholder avarice and flimsy user agreements.”

If we take 9/11 and the rollout of the Patriot Act as Year Zero for the USA’s massive data gathering, encapsulation, and digestion campaign (one that we are only now beginning to comprehend, even as parallel projects from China, Russia, and Europe are sure to come to light in time), then we can imagine the entirety of network communication for the last decade—the Big Haul—as a single, deep-and-wide digital simulation of the world (or a significant section of it). It is an archive, a library of the real. Its existence as the purloined property of a state, just as a physical fact, is almost occult. Almost.

The geophilosophical profile of the Big Haul, from the energy necessary to preserve it to its governing instrumentality understood as both a text (a very large text) and as a machine with various utilities, overflows the traditional politics of software. Its story is much more Borges than Lawrence Lessig. As is its fate. Can it be destroyed? Is it possible to delete this simulation, and is it desirable to do so? Is there a trash can big enough for the Big Delete? Even if the plug could be pulled on all future data hauls, surely there must be a backup somewhere, the identical double of the simulation, such that if we delete one, the other will forever haunt history until it is rediscovered by future AI archaeologists interested in their own Paleolithic origins. Would we bury it, even if we could? Would we need signs around it like those designed for the Yucca Mountain nuclear waste disposal site that warn off unknowable future excavations? Those of us “lucky” enough to be alive during this fifteen-year span would enjoy a certain illegible immortality, curiosities to whatever meta-cognitive entity pieces us back together using our online activities, both public and private, proud and furtive, each of us rising again centuries from now, each of us a little Ozymandias of cat videos and Pornhub.

In light of this, the Black Stack could come to mean very different things. On the one hand, it would imply that this simulation is opaque and unmappable—not disappeared, but ultimately redacted entirely. It could imply that, from the ruined fragments of this history, another coherent totality can be carved against the grain, even from the deep recombinancy at and below the Earth layer of the Stack. Its blackness is the surface of a world that can no longer be composed by addition because it is so absolutely full, overwritten, and overdetermined, that to add more is just so much ink in the ocean. Instead of tabula rasa, this tabula plenus allows for creativity and figuration only by subtraction, like scratching paint from a canvas—only by carving away, by death, by replacement.

The structural logic of any Stack system allows for the replacement of whatever occupies one layer with something else, and for the rest of the architecture to continue to function without pause. For example, the content of any one layer—Earth, Cloud, City, Address, Interface, User—could be replaced (including the masochistic hysterical fiction of the individual User, both neoliberal and neo-other-things), while the rest of the layers remain a viable armature for global infrastructure. The Stack is designed to be remade. That is its technical form, but unlike replacing copper wire with fiber optics in the transmission layer of TCP/IP, replacing one kind of User with another is more difficult. Today, we are doing it by adding more and different kinds of things into the User position, as described above. We should, however, also allow for more comprehensive displacements, not just by elevating things to the status of political subjects or technical agents, but by making way for genuinely posthuman and ahuman positions.

In time, perhaps at the eclipse of the Anthropocene, the historical phase of Google Gosplan will give way to stateless platforms for multiple strata of synthetic intelligence and biocommunication to settle into new continents of cyborg symbiosis. Or perhaps instead, if nothing else, the carbon and energy appetite osf this ambitious embryonic ecology will starve its host.

For some dramas, but hopefully not for the fabrication of the Stack-to-come (Black or otherwise), a certain humanism and companion figure of humanity still presumes its traditional place in the center of the frame. We must let go of the demand that any Artificial Intelligence arriving at sentience or sapience must care deeply about humanity—us specifically—as the subject and object of its knowing and its desire. The real nightmare, worse than the one in which the big machine wants to kill you, is the one in which it sees you as irrelevant, or as not even a discrete thing to know. Worse than being seen as an enemy is not being seen at all. As Eliezer Yudkowsky puts it, “The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.”6

One of the integral accidents of the Stack may be an anthrocidal trauma that shifts us from a design career as the authors of the Anthropocene, to the role of supporting actors in the arrival of the Post-Anthropocene. The Black Stack may also be black because we cannot see our own reflection in it. In the last instance, its accelerationist geopolitics is less eschatological than chemical, because its grounding of time is based less on the promise of historical dialectics than on the rot of isotope decay. It is drawn, I believe, by an inhuman and inhumanist molecular form-finding: pre-Cambrian flora changed into peat oil changed into children’s toys, dinosaurs changed into birds changed into ceremonial headdresses, computation itself converted into whatever meta-machine comes next, and Stack into Black Stack.

Through the chapters that follow, we demonstrate that such questioning is not the end of anthropology but, to the contrary, a fruitful endeavor leading to the discovery of new ends and purposes for our enduring commitment to engage and interpret other lifeways. A critical examination of the “human” sheds light on how the anthropocentric presumptions of much anthropology ignore not just the “unhuman” but also the “animal” and the “not-quitehuman” (transgendered, disabled, or psychologically impaired persons), inevitably leading to a challenge, and perhaps an outright rejection, of the whole category of the human, at least as a core concept for anthropological theory. Anthropology is only one academic discipline currently engaged with the posthuman (Wolfe 2010), but arguably anthropology has the most to contribute to such debates through ethnographic engagement with cultural worlds in which Western Enlightenment definitions and exclusions (Latour 1993) are not so prominent. In this regard, the essays here demonstrate that new forms of ethnographic engagement with “unhuman” populations (as in the chapters by Heckenberger, Whitehead, and Wisniewski) can inform and be informed by studies of online phenomena that also challenge and subvert traditional notions of the human. The chapters here illustrate emergent cultural contexts in which embodied, “rational” individuals are but one of the forms of agency present in virtual and socially occluded worlds. As Matt Bernius demonstrates, software programs create chatbots, spambots, searchbots, and even ballot-stuffing bots, some of which are fully equipped to interact with “real humans” socially, sexually, and financially. Such bots appear alongside and engage with simulacra of our offline selves.

The theme of these chapters—human no more?—resonates as a question throughout. Although Whitehead answers firmly in the affirmative and eagerly embraces the posthuman as a potential liberation from the late capitalist disciplines of the corporeal and the mental, others more cautiously question whether we are in fact posthuman at all and whether we should radically rethink anthropology. This is partly the reaction of Anne Allison (whose afterword was originally a response to the American Anthropological Association (AAA) panel at which these chapters were first presented), but her highly constructive and open-ended engagement with the question of “humanity” contrasts with other commentators (Boellstorff 2008) who find the prospect of the posthuman threatening, both subjectively and with regard to the preservation of the iconic founding figures of the discipline. Just as the idea of the “postmodern” provoked surprisingly conservative reactions among otherwise innovative thinkers such as Johannes Fabian, Marshall Sahlins, and Eric Wolf (Whitehead 2004), it is important to distinguish the critique of anthropology’s intellectual frameworks, which the asking of questions about the “human” permits, from the assumption that “posthumanism” is a disguised political gambit aimed at taking over the profession. Attempting to avoid such pitfalls, and in a similar vein to Allison, Tufekci notes that the first symbols ever created allowed for a form of “disembodiment” by separating the thought from the human, arguing that “the essence of humanity is that we have always been both symbolic and embodied.” “We were always human,” she suggests. “Or maybe, said alternatively, we were always posthuman.” Tufekci and others in this volume discover that even in these virtual worlds that create always expanding possibilities for disembodied sociality, embodiment remains crucial. It is the body that “centers and unites us,” even as we play with different social roles and personas. “Typing on a keyboard,” Tufekci suggests, does not “create an ontological split within the body.” But, as Tufekci rightly points out, this still begs the question of what we are to make of emerging technologies that now promise to immerse us in fully realistic simulated worlds—skin suits, goggles, and other devices that will bring digital inputs seamlessly into our increasingly augmented intelligence—or those futuristic Kurzwellian visions of nanobots swarming through our bodies, giving us access to all the information on the web from inside our own skin while repairing and rebuilding our cells and transporting us into virtual worlds whenever we desire (Kurzwell 2005).

Nonetheless, the bodily disciplines imposed by subjective engagements through such technological devices do represent a historical and cultural rupture and disjuncture. Thus, it is not our potential forms of disembodiment that make us posthuman but rather the way in which this historical movement away from prior cultural forms of embodiment are understood. As a result, Tufekci necessarily oscillates between the formulation “we were always human” and the formulation “we were always posthuman,” precisely because the notion of the “human” is always a contingent category and different regimes of “humanity” have been deployed throughout history to produce the exclusions and inclusions so necessary for the construction of power through difference. The whole realm of ecstatic experience through ritual and shamanism, for example, is a perfect example of the persistent presence of an instability in the human/non-human boundary and in ideas about embodiment. Consequently, the ethnographic literature on shamanism, particularly from Amazonia (see Whitehead, this volume), has been inspirational and reinvigorating for this kind of discussion, especially for some of our authors (see also Alemán, Hoesterey, Wisniewski). Bringing not just cross-cultural but historical sensibilities to our analyses thus allows us to see both the contingency of current posthuman forms and also how there have been perhaps many “posthumanisms.”

Matthew Bernius provides an example that is especially “good to think” in this regard by examining how the “human” is manufactured in the creation of artificial intelligence (AI). As he demonstrates, AI-bots are becoming increasingly common in chatrooms, creating a social space in which not all “subjects” (or apparent subjects) are human. In a second example, Bernius looks at the Virtual Peers project at Northwestern University, where animated AI characters on screen are paired with digital objects throughout the physical environment, so that “the virtual peer is a complex system that extends itself into much of the environment in which she and her human interlocutors interact.” As tracking systems, two-dimensional barcodes, sensors, and other digital objects become increasingly part of our everyday lived spaces of sociality, such an example becomes especially important to examine. Jenny Ryan moves us into the realm of the truly “posthuman,” exploring how the dead live on through their digital traces and in virtual spaces where grievers gather to share and post their memories. Perhaps most interesting in this regard is the way in which most posters address their comments directly to the deceased. Here Ryan makes the point that “embodiment” may need to be reconfigured as the highly immersive nature of online interaction can create the sense that the deceased is really “there,” even after death. Perhaps nothing could say this more forcefully than the protest movement against Facebook’s policy of removing personal information from profiles after somebody dies, aptly named “Facebook Memoralization Is Misguided: Dead Friends are Still People.” For decades now scholars have speculated about and documented the possibilities of identity play online in virtual worlds (see, e.g., Rheingold 1993; Turkle 1995), a notion captured in the public imagination by the now classic New Yorker cartoon “On the Internet, nobody knows you are a dog.” But more recent forms of online sociality, such as Facebook, allow for what Zeynep Tufekci calls “grassroots surveillance,” in which ubiquitous peer surveillance of the most mundane activities monitored through status updates, photos, and videos make identity play much harder. In a play on the New Yorker title at a panel discussion Tufekci noted, “On the Internet, everybody knows you are a dog."
Wesch examines one of these new forms in the phenomenon of Anonymous, an “ongoing collective happening” that challenges our traditional notion of identity and group as all interlocutors on the site remain anonymous, communicating through text, links, and imagery. Their peculiar form of sociality, along with the values they express, presents a scathing critique of our cultural obsession with individualism and identity and the cult of celebrity that emerges from this obsession. Moreover, following the furor around Wikileaks and the support Anonymous gave to the figure of Julian Assange, as well as ongoing AnonOps against various governments and their agencies, there is an important way in which Anonymous’s challenges to individualism and identity represent a potentially new form of political engagement and resistance. Traditional notions of human identity become increasingly irrelevant to life as it is lived even as they begin to seem the most pertinent to the procedures of power and governance. As Wesch forcefully illustrates, we must recognize the ways in which “identity” is used as a way of rendering people legible to those who exercise power; and this remains a powerful structural element in current academic practice, no less than in the panopticon of the security state. In this way, questions of identity emerge as less important for scientific fact than for the way in which such notions validate the truth of the cultural quest for ever-expanding discovery and knowledge of others.

Radhika Gajjala has been rethinking “participant observation” during her two decades of research on South Asian diasporas, developing an especially acute sense of how different media “mediate” the communities we study. In this volume, she teams up with her colleague Sue Ellen McComas to explore how Indian diasporas have been mediated across generations, from pre-digital mediated narratives to a study of how members of a diaspora interact as avatars among the scripted objects of Second Life. There she has the opportunity to re-encounter those recurring critical questions that she has raised in earlier work: How do ethnographic practices and the ethnographer evolve in the online context? How are they revolutionized? What constitutes the field? For Gray Graffam, the “field” is the World of Warcraft, a virtual reality and massive social space that to date has more than 10 million players. In adapting ethnography to suit this field, Graffam found that interviews with players outside the game lacked the proper context that provides the performative cues for people to “be themselves” and answer his questions effectively. Fieldwork experiences like this remind us that observation of any kind (online and offline) is by definition utterly dependent on the forms of participation that the ethnographer may choose or have available. Such issues affect ethnography even as it is most traditionally conceived, especially as there is a growing worldwide cultural investment in online life, even among the most remote and marginal populations. In this volume, Stephanie Alemán reports on her experiences in a remote Guyanese village where the Waiwai first encounter the Internet and start to present themselves online. In one telling example, a young Waiwai man presents himself on Facebook “in a Taekwondo uniform and pose, and another, in a gangsta pose with a knit hat, with dark glasses and headphones and making hand signs.” Meanwhile, as the anthropologist, Alemán has collected images of him in more traditional “native activities”: adorned in full black body paint, shooting a bow and arrow, and dancing in the communal house. Alemán begins to see a clear dilemma for many anthropologists today: how do we represent those who can and will represent themselves? How do we address “their multiple and complex entailments with the regional and global networks to which they not only now have access but actively seek to engage”?

In this context of increasing anthropological anxiety, there is a growing need for the ethnographer to explicitly theorize participation no less adequately than we have painstakingly theorized observation and representation. As Wisniewski notes in this volume about his study of the invisible caboclos of Brazil in which he teamed up with two hippie “vagabond ethnographers,” “theorizing participation will give us a clearer understanding of how ethnographic knowledge is produced, revealing it as a shared product, an intersubjective product, not just of and about humans but of and about human interaction with all categories in a way that does not privilege or overvalue the role of the anthropologist in its production.” The need to theorize participation extends beyond the emerging contexts of online life to all spaces of virtuality where traditional notions of the human limit us or fail altogether. As Michael Heckenberger points out in this volume, not all “virtual realities” are digital or online, so the import of engaging digital subjectivities for wider theory lies precisely in how it may offer the possibility of eluding the Foucaldian/Marcusian nexus of knowledge-powermedia. In São Paulo, public discourses about public health and security create the unhuman nóias, a drug addict who has taken center stage as the persona of the irrational and subnormal in public discourse. Also referred to as zombies, inhabiting a space between life and death, such unhumans invoke different forms and moments of marginalization and oppression than those of the South Asian digital online residents, but nonetheless they experience displacement and disorientation similar to that produced in social encounters within a geographical diaspora. Like the zombies of the cityscapes, the diasporic are also engaged in an attempt to reframe their cultural identities to stave off the threat of cultural—if not physical—genocide through the effects of a rampant globalization.

In fact, such categories have revealed a vast field of social and cultural continuities among the human, animal, and technological. Current notions of biopower, the deployment of artificial intelligence and robotic systems in warfare and law enforcement, and the cultural logic of cinematic and televisual representations are all indications of the urgency with which anthropology should engage its new subjects. Regardless of where one stands on the question of the posthuman, it is clear that a jailbreak from late modernity does not go unchallenged. Stalwart symbols and institutions, such as hungry profiteers and militarized governance, seek to delimit the “human terrain” in both online and offline contexts, creating yet one more piece of the complex contexts and new spaces of cultural and social significance that have proliferated in the last decade. Here the “native populations”—the freaks, geeks, weirdos, techies, and net-addicts— like the savages at the margins of an earlier colonial order, defy simple inclusion into the frameworks of the state and its ethnographies. Living with the Mek, or the caboclos or the Waiwai, no less than the character-subjects of Anonymous or the online worlds of Second Life or My(Death)Space, must now take account of the endless interplay between offline and online subjectivity, while also expanding our notions and understandings of the vast potential of human diversity and social interaction. The stakes are high. Quoting Judith Butler, Heckenberger nicely observes how our cultural frames for thinking the human set limits on certain lives that are not considered lives at all: “[V]iolence against those who are already not quite living, living in a state of suspension between life and death, leaves a mark that is no mark.” These are the “killable bodies” discussed by Giorgio Agamben (1995) in his characterization of contemporary power and governance. As Wisniewski notes in this volume, it is time to expand and refine our approach so that we are equipped to grapple with the relationship between humans and technology, while also recognizing that humans are part of much larger systems that include relationships with animals, insects, microorganisms, spirits, and people who are not always considered human by others. And as humans become more digitally connected, we must also recognize that the sociality that emerges from such connections might not always be immediately analogous to traditional social formations and may involve unhuman actors and agencies (which may or may not be conceptualized or treated as human).

This is a book about the things people say about images. It is not primarily concerned with specific pictures and the things people say about them, but rather with the way we talk about the idea of imagery, and all its related notions of picturing, imagining, perceiving, likening, and imitating. It is a book about images, therefore, that has no illustrations except for a few schematic diagrams, a book about vision written as if by a blind author for a blind reader. If it contains any insight into real, material pictures, it is the sort that might come to a blind listener, overhearing the conversation of sighted speakers talking about images. M y hypothesis is that such a listener might see patterns in these conversations that would be invisible to the sighted participant. The book reflects on answers to two questions that come up regularly in these conversations: What is an image? What is the difference between images and words? It attempts to understand the traditional answers to these questions in relation to the human interests that give them urgency in particular situations. Why does it matter what an image is? What is at stake in marking off or erasing the differences between images and words? What are the systems of power and canons of value—that is, the ideologies—that inform the answers to these questions and make them matters of polemical dispute rather than purely theoretical interest?
If all this sounds impossibly comprehensive, it may help to note that this study has very definite limits, both in terms of the questions it raises and the body of texts it considers. Except for the first chapter this is primarily a series of close readings of a few important texts in the theory of imagery, and these readings revolve around two historical centers, one in the late eighteenth century (roughly, the era of the French Revolution and the rise of Romanticism), the other in the era of modern criticism. The aim of these readings is to show how the notion of imagery serves as a kind of relay connecting theories of art, language, and the mind with conceptions of social, cultural, and political value.

My only apology for these strange conjunctions of topics and texts is that they seemed to surface as I pursued the theoretical questions that inspired the study in the first place. Every theoretical answer to the questions, What is an image? Ho w are images different from words? seemed inevitably to fall back into prior questions of value and interest that could only be answered in historical terms. The simplest way of stating this is to admit that a book which began with the intention of producing a valid theory of images became a book about the fear of images. "Iconology" turned out to be, not just the science of icons, but the political psychology of icons, the study of iconophobia, iconophilia, and the struggle between iconoclasm and idolatry. The movement of this book is thus from modern attempts to establish a true theory of imagery (Gombrich, Goodman, the early Wittgenstein) to the "classic" accounts of imagery these theories sought to replace. In the process, my theoretical ambitions have inevitably been chastened by my narrow limits as an intellectual historian. My hope is that this critical fall into the space between theory and history will open up a region for other scholars to explore, and that it will suggest something about the necessary limits of any attempt to provide a theoretical account of symbolic practices.

Two things must immediately strike the notice of anyone who tries to take a general view of the phenomena called by the name of imagery. The first is simply the wide variety of things that go by this name. We speak of pictures, statues, optical illusions, maps, diagrams, dreams, hallucinations, spectacles, projections, poems, patterns, memories, and even ideas as images, and the sheer diversity of this list would seem to make any systematic, unified understanding impossible. The second thing that may strike us is that the calling of all these things by the name of "image" does not necessarily mean that they all have something in common. It might be better to begin by thinking of images as a far-flung family which has migrated in time and space and undergone profound mutations in the process. If images are a family, however, it may be possible to construct some sense of their genealogy. If we begin by looking, not for some universal definition of the term, but at those places where images have differentiated themselves from one another on the basis of boundaries between different institutional discourses, we come up with a family tree something like the following.

The mental and verbal images on the right side of our diagram, for instance, would seem to be images only in some doubtful, metaphork sense. People may report experiencing images in their heads while reading or dreaming, but we have only their word for this; there is no way (so the argument goes) to check up on this objectively. An d even if we trust the reports of mental imagery, it seems clear that they must be different from real, material pictures. Mental images don't seem to be stable and permanent the way real images are, and they vary from one person to the next: if I say "green," some listeners may see green in their mind's eye, but some may see a word, or nothing at all. An d mental images don't seem to be exclusively visual the way real pictures are; they involve all the senses. Verbal imagery, moreover, can involve all the senses, or it may involve no sensory component at all, sometimes suggesting nothing more than a recurrent abstract idea like justice or grace or evil. It is no wonder that literary scholars get very nervous when people start taking the notion of verbal imagery too literally." An d it is hardly surprising that one of the main thrusts of modern psychology and philosophy has been to discredit the notions of both mental and verbal imagery.
Wittgenstein's way of attacking mental imagery is not, however, the direct strategy of denying the existence of such images. He freely concedes that we may have mental images associated with thought or speech, insisting only that these images should not be thought of as private, metaphysical, immaterial entities any more than real images are. Wittgenstein's tactic is to demystify the mental image by bringing it right out in the open where we can see it: "Mental images of colours, shapes, sounds, etc., etc., which play a role in communication by means of language we put in the same category with patches of color actually seen, sounds heard."  It is a bit hard, however, to see how we can put mental and physical images "in the same category." We certainly can't do it by cutting open someone's head to compare mental pictures with the ones on our walls. A better strategy, and more in the Wittgensteinian spirit, would be to examine the ways we put those images "into our heads" in the first place by trying to picture the sort of world in which this move would make sense. I offer the figure on the next page as just such a picture.

But if the key to the recognition of real, material images in the world is our curious ability to say "there" and "not there" at the same time, we must then ask why mental images should be seen as any more—or less—mysterious than "real" images. The problem philosophers and ordinary people have always had with the notion of mental images is that they seem to have a universal basis in real, shared experience (we all dream, visualize, and are capable, in varying degrees, of re-presenting concrete sensations to ourselves), but we cannot point to them and say "There—that is a mental image." Exacdy the same sort of problem occurs, however, if I try to point to a real image and explain what it is to someone who doesn't already know what an image is. I point at Xeuxis's painting and say "There, that is an image." And the reply is, "D o you mean that colored surface?" Or "Do you mean those grapes?"

This is not to be taken as a claim that the mind really is a blank slate or a mirror—only that these are ways the mind is capable of picturing itself. It might picture itself in other ways: as a building, a statue, as an invisible gas or fluid, as a text, a narrative, or a melody, or as nothing in particular. It might decline to have a picture of itself, and refuse all selfrepresentation, just as we can look at a picture, a statue, or a mirror and not see it as a representational object. We might look at mirrors as shiny vertical objects, paintings as masses of colors on flat surfaces. There is no rule that the mind has to picture itself, or see pictures in itself, any more than there is a rule that we must go into a picture gallery, or that once inside we must look at the pictures. If we eliminate the notion that there is something necessary, natural, or automatic about the formation of both mental and material images, then we can do as Wittgenstein suggests, and put them "in the same category" as functional symbols, or, as in our model, in the same logical space. 17 This does not eliminate all differences between mental and physical images, but it may help to demystify the metaphysical or occult quality of this difference, and to allay our suspicion that mental images are somehow improper or illegitimately modeled on the "real thing." The path of derivation from original model to illegitimate analogy could as easily be traced in the opposite direction.

A good way to clarify the relation of mental and physical images is to reflect on the way we have just used a diagram to illustrate the matrix of analogies that connects theories of representation to theories of mind. We might be tempted to say that a mental version of this diagram was in our heads all along, before it appeared on the page, and that it was governing the way we discussed the boundary between mental and physical images. Well, perhaps it was; or perhaps it only occurred to us at a certain point in the discussion, when we began to use words like "boundary line" and "realm." Or perhaps it never occurred to us at all while thinking about these things or writing them down, and it was only later, after many revisions, that it came to mind. Does that mean that the mental diagram was there all along as a kind of unconscious deep structure determining our usage of the word "image"? Or is it a posterior construction, a graphic projection of the logical space implied in our propositions about imagery? In either case we certainly cannot regard the diagram as something mental in the sense of "private" or "subjective"; it is rather something that surfaced in language, and not just my language, but a way of speaking that we inherit from a long tradition of talking about minds and pictures. Our diagram might just as well be called a "verbal image" as a mental one, which brings us to that other notoriously illegitimate branch in the family tree of imagery, the notion of imagery in language.

This view of poetry, and of language in general, as a process of pictorial production and reproduction was accompanied in seventeenthand eighteenth-century English literary theory by a decline in the prestige of rhetorical figures and tropes. The notion of "image" replaced that of "figure," which began to be regarded as a feature of old-fashioned "ornamented" language. The literary style of verbal imagery is "plain" and "perspicuous," a style that reaches right out to objects, representing them (as Addison claims) even more vividly than the objects can represent themselves. This in contrast to the "deceptive ornament" of rhetoric, which is now seen as nothing but a matter of relations among signs. When the rhetorical figures are mentioned, they are either dismissed as the artificial excesses of a prerational, prescientific age, or they are redefined in ways that accommodate them to the hegemony of the verbal image. Metaphors are redefined as "short descriptions"; "allusions and similes are descriptions placed in an opposite point of view . . . and hyperbole is often nothing more than a description carried beyond the bounds of probability." Even abstractions are treated as pictorial, visual objects, projected in the verbal imagery of personification. In Romantic and modern poetics the verbal image retained its hold over the understanding of literary language, and the confused application of the term to both literal and figurative expression continued to encourage a lumping of notions such as description, concrete nouns, tropes, "sensory" terms, and even recurrent semantic, syntactic, or phonemic motifs under the rubric of "imagery." In order to do all this work, however, the notion of imagery had to be sublimated and mystified. Romantic writers typically assimilate mental, verbal, and even pictorial imagery into the mysterious process of "imagination," which is typically defined in contrast to the "mere" recall of mental pictures, the "mere" description of external scenes, and (in painting) the "mere" depiction of external visibilia, as opposed to the spirit, feeling, or "poetry" of a scene.

This progressive sublimation of the image reaches its logical culmination when the entire poem or text is regarded as an image or 'Verbal icon," and this image is defined, not as a pictorial likeness or impression, but as a synchronic structure in some metaphorical space—"that which" (in Pound's words) "presents an intellectual and emotional complex in an instant of time." The Imagists's emphasis on concrete, particular descriptions in their poetry is, by itself, a residue of the eighteenthcentury notion we have seen in Addison that poetry strives to outdo in vividness and immediacy the "images which flow from objects themselves" (Williams's "no ideas but in things" would seem to be another version of this idea). But the distinctive modernist emphasis is on the image as a sort of crystalline structure, a dynamic pattern of the intellectual and emotional energy bodied forth by a poem. Formalist criticism is both a poetics and a hermeneutics for this kind of verbal image, showing us how poems contain their energies in matrices of architectonic tension, and demonstrating the congruence of these matrices with the propositional content of the poem.

If the figure of the pictogram or hieroglyph demands a viewer who knows what to say, it also has a way of shaping the things that can be said. Consider further the ambiguous emblem/signature/ideogram of the petroglyph "eagle." If the warrior is an eagle, or "like" an eagle, or (more likely) if "Eagle himself" goes to war, and returns to tell about it, we can expect the picture to be extended. Eagle will no doubt see his enemies from afar and swoop down on them without warning. The "verbal image" of Eagle is a complex of speech, depiction, and writing that not only describes what he does, but predicts and shapes what he can and will do. It is his "character," a signature that is both verbal and pictorial, both a narrative of his actions and a summation of what he is.

But what exactly is this "spiritual" likeness which is not to be confused with any material image? We should note first that it seems to include a presumption of difference. To say that one tree, or one member of a species of tree, is like another, is not to argue that they are identical but that they are similar in some respects and not in others. Normally, however, we don't say that every likeness is an image. One tree is like another, but we don't call one the image of the other. The word "image" only comes up in relation to this sort of likeness when we try to construct a theory about the way we perceive the likeness between one tree and another. This explanation will typically resort to some intermediate or transcendental object—an idea, form, or mental image—that provides a mechanism for explaining how our categories arise. The "origin of species" is not just a matter of biological evolution then, but of the mechanisms of consciousness as they are described in representational models of the mind.

Is man created in the image of God, then, in that he looks like God, or in that we can say similar things about man and God? Milton wants to have it both ways, a desire we can trace to his rather unorthodox materialism, or perhaps more fundamentally, to a historic transformation in the concept of imagery which tended to identify the notion of spiritual likeness—particularly the "rational soul" that makes man an image of God—with a certain kind of material image. Milton's poetry is the scene of a struggle between iconoclastic distrust of the outward image and iconophilic fascination with its power, a struggle which manifests itself in his practice of proliferating visual images in order to prevent readers from focusing on any particular picture or scene. In order to see how the stage was set for this struggle we need to look more closely at the revolution which identified pictures or "artificial forms" with images as "likenesses" (Maimonides' "specific forms").

The revolution I am thinking of here was, of course, the invention of artificial perspective, first systematized by Alberti in 1435. The effect of this invention was nothing less than to convince an entire civilization that it possessed an infallible method of representation, a system for the automatic and mechanical production of truths about the material and the mental worlds. The best index to the hegemony of artificial perspective is the way it denies its own artificiality and lays claims to being a "natural" representation of "the way things look," "the way we see," or (in a phrase that turns Maimonides on his head) "the way things really are." Aided by the political and economic ascendance of Western Europe, artificial perspective conquered the world o f representation under the banner of reason, science, and objectivity. N o amount of counterdemonstration from artists that there are other ways of picturing what "we really see" has been able to shake the conviction that these pictures have a kind of identity with natural human vision and objective external space. An d the invention of a machine (the camera) built to produce this sort of image has, ironically, only reinforced the conviction that this is the natural mode of representation. What is natural is, evidendy, what we can build a machine to do for us.

If we discount the obvious hostility from Twain and Lessing's comments on the poverty of pictorial expression, we find a rather perspicuous account of what is meant by the notion of painting the invisible. What expression amounts to is the artful planting of certain clues in a picture that allow us to form an act of ventriloquism, an act which endows the picture with eloquence, and particularly with a nonvisual and verbal eloquence. A picture may articulate abstract ideas by means of allegorical imagery, a practice which, as Lessing notes, approaches the notational procedures of writing systems. The image of an eagle may depict a feathered predator, but it expresses the idea of wisdom, and thus works as a hieroglyph. Or we may understand expression in dramatic, oratorical terms, as did the Renaissance humanists who formulated a rhetoric of history painting complete with a language of facial expression and gesture, a language precise enough to let us verbalize what depicted figures are thinking, feeling, or saying. And expression need not be limited to predicates we can attach to pictured objects: the setting, compositional arrangement, and color scheme may all carry expressive charge, so that we can speak of moods and emotional atmospheres whose appropriate verbal counterparts may be something on the order of a lyric poem.

If I seem to be taking Twain's ironic attitude toward the claims of pictorial expression, it is not because I think that expression is impossible or illusory, but because our understanding of it is so often clouded by the same mystique of "natural representation" that obstructs our understanding of mimetic representation. Twain says that the label is worth more, for information, than "a ton of significant expression." But we might ask Twain how much the label would be worth, for information or for anything else, without this picture by Guido Reni, or the entire tradition of representing in pictorial, dramatic, or literary images the story of the Cenci. The painting is a confluence of pictorial and verbal traditions, neither of which is apparent to the innocent eyes of Twain, and so he can scarcely see what it is, much less respond to it.

What are we to make of this contest between the interests of verbal and pictorial representation? I propose that we historicize it, and treat it, not as a matter for peaceful settlement under the terms of some allembracing theory of signs, but as a struggle that carries the fundamental contradictions of our culture into the heart of theoretical discourse itself. The point, then, is not to heal the split between words and images, but to see what interests and powers it serves. This view can only be had, of course, from a standpoint which begins with skepticism about the adequacy of any particular theory of the relation of words and images, but which also preserves an intuitive conviction that there is some difference that is fundamental. It seems to me that Lessing, for instance, is absolutely right insofar as he regards poetry and painting as radically different modes or representation, but that his "mistake" (which theory still participates in) is the reification of this difference in terms of analogous oppositions like nature and culture, space and time.

Emerson once noted that the most fruitful conversations are always between two persons, not three. This principle may help to explain why the dialogue between poetry and painting has tended to dominate general discussions of the arts, and why music has seemed something of an outsider to the conversation. Al l the arts may aspire to the condition of music, but when they set out to argue, poetry and painting hold the stage. One reason for this is that they both lay claim to the same territory (reference, representation, denotation, meaning), a territory that music has tended to renounce. Another reason is that the differences between words and images seem so fundamental. They are not merely different kinds of creatures, but opposite kinds. They attract to their contest all the contrarieties and oppositions that riddle the discourse of criticism, the very discourse that takes as one of its projects a unified theory of the arts, an "aesthetics" which aspires to a synoptic view of artistic signs, a "semiotics" which hopes to comprehend all signs whatsoever.

This gap has two important functions in discussions of the arts and their symbol systems: it lends an air of tough-minded common sense to assertions of difference between the arts, and it gives an air of paradoxical daring and ingenuity to assertions of sameness or transference. The topic of the text-image difference provides an occasion for the exercise of the two great rhetorical skills, wit and judgment, "wit," as Edmund Burke noted, being "chiefly conversant in tracing resemblances," and judgment concerned mainly with "finding differences."1 Since aesthetics and semiotics dream of a theory that will satisfy both the need to discriminate artistic signs and to identify the principles that unite them, both these approaches to the topic have established themselves as traditional alternatives within the discourse of criticism. The mode of wit, the "tracing of resemblances," is the foundation of the ut pictura poesis and "sister arts" tradition in criticism, the construction of analogies or critical conceits that identify points of transference and resemblance between texts and images. Although these conceits are almost always accompanied by acknowledgments of differences between the arts, they are generally perceived as violations of good judgment that criticism ought to correct. Lessing opens the Laocoon by observing that "the first who compared painting with poetry was a man of fine feeling," not a critic or philosopher. 2 H e was, as Lessing goes on to explain, Simonides of Ceos, the legendary founder of the ut pictura poesis tradition. Lessing characterizes Simonides as a man of feeling and wit, "the Greek Voltaire," whose "dazzling antithesis that painting is dumb poetry and poetry speaking painting, stood in no textbook. It was one of those conceits, occurring frequendy in Simonides, the inexactness and falsity of which we feel constrained to overlook for the sake of the truth they contain.

In suggesting that these judicious discriminations are figurative I do not mean to assert that they are simply false, illusory, or without efficacy. O n the contrary, I want to suggest that they are powerful distinctions that effect the way the arts arc practiced and understood. I do mean to imply, however, that they are literally false, or (more generously) figuratively true. M y argument here will be twofold: (i) there is no essential difference between poetry and painting, no difference, that is, that is given for all time by the inherent natures of the media, the objects they represent, or the laws of the human mind; (2) there arc always a number of differences in effect in a culture which allow it to sort out the distinctive qualities o f its ensemble of signs and symbols. These differences, as I have suggested, are riddled with all the antithetical values the culture wants to embrace or repudiate: the parqgone or debate of poetry and painting is never just a contest between two kinds of signs, but a struggle between body and soul, world and mind, nature and culture.

Modern discussions of the relation between texts and images have tended to reduce this question to a problem of grammar. The traditional distinctions expressed in notions like time and space, nature and convention, have, in the work of modern theorists, been replaced by distinctions between different kinds of sign functions and communicative systems. We now speak of the difference between images and texts in terms such as the analogical and the digital, the iconic and the symbolic, the singleand the double-articulated. 1 These terms, drawn from fields such as systems analysis, semiotics, and linguistics, seem to promise a new, more scientific understanding of the boundaries between painting and poetry. They hold out the hope for a more rigorous definition of the difference and, especially in the work of the structuralists, the hope for a systematic way of comparing the arts. Modern theory has, in short, promised something to both sides of the traditional quarrel between the witty comparatists and the judicious differentiators of the arts: to the former it promises a higher level of generality and the prospect of large structural homologies between the arts; to the latter it offers a rigorous taxonomy that allows precise differentiation of sign-types and aesthetic modes.

For an arch-conventionalist like Goodman, the phrase that would probably stand out in this passage is the one that equates "artificial divisions" between the arts with "false" divisions, thus implying that man-made, conventional distinctions are, by virtue of their artificiality, automatically false. The contrast implied between these "false, artificial divisions" and "essential" features founded on "empirical and important facts" sets off a warning bell in the mind of a conventionalist; it also ought to sound an alarm for anyone who dislikes red herrings in arguments, and send them off in search of counterexamples to those "empirical facts" that lead us to Langer's categorical imperative: "there can be no hybrid works." An empirical survey of works that attempt graftings of verbal and pictorial signs (illustrated books, narrative paintings, films and dramas) does not immediately lead us to the conclusion that such hybrids are impossible. Even Langer's own logic of "essential" differences between the arts, aside from what the empirical facts tell us, leads to no such conclusion. One could as easily argue that such differences are a necessary condition for hybridization; the "crossing" of disparate forms to form new, composite unities makes no sense without an established set of differences, artificial or natural, to be overcome. Langer's notorious claim that "there are no happy marriages in art—only successful rape" illustrates perfecdy the sense of violence and violation she associates with the conjunction of artistic media, and hints (rather vividly) at its ideological basis in categories of gender.

Or , more accurately, one might say that the same old distinctions whose inadequacy motivated the search for a "general science of signs" tend to crop up in spite of the best efforts to weed them out. The disparities within the field of iconic signs that lead Eco to regard it as an incoherent category are precisely those sorts of oppositions that have traditionally figured the difference between texts and images. Some icons are "ruled by convention but are at the same time motivated." The word "motivated" in this context stands in the place occupied by terms like "nature" in traditional accounts of the text-image difference: "motivated" signs have a natural, necessary connection with what they signify; "unmotivated" signs are arbitrary and conventional. Eco's observation that icons sometimes seem "to be more firmly linked to the basic mechanisms of perception than to explicit cultural habits" is, similarly, a semiotic redaction of the notion that (some) images are "natural signs," and amounts to a contradiction in terms for a system that begins with the notion of the sign based in language.

There would be nothing wrong with this sort of redescription if it were not advertised as a liberation from metaphysics into a new science. The translation of Hume's laws of association into sign-types or modes of figuration has considerable interest. Among other things, it helps us to see just how riddled with notions of indirect, symbolic mediation are the supposedly "direct" perceptual mechanisms of the empirical tradition. The most striking example of this sort of mediation is, as we have seen, the notion of the mental or perceptual image ("ideas" and "sense-data"), which, on the one hand, seem to guarantee veridical access to the world, on the other hand to indefinitely and irretrievably distance the world through a system of intermediate signs. This double bind may be seen most clearly in the attempt of semioticians to come up with an account of the "photographic sign."

These doubly natural signs, iconic and indexical, then serve as the foundation for all further intellection and discourse. Among other things, they stand as the referents for words, which unlike the ideaimpression-mental image, signify (as Locke puts it) "not by any natural connexion . . . but by a voluntary imposition, whereby such a word is made arbitrarily the mark of such an idea." Ideas, by contrast, are naturally imprinted by experience and reflection: they are natural signs that (ideally) stand behind the arbitrary signs of language. The relation of words and ideas, discourse and thought, turns on the very same hinge that, in semiotics, connects the symbol with the indexical icon, the arbitrary code with the "natural" code.  Small wonder, then, that Roland Bardies finds himself saying the following sorts of things about photographs.

When Wilson catalogues what he shares with his Furby, there are things of the body (the burping) and there are things of the mind. Like many children, he thinks that because Furbies have language, they are more “peoplelike” than a “regular” pet. They arrive speaking Furbish, a language with its own dictionary, which many children try to commit to memory because they would like to meet their Furbies more than half way. The Furby manual instructs children, “I can learn to speak English by listening to you talk. The more you play with me, the more I will use your language.” Actually, Furby English emerges over time, whether or not a child talks to the robot. (Furbies have no hearing or language-learning ability.5) But until age eight, children are convinced by the illusion and believe they are teaching their Furbies to speak. The Furbies are alive enough to need them. Children enjoy the teaching task. From the first encounter, it gives them something in common with their Furbies and it implies that the Furbies can grow to better understand them. “I once didn’t know English,” says one six-year-old. “And now I do. So I know what my Furby is going through.” In the classroom with Furbies, children shout to each other in competitive delight: “My Furby speaks more English than yours! My Furby speaks English.” I have done several studies in which I send Furbies home with schoolchildren, often with the request that they (and their parents) keep a “Furby diary.” In my first study of kindergarten to third graders, I loan the Furbies out for two weeks at a time. It is not a good decision. I do not count on how great will be children’s sense of loss when I ask them to return the Furbies. I extend the length of the loans, often encouraged by parental requests. Their children have grown too attached to give up the robots. Nor are they mollified by parents’ offers to buy them new Furbies. Even more so than with Tamagotchis, children attach to a particular Furby, the one they have taught English, the one they have raised. For three decades, in describing people’s relationships with computers, I have often used the metaphor of the Rorschach, the inkblot test that psychologists use as a screen onto which people can project their feelings and styles of thought. But as children interact with sociable robots like Furbies, they move beyond a psychology of projection to a new psychology of engagement. They try to deal with the robot as they would deal with a pet or a person. Nineyear-old Leah, in an after-school playgroup, admits, “It’s hard to turn it [the Furby] off when it is talking to me.” Children quickly understand that to get the most out of your Furby, you have to pay attention to what it is telling you. When you are with a Furby, you can’t play a simple game of projective make-believe. You have to continually assess your Furby’s “emotional” and “physical” state. And children fervently believe that the child who loves his or her Furby best will be most loved in return.
This mutuality is at the heart of what makes the Furby, a primitive exemplar of sociable robotics, different from traditional dolls. As we’ve seen, such relational artifacts do not wait for children to “animate” them in the spirit of a Raggedy Ann doll or a teddy bear. They present themselves as already animated and ready for relationship. They promise reciprocity because, unlike traditional dolls, they are not passive. They make demands. They present as having their own needs and inner lives. They teach us the rituals of love that will make them thrive. For decades computers have asked us to think with them; these days, computers and robots, deemed sociable, affective, and relational, ask us to feel for and with them. Children see traditional dolls as they want them or need them to be. For example, an eight-year-old girl who feels guilty about breaking her mother’s best crystal pitcher might punish a row of Barbie dolls. She might take them away from their tea party and put them in detention, doing unto the dolls what she imagines should be done unto her. In contrast, since relational artifacts present themselves as having minds and intentions of their own, they cannot be so easily punished for one’s own misdeeds. Two eight-year-old girls comment on how their “regular dolls” differ from the robotic Furbies. The first says, “A regular doll, like my Madeleine doll . . . you can make it go to sleep, but its eyes are painted open, so, um, you cannot get them to close their eyes.... Like a Madeleine doll cannot go, ‘Hello, good morning.’” But this is precisely the sort of thing a Furby can do. The second offers, “The Furby tells you what it wants.” Indeed, Furbies come with manuals that provide detailed marching orders. They want language practice, food, rest, and protestations of love. So, for example, the manual instructs, “Make sure you say ‘HEY FURBY! I love you!’ frequently so that I feel happy and know I’m loved.” There is general agreement among children that a penchant for giving instructions distinguishes Furbies from traditional dolls. A seven-year-old girl puts it this way: “Dolls let you tell them what they want. The Furbies have their own ideas.” A nine-year-old boy sums up the difference between Furbies and his action figures: “You don’t play with the Furby, you sort of hang out with it. You do try to get power over it, but it has power over you too.” Children say that traditional dolls can be “hard work” because you have to do all the work of giving them ideas; Furbies are hard work for the opposite reason. They have plenty of ideas, but you have to give them what they want and when they want it. When children attach to a doll through the psychology of projection, they attribute to the doll what is most on their mind. But they need to accommodate a Furby. This give-and-take prepares children for the expectation of relationship with machines that is at the heart of the robotic moment.

Daisy, six, with a Furby at home, believes that each Furby’s owner must help his or her Furby fulfill its mission to learn about people. “You have to teach it; when you buy it, that is your job.” Daisy tells me that she taught her Furby about Brownie Girl Scouts, kindergarten, and whales. “It’s alive; I teach it about whales; it loves me.” Padma, eight, says that she likes meeting what she calls “Furby requests” and thinks that her Furby is “kind of like a person” because “it talks.” She goes on: “It’s kind of like me because I’m a chatterbox.” After two weeks, it is time for Padma to return her Furby, and afterward she feels regret: “I miss how it talked, and now it’s so quiet at my house.... I didn’t get a chance to make him a bed.” After a month with her Furby, Bianca, seven, speaks with growing confidence about their mutual affection: “I love my Furby because it loves me. . . . It was like he really knew me.”6 She knows her Furby well enough to believe that “it doesn’t want to miss fun . . . at a party.” In order to make sure that her social butterfly Furby gets some rest when her parents entertain late into the evening, Bianca clips its ears back with clothespins to fool the robot into thinking that “nothing is going on . . . so he can fall asleep.” This move is ineffective, and all of this activity is exhausting, but Bianca calmly sums up her commitment: “It takes lots of work to take care of these.” When Wilson, who so enjoys burping in synchrony with his Furby, faces up to the hard work of getting his Furby to sleep, he knows that if he forces sleep by removing his Furby’s batteries, the robot will “forget” whatever has passed between them—this is unacceptable. So Furby sleep has to come naturally. Wilson tries to exhaust his Furby by keeping it up late at night watching television. He experiments with Furby “sleep houses” made of blankets piled high over towers of blocks. When Wilson considers Furby sleep, his thoughts turn to Furby dreams. He is sure his Furby dreams “when his eyes are closed.” What do Furbies dream of? Second and third graders think they dream “of life on their flying saucers.”7 And they dream about learning languages and playing with the children they love.
In the 1980s, the computer toy Merlin made happy and sad noises depending on whether it was winning or losing the sound-and-light game it played with children. Children saw Merlin as “sort of alive” because of how well it played memory games, but they did not fully believe in Merlin’s shows of emotion. When a Merlin broke down, children were sorry to lose a playmate. When a Furby doesn’t work, however, children see a creature that might be in pain. Lily, ten, worries that her broken Furby is hurting. But she doesn’t want to turn it off, because “that means you aren’t taking care of it.” She fears that if she shuts off a Furby in pain, she might make things worse. Two eight-year-olds fret about how much their Furbies sneeze. The first worries that his sneezing Furby is allergic to him. The other fears his Furby got its cold because “I didn’t do a good enough job taking care of him.” Several children become tense when Furbies make unfamiliar sounds that might be signals of distress. I observe children with their other toys: dolls, toy soldiers, action figures. If these toys make strange sounds, they are usually put aside; broken toys lead easily to boredom. But when a Furby is in trouble, children ask, “Is it tired?” “Is it sad?” “Have I hurt it?” “Is it sick?” “What shall I do?” Taking care of a robot is a high-stakes game. Things can—and do—go wrong. In one kindergarten, when a Furby breaks down, the children decide they want to heal it. Ten children volunteer, seeing themselves as doctors in an emergency room. They decide they’ll begin by taking it apart. The proceedings begin in a state of relative calm. When talking about their sick Furby, the children insist that this breakdown does not mean the end: people get sick and get better. But as soon as scissors and pliers appear, they become anxious. At this point, Alicia screams, “The Furby is going to die!” Sven, to his classmates’ horror, pinpoints the moment when Furbies die: it happens when a Furby’s skin is ripped off. Sven considers the Furby as an animal. You can shave an animal’s fur, and it will live. But you cannot take its skin off. As the operation continues, Sven reconsiders. Perhaps the Furby can live without its skin, “but it will be cold.” He doesn’t back completely away from the biological (the Furby is sensitive to the cold) but reconstructs it. For Sven, the biological now includes creatures such as Furbies, whose “insides” stay “all in the same place” when their skin is removed. This accommodation calms him down. If a Furby is simultaneously biological and mechanical, the operation in process, which is certainly removing the Furby’s skin, is not necessarily destructive. Children make theories when they are confused or anxious. A good theory can reduce anxiety.
But some children become more anxious as the operation continues. One suggests that if the Furby dies, it might haunt them. It is alive enough to turn into a ghost. Indeed, a group of children start to call the empty Furby skin “the ghost of Furby” and the Furby’s naked body “the goblin.” They are not happy that this operation might leave a Furby goblin and ghost at large. One girl comes up with the idea that the ghost of the Furby will be less fearful if distrib- uted. She asks if it would be okay “if every child took home a piece of Furby skin.” She is told this would be fine, but, unappeased, she asks the same question two more times. In the end, most children leave with a bit of Furby fur.8 Some talk about burying it when they get home. They leave room for a private ritual to placate the goblin and say good-bye. Inside the classroom, most of the children feel they are doing the best they can with a sick pet. But from outside the classroom, the Furby surgery looks alarming. Children passing by call out, “You killed him.” “How dare you kill Furby?” “You’ll go to Furby jail.” Denise, eight, watches some of the goings-on from the safety of the hall. She has a Furby at home and says that she does not like to talk about its problems as diseases because “Furbies are not animals.” She uses the word “fake” to mean nonbiological and says, “Furbies are fake, and they don’t get diseases.” But later, she reconsiders her position when her own Furby’s batteries run out and the robot, so chatty only moments before, becomes inert. Denise panics: “It’s dead. It’s dead right now.... Its eyes are closed.” She then declares her Furby “both fake and dead.” Denise concludes that worn-out batteries and water can kill a Furby. It is a mechanism, but alive enough to die. Linda, six, is one of the children whose family has volunteered to keep a Furby for a twoweek home study. She looked forward to speaking to her Furby, sure that unlike her other dolls, this robot would be worth talking to. But on its very first night at her home, her Furby stops working: “Yeah, I got used to it, and then it broke that night—the night that I got it. I felt like I was broken or something.... I cried a lot. . . . I was really sad that it broke, ’cause Furbies talk, they’re like real, they’re like real people.” Linda is so upset about not protecting her Furby that when it breaks she feels herself broken. Things get more complicated when I give Linda a new Furby. Unlike children like Zach who have invested time and love in a “first Furby” and want no replacements, Linda had her original Furby in working condition for only a few hours. She likes having Furby #2: “It plays hide-and-seek with me. I play red light, green light, just like in the manual.” Linda feeds it and makes sure it gets enough rest, and she reports that her new Furby is grateful and affectionate. She makes this compatible with her assessment of a Furby as “just a toy” because she has come to see gratitude, conversation, and affection as something that toys can manage. But now she will not name her Furby or say it is alive. There would be risk in that: Linda might feel guilty if the new Furby were alive enough to die and she had a replay of her painful first experience.
When a mechanism breaks, we may feel regretful, inconvenienced, or angry. We debate whether it is worth getting it fixed. When a doll cries, children know that they are themselves creating the tears. But a robot with a body can get “hurt,” as we saw in the improvised Furby surgical theater. Sociable robotics exploits the idea of a robotic body to move people to relate to machines as subjects, as creatures in pain rather than broken objects. That even the most primitive Tamagotchi can inspire these feelings demonstrates that objects cross that line not because of their sophistication but because of the feelings of attachment they evoke. The Furby, even more than the Tamagotchi, is alive enough to suggest a body in pain as well as a troubled mind. Furbies whine and moan, leaving it to their users to discover what might help. And what to make of the moment when an upside down Furby says, “Me scared!”? Freedom Baird takes this question very seriously.9 A recent graduate of the MIT Media Lab, she finds herself engaged with her Furby as a creature and a machine. But how seriously does she take the idea of the Furby as a creature? To determine this, she proposes an exercise in the spirit of the Turing test. In the original Turing test, published in 1950, mathematician Alan Turing, inventor of the first general-purpose computer, asked under what conditions people would consider a computer intelligent. In the end, he settled on a test in which the computer would be declared intelligent if it could convince people it was not a machine. Turing was working with computers made up of vacuum tubes and Teletype terminals. He suggested that if participants couldn’t tell, as they worked at their Teletypes, if they were talking to a person or a computer, that computer would be deemed “intelligent.” 10 A half century later, Baird asks under what conditions a creature is deemed alive enough for people to experience an ethical dilemma if it is distressed. She designs a Turing test not for the head but for the heart and calls it the “upside-down test.” A person is asked to invert three creatures: a Barbie doll, a Furby, and a biological gerbil. Baird’s question is simple: “How long can you hold the object upside down before your emotions make you turn it back?” Baird’s experiment assumes that a sociable robot makes new ethical demands. Why? The robot performs a psychology; many experience this as evidence of an inner life, no matter how primitive. Even those who do not think a Furby has a mind—and this, on a conscious level, includes most people—find themselves in a new place with an upside-down Furby that is whining and telling them it is scared. They feel themselves, often despite themselves, in a situation that calls for an ethical response. This usually happens at the moment when they identify with the “creature” before them, all the while knowing that it is “only a machine.” This simultaneity of vision gives Baird the predictable results of the upside-down test. As Baird puts it, “People are willing to be carrying the Barbie around by the feet, slinging it by the hair . . . no problem.... People are not going to mess around with their gerbil.” But in the case of the Furby, people will “hold the Furby upside down for thirty seconds or so, but when it starts crying and saying it’s scared, most people feel guilty and turn it over.” The work of neuroscientist Antonio Damasio offers insight into the origins of this guilt. Damasio describes two levels of experiencing pain. The first is a physical response to a painful stimulus. The second, a far more complex reaction, is an emotion associated with pain. This is an internal representation of the physical. 11 When the Furby says, “Me scared,” it signals that it has crossed the line between a physical response and an emotion, the internal representation. When people hold a Furby upside down, they do something that would be painful if done to an animal. The Furby cries out—as if it were an animal. But then it says, “Me scared”—as if it were a person.
People are surprised by how upset they get in this theater of distress. And then they get upset that they are upset. They often try to reassure themselves, saying things like, “Chill, chill, it’s only a toy!” They are experiencing something new: you can feel bad about yourself for how you behave with a computer program. Adults come to the upside-down test knowing two things: the Furby is a machine and they are not torturers. By the end, with a whimpering Furby in tow, they are on new ethical terrain.12 We are at the point of seeing digital objects as both creatures and machines. A series of fractured surfaces—pet, voice, machine, friend—come together to create an experience in which knowing that a Furby is a machine does not alter the feeling that you can cause it pain. Kara, a woman in her fifties, reflects on holding a moaning Furby that says it is scared. She finds it distasteful, “not because I believe that the Furby is really scared, but because I’m not willing to hear anything talk like that and respond by continuing my behavior. It feels to me that I could be hurt if I keep doing this.” For Kara, “That is not what I do.... In that moment, the Furby comes to represent how I treat creatures.” When the toy manufacturer Hasbro introduced its My Real Baby robot doll in 2000, it tried to step away from these complex matters. My Real Baby shut down in situations where a real baby might feel pain. This was in contrast to its prototype, a robot called “IT,” developed by a team led by MIT roboticist Rodney Brooks. “IT” evolved into “BIT” (for Baby IT), a doll with “states of mind” and facial musculature under its synthetic skin to give it expression.13 When touched in a way that would induce pain in a child, BIT cried out. Brooks describes BIT in terms of its inner states: If the baby were upset, it would stay upset until someone soothed it or it finally fell asleep after minutes of heartrending crying and fussing. If BIT . . . was abused in any way—for instance, by being swung upside down—it got very upset. If it was upset and someone bounced it on their knee, it got more upset, but if the same thing happened when it was happy, it got more and more excited, giggling and laughing, until eventually it got overtired and started to get upset. If it were hungry, it would stay hungry until it was fed. It acted a lot like a real baby.14 BIT, with its reactions to abuse, became the center of an ethical world that people constructed around its responses to pleasure and pain. But when Hasbro put BIT into mass production as My Real Baby, the company decided not to present children with a toy that responded to pain. The theory was that a robot’s response to pain could “enable” sadistic behavior. If My Real Baby were touched, held, or bounced in a way that would hurt a real baby, the robot shut down.
In its promotional literature, Hasbro marketed My Real Baby as “the most real, dynamic baby doll available for young girls to take care of and nurture.” They presented it as a companion that would teach and encourage reciprocal social behavior as children were trained to respond to its needs for amusement as well as bottles, sleep, and diaper changes. Indeed, it was marketed as realistic in all things—except that if you “hurt” it, it shut down. When children play with My Real Baby, they do explore aggressive possibilities. They spank it. It shuts down. They shake it, turn it upside down, and box its ears. It shuts down. Hasbro’s choice—maximum realism, but with no feedback for abuse—inspires strong feelings, especially among parents. For one group of parents, what is most important is to avoid a child’s aggressive response. Some believe that if you market realism but show no response to “pain,” children are encouraged to inflict it because doing so seems to have no cost. Others think that if a robot simulates pain, it enables mistreatment. Another group of parents wish that My Real Baby would respond to pain for the same reason that they justify letting their children play violent video games: they see such experiences as “cathartic.” They say that children (and adults too) should express aggression (or sadism or curiosity) in situations that seem “realistic” but where nothing “alive” is being hurt. But even these parents are sometimes grateful for My Real Baby’s unrealistic show of “denial.” They do not want to see their children tormenting a screaming baby. No matter what position one takes, sociable robots have taught us that we do not shirk from harming realistic simulations of life. This is, of course, how we now train people for war. First, we learn to kill the virtual. Then, desensitized, we are sent to kill the real. The prospect of studying these matters raises awful questions. Freedom Baird had people hold a whining, complaining Furby upside down, much to their discomfort. Do we want to encourage the abuse of increasingly realistic robot dolls? When I observe children with My Real Baby in an after-school playgroup for eight-yearolds, I see a range of responses. Alana, to the delight of a small band of her friends, flings My Real Baby into the air and then shakes it violently while holding it by one leg. Alana says the robot has “no feelings.” Watching her, one wonders why it is necessary then to “torment” something without feelings. She does not behave this way with the many other dolls in the playroom. Scott, upset, steals the robot and brings it to a private space. He says, “My Real Baby is like a baby and like a doll.... I don’t think she wants to get hurt.” As Scott tries to put the robot’s diaper back on, some of the other children stand beside him and put their fingers in its eyes and mouth. One asks, “Do you think that hurts?” Scott warns, “The baby’s going to cry!” At this point, one girl tries to pull My Real Baby away from Scott because she sees him as an inadequate protector: “Let go of her!” Scott resists. “I was in the middle of changing her!” It seems a good time to end the play session. As the research team, exhausted, packs up to go, Scott sneaks behind a table with the robot, gives it a kiss, and says good-bye, out of the sight of the other children.
In the pandemonium of Scott and Alana’s playgroup, My Real Baby is alive enough to torment and alive enough to protect. The adults watching this—a group of teachers and my research team—feel themselves in an unaccustomed quandary. If the children had been tossing around a rag doll, neither we, nor presumably Scott, would have been as upset. But it is hard to see My Real Baby treated this way. All of this—the Furbies that complain of pain, the My Real Babies that do not—creates a new ethical landscape. The computer toys of the 1980s only suggested ethical issues, as when children played with the idea of life and death when they “killed” their Speak & Spells by taking out the toys’ batteries. Now, relational artifacts pose these questions directly. One can see the new ethics at work in my students’ reactions to Nexi, a humanoid robot at MIT. Nexi has a female torso, an emotionally expressive face, and the ability to speak. In 2009, one of my students, researching a paper, made an appointment to talk with the robot’s development team. Due to a misunderstanding about scheduling, my student waited alone, near the robot. She was upset by her time there: when not interacting with people, Nexi was put behind a curtain and blindfolded. At the next meeting of my graduate seminar, my student shared her experience of sitting alongside the robot. “It was very upsetting,” she said. “The curtain—and why was she blindfolded? I was upset because she was blindfolded.” The story of the shrouded and blindfolded Nexi ignited the seminar. In the conversation, all the students talked about the robot as a “she.” The designers had done everything they could to give the robot gender. And now, the act of blindfolding signaled sight and consciousness. In class, questions tumbled forth: Was the blindfold there because it would be too upsetting to see Nexi’s eyes? Perhaps when Nexi was turned off, “her” eyes remained open, like the eyes of a dead person? Perhaps the robot makers didn’t want Nexi to see “out”? Perhaps they didn’t want Nexi to know that when not in use, “she” is left in a corner behind a curtain? This line of reasoning led the seminar to an even more unsettling question: If Nexi is smart enough to need a blindfold to protect “her” from fully grasping “her” situation, does that mean that “she” is enough of a subject to make “her” situation abusive? The students agreed on one thing: blindfolding the robot sends a signal that “this robot can see.” And seeing implies understanding and an inner life, enough of one to make abuse possible. I have said that Sigmund Freud saw the uncanny as something long familiar that feels strangely unfamiliar. The uncanny stands between standard categories and challenges the categories themselves. It is familiar to see a doll at rest. But we don’t need to cover its eyes, for it is we who animate it. It is familiar to have a person’s expressive face beckon to us, but if we blindfold that person and put them behind a curtain, we are inflicting punishment. The Furby with its expressions of fear and the gendered Nexi with her blindfold are the new uncanny in the culture of computing.
Soon, it may seem natural to watch a robot “suffer” if you hurt it. It may seem natural to chat with a robot and have it behave as though pleased you stopped by. As the intensity of experiences with robots increases, as we learn to live in new landscapes, both children and adults may stop asking the questions “Why am I talking to a robot?” and “Why do I want this robot to like me?” We may simply be charmed by the pleasure of its company. The romantic reaction of the 1980s and 1990s put a premium on what only people can contribute to each other: the understanding that grows out of shared human experience. It insisted that there is something essential about the human spirit. In the early 1980s, David, twelve, who had learned computer programming at school, contrasted people and programs this way: “When there are computers who are just as smart as the people, the computers will do a lot of the jobs, but there will still be things for the people to do. They will run the restaurants, taste the food, and they will be the ones who will love each other, have families and love each other. I guess they’ll still be the only ones who go to church.”15 Adults, too, spoke of life in families. To me, the romantic reaction was captured by how one man rebuffed the idea that he might confide in a computer psychotherapist: “How can I talk about sibling rivalry to something that never had a mother?” Of course, elements of this romantic reaction are still around us. But a new sensibility emphasizes what we share with our technologies. With psychopharmacology, we approach the mind as a bioengineerable machine.16 Brain imaging trains us to believe that things—even things like feelings—are reducible to what they look like. Our current therapeutic culture turns from the inner life to focus on the mechanics of behavior, something that people and robots might share. A quarter of a century stands between two conversations I had about the possibilities of a robot confidant, the first in 1983, the second in 2008. For me, the differences between them mark the movement from the romantic reaction to the pragmatism of the robotic moment. Both conversations were with teenage boys from the same Boston neighborhood; they are both Red Sox fans and have close relationships with their fathers. In 1983, thirteen-year-old Bruce talked about robots and argued for the unique “emotionality” of people. Bruce rested his case on the idea that computers and robots are “perfect,” while people are “imperfect,” flawed and frail. Robots, he said, “do everything right”; people “do the best they know how.” But for Bruce it was human imperfection that makes for the ties that bind. Specifically, his own limitations made him feel close to his father (“I have a lot in common with my father.... We both have chaos”). Perfect robots could never understand this very important relationship. If you ever have a problem, you go to a person.
Twenty-five years later, a conversation on the same theme goes in a very different direction. Howard, fifteen, compares his father to the idea of a robot confidant, and his father does not fare well in the comparison. Howard thinks the robot would be better able to grasp the intricacies of high school life: “Its database would be larger than Dad’s. Dad has knowledge of basic things, but not enough of high school.” In contrast to Bruce’s sense that robots are not qualified to have an opinion about the goings-on in families, Howard hopes that robots might be specially trained to take care of “the elderly and children”—something he doesn’t see the people around him as much interested in. Howard has no illusions about the uniqueness of people. In his view, “they don’t have a monopoly” on the ability to understand or care for each other. Each human being is limited by his or her own life experience, says Howard, but “computers and robots can be programmed with an infinite amount of information.” Howard tells a story to illustrate how a robot could provide him with better advice than his father. Earlier that year, Howard had a crush on a girl at school who already had a boyfriend. He talked to his father about asking her out. His father, operating on an experience he had in high school and what Howard considers an outdated ideal of “macho,” suggested that he ask the girl out even though she was dating someone else. Howard ignored his father’s advice, fearing it would lead to disaster. He was certain that in this case, a robot would have been more astute. The robot “could be uploaded with many experiences” that would have led to the right answer, while his father was working with a limited data set. “Robots can be made to understand things like jealousy from observing how people behave.... A robot can be fully understanding and open-minded.” Howard thinks that as a confidant, the robot comes out way ahead. “People,” he says, are “risky.” Robots are “safe.” There are things, which you cannot tell your friends or your parents, which . . . you could tell an AI. Then it would give you advice you could be more sure of.... I’m assuming it would be programmed with prior knowledge of situations and how they worked out. Knowledge of you, probably knowledge of your friends, so it could make a reasonable decision for your course of action. I know a lot of teenagers, in particular, tend to be caught up in emotional things and make some really bad mistakes because of that.
I ask Howard to imagine what his first few conversations with a robot might be like. He says that the first would be “about happiness and exactly what that is, how do you gain it.” The second conversation would be “about human fallibility,” understood as something that causes “mistakes.” From Bruce to Howard, human fallibility has gone from being an endearment to a liability. No generation of parents has ever seemed like experts to their children. But those in Howard’s generation are primed to see the possibilities for relationships their elders never envisaged. They assume that an artificial intelligence could monitor all of their e-mails, calls, Web searches, and messages. This machine could supplement its knowledge with its own searches and retain a nearly infinite amount of data. So, many of them imagine that via such search and storage an artificial intelligence or robot might tune itself to their exact needs. As they see it, nothing technical stands in the way of this robot’s understanding, as Howard puts it, “how different social choices [have] worked out.” Having knowledge and your best interests at heart, “it would be good to talk to . . . about life. About romantic matters. And problems of friendship.” Life? Romantic matters? Problems of friendship? These were the sacred spaces of the romantic reaction. Only people were allowed there. Howard thinks that all of these can be boiled down to information so that a robot can be both expert resource and companion. We are at the robotic moment. As I have said, my story of this moment is not so much about advances in technology, impressive though these have been. Rather, I call attention to our strong response to the relatively little that sociable robots offer—fueled it would seem by our fond hope that they will offer more. With each new robot, there is a ramp-up in our expectations. I find us vulnerable—a vulnerability, I believe, not without risk.
In April 1999, a month before AIBO’s commercial release, Sony demonstrated the little robot dog at a conference on new media in San Jose, California. I watched it walk jerkily onto an empty stage, followed by its inventor, Toshitado Doi. At his bidding, AIBO fetched a ball and begged for a treat. Then, with seeming autonomy, AIBO raised its back leg to some suggestion of a hydrant. Then, it hesitated, a stroke of invention in itself, and lowered its head as though in shame. The audience gasped. The gesture, designed to play to the crowd, was wildly successful. I imagined how audiences responded to Jacques de Vaucanson’s eighteenth-century digesting (and defecating) mechanical duck and to the chess-playing automata that mesmerized Edgar Alan Poe. AIBO, like these, was applauded as a marvel, a wonder.1 Depending on how it is treated, an individual AIBO develops a distinct personality as it matures from a fall-down puppy to a grown-up dog. Along the way, AIBO learns new tricks and expresses feelings: flashing red and green eyes direct our emotional traffic; each of its moods comes with its own soundtrack. A later version of AIBO recognizes its primary caregiver and can return to its charging station, smart enough to know when it needs a break. Unlike a Furby, whose English is “destined” to improve as long as you keep it turned on, AIBO stakes a claim to intelligence and impresses with its ability to show what’s on its mind. If AIBO is in some sense a toy, it is a toy that changes minds. It does this in several ways. It heightens our sense of being close to developing a postbiological life and not just in theory or in the laboratory. And it suggests how this passage will take place. It will begin with our seeing the new life as “as if ” life and then deciding that “as if ” may be life enough. Even now, as we contemplate “creatures” with artificial feelings and intelligence, we come to reflect differently on our own. The question here is not whether machines can be made to think like people but whether people have always thought like machines. The reconsiderations begin with children. Zane, six, knows that AIBO doesn’t have a “real brain and heart,” but they are “real enough.” AIBO is “kind of alive” because it can function “as if it had a brain and heart.” Paree, eight, says that AIBO’s brain is made of “machine parts,” but that doesn’t keep it from being “like a dog’s brain.... Sometimes, the way [AIBO] acted, like he will get really frustrated if he can’t kick the ball. That seemed like a real emotion . . . so that made me treat him like he was alive, I guess.” She says that when AIBO needs its batteries charged, “it is like a dog’s nap.” And unlike a teddy bear, “an AIBO needs its naps.” As Paree compares her AIBO’s brain to that of a dog, she clears the way for other possibilities. She considers whether AIBO might have feelings like a person, wondering if AIBO “knows its own feelings”—or “if the controls inside know them.” Paree says that people use both methods. Sometimes people have spontaneous feelings and “just become aware” of them (this is “knowing your own feelings”). But other times, people have to program themselves to have the feelings they want. “If I was sad and wanted to be happy”—here Paree brings her fists up close to her ears to demonstrate concentration and intent—“I would have to make my brain say that I am set on being happy.” The robot, she thinks, probably has the second kind of feelings, but she points out that both ways of getting to a feeling get you to the same place: a smile or a frown if you are a person, a happy or sad sound if you are an AIBO. Different inner states lead to the same outward states, and so inner states cease to matter. AIBO carries a behaviorist sensibility.
Keith, seventeen, is going off to college next year and taking his AIBO with him. He treats the robot as a pet, all the while knowing that it is not a pet at all. He says, “Well, it’s not a pet like others, but it is a damn good pet. . . . I’ve taught it everything. I’ve programmed it to have a personality that matches mine. I’ve never let it reset to its original personality. I keep it on a program that lets it develop to show the care I’ve put into it. But of course, it’s a robot, so you have to keep it dry, you have to take special care with it.” His classmate Logan also has an AIBO. The two have raised the robots together. If anything, Logan’s feelings are even stronger than Keith’s. Logan says that talking to AIBO “makes you better, like, if you’re bored or tired or down . . . because you’re actually, like, interacting with something. It’s nice to get thoughts out.” The founders of artificial intelligence were much taken with the ethical and theological implications of their enterprise. They discussed the mythic resonance of their new science: Were they people putting themselves in the place of gods?2 The impulse to create an object in one’s own image is not new—think Galatea, Pygmalion, Frankenstein. These days, what is new is that an off-the-shelf technology as simple as an AIBO provides an experience of shaping one’s own companion. But the robots are shaping us as well, teaching us how to behave so that they can flourish.3 Again, there is psychological risk in the robotic moment. Logan’s comment about talking with the AIBO to “get thoughts out” suggests using technology to know oneself better. But it also suggests a fantasy in which we cheapen the notion of companionship to a baseline of “interacting with something.” We reduce relationship and come to see this reduction as the norm. As infants, we see the world in parts. There is the good—the things that feed and nourish us. There is the bad—the things that frustrate or deny us. As children mature, they come to see the world in more complex ways, realizing, for example, that beyond black and white, there are shades of gray. The same mother who feeds us may sometimes have no milk. Over time, we transform a collection of parts into a comprehension of wholes.4 With this integration, we learn to tolerate disappointment and ambiguity. And we learn that to sustain realistic relationships, one must accept others in their complexity. When we imagine a robot as a true companion, there is no need to do any of this work. The first thing missing if you take a robot as a companion is alterity, the ability to see the world through the eyes of another.5 Without alterity, there can be no empathy. Writing before robot companions were on the cultural radar, the psychoanalyst Heinz Kohut described barriers to alterity, writing about fragile people—he calls them narcissistic personalities—who are characterized not by love of self but by a damaged sense of self. They try to shore themselves up by turning other people into what Kohut calls self objects. In the role of selfobject, another person is experienced as part of one’s self, thus in perfect tune with a fragile inner state. The selfobject is cast in the role of what one needs, but in these relationships, disappointments inevitably follow. Relational artifacts (not only as they exist now but as their designers promise they will soon be) clearly present themselves as candidates for the role of selfobject.
With a price tag of $1,300 to $2,000, AIBO is meant for grown-ups. But the robot dog is a harbinger of the digital pets of the future, and so I present it to children from age four to thirteen as well as to adults. I bring it to schools, to after-school play centers, and, as we shall see in later chapters, to senior centers and nursing homes. I offer AIBOs for home studies, where families get to keep them for two or three weeks. Sometimes, I study families who have bought an AIBO of their own. In these home studies, just as in the home studies of Furbies, families are asked to keep a “robot diary.” What is it like living with an AIBO? The youngest children I work with—the four- to six-year-olds—are initially preoccupied with trying to figure out what the AIBO is, for it is not a dog and not a doll. The desire to get such things squared away is characteristic of their age. In the early days of digital culture, when they met their first electronic toys and games, children of this age would remain preoccupied with such questions of categories. But now, faced with this sociable machine, children address them and let them drop, taken up with the business of a new relationship. Maya, four, has an AIBO at home. She first asks questions about its origins (“How do they make it?”) and comes up with her own answer: “I think they start with foil, then soil, and then you get some red flashlights and then put them in the eyes.” Then she pivots to sharing the details of her daily life with AIBO: “I love to play with AIBO every day, until the robot gets tired and needs to take a nap.” Henry, four, follows the same pattern. He begins with an effort to categorize AIBO: AIBO is closest to a person, but different from a person because it is missing a special “inner power,” an image borrowed from his world of Pokémon. 6 But when I see Henry a week later, he has bonded with AIBO and is stressing the positive, all the things they share. The most important of these are “remembering and talking powers, the strongest powers of all.” Henry is now focused on the question of AIBO’s affection: How much does this robot like him? Things seem to be going well: he says that AIBO favors him “over all his friends.” By eight, children move even more quickly from any concern over AIBO’s “nature” to the pleasures of everyday routines. In a knowing tone, Brenda claims that “people make robots and . . . people come from God or from eggs, but this doesn’t matter when you are playing with the robot.” In this dismissal of origins we see the new pragmatism. Brenda embraces AIBO as a pet. In her robot diary, she reminds herself of the many ways that this pet should not be treated as a dog. One early entry reminds her not to feed it, and another says, “Do not take AIBO on walks so it can poop.” Brenda feels guilty if she doesn’t keep AIBO entertained. She thinks that “if you don’t play with it,” its lights get red to show its discontent at “playing by itself and getting all bored.” Brenda thinks that when bored, AIBO tries to “entertain itself.” If this doesn’t work, she says, “it tries to get my attention.” Children believe that AIBO asks for attention when it needs it. So, for example, a sick AIBO will want to get better and know it needs human help. An eight-year-old says, “It would want more attention than anything in the whole world.”
Yolanda’s feelings about AIBO also go through all the stages. She first sees AIBO as a substitute: “AIBO might be good practice for all children whose parents aren’t ready to take care of a real dog.” But then she takes another step: in some ways AIBO might be better than a real dog. “The AIBO,” says Yolanda, “doesn’t shed, doesn’t bite, doesn’t die.” More than this, a robotic companion can be made as you like it. Yolanda muses about how nice it would be to “keep AIBO at a puppy stage for people who like to have puppies.” Children imagine that they can create a customized AIBO close to their heart’s desire.8 Sometimes their heart’s desire is to have affection when that pleases them and license to walk away, something not possible with a biological pet. Two nine-year-olds—Lydia and Paige—talk through the steps that take a robot from better than nothing to better than anything. Lydia begins by thinking of AIBO as a substitute for a real pet if you can’t have one: “An AIBO, since you can’t be allergic to a robot, that would be very nice to have.” But as she gets to know AIBO better, she sees a more enticing possibility. “Sometimes,” she says, “I might like [AIBO] more than a real living animal, like a real cat or a real dog, because, like if you had a bad day . . . then you could just turn this thing off and it wouldn’t bug you.” Paige has five pets—three dogs, two cats—and when she is sad, she says, “I cuddle with them.” This is a good thing, but she complains that pets can be trouble: “All of them want your attention. If you give one attention you have to give them all attention, so it’s kinda hard.... When I go somewhere, my kitten misses me. He’ll go into my room and start looking for me.” AIBO makes things easy: “AIBO won’t look at you like ‘play with me’; it will just go to sleep if there is nothing else to do. It won’t mind.” Paige explains that the worst thing that ever happened to her was when her family “had to put their dog to sleep.” She hasn’t wanted a new one since. “But the thing about AIBO,” she says, “is that you don’t have to put him to sleep.... I think you could fix [AIBO] with batteries . . . but when your dog actually dies, you can’t fix it.” For now, the idea that AIBO, as she puts it, “will last forever” makes it better than a dog or cat. Here, AIBO is not practice for the real. It offers an alternative, one that sidesteps the necessity of death.9 For Paige, simulation is not necessarily second best. Pets have long been thought good for children because they teach responsibility and commitment. AIBO permits something different: attachment without responsibility. Children love their pets, but at times, like their overextended parents, they feel burdened by their pets’ demands. This has always been true. But now children see a future where something different may be available. With robot pets, children can give enough to feel attached, but then they can turn away. They are learning a way of feeling connected in which they have permission to think only of themselves. And yet, since these new pets seem betwixt and between what is alive and what is not, this turning away is not always easy. It is not that some children feel responsible for AIBO and other do not. The same children often have strong feelings on both sides of the matter.
As soon as children met computers and computer toys in the late 1970s and early 1980s, they used aggression as a way to animate them and to play with ideas about life and death. Children crashed and revived computer programs; they “killed” Merlin, Simon, and Speak & Spell by pulling out their batteries and then made them come back to life. Aggression toward sociable robots is more complex because children are trying to manage more significant attachments. To take only one example, robots disappoint when they do not display the affection children lead themselves to expect. To avoid hurt, children want to dial things down. Turning robots into objects that can be hurt with impunity is a way to put them in their place. Whether we have permission to hurt or kill an object influences how we think about its life. 10 To children, being able to kill spiders without punishment makes spiders seem less alive, and hurting a robot can make it seem less alive as well. But as in the discussion about whether My Real Baby should cry in “pain,” things are complicated. For the idea that you can hurt a robot can also make it seem more alive. Like Henry, twelve-year-old Tamara is aggressive toward AIBO and troubled by what this implies. She wants to play with AIBO in the same way that she plays with her much-loved cat. But she worries that AIBO’s responses to her are generic. She says, “AIBO acts the same to everyone. It doesn’t attach herself to one person like most animals do.” Tamara says that sometimes she stops herself from petting AIBO: “I start to pet it, and then, like, I would start to be, like, ‘Oh wait. You’re not a cat. You’re not alive.’” And sometimes she gives in to an urge to “knock it over because it was just so cute when it was getting up and then it would, like, shake its head, because then it seemed really alive because that’s what dogs do.” She tries to reassure me: “I’m not like this with my animals.” From their earliest experiences with the electronic toys and games of the late 1970s, children split the notion of consciousness and life. You didn’t have to be biologically alive to have awareness. And so, Tamara who knows AIBO is not alive, imagines that it still might feel pain. In the end, her aggression puts her in a tough spot; AIBO is too much like a companion to be a punching bag. For Tamara, the idea that AIBO might “see” well enough to recognize her is frightening because it might know she is hitting it. But the idea of AIBO as aware and thus more lifelike is exciting as well.
Ashley, seventeen, is a bright and active young woman who describes herself as a cat lover. I have given her an AIBO to take home for two weeks, and now she is at my office at MIT to talk about the experience. During the conversation, Ashley’s AIBO plays on the floor. We do not attend to it; it does tricks on its own—and very noisily. After a while, it seems as though the most natural thing would be to turn AIBO off, in the same spirit that one might turn off a radio whose volume interferes with a conversation. Ashley moves toward the AIBO, hesitates, reaches for its off switch, and hesitates again. Finally, with a small grimace, she hits the switch. AIBO sinks to the ground, inert. Ashley comments, “I know it’s not alive, but I would be, like, talking to it and stuff, and then it’s just a weird experience to press a[n off] button. It made me nervous.... [I talk to it] how I would talk to my cat, like he could actually hear me and understand praise and stuff like that.” I am reminded of Leah, nine, who said of her Furby, “It’s hard to turn it off when it is talking to me.” Ashley knows AIBO is a robot, but she experiences it as a biological pet. It becomes alive for her not only because of its intelligence but because it seems to her to have real emotions. For example, she says that when AIBO’s red lights shone in apparent frustration, “it seemed like a real emotion.... So that made me treat him like he was alive.... And that’s another strange thing: he’s not really physically acting those emotions out, but then you see the colors and you think, ‘Oh, he’s upset.’” Artificial intelligence is often described as the art and science of “getting machines to do things that would be considered intelligent if done by people.” We are coming to a parallel definition of artificial emotion as the art of “getting machines to express things that would be considered feelings if expressed by people.” Ashley describes the moment of being caught between categories: she realizes that what the robot is “acting out” is not emotion, yet she feels the pull of seeing “the colors” and experiencing AIBO as “upset.” Ashley ends up seeing AIBO as both machine and creature. So does John Lester, a computer scientist coming from a far more sophisticated starting point. From the early 1990s, Lester pioneered the use of online communities for teaching, learning, and collaboration, including recent work developing educational spaces on the virtual world of Second Life. Lester bought one of the first AIBOs on the market. He called it Alpha in deference to its being “one of the first batch.”12 When Lester took Alpha out of its box, he shut the door to his office and spent the entire day “hanging out with [my] new puppy.” He describes the experience as “intense,” comparing it to the first time he saw a computer or typed into a Web browser. He quickly mastered the technical aspects of AIBO, but this kind of understanding did not interfere with his pleasure in simply being with the puppy. When Sony modified the robot’s software, Lester bought a second AIBO and named it Beta. Alpha and Beta are machines, but Lester does not like anyone to treat them as inanimate metal and plastic. “I think about my AIBOs in different ways at the same time,” Lester says. In the early days of cubism, the simultaneous presentation of many perspectives of the human face was subversive. But at a certain point, one becomes accustomed to looking at a face in this new way. A face, after all, does have multiple aspects; only representational conventions keep us from appreciating them together. But once convention is challenged, the new view of the face suggests depth and new complexities. Lester has a cubist view of AIBO; he is aware of it as machine, bodily creature, and mind. An AIBO’s sentience, he says, is “awesome.” The creature is endearing. He appreciates the programming behind the exact swing of the “floppy puppy ears.” To Lester, that programming gives AIBO a mind.
Lester understands the mechanisms that AIBO’s designers have used to draw him in: AIBO’s gaze, its expressions of emotion, and the fact that it “grows up” under his care. But this understanding does not interfere with his attachment, just as knowing that infants draw him in with their big, wide eyes does not threaten his connection with babies. Lester says that when he is with AIBO, he does not feel alone. He says that “from time to time” he “catches himself ” in engineer mode, remarking on a technical detail of AIBO that he admires, but these moments do not pull him away from enjoying the companionship of his AIBO puppies. This is not a connection he plays at. It is a big step from accepting AIBO as a companion, and even a solace, to the proposals of David Levy, the computer scientist who imagines robots as intimate partners. But today’s fantasies and Levy’s dreams share something important: the idea that after a robot serves as a better-than-nothing substitute, it might become equal, or even preferable, to a pet or person. In Yolanda’s terms, if your pet is a robot, it might always stay a cute puppy. By extension, if your lover were a robot, you would always be the center of its universe. A robot would not just be better than nothing or better than something, but better than anything. From watching children play with objects designed as “amusements,” we come to a new place, a place of cold comforts. Child and adult, we imagine made to measure companions. Or, at least we imagine companions who are always interested in us. Harry, a forty-two-year-old architect, enjoys AIBO’s company and teaching it new tricks. He knows that AIBO is not aware of him as a person but says, “I don’t feel bad about this. A pet isn’t as aware of me as a person might be.... Dogs don’t measure up to people.... Each level of creature simply does their best. I like it that he [AIBO] recognizes me as his master.” Jane, thirty-six, a grade school teacher, is similarly invested in her AIBO. She says she has “adopted my husband’s AIBO . . . because it is so cute. I named it and love to spend time with it.” Early in our conversation, Jane claims that she turns to AIBO for “amusement,” but she ends up saying that she also turns to it when she is lonely. Jane looks forward to its company after a long workday. Jane talks to her AIBO. “Spend[ing] time” with AIBO means sharing the events of her day, “like who I’m having lunch with at school, which students give me trouble.” Her husband, says Jane, is not interested in these topics. It is more comfortable to talk to AIBO than to force him to listen to stories that bore him. In the company of their robots, Jane and Harry are alone in a way that encourages them to give voice to their feelings. Is there harm here?
Wesley knows he is difficult to live with. He once saw a psychiatrist who told him that his “cycles” were out of the normal range. Ex-wives, certainly, have told him he is “too moody.” He sees himself as “pressure” on a woman, and he feels pressure as well because he has not been able to protect women he cared for from his “ups and downs.” He likes the idea of a robot because he could act naturally—it could not be hurt by his dark moods. Wesley considers the possibility of two “women,” one real and the other artificial: “Maybe I would want a robot that would be the perfect mate—less needs—and a real woman. The robot could take some of the pressure off the real woman. She wouldn’t have to perform emotionally at such a high level, really an unrealistic level.... I could stay in my comfort zone.” Rudimentary versions of Wesley’s fantasy are in development. I have spoken briefly of the Internet buzz over Roxxxy, put on the market in January 2010, advertised as “the world’s first sex robot.” Roxxxy cannot move, although it has electronically warmed skin and internal organs that pulse. It does, however, make conversation. The robot’s creator, Douglas Hines, helpfully offers, “Sex only goes so far—then you want to be able to talk to the person.”13 So, for example, when Roxxxy senses that its hand is being held, the robot says, “I love holding hands with you,” and moves into more erotic conversation when the physical caresses become more intimate. One can choose different personalities for Roxxxy, ranging from wild to frigid. The robot will be updated over the Internet to expand its capabilities and vocabulary. It can already discuss soccer. Hines, an engineer, says that he got into the robot business after a friend died in the September 11 attacks on the Twin Towers. Hines wanted to preserve his friend’s personality so that his children could interact with him as they grew up. Like AI scientist and inventor Raymond Kurzweil, who dreams of a robotic incarnation of his father who died tragically young, Hines committed himself to the project of building an artificial personality. At first, he considered building a home health aid for the elderly but decided to begin with sex robots, a decision that he calls “only marketing.” His long-term goal is to take artificial personalities into the mainstream. He still wants to recreate his lost friend. The well-publicized launch of Roxxxy elicits a great deal of online discussion. Some postings talk about how “sad” it is that a man would want such a doll. Others argue that having a robot companion is better than being lonely. For example, “There are men for who attaining a real woman is impossible.... This isn’t simply a matter of preference.... In the real world, sometimes second best is all they can get.”

This book began with a roboticist's dream that struck me as a nightmare. I was reading Hans Moravec's Mind Children: The Future of Robot and Human Intelligence, enjoying the ingenious variety of his robots, when I happened upon the passage where he argues it will soon be possible to download human consciousness into a computer. l To illustrate, he invents a fantasy scenario in which a robot surgeon purees the human brain in a kind of cranial liposuction, reading the information in each molecular layer as it is stripped away and transferring the information into a computer. At the end of the operation, the cranial cavity is empty, and the patient, now inhabiting the metallic body of the computer, wakens to find his consciousness exactly the same as it was before.

Following this thread, I was led into a maze of developments that turned into a six-year odyssey of researching archives in the history of cybernetics, interviewing scientists in computational biology and artificial life, reading cultural and literary texts concerned with information technologies, visiting laboratories engaged in research on virtual reality, and grappling with technical articles in cybernetics, information theory, autopoiesis, computer simulation, and cognitive science. Slowly this unruly mass of material began taking shape as three interrelated stories. The first centers on how information lost its body, that is, how it came to be conceptualized as an entity separate from the materialforms in which it is thought to be embedded. The second story concerns how the cyborg was created as a technological artifact and cultural icon in the years follOwing World War II. The third, deeply implicated with the first two, is the unfolding story of how a historically specific construction called the human is giving way to a different construction called the posthuman. Interrelations between the three stories are extensive. Central to the construction of the cyborg are informational pathways connecting the organic body to its prosthetic extensions. This presumes a conception of information as a (disembodied) entity that can flow between carbon-based organic components and silicon-based electronic components to make protein and silicon operate as a Single system. When information loses its body, equating humans and computers is especially easy, for the materiality in which the thinking mind is instantiated appears incidental to its essential nature. Moreover, the idea of the feedback loop implies that the boundaries of the autonomous subject are up for grabs, since feedback loops can flow not only within the subject but also between the subject and the environment. From Norbert Wiener on, the flow of information through feedback loops has been associated with the deconstruction of the liberal humanist subject, the version of the "human" with which I will be concerned. Although the "posthuman" differs in its articulations, a common theme is the union of the human with the intelligent machine.

What to make of this shift from the human to the posthuman, which both evokes terror and excites pleasure? The liberal humanist subject has, of course, been cogently criticized from a number of perspectives. Feminist theorists have pointed out that it has historically been constructed as a white European male, presuming a universality that has worked to suppress and disenfranchise women's voices; postcolonial theorists have taken issue not only with the universality of the (white male) liberal subject but also with the very idea of a unified, consistent identity, fOCUSing instead on hybridity; and postmodern theorists such as Gilles Deleuze and Felix Guattari have linked it with capitalism, arguing for the liberatory potential of a dispersed subjectivity distributed among diverse desiring machines they call "body without organs."7 Although the deconstruction of the liberal humanist subject in cybernetiCS has some affinities with these perspectives, it proceeded primarily along lines that sought to understand human being as a set of informational processes. Because information had lost its body, this construction implied that embodiment is not essential to human being. Embodiment has been systematically downplayed or erased in the cybernetic construction of the posthuman in ways that have not occurred in other critiques of the liberal humanist subject, espeCially in feminist and postcolonial theories.

In tracing these continuities and discontinuities between a "natural" self and a cybernetic posthuman, I am not trying to recuperate the liberal subject. Although I think that serious consideration needs to be given to how certain characteristics associated with the liberal subject, especially agency and choice, can be articulated within a posthuman context, I do not mourn the passing of a concept so deeply entwined with projects of domination and oppression. Rather, I view the present moment as a critical juncture when interventions might be made to keep disembodiment from being rewritten, once again, into prevailing concepts of subjectivity. I see the deconstruction of the liberal humanist subject as an opportunity to put back into the picture the flesh that continues to be erased in contemporary discussions about cybernetic subjects. Hence my focus on how information lost its body, for this story is central to creating what Arthur Kroker has called the "flesh-eating 90s."11 If my nightmare is a culture inhabited by posthumans who regard their bodies as fashion accessories rather than the ground of being, my dream is a version of the posthuman that embraces the possibilities of information technologies without being seduced by fantasies of unlimited power and disembodied immortality, that recognizes and celebrates finitude as a condition of human being, and that understands human life is embedded in a material world of great complexity, one on which we depend for our continued survival.

Perhaps it will now be clear that I mean my title, How We Became Posthuman, to connote multiple ironies, which do not prevent it from also being taken seriously. Taken straight, this title points to models of subjectivity sufficiently different from the liberal subject that if one assigns the term "human" to this subject, it makes sense to call the successor "posthuman." Some of the historical processes leading to this transformation are documented here, and in this sense the book makes good on its title. Yet my argument will repeatedly demonstrate that these changes were never complete transformations or sharp breaks; without exception, they reinscribed traditional ideas and assumptions even as they articulated something new. The changes announced by the title thus mean something more complex than "That was then, this is now." Rather, "human" and "posthuman" coexist in shifting configurations that vary with historically specific contexts. Given these complexities, the past tense in the title-"became" -is intended both to offer the reader the pleasurable shock of a double take and to reference ironically apocalyptic visions such as Moravec's prediction of a "postbiological" future for the human race. Amplifying the ambiguities of the past tense are the ambiguities of the plural. In one sense, "we" refers to the readers of this book-readers who, by becoming aware of these new models of subjectivity (if they are not already familiar with them), may begin thinking of their actions in ways that have more in common with the posthuman than the human. Speaking for myself, I now find myself saying things like, "Well, my sleep agent wants to rest, but my food agent says I should go to the store." Each person who thinks this way begins to envision herself or himself as a posthuman collectivity, an "I" transformed into the "we" of autonomous agents operating together to make a self. The infectious power of this way of thinking gives "we" a performative dimension. People become posthuman because they think they are posthuman. In another sense "we," like "became," is meant ironically, positioning itself in opposition to the techno-ecstasies found in various magazines, such as Mondo 2000, which customarily speak of the transformation into the posthuman as if it were a universal human condition when in fact it affects only a small fraction of the world's populationa pOint to which I will return.

During the foundational era of cybernetics, Norbert Wiener, John von Neumann, Claude Shannon, Warren McCulloch, and dozens of other distinguished researchers met at annual conferences sponsored by the JOSiah Macy Foundation to formulate the central concepts that, in their high expectations, would coalesce into a theory of communication and control applying equally to animals, humans, and machines. Retrospectively called the Macy Conferences on Cybernetics, these meetings, held from 1943 to 1954, were instrumental in forging a new paradigm. 12 To succeed, they needed a theory of information (Shannon's bailiwick), a model of neural functioning that showed how neurons worked as information-processing systems (McCulloch's lifework), computers that processed binary code and that could conceivably reproduce themselves, thus reinforcing the analogy with biolOgical systems (von Neumann's specialty), and a visionary who could articulate the larger implications of the cybernetic paradigm and make clear its cosmic significance (Wiener's contribution). The result of this breathtaking enterprise was nothing less than a new way oflooking at human beings. Henceforth, humans were to be seen primarily as information-processing entities who are essentially similar to intelligent machines. The revolutionary implications of this paradigm notwithstanding, Wiener did not intend to dismantle the liberal humanist subject. He was less interested in seeing humans as machines than he was in fashioning human and machine alike in the image of an autonomous, self-directed individual. In aligning cybernetiCS with liberal humanism, he was following a strain of thought that, since the Enlightenment, had argued that human beings could be trusted with freedom because they and the social structures they devised operated as self-regulating mechanisms. 13 For Wiener, cybernetics was a means to extend liberal humanism, not subvert it. The point was less to show that man was a machine than to demonstrate that a machine could function like a man.

This definition of reflexivity has much in common with some of the most influential and provocative recent work in critical theory, cultural studies, and the social studies of science. Typically, these works make the reflexive move of showing that an attribute previously considered to have emerged from a set of preexisting conditions is in fact used to generate the conditions. In Nancy Armstrong's Desire and Domestic Fiction: A Political History of the Novel, for example, bourgeOiS femininity is shown to be constructed through the domestic fictions that represent it as already in place. 16 In Michael Warner's The Letters of the RepubliC: Publication and the Public Sphere in Eighteenth-Century America, the founding document of the United States, the Constitution, is shown to produce the very people whose existence it presupposes. 17 In Bruno Latour's Science in Action: How to Follow Scientists and Engineers through Society, scientific experiments are shown to produce the nature whose existence they predicate as their condition of possibility. 18 It is only a slight exaggeration to say that contemporary critical theory is produced by the reflexivity that it also produces (an observation that is, of course, also reflexive).
Reflexivity entered cybernetics primarily through discussions about the observer. By and large, first-wave cybernetics followed traditional scientific protocols in considering observers to be outside the system they observe. Yet cybernetics also had implications that subverted this premise. The objectivist view sees information flOwing from the system to the observers, but feedback can also loop through the observers, drawing them in to become part of the system being observed. Although participants remarked on this aspect of the cybernetic paradigm throughout the Macy transcripts, they lacked a single word to describe it. To my knowledge, the word "reflexivity" does not appear in the transcripts. This meant they had no handle with which to grasp this slippery concept, no Signifier that would help to constitute as well as to describe the changed perspective that reflexivity entails. Discussions of the idea remained diffuse. Most participants did not go beyond remarking on the shifting boundaries between observer and system that cybernetics puts into play. With some exceptions, deeper formulations of the problem failed to coalesce during the Macy discussions.

The second wave of cybernetics grew out of attempts to incorporate reflexivity into the cybernetic paradigm at a fundamental level. The key issue was how systems are constituted as such, and the key problem was how to redefine homeostatic systems so that the observer can be taken into account. The second wave was initiated by, among others, Heinz von Foerster, the Austrian emigre who became coeditor of the Macy transcripts. This phase can be dated from 1960, when von Foerster wrote the first of the essays that were later collected in his influential book Observing Systems. 19 As von Foerster's punning title recognizes, the observer of systems can himself be constituted as a system to be observed. Von Foerster called the models he presented in these essays "second-order cybernetics" because they extended cybernetic principles to the cyberneticians themselves. The second wave reached its mature phase with the publication of Humberto Maturana and Francisco Varela's Autopoiesis and Cognition: The Realization of the Living. 20 Building on Maturana's work on reflexivity in sensory processing and Varela's on the dynamics of autonomous biological systems, the two authors expanded the reflexive tum into a fully articulated epistemology that sees the world as a set of informationally closed systems. Organisms respond to their environment in ways determined by their internal self-organization. Their one and only goal is continually to produce and reproduce the organization that defines them as systems. Hence, they not only are self-organizing but also are autopoietic, or selfmaking. Through Maturana and Varela's work and that of other influential theorists such as German SOCiologist Niklas Luhmann,21 cybernetics by 1980 had spun off from the idea of reflexive feedback loops a theory of autopoiesis with sweeping epistemological implications.
The third wave swelled into existence when self-organization began to be understood not merely as the (re)production of internal organization but as the springboard to emergence. In the rapidly emerging field of artificiallife, computer programs are designed to allow "creatures" (that is, discrete packets of computer codes) to evolve spontaneously in directions the programmer may not have anticipated. The intent is to evolve the capacity to evolve. Some researchers have argued that such self-evolving programs are not merely models oflife but are themselves alive. What assumptions make this claim plausible? If one sees the universe as composed essentially of information, it makes sense that these "creatures" are life forms because they have the form oflife, that is, an informational code. As a result, the theoretical bases used to categorize all life undergo a significant shift. As we shall see in chapters 9 and 10, when these theories are applied to human beings, H onw sapiens are so transfigured in conception and purpose that they can appropriately be called posthuman. The emergence of the posthuman as an informational-material entity is paralleled and reinforced by a corresponding reinterpretation of the deep structures of the phYSical world. Some theorists, notably Edward Fredkin and Stephen Wolfram, claim that reality is a program run on a cosmic computer.22 In this view, a universal informational code underlies the structure of matter, energy, spacetime-indeed, of everything that exists. The code is instantiated in cellular automata, elementary units that can occupy two states: on or off. Although the jury is still out on the cellular automata model, it may indeed prove to be a robust way to understand reality. Even now, a research team headed by Fredkin is working on shOwing how quantum mechanics can be derived from an underlying cellular automata model.

It is this materiality/information separation that I want to contest-not the cellular automata model, information theory, or a host of related theories in themselves. My strategy is to complicate the leap from embodied reality to abstract information by pointing to moments when the assumptions involved in this move were contested by other researchers in the field and so became especially visible. The point of highlighting such moments is to make clear how much had to be erased to arrive at such abstractions as bodiless information. Abstraction is of course an essential component in all theOrizing, for no theory can account for the infinite multiplicity of our interactions with the real. But when we make moves that erase the world's multiplicity, we risk losing Sight of the variegated leaves, fractal branchings, and particular bark textures that make up the forest. In the pages that follow, I will identifY two moves in particular that played important roles in constructing the information/materiality hierarchy. Irreverently, I think of them as the Platonic backhand and forehand.

Whether the enabling assumptions for this conception of information occur in information theory, cybernetics, or popular science books such as Mind Children, their appeal is clear. Information viewed as pattern and not tied to a particular instantiation is information free to travel across time and space. Hackers are not the only ones who believe that information wants to be free. The great dream and promise of information is that it can be free from the material constraints that govern the mortal world. Marvin Minsky precisely expressed this dream when, in a recent lecture, he suggested it will soon be possible to extract human memories from the brain and import them, intact and unchanged, to computer disks.23 The clear implication is that if we can become the information we have constructed, we can achieve effective immortality. In the face of such a powerful dream, it can be a shock to remember that for information to exist, it must always be instantiated in a medium, whether that medium is the page from the Bell Laboratories Journal on which Shannon's equations are printed, the computer-generated topolOgical maps used by the Human Genome Project, or the cathode ray tube on which virtual worlds are imaged. The point is not only that abstracting information from a material base is an imaginary act but also, and more fundamentally, that conceiving of information as a thing separate from the medium instantiating it is a prior imaginary act that constructs a holistic phenomenon as an information/matter duality.
The complex psychological functions a skeuomorph performs can be illustrated by an installation exhibited at SIGGRAPH '93. Called the "Catholic Turing Test," the simulation invited the viewer to make a confession by chOOSing selections from the video screen; it even had a bench on which the viewer could kneel. 28 On one level, the installation alluded to the triumph of science over religion, for the role of divinely authorized interrogation and absolution had been taken over by a machine algorithm. On another level, the installation pointed to the intransigence of conditioned behavior, for the machine's form and function were determined by its religious predecessor. Like a Janus figure, the skeuomorph looks to past and future, Simultaneously reinforcing and undermining both. It calls into a playa psychodynamic that finds the new more acceptable when it recalls the old that it is in the process of displacing and finds the traditional more comfortable when it is presented in a context that reminds us we can escape from it into the new. In the history of cybernetics, skeuomorphs acted as threshold devices, smoothing the transition between one conceptual constellation and another. Homeostasis, a foundational concept during the first wave, functioned during the second wave as a skeuomotph. Although homeostasis remained an important concept in biology, by about 1960 it had ceased to be an initiating premise in cybernetics. Instead, it performed the work of a gesture or an allusion used to authenticate new elements in the emerging constellation of reflexivity. At the same time, it also exerted an inertial pull on the new elements, limiting how radically they could transform the constellation.
Shannon's theory defines information as a probability function with no dimensions, no materiality, and no necessary connection with meaning. It is a pattern, not a presence. (Chapter 3 talks about the development of information theory in more detail, and the relevant equations can be found there.) The theory makes a strong distinction between message and signal. Lacan to the contrary, a message does not always arrive at its destination. In information theoretic terms, no message is ever sent. What is sent is a signal. Only when the message is encoded in a Signal for transmission through a medium-for example, when ink is printed on paper or when electrical pulses are sent racing along telegraph wires-does it assume material form. The very definition of "information," then, encodes the distinction between materiality and information that was also becoming important in molecular biology during this period.
Shannon's approach had other advantages that turned out to incur large (and mounting) costs when his premise interacted with certain predispositions already at work within the culture. Abstracting information from a material base meant that information could become free-floating, unaffected by changes in context. The technical leverage this move gained was considerable, for by formalizing information into a mathematical function, Shannon was able to develop theorems, powerful in their generality, that hold true regardless of the medium in which the information is instantiated. Not everyone agreed this move was a good idea, however, despite its theoretical power. As Carolyn Marvin notes, a decontextualized construction of information has important ideological implications, including an Anglo-American ethnocentrism that regards digital information as more important than more context-bound analog information.33 Even in Shannon's day, malcontents grumbled that divorcing information from context and thus from meaning had made the theory so narrowly formalized that it was not useful as a general theory of communication. Shannon himself frequently cautioned that the theory was meant to apply only to certain technical situations, not to communication in generaP4 In other circumstances, the theory might have become a dead end, a victim of its own excessive formalization and decontextualization. But not in the post -World War II era. The time was ripe for theories that reified information into a free-floating, decontextualized, quantifiable entity that could serve as the master key unlocking secrets of life and death. Technical artifacts help to make an information theoretic view a part of everyday life. From ATMs to the Internet, from the morphing programs used in Terminator II to the sophisticated visualization programs used to guide microsurgeries, information is increasingly perceived as interpenetrating material forms. EspeCially for users who may not know the material processes involved, the impression is created that pattern is predominant over presence. From here it is a small step to perceiving information as more mobile, more important, more essential than material forms. When this impression becomes part of your cultural mindset, you have entered the condition of virtuality.

Nevertheless, I think it is a mistake to underestimate the importance of virtuality, for it wields an influence altogether disproportionate to the number of people immersed in it. It is no accident that the condition of virtuality is most pervasive and advanced where the centers of power are most concentrated. Theorists at the Pentagon, for example, see it as the theater in which future wars will be fought. They argue that coming conflicts will be decided not so much by overwhelming force as by "neocortical warfare," waged through the techno-sciences of information. 37 If we want to contest what these technologies SignifY, we need histories that show the erasures that went into creating the condition of virtuality, as well as visions arguing for the importance of embodiment. Once we understand the complex interplays that went into creating the condition of virtuality, we can demystifY our progress toward virtuality and see it as the result of historically specific negotiations rather than of the irresistible force of technological determinism. At the same time, we can acquire resources with which to rethink the assumptions underlying virtuality, and we can recover a sense of the virtual that fully recognizes the importance of the embodied processes constituting the lifeworld of human beings.38 In the phrase "virtual bodies," I intend to allude to the historical separation between information and materiality and also to recall the embodied processes that resist this division.

A second way to think about the organization of How We Became Posthuman is narratively. In this arrangement, the three divisions proceed not so much through chronolOgical progression as through the narrative strands about the (lost) body of information, the cyborg body, and the posthuman body. Here the literary texts playa central role, for they display the passageways that enabled stories coming out of narrowly focused scientific theories to circulate more widely through the body politic. Many of the scientists understood very well that their negotiations involved premises broader than the formal scope of their theories strictly allowed. Because of the wedge that has been driven between science and values in U.S. culture, their statements on these wider implications necessarily occupied the position of ad hoc pronouncements rather than "scientific" arguments. Shaped by different conventions, the literary texts range across a spectrum ofissues that the scientific texts only fitfully illuminate, including the ethical and cultural implications of cybernetiC technologies.

What does this emphasis on narrative have to do with virtual bodies? FollOwing J ean-Franc:;ois Lyotard, many theorists of postmodernity accept that the postmodern condition implies an incredulity toward metanarrative. 41 As we have seen, one way to construct virtuality is the way that Moravec and Minsky do-as a metanarrative about the transformation of the human into a disembodied posthuman. I think we should be skeptical about this metanarrative. To contest it, I want to use the resources of narrative itself, particularly its resistance to various forms of abstraction and disembodiment. With its chronolOgical thrust, polymorphous digreSSions, located actions, and personified agents, narrative is a more embodied form of discourse than is analytically driven systems theory. By turning the technolOgical determinism of bodiless information, the cyborg, and the posthuman into narratives about the negotiations that took place between particular people at particular times and places, I hope to replace a teleology of disembodiment with historically contingent stories about contests between competing factions, contests whose outcomes were far from obvious. Many factors affected the outcomes, from the needs of emerging technologies for reliable quantification to the personalities of the people involved. Though overdetermined, the disembodiment ofinformation was not inevitable, any more than it is inevitable we continue to accept the idea that we are essentially informational patterns.

The first literary text I discuss in detail is Bernard Wolfe's Limbo. 42 Written in the 1950s, Limbo has become something of an underground classic. It imagines a postwar society in which an ideology, Immob, has developed; the ideology equates aggression with the ability to move. "Pacifism equals passivity," Immob slogans declare. True believers volunteer to banish their mobility (and presumably their aggreSSion) by having amputations, which have come to be regarded as signifiers of social power and influence. These amputees get bored with lying around, however, so a vigorous cyberneticS industry has grown up to replace their missing limbs. As this brief summary suggests, Limbo is deeply influenced by cybernetiCS. But the technical achievements of cybernetics are not at the center of the text. Rather, they serve as a springboard to explore a variety of social, political, and psychological issues, ranging from the perceived threat that women's active sexuality poses for Immob men to global East-West tensions that explode into another world war at the end of the text. Although it is unusually didactic, Limbo does more than discuss cyberneticS; it engages a full range of rhetorical and narrative devices that work both with and against its explicit pronouncements. The narrator seems only partially able to control his verbally extravagant narrative. There are, I will argue, deep connections between the narrator's struggle to maintain control of the narrative and the threat to "natural" body boundaries posed by the cybernetiC paradigm. Limbo interrogates a dynamiC that also appears in Norbert Wiener's work-the intense anxiety that erupts when the perceived boundaries of the body are breached. In addition, it illustrates how the body of the text gets implicated in the processes used to represent bodies within the text.

gs of race, gender, and sexuality. The chapter on contemporary speculative fictions constructs a semiotics of virtuality by shOwing how the central concepts ofinformation and materiality can be mapped onto a multilayered semiotic square. The tutor texts for this analYSis, which include Snow Crash, Blood Music, Galatea 2.2, and Terminal Games, indicate the range of what counts as the posthuman in the age of virtuality, from neural nets to hackers, biolOgically modified humans, and entities who live only in computer simulations.44 In follOwing the construction of the posthuman in these texts, I will argue that older ideas are reinscribed as well as contested. As was the case for the scientific models, change occurs in a seriated pattern of overlapping innovation and replication. I hope that this book will demonstrate, once again, how crucial it is to recognize interrelations between different kinds of cultural productions, specifically literature and science. The stories I tell here-how information lost its body, how the cyborg was created as a cultural icon and technolOgical artifact, and how humans became posthumans-and the waves of historical change I chart would not have the same resonance or breadth if they had been pursued only through literary texts or only through scientific discourses. The scientific texts often reveal, as literature cannot, the foundational assumptions that gave theoretical scope and artifactual efficacy to a particular approach. The literary texts often reveal, as scientific work cannot, the complex cultural, social, and representational issues tied up with conceptual shifts and technological innovations. From my point of view, literature and science as an area of specialization is more than a subset of cultural studies or a minor activity in a literature department. It is a way of understanding ourselves as embodied creatures living within and through embodied worlds and embodied words.
What, finally, are we to make of the posthuman?l At the beginning of this book, I suggested that the prospect of becoming posthuman both evokes terror and excites pleasure. At the end of the book, perhaps I can summarize the implications of the posthuman by interrogating the sources of this terror and pleasure. The terror is relatively easy to understand. "Post," with its dual connotation of superseding the human and coming after it, hints that the days of "the human" may be numbered. Some researchers (notably Hans Moravec but also my UCLA colleague Michael Dyer and many others) believe that this is true not only in a general intellectual sense that displaces one definition of "human" with another but also in a more disturbingly literal sense that envisions humans displaced as the dominant form of life on the planet by intelligent machines. Humans can either go gently into that good night, joining the dinosaurs as a species that once ruled the earth but is now obsolete, or hang on for a while longer by becoming machines themselves. In either case, Moravec and like-minded thinkers believe, the age of the human is drawing to a close. The view echoes the deeply pessimistic sentiments of Warren McCulloch in his old age. As noted earlier, he remarked: "Man to my mind is about the nastiest, most destructive of all the animals. I don't see any reason, ifhe can evolve machines that can have more fun than he himself can, why they shouldn't take over, enslave us, quite happily. They might have a lot more fun. Invent better games than we ever did."2 Is it any wonder that faced with such dismal scenarios, most people have understandably negative reactions? If this is what the posthuman means, why shouldn't it be resisted?

What about the pleasures? For some people, including me, the posthuman evokes the exhilarating prospect of getting out of some of the old boxes and opening up new ways of thinking about what being human means. In positing a shift from presence/absence to pattern/randomness, I have sought to show how these categories can be transformed from the inside to arrive at new kinds of cultural configurations, which may soon render such dualities obsolete if they have not already. This process of transformation is fueled by tensions between the assumptions encoded in pattern/randomness as opposed to presence/absence. In Jacques Derrida's performance of presence/absence, presence is allied with Logos, God, teleology-in general, with an originary plenitude that can act to ground signification and give order and meaning to the trajectory of history. 6 The work of Eric Havelock, among others, demonstrates how in Plato's Republic this view of originarypresence authorized a stable, coherent self that could witness and testifY to a stable, coherent reality. 7 Through these and other means, the metaphysics of presence front-loaded meaning into the system. Meaning was guaranteed because a stable origin existed. It is now a familiar story how deconstruction exposed the inability of systems to posit their own origins, thus ungrounding signification and rendering meaning indeterminate. As the presence/absence hierarchy was destabilized and as absence was privileged over presence, lack displaced plenitude, and desire usurped certitude. Important as these moves have been in late-twentieth-century thought, they still took place within the compass of the presence/absence dialectic. One feels lack only if presence is posited or assumed; one is driven by desire only if the object of desire is conceptualized as something to be possessed. Just as the metaphysics of presence required an originaryplenitude to articulate a stable self, deconstruction required a metaphysics of presence to articulate the destabilization of that self.

Indeed, it is not too much to say that in these and similar models, randomness rather than pattern is invested with plenitude. If pattern is the realization of a certain set of possibilities, randomness is the much, much larger set of everything else, from phenomena that cannot be rendered coherent by a given system's organization to those the system cannot perceive at all. In Gregory Bateson's cybernetiC epistemology, randomness is what exists outside the confines of the box in which a system is located; it is the larger and unknowable complexity for which the perceptual processes of an organism are a metaphor. 11 Significance is achieved by evolutionary processes that ensure the surviving systems are the ones whose organizations instantiate metaphors for this complexity, unthinkable in itself. When Varela and his coauthors argue in Embodied Mind that there is no stable, coherent self but only autonomous agents running programs, they envision pattern as a limitation that drops away as human awareness expands beyond consciousness and encounters the emptiness that, in another guise, could equally well be called the chaos from which all forms emerge.
To explore these resources, let us return to Bateson's idea that those organisms that survive will tend to be the ones whose internal structures are good metaphors for the complexities without. What kind of environments will be created by the expanding power and sophistication of intelligent machines? As Richard Lanham has pOinted out, in the information-rich environments created by ubiquitous computing, the limiting factor is not the speed of computers, or the rates of transmission through fiber-optic cables, or the amount of data that can be generated and stored. Rather, the scarce commodity is human attention. 14 It makes sense, then, that technological innovation will focus on compensating for this bottleneck. An obvious solution is to design intelligent machines to attend to the choices and tasks that do not have to be done by humans. For example, there are already intelligent -agent programs to sort email, discarding unwanted messages and pri- 0ritizing the rest. The programs work along lines similar to neural nets. They tabulate the choices the human operators make, and they feed back this information in recursive loops to readjust the weights given to various kinds of email addresses. After an initial learning period, the sorting programs take over more and more of the email management, freeing humans to give their attention to other matters.
In the posthuman view, by contrast, conscious agency has never been "in control." In fact, the very illusion of control bespeaks a fundamental ignorance about the nature of the emergent processes through which consciousness, the organism, and the environment are constituted. Mastery through the exercise of autonomous will is merely the story consciousness tells itself to explain results that actually come about through chaotic dynamics and emergent structures. If, as Donna Haraway, Sandra Harding, Evelyn Fox Keller, Carolyn Merchant, and other feminist critics of science have argued, there is a relation among the desire for mastery, an objectivist account of science, and the imperialist project of subdUing nature, then the posthuman offers resources for the construction of another kind of account.I8 In this account, emergence replaces teleology; reflexive epistemology replaces objectivism; distributed cognition replaces autonomous will; embodiment replaces a body seen as a support system for the mind; and a dynamic partnership between humans and intelligent machines replaces the liberal humanist subject's manifest destiny to dominate and control nature. Of course, this is not necessarily what the posthuman will mean-only what it can mean if certain strands among its complex seriations are highlighted and combined to create a vision of the human that uses the posthuman as leverage to avoid reinscribing, and thus repeating, some of'the mistakes of the past.


As we have seen, cybernetics was born in a froth of noise when Norbert Wiener first thought of it as a way to maximize human potential in a world that is in essence chaotic and unpredictable. Like many other pioneers, Wiener helped to initiate a journey that would prove to have consequences more far-reaching and subversive than even his formidable powers of imagination could conceive. As Bateson, Varela, and others would later argue, the noise crashes within as well as without. The chaotic, unpredictable nature of complex dynamics implies that subjectivity is emergent rather than given, distributed rather than located solely in consciousness, emerging from and integrated into a chaotic world rather than occupying a position of mastery and control removed from it. Bruno Latour has argued that we have never been modem; the seriated history of cybernetics-emerging from networks at once materially real, socially regulated, and discurSively constructed-suggests, for similar reasons, that we have always been posthuman. The purpose of this book has been to chronicle the journeys that have made this realization pOSSible. If the three stories told here-how information lost its body, how the cyborg was constructed in the postwar years as technological artifact and cultural icon, and how the human became the posthuman-have at times seemed to present the posthuman as a transformation to be feared and abhorred rather than welcomed and embraced, that reaction has everything to do with how the posthuman is constructed and understood. The best possible time to contest for what the posthuman means is now, before the trains of thought it embodies have been laid down so firmly that it would take dynamite to change them.24 Although some current versions of the posthuman point toward the antihuman and the apocalyptic, we can craft others that will be conducive to the long-range survival of humans and of the other life-forms, biological and artificial, with whom we share the planet and ourselves.

A few words on the two neoteric terms, cybertext and ergodic, are in order. Cybertext is a neologism derived from Norbert Wiener's book (and discipline) called Cybernetics, and subtitled Control and Communication in the Animal and the Machine (1948). Wiener laid an important foundation for the development of digital computers, but his scope is not limited to the mechanical world of transistors and, later, of microchips. As the subtitle indicates, Wiener's perspective includes both organic and inorganic systems; that is, any system that contains an information feedback loop. Likewise, the concept of cybertext does not limit itself to the study of computer-driven (or "electronic") textuality; that would be an arbitrary and unhistorical limitation, perhaps comparable to a study of literature that would only acknowledge texts in paper-printed form. While there might be sociological reasons for such a study, we would not be able to claim any understanding of how different forms of literature vary. The concept of cybertext focuses on the mechanical organization of the text, by positing the intricacies of the medium as an integral part of the literary exchange. However, it also centers attention on the consumer, or user, of the text, as a more integrated figure than even reader-response theorists would claim. The performance of their reader takes place all in his head, while the user of cybertext also performs in an extranoematic sense. During the cybertextual process, the user will have effectuated a semiotic sequence, and this selective movement is a work of physical construction that the various concepts of "reading" do not account for. This phenomenon I call ergodic, using a term appropriated from physics that derives from the Greek words ergon and hodos, meaning "work" and "path." In ergodic literature, nontrivial effort is required to allow the reader to traverse the text. If ergodic literature is to make sense as a concept, there must also be nonergodic literature, where the effort to traverse the text is trivial, with no extranoematic responsibilities placed on the reader except (for example) eye movement and the periodic or arbitrary turning of pages.

Typically, these objections came from persons who, while well versed in literary theory, had no firsthand experience of the hypertexts, adventure games, or multi-user dungeons I was talking about. At first, therefore, I thought this was simply a didactical problem: if only I could present examples of my material more clearly, everything would become indisputable. After all, can a person who has never seen a movie be expected to understand the unique characteristics of that medium? A text such as the I Ching is not meant to be read from beginning to end but entails a very different and highly specialized ritual of perusal, and the text in a multi-user dungeon is without either beginning or end, an endless labyrinthine plateau of textual bliss for the community that builds it. But no matter how hard I try to describe these texts to you, the reader, their essential difference will remain a mystery until they are experienced firsthand. In my campaign for the study of cybertextuality I soon realized that my terminology was a potential source of confusion. Particularly problematic was the word nonlinear. For some it was a common literary concept used to describe narratives that lacked or subverted a straightforward story line; for others, paradoxically, the word could not describe my material, since the act of reading must take place sequentially, word for word. This aporia never ceased to puzzle me. There was obviously an epistemological conflict. Part of the problem is easily resolved: hypertexts, adventure games, and so forth are not texts the way the average literary work is a text. In what way, then, are they texts? They produce verbal structures, for aesthetic effect. This makes them similar to other literary phenomena. But they are also something more, and it is this added paraverbal dimension that is so hard to see. A cybertext is a machine for the production of variety of expression. Since literary theorists are trained to uncover literary ambivalence in texts with linear expression, they evidently mistook texts with variable expression for texts with ambiguous meaning. When confronted with a forking text such as a hypertext, they claimed that all texts are produced as a linear sequence during reading, so where was my problem? The problem was that, while they focused on what was being read, I focused on what was being read from. This distinction is inconspicuous in a linear expression text, since when you read from War and Peace, you believe you are reading War and Peace. In drama, the relationship between a play and its (varying) performance is a hierarchical and explicit one; it makes trivial sense to distinguish between the two. In a cybertext, however, the distinction is crucial--and rather different; when you read from a cybertext, you are constantly reminded of inaccessible strategies and paths not taken, voices not heard. Each decision will make some parts of the text more, and others less, accessible, and you may never know the exact results of your choices; that is, exactly what you missed. This is very different from the ambiguities of a linear text. And inaccessibility, it must be noted, does not imply ambiguity but, rather, an absence of possibility--an aporia.

A reader, however strongly engaged in the unfolding of a narrative, is powerless. Like a spectator at a soccer game, he may speculate, conjecture, extrapolate, even shout abuse, but he is not a player. Like a passenger on a train, he can study and interpret the shifting landscape, he may rest his eyes wherever he pleases, even release the emergency brake and step off, but he is not free to move the tracks in a different direction. He cannot have the player's pleasure of influence: "Let's see what happens when I do this." The reader's pleasure is the pleasure of the voyeur. Safe, but impotent. The cybertext reader, on the other hand, is not safe, and therefore, it can be argued, she is not a reader. The cybertext puts its would-be reader at risk: the risk of rejection. The effort and energy demanded by the cybertext of its reader raise the stakes of interpretation to those of intervention. Trying to know a cybertext is an investment of personal improvisation that can result in either intimacy or failure. The tensions at work in a cybertext, while not incompatible with those of narrative desire, are also something more: a struggle not merely for interpretative insight but also for narrative control: "I want this text to tell my story; the story that could not be without me." In some cases this is literally true. In other cases, perhaps most, the sense of individual outcome is illusory, but nevertheless the aspect of coercion and manipulation is real. The study of cybertexts reveals the misprision of the spaciodynamic metaphors of narrative theory, because ergodic literature incarnates these models in a way linear text narratives do not. This may be hard to understand for the traditional literary critic who cannot perceive the difference between metaphorical structure and logical structure, but it is essential. The cybertext reader is a player, a gambler; the cybertext is a game-world or world-game; it is possible to explore, get lost, and discover secret paths in these texts, not metaphorically, but through the topological structures of the textual machinery. This is not a difference between games and literature but rather between games and narratives. To claim that there is no difference between games and narratives is to ignore essential qualities of both categories. And yet, as this study tries to show, the difference is not clear-cut, and there is significant overlap between the two. It is also essential to recognize that cybertext is used here to describe a broad textual media category. It is not in itself a literary genre of any kind. Cybertexts share a principle of calculated production, but beyond that there is no obvious unity of aesthetics, thematics, literary history, or even material technology. Cybertext is a perspective I use to describe and explore the communicational strategies of dynamic texts. To look for traditions, literary genres, and common aesthetics, we must inspect the texts at a much more local level, and I suggest one way to partition the field in chapters 4 through 7, each chapter dealing with a subgroup of ergodic textuality.

However, after the invention of digital computing in the middle of the twentieth century, it soon became clear that a new textual technology had arrived, potentially more flexible and powerful than any preceding medium. Digital systems for information storage and retrieval, popularly known as databases, signified new ways of using textual material. The database is in principle similar to the filing cabinet but with a level of automation and speed that made radically different textual practices possible. On the physical level, the surface of reading was divorced from the stored information. For the first time, this breaks down concepts such as "the text itself" into two independent technological levels: the interface and the storage medium. On the social level, huge texts could be browsed, searched, and updated by several people at once, and from different places on the globe, operations that only superficially seem to resemble what we used to call "reading" and "writing." Armed with a good search engine and a digital library, any college dropout can pass for a learned scholar, quoting the classics without having read any of them. Several new textual genres have emerged with digital computing and automation. Computer programs, complex lists of formal instructions written in specially designed, artificial languages, can be seen as a new type of the rhetorical figure apostrophe, the addressing of inanimate or abstract objects, with the magical difference that it actually provokes a response. Short, simple programs are often linear, but longer programs generally consist of collections of interdependent fragments, with repeating loops, cross-references, and discontinuous "jumps" back and forth between sections. Given the seminatural vocabulary of some modern programing languages, it is not uncommon for programers to write poems in them, often with the constraint that the "poegrams" (or whatever) must make sense to the machine as well. Programs are normally written with two kinds of receivers in mind: the machines and other programers. This gives rise to a double standard of aesthetics, often in conflict: efficiency and clarity. Since speed is a major quality in computer aesthetics, an unreadable program might perform much faster than a comprehensible one. The poetics of computer program writing is constantly evolving, and through paradigms such as object orientation it inspires practical philosophies and provides hermeneutic models for organizing and understanding the world, both directly (through programed systems) and indirectly (through the worldviews of computer engineers).

A related but reverse problem is the tendency to describe the new text media as radically different from the old, with attributes solely determined by the material technology of the medium. In these analyses, technical innovation is presented as a cause of social improvement and political and intellectual liberation, a historical move away from the old repressive media. This kind of technological determinism (the belief that technology is an autonomous force that causes social change) has been refuted eloquently by Langdon Winner (1986), James W. Carey (1988), and others but continues, nevertheless, to dominate the discussion. In the context of literature, this has led to claims that digital technology enables readers to become authors, or at least blurs the (supposedly political) distinction between the two, and that the reader is allowed to create his or her own "story" by "interacting" with "the computer." The ideological forces surrounding new technology produce a rhetoric of novelty, differentiation, and freedom that works to obscure the more profound structural kinships between superficially heterogeneous media. Even the inspiring and perceptive essays of Richard Lanham (1993) are suffused by this binary rhetoric and, ultimately, dominated by politics at the expense of analysis. fragment.. Whether concepts such as "computer literature" or "electronic textuality" deserve to be defended theoretically is by no means obvious, and they will not be given axiomatic status in this book. The idea that "the computer" is in itself capable of producing social and historical change is a strangely ahistorical and anthropomorphic misconception, yet it is as popular within literary-cultural studies as it is in the science fiction texts they sometimes study.Often, in fact, science fiction portrays the technology with an irony that the critical studies lack (see, e.g., William Gibson's short story, "Burning Chrome," in Gibson 1986). Most literary theories take their object medium as a given, in spite of the blatant historical differences between, for instance, oral and written literature. The written, or rather the printed, text has been the privileged form, and the potentially disruptive effects of media transitions have seldom been an issue, unlike semantic transitions such as language translation or intertextual practices. At this point, in the age of the dual ontology of everyday textuality (screen or paper), this ideological blindness is no longer possible, and so we have to ask an old question in a new context: What is a text? In a limited space such as this, it is impossible to recapture the arguments of previous discussions of this question. And since the empirical basis for this study is different from the one assumed in these discussions, the arguments would be of limited value. In the context of this study, the question of the text becomes a question of verbal media and their functional differences (what role does a medium play?), and only subsequently a question of semantics, influence, otherness, mental events, intentionality, and so forth. These philosophical problems have not left us, but they belong to a different level of textuality. In order to deal with these issues responsibly, we must first construct a map of the new area in which we want to study them, a textonomy (the study of textual media) to provide the playing ground of textology (the study of textual meaning).

Strangely, the struggle between the proponents and opponents of "digital literature" deteriorates usually on both sides into material arguments of a peculiar fetishist nature. One side focuses on the exotic hardware of the shiny new technologies, like CD-ROM. Witness especially the computer industry slogan, "information at your fingertips," as if information were somehow a touchable object. The other side focuses on the well-known hardware of the old technology, the "look and feel" of a book, compared to the crude letters on a computer screen. "You can't take it to bed with you" is the sensuous (but no longer true) refrain of the book chauvinists. Isn't the content of a text more important than these materialistic, almost ergonomic, concerns? What these strangely irrelevant exuberances reveal, I think, is that beyond the obvious differences of appearance, the real difference between paper texts and computer texts is not very clear. Does a difference even exist? Instead of searching for a structural divide, this study begins with the premise that no such essential difference is presumed. If it exists, it must be described in functional, rather than material or historical, terms. The alternative, to propose an essential difference and then proceed to describe it, does not allow for the possibility that it does not exist and is, therefore, not an option. Whether it exists or not is not of great importance to this thesis, however, as such knowledge would not make much practical difference in the world. The emerging new media technologies are not important in themselves, nor as alternatives to older media, but should be studied for what they can tell us about the principles and evolution of human communication. My main effort is, therefore, to show what the functional differences and similarities among the various textual media imply about the theories and practices of literature. The exploration is based on the concepts and perspectives of narratology and rhetoric but is not limited to these two disciplines. I argue that existing literary theory is incomplete (but not irrelevant) when it comes to describing some of the phenomena studied here, and I try to show why and where a new theoretical approach is needed. My final aim is to produce a framework for a theory of cybertext or ergodic literature and to identify the key elements for this perspective.

In the current discussions of "computer literacy," hypertext, "electronic language," and so on, there seems to emerge an explicit distinction between the printed, or paper-based, text and the electronic text, both with singular and remarkably opposing qualities. The arguments for this distinction are sometimes historical, sometimes technological, but eminently political; that is, they don't focus on what these textual genres or modes are but on their assumed functional difference from each other. Such a strategy is useful for drawing attention to, but less so for the analysis of, the objects thus constructed. It might have been tempting to follow this rhetoric in my investigation of the concept of cybertext and to describe a dichotomy between it and traditional, conventional literature; but the meaning of these concepts is unstable to the point of incoherence, and my construct would therefore probably have reached a similar degree of uselessness. Cybertext, then, is not a "new," "revolutionary" form of text, with capabilities only made possible through the invention of the digital computer. Neither is it a radical break with old-fashioned textuality, although it would be easy to make it appear so. Cybertext is a perspective on all forms of textuality, a way to expand the scope of literary studies to include phenomena that today are perceived as outside of, or marginalized by, the field of literature--or even in opposition to it, for (as I make clear later) purely extraneous reasons. In this study I investigate the literary behavior of certain types of textual phenomena and try to construct a model of textual communication that will accommodate any type of text. This project is not as ambitious as it might sound, since the model is provisional and empirical and subject to future modification should any "falsificatory" evidence (such as an unpredictable object) appear. This pragmatic model is presented in detail in chapter 3. Multi-User Dungeons The rest of this introductory chapter discusses the conceptual foundations and implications of this approach and establishes the terminology applied in the analytical chapters. These chapters (4 through 7) each takes on a main category (or genre) of cybertext roughly corresponding to the results of the analysis in chapter 3: hypertext, the textual adventure game, computer-generated narrative and participatory world-simulation systems, and the social-textual MUDs of the global computer networks. This pragmatic partitioning, which derives from popular convention rather than from my own theoretical model, is motivated by my strong belief that, in such a newly awakened field, theoretical restraint is imperative. Theories of literature have a powerful ability to co-opt new fields and fill theoretical vacuums, and in such a process of colonization, where the "virgin territory" lacks theoretical defense, important perspectives and insights might be lost or at least overlooked. When we invade foreign ground, the least we can do is to try to learn the native language and study the local customs. Although several studies have already been carried out within most of these subfields, almost none have produced overarching, or universal, perspectives or engaged in a comparative analysis of all the forms of textuality examined here. Therefore, these previous approaches are discussed in their respective chapters rather than in this general introduction.

As can be inferred from its etymology, a cybertext must contain some kind of information feedback loop. In one sense, this holds true for any textual situation, granted that the "text" is something more than just marks upon a surface. A reader peruses a string of words, and depending on the reader's subsequent actions, the significance of those words may be changed, if only imperceptibly. The act of rereading is a crucial example: the second time we read a text, it is different, or so it seems. How can we know the text from the reading? Sometimes, a reader may influence the text for other readers, even if all the "marks on the pages" stay the same: a dramatic example is the ayatollah Khomeiny's reaction to The Satanic Verses. The conventional split between text and reading (between the "intentional object" and the "mental event"), or signifiant and signifié, is not an impermeable membrane: leaks occur constantly; through various stages of reception such as editing, marketing, translation, criticism, rediscovery, canonization, or banishment. These well-known processes are not entirely trivial, however, because they remind us that a text can never be reduced to a stand-alone sequence of words. There will always be context, convention, contamination; sociohistorical mediation in one form or another. Distinguishing between a text and its readings is not only necessary, it is also quite impossible--an ideal, in other words. On the one hand we need the image of "the text" in order to focus on anything at all; on the other hand we use the metaphor of "reading" to signal that our apprehension of a text will always be partial, that we never quite reach the "text itself," a realization that has led certain critics to question the very existence of such an object (see, for instance, Fish 1980). This hermeneutic movement or desire--perhaps better described as asymptotic than circular--holds true for all kinds of textual communication, but the particular organization of a text can make both the reader's strategic approach and the text's perceived teleology very distinctive, perhaps to the point where interpretation is stretched beyond the cognitive bounds of a singular concept. It is this field of varying textual organization that this study attempts to clarify. The differences in teleological orientation--the different ways in which the reader is invited to "complete" a text--and the texts' various self-manipulating devices are what the concept of cybertext is about. Until these practices are identified and examined, a significant part of the question of interpretation must go unanswered.

Previous models of textuality have not taken this performative aspect into account and tend to ignore the medium end of the triangle and all that goes with it. In his phenomenology of literature, Ingarden (1973, 305-13) insists that the integrity of the "literary work of art" depends on the "order of sequence" of its parts; without this linear stability the work would not exist. While Ingarden here certainly acknowledges the importance of the objective shape of the text, he also reduces it to a given. This taken-for-grantedness is hardly strange, since it is only after we have started to notice the "medium" and its recent shifting appearances that we can begin to observe the effect this instability has on the rest of the triangle. As Richard Lanham (1989, 270) observes, literary theorists have for a long time been in the "codex book business," restricting their observations (but not their arguments) to literature mediated in a certain way. Even within the field of codex literature there is room, as experimentalists from Laurence Sterne to Milorad Pavic have demonstrated, for mediational variation, but these attempts have not, apparently, produced sufficient contrast to provoke a systematic investigation of the aesthetic role of the medium (a notable but much too brief exception being McHale 1987, chap 12). There is also the fascinating phenomenon known as "Artists' Books," an art movement that originated in the sixties and dedicated to the creation of unique works of art that challenge the presumed properties of the book from within (cf. Strand 1992b and Lyons 1985). Cybertext, as now should be clear, is the wide range (or perspective) of possible textualities seen as a typology of machines, as various kinds of literary communication systems where the functional differences among the mechanical parts play a defining role in determining the aesthetic process. Each type of text can be positioned in this multidimensional field according to its functional capabilities, as we shall see in chapter 3. As a theoretical perspective, cybertext shifts the focus from the traditional threesome of author/sender, text/message, and reader/receiver to the cybernetic intercourse between the various part(icipant)s in the textual machine. In doing so, it relocates attention to some traditionally remote parts of the textual galaxy, while leaving most of the luminous clusters in the central areas alone. This should not be seen as a call for a renegotiation of "literary" values, since most of the texts drawn attention to here are not well suited for entry into the competition for literary canonization. The rules of that game could no doubt change, but the present work is not (consciously, at least) an effort to contribute to the hegemonic worship of "great texts." The reason for this is pragmatic rather than ethical: a search for traditional literary values in texts that are neither intended nor structured as literature will only obscure the unique aspects of these texts and transform a formal investigation into an apologetic crusade. If these texts redefine literature by expanding our notion of it--and I believe that they do--then they must also redefine what is literary, and therefore they cannot be measured by an old, unmodified aesthetics. I do not believe it is possible to avoid the influence from literary theory's ordinary business, but we should at least try to be aware of its strong magnetic field as we approach the whiter spaces--the current final frontiers--of textuality.

Weibel had above all placed emphasis in his exhibition concept on examining the problematic issues of totalitarian systems such as Fascism, Communist dictatorship or National Socialism and to present in critical terms his long considered views of Austria’s identity as a “country without qualities” (a reference to the title “Man Without Qualities” of the key Robert Musil novel). Austria had never enjoyed a conflict-free relationship to modernity, especially in the interwar years, when its representatives had been banished and exiled, and after the “Anschluss”, the incorporation of Austria into the Third Reich, Austria had denied and suppressed its pernicious role as a guilty party in the Holocaust. This project of negation of criminal involvement and an abysmal identity transfer to that of a victim had been only too gladly passed down to the present time and the problems blithely swept under the carpet. Peter Weibel has rebelled from his earliest youth against this public denial of an “expulsion of the intelligentsia from Austria” both in spectacular art events and performances as also in his theoretical writings and has remained an intellectual authority on contemporary political and topical social issues.
In the meantime we had also engaged an architect, since this exhibition was to be spread among three venues due to the enormous volume of art works covered. The choice fell on Manfred Wolff-Plottegg, who had congenial relations with Weibel in implementing exhibition projects from his youth through to the present. This background naturally meant that he was no easy case to deal with either, and he held with unrelenting firmness to each of his “Utopias,” whether the issue was a deconstruction of the Kuenstlerhaus roof, or pushing through his plan for the development of unconventional wall structures in steel for the Baroque halls of the Neue Galerie, in which he brought our restorer to the point of madness, or even his plan of filling the city with silage bales for use as an advertising medium. The conflict potential rose with the temperature in the course of an unusually hot summer. All holidays were cancelled and any of the usual divisions between the working day and the weekend, day, and night gradually vanished. Our eating habits changed – it became a joy to have the pizza service arrive somewhere around midnight bringing us our “lunch”; Red Bull and Bach-flower drops were used to keep up our concentration through the early hours of the day while reading and editing the galley proofs that were being set manually. Alexandra measured out the dimensions for the illustrations using a ruler, she cut and glued them for the layout that Weibel had specified.
Avant-gardes of the twentieth century fought against the (art) institutions in the name of each individual artist’s freedom – the freedom to overcome the limits of these institutions, to break free from them, flout obsolete standards and create the outrageous. Today people still like to criticize institutions, aiming at their disempowerment and abolition. Meanwhile, however, the meaning of this critique has changed completely. Today criticizing institutions serves only to conceal the fact that these mighty, normative institutions de facto no longer exist. Today’s art institutions are usually weak willed, have no money and are scarcely perceived by the wider public. They cannot and do not want any longer to set up binding aesthetic standards. They would rather chase after fashions spread by the media; they seek to be cool and hip to appeal to “young visitors.” The great, mighty museum director with indisputable power over his collection, or the art critic with a farreaching influence who sets directions and dictates his taste to the public: these are figures that belong to a long-gone age. Today’s criticism regarding cultural institutions can only be interpreted as a nostalgic invocation of this past epoch – an attempt to conceal the real weakness of today’s art institutions. Under these conditions the artist no longer has a chance to orientate himself according to existing art institutions, be it in the sense of adapting to the standards they have established, or in the sense of a revolution against these standards. Basically, the artist is left with but one possibility: to found an institution of his/her own, making themselves an institution. Certainly this way is much more cumbersome than the easier way of common criticism of institutions. Hence there are only very few artists who have been down this road consistently. And these artists deserve our admiration. Amongst these very few, we most assuredly find Peter Weibel. Weibel understood from early on that advanced art has no stable place in our society and that this place has yet to be created – on all levels of topical artistic practice. As an artist, if one does not choose to make oneself a curator, one is dependent on the curator. And when the artist wishes to assume the role of curator, his best move is to keep a museum space constantly available for himself. A space in which he or she can experiment with all kinds of exhibitions. Today’s artists cannot count on a binding comment on their work unless they become theoreticians in their own right and comment on their own work. They could also invite other theoreticians to write for a publication conceived by the artist, and then design and edit the book themselves. Ideally they would print it themselves as well. In this the artist would always be dependent on the technology continuously developing in art production and distribution. If they don’t develop the production on their own account. In this way they could avoid being dependent on using only the things other people had produced as a technological progress. This would mean the artists themselves must create and manage an institution, which would further develop artistic techniques. They must run this institution, employ people and have the means to pay them. So the artists have to ask themselves how they would not only passively reflect and criticize the economical, political and administrative aspects of their artistic practice, but also how they could actively design, organize and manage them on a daily basis. In other words: Once the artists stop to act as a decorator for the state, they have no choice but to become a state within a state, if they choose not just to cater for the art market.

Weibel’s art, however, is not limited to the creation of this image of artistic sovereignty. Beyond art institutions there is another dimension that increasingly determines today’s art world, i.e. the beholder. The avant-garde promised the artists absolute individual freedom regarding all their artistic decisions – the freedom not only to choose freely the theme of their work, but also to dispose freely of all artistic means, independent of all traditions, conventions and criteria of good taste or the mastership that still largely determined artistic creation before the rise of radical avant-garde at the beginning of the twentieth century. This newly promised freedom had a drug-like effect on the artists at first and created an unprecedented intoxication. This is why so many works arose from these early times which we still admire today. At the same time, however, one cannot deny that little has remained of this early euphoria. Today, artists no longer feel free – rather, they feel that their subjectivity has but little or no relevance for the art world. The reason for this new powerlessness on the part of the artist can be put down to the fact that, in the attempt to free themselves, at the same time the artists of the historical avant-garde committed a somewhat fatal act, and one whose importance they themselves did not notice in the beginning: they freed the beholder. The beholder was relieved of any of the criteria he used, or was supposed to use, for his judgment of a piece of art. The artist actively liberated himself from these criteria – and the beholder lost them. This made the beholder extremely insecure. He no longer knew how to react to the pieces of art that were presented to him. He felt helpless vis-à-vis the arbitrariness of the artist, utterly un-able to defend himself by means of a well-founded judgment. This initial insecurity reduced significantly with time. The further modern art developed, the more the beholder began both to cherish and relish the insecurity of freedom given to him by the artist. There are of course more beholders than artists and, according to democratic conditions, we all know that the minority must bend to the will of the majority. In earlier times the artist was able to reject the public’s judgment by asserting that they had not properly understood his work. The artist could call upon his mastership, the historical originality of his work and the strength of his inspiration, etc. Today, however, these justifications no longer apply, as the public feels totally free in their judgment. If they simply do not like something, they are not prepared to let themselves be convinced otherwise; the tables have turned. The avant-garde has directed the artist’s freedom against all of the public’s judgments. Now the freedom of the public’s judgment is directed against all reasoning and explanations on the part of the artist.
Even if our culture no longer believes in the criteria of taste, it certainly believes in knowledge and in technology. Art’s liberation from the beholder’s taste can only happen in terms of a mechanization of art. Or, in other words, its re-mechanization. Art was always, in the first place, a matter of technique. Only the historical avant-garde discredited and then abolished art production’s old techniques. Malevich’s Black Square and Duchamp’s Ready-mades introduced an age of the de-mechanization of art. It was particularly at this point that artists lost their cultural privilege regarding the beholder. If an artistic decision is totally free to declare an object or a random form a work of art, then the beholder must also be free not to accept this decision. Everything becomes a question of personal or individual taste. If the artist’s taste is contrary to the beholder’s taste then, as we all know, the loser is the artist. The avant-garde was an attempt to liberate art by de-mechanizing it, by liberating it from capability and knowledge, by equalizing the act of artistic creation with aesthetic judgment. Today, this de-mechanization of art, which was carried out by the historical avant-garde, increasingly presents itself as the preparation for a new phase of its radical re-mechanization. The avantgarde abolished old techniques such as drawing, painting and sculpture – or relativized them in their meaning. However, after a phase of being freed from technology, the re-mechanization of art has begun. It started with the use of new digital techniques of image production and distribution. The artist has again become a technician, a specialist and a producer, thereby once again establishing cultural distance between himself and the beholder. Peter Weibel had an early understanding of the opportunity offered by the new determination of arts and he seized it. Many artists and theorists have preached the return of art to technology and knowledge in the times of the Post-modernism of the 1970s and 1980s. However, this almost always implied some kind of ironic return to traditional artistic techniques. One wanted to protect oneself from the all too mighty mass media and define other, alternative spaces. For the same reason others use to legitimate their farewell from the avant-garde, as Weibel connects to the avant-garde’s traditions. The opportunity and even the need to restructure the avant-garde program, results for Weibel from the fact that art has definitely left behind its traditionally closed rooms and must act amidst today’s technological, medialized world. Hence the avant-garde was right in letting go of traditional art techniques, directing the view of the beholder onto the new technical world – even with a critical purpose. Today, however, it is widely pronounced that art should give up its critical, elitist, avant-gardist attitude and should be easy to digest. In short: that art, as soon as it starts to be produced in the context of and by using mass-media procedures, should be obliged to adapt aesthetically to the laws of mass-media dissemination too.
Here, the avant-garde is not dismissed as obsolete but rather transcended by means of a re-mechanization of art and continued in its critical impetus. The repertoire of images circulating in mass media networks is very limited. Not only so-called images of the other, but also images created by science, interactive images, purely subjective images, let alone abstract images, usually have no chance of entering large mass-media networks. It is, however, just these denied, excluded images that Weibel works with. Images which can be created using technical media and are hence indeed technically compatible with the networks of massmedia dissemination. At the same time, they display an aesthetic incompatibility with these networks. In this sense Weibel’s artistic practice is a direct continuation of the artistic practice of the historical avant-garde under the conditions of its technical and medial topicality. This would imply a continuation of the critical discussion of mimetic illusion.

This insight, however, would only worry an engineer, not an artist. Weibel is very well informed about the limits of logic, since he has seriously and systematically researched the field. Artists like to use computer programming and simulation because they like to demonstrate how the system loses its own systematic, how strict logic leads to paradoxes and how an ambivalence is created by an obsessive search for unambiguousness. Art is indeed interested in system, structure and program, but mainly where they lead themselves ad absurdum. It is exactly in this drift toward the absurd that it becomes clear how the image of thinking differs from thinking itself and that the living cannot easily be mimetically duplicated. Today’s artists, working with communicative and “intelligent” media, are more interested in the blind alleys, disturbances and absurdities of media communication than in its acquisitions. This is why the re-mechanization of art is certainly not just about naive enthusiasm or worship of technology. On the contrary: it allows not only an ideological claim of the limits of technology but also their technical analyses. But it is not mere pleasure in chaos that makes artists interested in the dysfunctional, in the deceptive element of computer-controlled and communicative processes. It is instead a systematic and critical analysis of mimetic procedure by means of art. This is a task modern art has imposed on itself and in the meantime it has become tradition. Except that today it is the mimesis of thinking, not reality. The analytical, critical dimension of Weibel’s occupation with the operational logic of today’s computer-controlled and communication systems, which he pursues in the context of his work at ZKM Karlsruhe, is already detectable in his early works dealing with the logic of mass media and everyday communication. These earlier art works can be seen mainly as examples of the artist’s critical occupation with the conventions of today’s media culture. These works often have a clear political dimension. They frequently refer to the chances as well as to the shortcomings and absurdities of today’s media world. However, mostly these works display a very specific sense of humor. As we all know, Soren Kierkegaard drew a strict line between irony and humor. For Kierkegaard, irony was the manifestation of a seemingly endless subjectivity aiming for a triumph over the finite character of things. Humor was for him the result of this subjectivity’s insight about its own finiteness.

Don’t worry, this is not an attempt to trace family circumstances or events way back in Odessa. The making of spiteful allegations is not our purpose either but rather to take a leap in the dark and to attempt an investigation of one of the most lucid creative contributions to Austrian art since 1945 – the work of Peter Weibel. Lügt (lies) was the message written on a board the artist held up beneath the sign of a police station in 1971. Let us forget for a moment that it is above all federal institutions, which through their practice contribute in no way at all to the good of certain groups of people. Indeed, these institutional inventions have set themselves objectives such as the freedom of the individual or social progress, or simply, the welfare of mankind, yet, there are also some quite contrary side effects, extending as far as the blatant discrimination for some of the so-called “supported.” If this were not the case, in our representational democracies we would be obliged to assume that the police are ultimately the tool of the representatives of the people (politicians). They again, are elected by the people (us), representing it (us). They are the people (us). So if the policeman lies, this is first of all an incident, maybe resulting from a certain specific personality structure. For instance because the policeman as an individual is given too much personal power which as a consequence he might abuse to the detriment of other individuals. Lies are told, however, also by the institution, the state, the representatives of the people, by the people themselves – and finally by us. In this paradoxical situation you too, dear reader, are lying every bit as much as the author of these lines – and consequently Weibel too. We know what the artist is on about and what he is denouncing, at the same time, however, we understand the limits of linguistic possibilities. Thus in striving for freedom and knowledge, we are forced to contrast the state lie with our own lie, whilst the notion is relativized as soon as the viewpoint changes. In the most radical moment of anti-art, in the 1960’s, avant-garde artists – building their work on the heritage of Dada, Surrealism and Constructivism – were able to shatter the foundations of bourgeois society, and thereby create fear. The traditional self-conception of art at that time was after all based on the assumption that art per se was a part of bourgeois society and thus part of its own system of rule and power. So if art crosses the divide to become an expression of both cultural and political rebellion, its demand for truth will change. For its “counterpart” this becomes a lie that must be fought against. Whilst for the avant-garde, art, now transformed into a weapon, will reveal the lie of the bourgeois capitalist society and hence of the federal power. So we should look at Peter Weibel’s practice with regard to this field of conflict. Seemingly remote sources from anarchism to scientific-theoretical models were suddenly applied in order to help art to get out, to transform, helping it to new relevance.

Polizei lügt (police lie) is one of a series of actions that was documented photographically. Weibel changed the script on public notices. Oberlandesgericht became Oberschandesgericht (Higher Regional Court became Higher Disgrace Court), and Rechtsanwalt became Rechtsgewalt (attorney at law became violence of the law). This “scriptual terrorism” came into existence simply through holding the appropriate letters at the corresponding positions in the word. These interventions were a more direct invasion of public space than the graffiti being created at this time in New York City. As a potential for the denotation of an actual content, a warning, a revolt against state institutions, these “attacks” create feelings of insecurity. The graffiti by contrast no longer contains any direct message, for they have internalized the message and have been radicalized in their formal qualities. “Being neither denotation nor con-notation, they evade the principle of nomenclature and break as empty signifiers into the sphere of the fulfilled signs of the city, which is dissolved in turn by their pure presence.”2 Thus graffiti represent a variation – derived from the sub-culture – on linguistic criticism. Weibel’s series were titled attacks and still speak the language of the protest movement: do it! It also represents a form of subversive poetic statement, reducing content to its opposite through linguistic intervention, while plumbing the limitations of what language in fact can do. Weibel has himself studied and done academic work on the issue of philosophy as criticism of language, culminating in the formal thinking of the mathematician Kurt Gödel. Gödel has provided the quasi salvation of the “liar” paradox in his statement “I cannot be proved.” If Nietzsche took God from us, then Gödel would seem to have robbed us of ourselves. Because where are we if we cannot be proved or demonstrated? There, perhaps, where God is... A connection between linguistic skepticism and individual social rebellion was developed still further in the protest movement of the 1960’s of which Weibel was a part. This connection provides a basic framework, in which the content and socially critical expressions within Weibel’s art can be developed. He did this in the mid 1960’s in literature and as an extension from this into the fine arts. He would not be Weibel if he had not also created a theoretical foundation for his artistic actions. He provides coordinates for the clarification of his procedure in his Sub-geschichte der Literatur (A Sub-History of Literature).3 Here we learn that the official history of literature is not “a history of human invention through, with, and in language” but “rather a collection of felicities and self-help.” Thus no basic or social change is possible without bringing about a change in the formal structure. The hero, in content that is, is not an adequate figure however when presented in traditional costume. The Russian linguist Roman Jakobson provided a formulation of this development in 1921, by describing the poetic procedure as the “sole hero” of literature. The formulation of a sub-history, such as that of the avant-garde, comprises in itself a social-political dimension, precisely analogous to the content which it reports in its special cases.

Weibel’s early experiments began in 1964, downwind of the Vienna Group that had begun to assemble in the 1950’s. Direct work was done by this movement during the 1950’s and 1960’s as a sort of continuation, in the most radical manner possible, of the results achieved by the historical avant-garde (Dada, Futurism, Surrealism, Constructivism u.s.f.). Vienna was by no means ahead of the times in this decade – it was provincial, traditional and marked with the brown striations of a Nazi past. Neo-avant-gardist movements such as the Vienna Group had an absolutely unsettling character; the conviction that the transformation of bourgeois aesthetics would of necessity go hand in hand with the transformation of bourgeois society was here understood as a matter of course. Criticism of both the state and of reality by means of a criticism of language contributed on the one hand to an overstepping of the aesthetic consensus of the times and on the other hand to the transgression of genre frontiers. In anticipation of the “linguistic turn,” the artists expanded into other media such as photography and film, and, even more radical forms of artistic practice such as happenings, Concept Art, and actions. In other words, change-related thinking was by no means solely restricted to language and the arts, and the borderline between art and life was shifting in consequence. The Eight–Points–Proclamation of the Poetic Act, written in 1953 by H.C. Artmann, acts here to a certain extent as godfather.4 The expansion of the definition of language first took place through the inclusion of sounds and words, but continued with another phase of development that included other materials as well, up to the point where the poet could get by without using any kind of verbal language whatsoever. The visual dimension of the written text was rapidly overcome by Weibel as well, whose own work developed along the lines of action texts, motion poems, photo poems, object poems, process poems, and, as a concluding sort of gesture, the radical use of one’s own body. In works such as raum der sprache – skulptur mit angeschlossenem organismus (space of speech – sculpture with a connected living organism) Weibel had his own tongue set in a block of concrete. The point that is articulated in this gesture ranges from general speechlessness to the achievement of a bodily and social effect on people through the use of speech. What else can it be but a lack of freedom, after all, when the letter of the law is interpreted by a judge? What is possible for a judge, in setting his organ of speech in motion, is altogether impossible for the person whose tongue is sealed into a wall of concrete and marked by bodily repression.

Herbert Marcuse spontaneously offers a helping hand here: “Only when art remains negative is it in a position to negate existing reality itself.” And he continues: “Only authentic art is negative and this in the sense that it refuses to obey the existing reality, with its language, its order, its conventions and in its images. (...) To this extent art provides a representation of a free society and of closer human relationships. But art cannot go beyond this.” During the student revolts of the 1960’s, artists in a wide range of fields argued about the commitment of art within the context of the revolutionary process. There was relatively little to hear about these ideas in this respect in Austria, however. The work of the Viennese Actionists (e.g. Kunst und Revolution / Art and Revolution, 1968) represented one of the few contributions to this. Weibel, who was to some extent an active participant in Viennese Actionism and provided theoretical support for it, was once again the exception, however, for no other significant artistic work of Austrian origin provided such thoroughly committed contributions to these various processes. Michail Bakunin should be permitted to have a word here as well. For him, positive means reactionary, an acceptance of the status quo and thus ultimately a sort of quietness. Negativity is for Bakunin, as he wrote in 1842 in his essay Die Reaktion in Deutschland (Reaction in Germany), the democratic principle, which in his day represented the negation of an existing reality, and indeed ultimately represented a sort of movement. He argues from this that the negative alone has the right to existence, closing with the celebrated words: “The lust to destroy is also a creative lust!”6 This sentence has had a great influence of course in artistic and intellectual circles through to the present day. The intensive desire for change in the midst of an unbearable reality sounds like a trumpet call for all avant-garde demands and manifestos. This same phrase applies to the work of Dada, through Surrealism, and even the Viennese Actionists, just as it is to be encountered in linguistic criticism of that time, in its most radical and brutal formulations. In short, Bakunin’s sentence is the legitimizing basis for all such antipositions, and not just in art.

It is true that communicative media already have established a continuity in time and space: telephone, fax, electronic mail, numerical and telematic networks, radio, television, the press, etc. This continuity is still not the continuity of the active and living thought, singular and differentiated, emergent and cohering everywhere, but rather a network for the transportation of information. Do the viewers of a televised transmission share a community? Do they bring together their experiences and their intellectual powers? Do they envisage and perfect new mental models of a situation together? Do they even exchange arguments? No. Their brains are not yet cooperating. The continuity effected by the media is only physical. It is a necessary prerequisite of the intellectual continuity but not sufficient in itself. Until this morning the work of writing was undoubtedly one of the most efficient means for the production of collective thought ever invented. The network of libraries keeps records of the creation and the experience of myriads of dead and living human beings. The fragile filament of memory is re-established, dormant thoughts revivified from generation to generation through the processes of reading and of interpreting. Translations from one language to another, or from one discipline to another, assure the communication between detached spaces of thought.
But by its nature the classical form of writing is a static and discontinuous system of signs. It is an inert, parcelled, dispersed body becoming more and more enormous each moment, and its unification and resuscitation requires that each individual sacrifice years and years to research, interpretation to the establishment of connections. As a remedy to the present situation, virtual worlds of collective intelligence will see the development of new forms of writing: animated pictograms, moving languages that will preserve traces of their interaction with navigators. By itself, the collective memory will organize itself, unfold itself anew for each navigator according to his interests and his previous traversings of the virtual world. The new space of signs will be sensitive, active, intelligent, at the service of its explorers. I ask again: what is interpretation? The subtle mind attempting to invite the inert body of letters into a dance.The evocation of the breath of the author in front of dead signs. The haphazard reconstruction of the knot of affects and of images in which the text originates. And, finally, the production of a new text, that of the interpreter. But what if the signs are alive? What if the image-text or the space-thought continuously grows, proliferates and metamorphoses itself to the beat of the collective intelligence? What if the leaden characters cede their place to some dynamic and translucent substance? What if the opacity of the gigantic stratifications of texts effaces itself in front of a flowing and continuous milieu the center of which is always occupied by its explorer? After the encounter between the vivifying spirit and the dead letter, after the dialectic of the corpus and the oral tradition, comes a new mode of the construction of the continuity of thought, a mode making possible the participation of everyone in the adventure of a nomadic language.

The first nomadic people followed after their flocks searching for nourishment, moving about following the rhytm of seasons and of rains. Today we are nomads following after the future of humanity, the future traversing us and made by us. The human being has become its own climate, an endless season with no return. We are hord and flock intermingled, more and more attached to our instruments and to the world moving with us, strolling on a new steppe each day. Neanderthal men, well adapted to the wonderful hunting expeditions on the glacial tundra, became extinct when the climate abruptly became warmer and more humid. Their natural game disappeared. Despite their intelligence these growling or mute men had no voice, no language with which to communicate with each other. Therefore the solutions found for their problems here and there could not be made more general. They remained dispersed even when they were faced by the transformation of the world surrounding them. They did not change with it. Today the homo sapiens is face to face with a fast modification of its surroundings, a transformation of which it is the collective involuntary agent. We may either cross a new threshold, a new stage in the evolution of man, by inventing some attribute of humanity as essential as language but on a superior level. Or we may continue to "communicate" through the media and to think in institutions detached from one another, organizing moreover the suffocation and division of intelligences. In the second case the only problems we would still be confronting would be problems of survival and of power. But if we were to take the route of the collective intelligence, we would gradually invent techniques, systems of signs, social forms of organization and of regulation permitting us to think together, to concentrate our intellectual and mental power, to multiply our imaginations and our experiences, to work out practical solutions for the complex problems affronting us in real time and on all levels. We would progressively learn to orientate ourselves in a new cosmos, constantly transforming itself and drifting, to become its authors as much as we can, to invent collectively ourselves as a species. Collective intelligence does not aim at the mastery of selves through human collectives but at an essential loosening of the grip changing the very conception of identity, the mechanisms of domination and of the breaking out of conflicts, the unblocking of confiscated communication, the mutual launching of isolated thoughts. So we are now in the same situation as a species whose each member would possess a good memory, would be perceptive and astute, but which would not yet have reached the stage of the collective intelligence of the culture because it would not have been capable of inventing an articulated language. How can one invent language if one has never spoken, if one's ancestors have never pronounced a single phrase, if one has no example to follow, not the slightest idea of what language could be? We are as nearly as possible in the same situation presently: we do not know what it is that we have to create, what we may already have obscurely began to envision. Still it only took a few millenia for the homo habilis to become the homo sapiens, to cross such an imposing threshold; it launched itself in the unknown, inventing the earth, the gods, and the endless world of signification. But languages are made for the "human scale" communication within small communities, perhaps even to guarantee the stability of their relations. Thanks to writing we have reached a new stage. The technique of writing effected the growth of the efficiency of communication and the organization of human groups; its scope was much wider than could ever have been that of shere speech. But this change took place at the expense of the unity of societies: it caused the division of societies into bureaucratic machineries for the handling and manipulation of information with the aid of writing and into those to be "administered". The task of the collective intelligence is to discover, or to invent, the other side of writing, the other side of language, so that the manipulation of information would be distributed everywhere, coordinated everywhere, that it would no longer be the priviledge of separate social organs but, on the contrary, would be naturally integrated into every human activity, as a tool in the hands of everyone. This new dimension of communication should evidently permit the mutuality of our knowledge and the reciprocality of its transmission which is the most rudimentary condition of the collective intelligence. In addition it would open up two major possibilities that would radically transform the fundamental facts of life within societies. First, we would have at our disposal simple and practical means of finding out what it is that we are doing together. Second, we could handle, even more easily than we write today, instruments allowing collective enunciation. And all of this no longer on the scale of paleolithic clans, or on that of States and historical institutions, but with the amplitude and velocity of gigantic turbulences, of deterritorialized processes, and of anthropological nomadism influencing us today. If our societies content themselves with mere intelligent government, they will almost certainly not attain to goals set by them. In order to have some chances of a better life, they will have to become intelligent by the masses. From beyond the media aerial machineries will make the voice of the multiplicity heard. It is still indiscernible, muffled by the mists of the future, bathing another kind of humanity in its murmur, but we are destined for an encounter with superlanguage.
The advantage on the other hand, is that these submicroscopic data can be transmitted without material movement in space and therefore without human accompaniment. Nontechnical media are bound to a physical carrier, they require the same network infrastructure as traffic of people and goods. The earliest wire networks had a point-to-point structure, requiring access points for in- and output, relay or refresher stations at regular intervals, and central nodes where messages are switched to their destination. In 1896, Marconi's wireless telegraphy extended the traffic of signals into the ethereal radio spectrum. The point-to-point cable was supplemented by omnipresent waves that can be intercepted by anyone owning a receiver. Radio extended the distance the voice carries virtually around the globe. With the broadcast networks of radio and TV, the center-to- all structure was invented. One speaks and all listen. These media are an extension of the public sphere, and therefore the radio spectrum is usually considered a public resource and regulated accordingly. Broadcast media create the masses they address,synchronizing millions of non-present, anonymous media recipients. McLuhan points to the origin of technical media in the medium print. In contrast to his own interpretation of the electronic media as fundamentally different from print, in their homogenizing function they are not. Reading this quote against its author, I see the program of 'homogenization of men and materials' rising to its ultimate violent power only in its military form under conditions of mass- mobilization during the Second World War (radio), and in its postwar civilian form under conditions of mass-markets, -media, -automobilization, -tourism etc. (TV). One further important aspect of technical media is that perception of the world shifted from the real thing to its stored mediatizations. Typists took dictation not from their superior's voice but from a gramophone or telegraphone recording. The question if Leland Stanford's horse had all four feet off the ground when in gallop could not be answered by observation through the naked eye, but Muybridge's serial photographs showed that it was in fact the case. The amount of live music we listen to is neglectable in comparison to prerecorded music. Whereas live broadcast implies a co-existence in time, a simultaneity that seems to warrant authenticity, much of what we see on TV is pre-recorded, edited, re-run - if we are not watching out of local storage of video anyway. Personal communication shifted from synchronous to asynchronous with the storage of answering machines, faxes, and email. The Matrix itself is a vast and rapidly growing-library. In short, large and exponentially growing parts of our media horizon are 'canned', and the two essential new operations besides transmission that technical media add to those of the Gutenberg Galaxy - copying and editing - are based on storage media.
The computer has its roots in mathematics which is indistinguishably linked to astronomy. Computing machines were built before Leibniz, like Schickhard's calculating clock (!) (1624) or Pascal's adding machine (1642). Still the primacy goes to Leibniz who produced a great confluence of streams of ideas, and contributed profoundly to symbolic logic, combinatorics, and therefore the history of the computer. Babbage should at least be mentioned in passing. His projected Analytical Engine was to have included most of the characteristics of modern computers realized only a hundred years later: a store, a mill (CPU), a transfer system, in- and output, and he also anticipated automatic operation, external data memories, conditional operations and programming. In 1847, Boole used a binary notation to represent truth-values in formal logic, 0 and 1 representing 'false' and 'true'. Shannon and Weaver's information theory translated the Boolean false and true into off and on states in electronic components. Signals, since this ultimate analytic cut with Ockham's razor, fall apart into basic indivisible yes/no units called bit. Like 'atom' for the material world and 'individuum' for society (both meaning 'indivisible'), 'bit' marks the smallest possible unit, the simplest building block of any possible symbolic system. Having mentioned some of the shoulders he was standing on, I can now turn to Alan Turing. I suggest to name the emerging horizon of binary digital media "Turing Galaxy", because its two central concepts were first formulated by him. One is the Universal Machine, the extremely primitive machine that can emulate any machine, the typewriter that reads and writes an operative text out of no more than two characters which freely models the appearance of the typewriter itself, the Universal Medium that precedes and empowers any possible multimedia to come. From then on every phenomenon and every process that can be described completely and unequivocally (the definition of both algorithm/automaton and the inter-subjectively scrutinizable knowledge of science) can be implemented in the one single machine to end all machines. The problem of building new machines has been replaced by the problem of writing an operational description of this new machine for the universal machine. (5) The other is the thought experiment known as the Turing Test which provided a comprehensive re-definition of man as a symbol processing system on a par with machines, and technically resolved the subjectivity problem. (6) Since then, 'intelligent' modelling, signal processing, and pattern recognition - so called thinking - has turned into a continuum across a range of possible technical or biological implementat ons. 'Mind' and machine have become interconnectable (if not interchangeable). Turing or bit media inherit properties from earlier media. They still operate largely in the mind-frame of the mathematical and the Gutenberg Galaxy. The most essential new operation introduced in the Turing Galaxy seems to me simulation. While models in the Gutenberg Galaxy become operational only after being read into and processed by the cortical CPU, models in the Turing Galaxy run inside a dynamic self-active technical medium. Bit words have the double function of addressing human readers as well as machines, i.e. themselves. Action unfolds and changes according to a script or in response to the action of the user and to its own results. Simulation allows to test hypotheses, to automatically control real life processes, and to construct alternate worlds. Today we observe the collapsing of all media into the universal medium computer. Turing media connect people, libraries, machines, and aartificial communicational entities. We are still exploring what the usage of computers in 'Turing mode' could mean. My suggestion: acting inside of media, and interacting with artificial agents.
form of computer games. Games are simulations. In the earliest form they simulate rules and strategies of board games. Later they simulate technical systems (notably with the military flight simulator reappearing as entertainment product), and social systems ( role playing games, SimCity etc.). In games (as in simulation) the computer takes on the function of agency, of a counter- player, an interlocutor, simulating dragons, enemy aliens, humans, governments, or simply fate. The computer also provides the playing-space into which the human player projects herself as a sprite, avatar, or persona, a marionette of herrself that she flies by the wire of the joystick. This is the first time not only the eye and ear, but the hand reaches through to the other side of the proverbial looking glass. With the emergence of data networks, games shifted from single-player stand-alone games to multi-player networked games or MUDs (Multi-User Dungeons/Dimensions). Here human others are re-introduced into the position of counter-player, next to and on a par with pieces of software simulating game characters. Originally games in the narrow sense of the word built around the sword-wielding-and-monster-slaying world of Dungeons & Dragons, MUDs are developing into common meeting grounds around diverse topics including professional conferencing facilities and educational institutions.
A typical host on the Matrix has a tree structure. It might be presented as a directory listing, menu, or in an index file pointing to the individual texts. One might browse or search by keywords with the search space inside one database, one host, or network-wide. At the final nodes of the tree one might find a text, in itself complete with author, pagination, footnotes, etc. The Gutenbergian resources on the Matrix are vast. Librarians were among the first to inhabit and develop it. Just a few examples are the US Library of Congress, including their Soviet and Vatican online archives, the Project Gutenberg, books.com, the first bookstore on the Net, magazines like Wired, and a sprouting new category of multimedia Metazines like the Electric Eclectic. Do people actually read books on the screen? Do they print them out? How are etexts used? The advantage of having reference books ready for automatic searches is obvious. Same for checking quotes in any sort of text. Maybe people will start to actually read electronic books when screens are light enough to hold, as pleasant to the eye and as 'interactive' as print on paper. Maybe people will have them rread to them by a voice synthesizer. Already now, the ASCII text is driving a Braille interface to allow blind people to read them. But rather than looking at the 'usage' of an individual text it is apparent that 'reading' will take on a different meaning when you imagine a library of 10,000 etexts in the form of a single text corpus available to you at any time. Even though somebody like Borges might be able to store a huge library in his memory and quote from it literally even after going blind, this is not given to most of us mortals. But thanks to the automatized external memory, we have random access to all the stored ideas. We can keyword search, browse with guaranteed serendipity effects, follow through on various threads, all the while creating hyperl inks on the go, leaving tracks inside the Gutenberg horizon that we can follow again next time we touch any of the texts. Every work (say Dante's Divine Comedy (8) appears in its own context, and in any other that we might create. All these operations could be done inside a library, but involving a lot more foot-work. The increased accessibility is already more than a quantitative difference. But what other automatic operations on texts may emerge, most of all what the new faculty of simulation will mean for writing/reading, ie. 'thinking' under conditions of the availability of the virtually complete library (Lyotard) in a dynamic format at the tick of a few keys, will have to be seen. If all these operations have to be done using raw Unix commands or exotic database query languages, the bookish Gutenbergian will likely not feel very at home. Luckily, there have emerged hypertext interfaces that make life a lot easier (World Wide Web under Mosaic). The reader/writer sees a text page complete with graphic design, that can be read like the page of a book. The reader can mouse-click his way around in the labyrinth of the global online library, make annotations, leave 'bookmarks' etc. A special feature is that the 'footnotes' to materials (text, image, sound, and video) outside the present text are active. By clicking on them, the corresponding file is retrieved across the Matrix, and presented immediately. All these are Gutenbergian operations, accelerated to the speed of electricity. Their model is the library.
The Letter. Postal mail is basically a point-to-point network, switched (sorted) at post offices, transported by diverse means of transportation. 'Snail Mail', as it is called in net.land, is usually private, to someone you know, but it can be extended to point-to-many. Mailing •lists exist also in RL for commercial, administrative or grass roots usage, but switching from traffic to transmission is more than a linear change. The Universal Network Medium adds the function of a reflector. A list server (like Majordomus) is an automatic forwarding program that sends every incoming message on to every subscriber, and drops it into his mailbox. Mailing lists can be unmoderated, ie. the information is provided as is, or moderated, ie. preprocessed by a wetware editor agent, which for certain purposes helps to raise the information-to-noise ratio significantly. Whether it holds together a professional special interest group, a hobby club or a speaker's ccorner - the mailing list constitutes a form of public. Electronic fora or bulletin boards - the metaphors reveal the heritage of earlier equivalents in public face-to-face real space. The most impressive are, without doubt, the Usenet newsgroups. Forming another network within the networks, the newsgroups permeate across Usenet, UUCP,Internet and selections also into the commercial networks. One does not subscribe to newsgroups, and the messages are not delivered to one's mailbox. They rather sit on one's local host to be read, browsed, participated in whenever one likes. Mailing lists and newsgroups constitute the basis for a written sense of community. In order to do so, they have to provide some form of space-time coordinates to anchor the social. The placement of a message in one electronic forum creates an unambiguous attribution in index space (an address). Their sequence creates a temporal order, a history of speech acts in which regulars build a sense of group identity. Fora are usually archived, so even though a message was deleted locally you can still look it up. FAQ (Frequently Asked Questions) documents are the collected common sense that is not the lowest common denominator, but common expertise. Like in every form of communal exchange, rules for accepted use and conduct (nettiquette), for the prevention of redundancy, various ideas on how to enforce these rules, etc. are negotiated on the go. Communicational conflicts are solved, of course, also within the interaction, but as a novelty there are technical solutions as well, e.g. the killfile, also sometimes referred to as 'the bozo filter', that locally tunes down or makes invisible unwanted traffic without having to have any censorship at source. Moderated newsgroups and mail- servers, like postal-based news-letters (or the xerox machine-borne mini-komi, as the Japanese call the genre) are already crossing over into publishing. These publications are in the Public Domain, and the moderators are most of the time volunteer editors. Unmoderated newsgroups are a running comment by Everybody attached to Everything. Large events like Tiananmen or the Gulf War, just as small events like a change in the design of Starship Enterprise bring forth their own forums. Strategies have evolved to prevent the slightest idea of censuring an unmoderated newsgroup. Any attempt at dominating or turning it into a PR device will cause a flood of flame - the power of the many.[9] They are specific, global, personal, and very powerful. And again, the total is more than the sum of its parts. MindVox offers many services, among them "a constantly growing library that chronicles the very inceptions of Cyberspace, with timeslices of systems dating all the way back to 1979 - the first bulletin boards ever to exist. "An orgiastic idea for any sociologist, media and market researcher, historian or linguist. The whole problem of sampling that is fundamental to every empirical social science evaporates when you can operate on the complete set. And it comes with a tool box that allows you to do searches, sorts, pattern recognition and other analysis automagically. Fractal algorithms are used to analyze huge amounts of earthquake data out of which it is otherwise very hard to make sense. What collective image might arise if you ran a similar chaotic pattern synthesizer on the subset of, say all utterances on the topic 'Internet' in unmoderated newsgroups, and how it changed over the years? The casual enquiry What's everybody talking about? will receive an unpredictable but mathematically precise answer. The idea of Man, Mina, Everybody, this collective chimera that broadcasting and marketing directors have in mind when they talk about 'the audience' and 'the consumer' - this non-entity will get a voice.
Radio. There were downloadable sound files on the Internet before, but the first regular radio station in cyberspace was pioneered by Carl Malamud in March 1993. You can receive Internet Talk Radio on your desktop or laptop radio either 'live' or 'on tape'. ITR publishes from 30 to 90 minutes of professionally produced radio programming per day. It reaches 100,000 people in 30 countries. TV. Malamud did not choose a TV metaphor, simply because it requires more bandwidth than the majority of the Matrix population has available right now. Also, production of video information still requires an order-of-magnitude larger investment in facilities. As a first step towards general use MIME allows to include little video and sound blips in email. Not yet live broadcast of concerts, but downloadable video clips one finds at mtv.com. Adam Curry, former star host of MTV Networks and net.veteran, created a multimedia site in 1993 that now attracts an average of 35,000 people daily, including music industry professionals. The cable TV on the Internet is the Multicasting Backbone or in short M-Bone. Multicast is a continuous stream of video and audio data packets running over a virtual network layered on top of portions of the physical Internet. It combines a global point-to-point structure with local 'narrowcasting' to everyone who is tuned in. What's on the digital tube? If you belong to the lucky group of power networkers you can watch keynote speeches by John Perry Barlow, co-founder of the EFF, or Vinton G. Cerf, president of the Internet Society, live multicasts from the deep-sea or outer space, footage from NASA satellites and telescopes, eg. Keck, the world's largest pair of binoculars in the solar system. The M-Bone has even emerged a solution to the hotly debated '500 TV channel' problem: 'sd' or session directory is a TV guide where all ongoing events are announced and can be joined on mouse-click. CU-SeeMe, an offshoot of the M-Bone for personal computers, was first developed as a TV metaphor for live video, only later voice was added. Latest news while I am writing this: "Coming Soon - Newscasts on Your PC. Intel and CNN have teamed up to test 'LAN TV' , a system that turns a regular broadcast TV signal into a compressed digital data stream, capable of being received on regular 486-type desktop PCs. While Intel tests the technology, CNN will concentrate on determining what it is people want to watch on their computers, in order to develop a special corporate news service."(10) As with desktop print and radio, desktop TV is not restricted to corporate providers. A CCD camera, a VCR, a video-capture board, and some editing software allow, in principle, TV production and multicasting on every PC. After the telephone answering machine made everyone a radio announcer, the desktop multimedia answering machine will turn everyone or his agent into a celebrity TV announcer. The Hunt. It is here that we see the Dungeon Masters and the Net-Gods at play. The Hunt is a kind of paper-chase, only without the paper. A game to encourage the players to "explore the Net, and traverse little known routes." Huntmaster Rick Gates, Student & Lecturer of the University of Arizona, got the idea "sometime in 1991 when I began to realize the enormous variety and volume of information available via what I will call the Net (Bitnet, Usenet, The Internet, etc.). [...] I suppose my initial ideas were based on the type of search exam that most library-school students have to go through during a class in Basic Reference.' Some of us enjoyed this type of challenge; we called it 'The Thrill of the Hunt". (11) The Hunt is edutainment at its best, "casual instruction in training for information resources. [...] It provides for training in context, which for most people works better than books or chalk on a board." For beginning net.citizens it provides a chance to look over the shoulders of media-literate experts. "It helps more novice users, or Net 'settlers', understand how to move around using the 'trails' that the more experienced Hunt players have 'blazed'. [...] Learning how to learn is critical, and this only comes from experience."
cky part, as always with Oracles, is to formulated the right search command. (12) At the risk of boring you by repeating myself: old media don't go away, they are the content of the new ones, transformed into metaphors. If the 'content' of the Net is magazines, radio, TV etc., then the 'content' of those is the Net. The whole Net is abuzz with questions of where it is heading. Self- reflectiveness is part of the constitution of a new medium. But this is a transient stage. As with Usenet newsgroups we will see that the early bias towards computing and networking itself will shift. Today the comp. groups are far outnumbered by the alt. and rec. groups. There are, of course, differences between the Meta-Medium and the other media it embraces. With text, sound and video editing capabilities on personal computers getting cheaper, one-person desktop publishing and multicasting houses become possible. This was also said when xerox machines spread, and again with laser printers . It did indeed happen to some degree, but it also showed that not everybody has the urge to publish. Most of all, cheap high-quality printing on a laser engine did not solve the problem of distributing and making your product known. This changes with the Universal Medium that is production, transmission and reception medium in one. To multicast does not require the concentration of capital and power necessary to produce a full daily broadcast schedule in one of The Networks. Anybody who finds a friendly host or scrapes up a few thousand dollars to set up her own can be media provider. Combining broadcast tools and communications networks, and private and various forms of public communications, makes all the difference. The implications for the changing nature of work become visible already. Everybody who offers informational products or services can do so - globally, from anywhere, at a price that a private person can afford. This is not to say that capitalism will crumble, and give way to an Anarchist's dream of self-expression for everyone. But it does mean the end of capitalism as we know it. People who are part of what is often called a revolution are very excited about the empowering qualities of the Net.
The oldest medium holds its entrée as the latest, the get-together. The basic social function of going somewhere and hanging out with friends and like-minded people requires a common place and time. While tree-shaped lists, keywords and hyperlinks are appropriate for retrieving data, 'human information' needs conversational tools, an anthropomorphic space not to consult repositories of passive information, but to meet people. Today the theater metaphor (Laurel) re-emerges and with it the idea of actors and agents (Brooks, Maes). It is here that we re-encounter the ars memoria. Cicero suggested to use personae for the memory image that anchors the 'things'. Yates' characterization of a classic memory image: consists of human figures, is active, dramatic, striking, under circumstances that recall the 'whole' thing, can be read directly parallel to Laurel's explications of a desirable human-computer interface. What is not lost in the transition from the art of memory to the art of interface design is most of all the dimension of mental space. The stage where the play is enacted is idea space, regardless whether the mental image is evoked by printed, pictorial or sound signs, or a Wagnerian multimedia Gesamtkunstwerk. An important difference between the two arts isthat the mental space of the ars memoria was not shared. An orator would, of course, share his idea space with his audience, but as he walked around the chambers of his memory, picking the points he wanted to touch upon from the statues where he had deposited them, he was alone. He would never encounter an other there. Net.operations in Gutenberg mode are mostly silent, ASCII, solitary, and asynchronous. Most of the time the netsurfer is not aware of others who 'are inside' the same host. While we have seen that newsgroups can turn into a home on the Net, the potential of the Universal Machine is by no means eexhausted there. The silent and iconoclast world is enlivened by the beginning multimediatization. But hypertext, radio and TV metaphor are precisely that, like horse-less carriage and wire-less radio we now have paper-less libraries and station-less massmedia. They are metaphors for different media, not for the market square in the Global Village (McLuhan). A different approach that does not come from Gutenberg (although it is not illiterate), nor from mathematics, and not from the technical network media, but from game are MUDs. According to one definition they are "detailed and realistic multi-player simulations that present ongoing campaigns and universes with evolving storylines, political systems, and landscapes being imagined into existence as play progresses." MUDs are shared places. You 'telnet' yourself there. Others 'are"there' as well, synchronously, even though from different 'real' time zones. From the theater metaphor we pick up the performing arts and the stage effects. From game/play we get the participatory elements and the challenge for the price at stake: recognition for wit, excellence, style, integrative qualities, for the craziness of thinking up something that nobody has ever done before.
In the Matrix also the physical body has no significance (except for the wetware break). But the mental body can travel along the wires and be re- incarnated in a remote Doppelganger. The same body can be played by a human, just as well as by the machine. On the behavior of human players there is much to be said and studied. Here I would rather take a closer look at the non-human players. Automatons and the game between man and machine carried tremendous fascination ever-since the days of the Ancients, with new boosts during the Renaissance (Maillardet's Magician, Vaucanson's Writer, etc.) and the industrial age. They were only sophisticated toys, but they triggered a philosopher like Descartes to think up a Turing Test avant la lettre. The philosophical and literary (eg. E.T.A. Hoffman) theme continues to fascinate mankind's phantasy. But it was only with Alan Turing and his influence on von Neumann, the cybernetics group, and others that a whole wave of mind-mirroring in Al, neuro nets, piano-playing robots and 'thinking machines' was triggered. This conceptual shift dismissed philosophy and literature, and made the Turing Testable machine the goal of a concrete effort of exact sciences. The first program that passed the Turing Test in a life-like situation was Weizenbaum's Eliza. Since then the Turing horizon has become populated by hosts of talkative and zealous homunculi, women, and daemons. One forum where the best of them come together is the Loebner Turing- Test competition, conducted annually since 1991. The New York business man has donated $100,000 prize money for a program that can pass as human in an unrestricted typed tele-conversation. Entries so far are required to be conversant - in "natural American English" - on one topic only. Entrants may selecttheir own topic areas, but the domains of knowledge must be "within the experience of ordinary people. One of the participants is Julia, a Maas-Neotek bot with an Al engine written by Michael Mauldin at Carnegie Mellon behind it. Between Loebner Turing Tests she logs onto a MUD and behaves like a regular player. She can be summoned, gives useful information, delivers mud.mai I messages to other players who are not currently logged in, dispenses witty quotes, can be nice to you, and kills you when teased too much - and next time you talk to her she will still be angry with you, because she even remembers.
It is envisaged that intelligent robots of the next generation be equipped with various sophistciated capabilities endowing them with desires and intentions, enabling them to perform hypothetical and defeasible reasoning, to solve problems creatively, to appreciate works of art, to achieve some form of cyberpleasure, etc. Understanding and the ability to develop explanations for observations and facts are fundamental for the realization of these capabilities. In fact explanation and understanding are 'two sides of the same coin' in both art and science. Our objective is to highlight techniques used in Artificial Intelligence which could provide mechanisms for modeling the aesthetic response of an intelligent robot, based on the causal explainability of complexity manifested in media such as electronic art. Leyton argues that art is related to explanation, in particular that the aesthetic response is the mind's evaluation of causal explanation. He maintains that the level of aesthetic response to art works is proportional to the level of complexity that an individual observes. He goes further arguing that the desire for art works is part of a general desire that the human mind has for complexity. Barratt also claims that humans seek to explicate complexity, and since the brain is finite, there must be a maximum degree of complexity that the mind is capable of explaining at any one time. If the degree of complexity is increased past this level, it exceeds the mind's capacity to explain it, artistic chaos is reached and consequently the viewer deems the art work to be incoherent. He concludes that the limit is set by the ability to give causal explanation, it is not complexity that is appetitive, but causal explanation itself. Clearly, if our aim is to develop intelligent robots with truly human-like characteristics, then they must be capable of artistic appreciation. For electronic art, appreciation must occur at the conceptual level and not at the physical (pixel) level. In the area of Artificial Intelligence the notion of explanation has been well explored. The complexity of explanations is often a reflection of the richness of the agent's background knowledge, and its ability to discern its surrounding world. Indeed, the aesthetic response to artistic chaos is equivalent to an explanation of a contradiction. Central to such an explanatory capability is the need for mechanisms supporting the modification or revision of knowledge, that is, learning. Belief revision models the process of accepting new information in such a way that an intelligent agent's epistemic state remains logically consistent, or coherent. Frameworks for explanation within the area of Artificial Intelligence can be used to support the aesthetic response of an intelligent agent. In particular, two important parameters of an explanation may assist in gauging an aesthetic response, namely the plausibility and the specificity of the explanation. In summary, if aesthetic response is the evaluation of causal explanation, then we can endow an intelligent robot with aesthetic responses which ebb and flow in accordance with the complexity of the causal explanation achieved.
I have always thought of computers as dynamic tools for introspection, exploration and discovery. Computer programming is instrumental in the externalization of ideas and algorithms are formal descriptions of what one hypothesis constitute the production of creative statements. The computer is a playground to speculate on the generative potential of ideas. As a matter of fact, the physical, tangible management of purely conceptual constructs becomes possible. However, the paradox is that while algorithmic specification allows the artist to touch the essence of his ideas it also creates a distance since all specification is indirect and seems to exclude spontaneous action. The idea is to view computers as partners in the process of creative decision-making. By way of algorithms we can explore various man- machine relations in this partnership: from studying total autonomy in computer programs to systems designed for expl icit interaction. The development of personal algorithms is the key to exploration and the gradual specification of objectives from incomplete knowledge, in sharp contrast to view the computer as slave, as a medium for deterministic visualization. I have characterized the interactive method where man and machine collaborate in a common effort and with common objectives as conceptual navigation; the artist-programmer gets feedback, his expectations are confirmed or contradicted by the program's behavior. Eventually, unexpected results may signal new and promising routes exposing unknown territories. Thus, man and machine contribute both to the creation of a computational climate that favours invention and to the development of a critical attitude towards the often complex relationships between programmed intention and actual result.
Writing algorithms has also forced me to evaluate experience vs. speculation. If one relies on models that have proven to be successful in the past, one 60 confirms what is already known. Algorithms that use rules reflecting this knowledge produce predictable results. Otherwise, designing processeswith the greatest possible freedom in pure speculation is like working outside of any known context making evaluation very hard indeed. The creation of new contexts for growing algorithmic activity mixing memories of the past and an open imagination is, I think, perhaps the most interesting challenge to algorithmic art.
Almost as if by magic - whatever procedure you dream of - you can probably extend the power of your dream to the computer and let it develop the dream beyond your wildest expectations. You may identify procedures for improvising with color, scale, and position - which is what artists have always done. Given sufficient definition you could develop a form generator and from your new vantage point see new possibilities for further elaboration on your routine. Through trial and error - interacting with the algorithm itself you proceed further into the new frontier. So what can we learn from this? We learn what artists have always known - that "CAD" programs, paint brush programs, paint brushes and drawing paraphernalia do not make art. Neither do artists or designers simply "make art". The one over-riding essential element to the process, "a developed artistic procedure", is necessarily unique for each artist and for each work of art. The procedure addresses a singular conjunction of elements for which there is no "universal" rule. The "calculus of form" may be placed in the service of such procedures but should not be confused with the art-making procedure. For the artist who writes the code the artistic procedure is the act of "writing the code", pretty much like the creative work of the composer when the composer writes a musical score. Making art does indeed require a "calculus of form". But the artist's instructions on how to employ the "calculus of form" precede the "calculus". One needs an "artistic procedure" which addresses the entire complex of elements for each specific work. The final form, unique and specific to each work, embraces more than the "calculus". While it embraces and grows from a "calculus" it might employ any of an infinite number of approaches to deliver the form. These may include metaphor, improvisations of the form phenomenon in and of itself, or reference to some other phenomenon or idea - historical, literary, political, mathematical or philosophic. Can an artist write an algorithm then for an artistic procedure? Emphatically yes! Such algorithms provide the artist with self-organizing form generators which manifest his or her own artistic concerns and interests. We are looking to an interesting time ahead of us when artists will be able to exchange and interact with each other's form-generating tools in ways we never dreamed. There are procedures yet to be developed to make this kind of interactive expression accessible - a time ahead when we will literally see an evolution of form including a genealogy associated with its creators.

Despite, or perhaps because of, a healthy skepticism, Artificial Intelligence (Al) has been making quiet progress in electronic arts. Artificial Intelligence has inspired traditional fields of electronic arts as well as it has developed new horizons for many artists working in electronic composition environments. Building on the success and shortcomings of previous experiences with computers in arts, the attempt to extend the paradigm of artificial intelligence systemsto the domain of electronic sonic arts is made now. Musicians are increasingly using intelligent machines to deal with tasks for which they are better equipped than humans. Computers are increasingly being used to address the brain-numbing complexity of modern electronic music products and processes, thereby allowing people to concentrate on their music and ideas. Expert systems, for example, help people by searching a book of rules to decide what to do in a particular situation; as machines do not forget, these systems can manage rules more consistently than people. Some musicians are using neural nets, which can recognize complex patterns, to apply precedents that are difficult to express in numbers or words. The real challenge facing technology is to recognize the uniqueness of machine intelligence and learn to work with it. Given enough memory, a computer can remember everything that ever happened to it or to anyone else. Furthermore, when faced with a logical problem or a theoretical model of how compositions or sounds should be, computers can deduce more results more quicklythan humans. Their complementary strengths should allow man and computer to work together and do things that neither can do separately.
Whereas the impossibility of physical death in cyberspace is one of its main attractions (certainly for the flight simulators used by the military), this absence of death and of death's possibility does not emasculate the project. For death becomes the ultimate ground for the cybernaught, not in terms of individual death, nor even death of the planet, but according to Lyotard, in the death of the solar system. On a number of occasions Lyotard mentions the inevitalbe the destruction of the solar system estimated to occur in 4.5 billion solar years. The task of technology, is to create an alternate non organic system that will survive this catastrophe. Not only does the certainty of this event constitute perhaps the most sublime of deaths, but the end of the solar system represents a finality, a resolution, that puts ultimate limits on human endeavour. Such closure however, comes at the end of a narrative space in which all the utopian and apocalyptic concerns that have defined twentieth century culture's relationship to technology, are able to play out their fictions. As a way of representing the body in space, according to a perspective that the logocentric apparatus has inherited form the renaissance, futurity is also associated with frontality, and opposed to anteriority. As a radiant, or irradiated subject, the cybernaught may transmit from a centre in all directions, nonetheless s/he is literally always looking in front. In front - to the absence of distance between the organic eye and the simulated scene, to the absence of difference between the real and the repesentation, to the unfolding in sequence of the virtual narrative, and to the future as a narrative of progress. This future space thus stands in for all the physical spaces which go missing in virtual worlds, and this future death defers the resolution of corporeality and the promise of transcendence that individual death promises. More than this, the future impossibility of organic embodiment provides the ultimate rationale for the numerical constitution, Cartesian co-ordination, and digital storage of the subject, who then shines with the necessity of survival. This is the radiant subject of art - the channel to the sublime, now irradiated. The subject who shares with radiant sound, the security of identity with the eventfullness, change and flux of the event. As Baudrillard says, we no longer need the VR glove or suit because 'we have swallowed our microphones' and 'internalized our aesthetic image.' 28 We have become the post holocaust meaning of radiant sound - transmissive but rotten at the core. And the realization of this subjectivity occurs, not at the point of solar explosion as radiance would suggest, but at the point of total computation. At this point, the signal continues to survive in outer space; the space of the future, but sound, and any vibrational body, is immediately extinguished by silence.
My first visit to virtual reality— a cartoon-like 'Virtual Seattle' at VPL Labs in California a number of years ago—indicated that for me at least, the great attraction was not the lure of computer technology or of interface devices, which included a cumbersome helmet ('eyephones') which put little video monitors over my eyes; and, the coarsely rendered, neon-colored artificial world, in which I had the illusion of being immersed was not a convincing imitation of the physical Seattle, or for that matter, any other landscape which could possibly have drawn me in. The allure of this cyberscape was the impression that it was responsive to me, as if my gaze itself was creating (or performing) this world and that I was to some extent enunciating it according to my own desire. My most abiding memory was of exhilarating ability to fly through the artificial world at great speed simply by cocking my hand like a gun—'navigation' is a poor term for this experience. Best of all, I had a sense of the weightlessness and super-power that I had imagined in childhood and had read about in myths and comic books, but had never before experienced, not even in my dreams. (My childhood friends in first and second grade and I tried fruitlessly to fly day after day by flapping blankets while jumping off walls and out of trees.) It is this feeling of transcendence of the mortal body and the gravity of earth that for me is a key to the desire and media attention which has been focused on 'cyberspace' and the subculture which has grown up around it.
The primordial virtual space is an utterly empty display, unlike the physical world, which is always 'full' and readymade. So far at least, cyberspace worlds are sparsely stocked with metaphors, now largely constituted from scratch with considerable graphic effort. Once these graphics are out of sight, it is easy to get lost in a void that is uniformly colored (usually black) and that wears infinity at its edges if not at a vanishing point. My first flight revealed Virtual Seattle, like most other virtual environments, to be relatively void but for a crude symbolscape of geometric objects. I remember my panic at flying through and out the swimming-pool-like image-space of Puget Sound and getting lost in utter emptiness. (I have also flown too far from the landingstrip metaphor of a Wall Street stock market program and have fallen off the checker-board world of 'Dactyl Nightmare.' The stock market program has an arrow function which points the way back to civilization.) What a comfort it was to find the traces of the human imagination in the spacescape near me again. On the other hand, why are these cyber-traces, the externalized imagination of electronic producers, filled with so little of our cultural legacy? I am thinking of metaphorically and graphically impoverished architectural flythroughs or crude male-centered fantasies of pornotopia ('Virtual Valerie') or a pseudo-prehistoric past wherein the only activity is relentless killing, (for instance, the aforementioned 'Dactyl Nightmare.') One task of art that commodity culture apparently eschews is to resituate the disengaged space of virtuality into a socio-historical context. For instance, Jeffrey Shaw's interactive city installations, such as virtual New York or Amsterdam, are richly symbolic, suggesting how the built environment may be refigured in a image-space as a kind of alphabet. Multiple and interlacing historical narratives are traced in a kind of writing motion over the display area via a bicycle interface. The Biblical reference in Shaw's piece at ars electronics 94, The Golden Calf, made what was otherwise a clever piece—a statue visible only through a mirror-like electronic display—into a commentary on electronic art itself. The uncanny and more sinister implications of my first flight occured to me later: A virtual space it is not just the ground or background or the landscape at which I look, or even that my look calls forth—that space looks at me, following my every move. Indeed, space constituted itself in response to various indices of my intention, for instance, the vectors of my gaze and the motion of my body or head. That is, in a virtual world, not just objects but space itself is interactive. As a consequence the virtual environment that surrounds the visitor itself can appear to be something 'live' or animate, 'that we cannot acknowledge as subject or persona in the traditional european sense, and which nonetheless constantly demonstrates that it sees us without revealing itself.
Of the many artistic responses to the Gulf War, I remember Frances Dyson's and Doug Kahn's sound and sculptural installation for its condensation of the sounds and images of birds in flight with the resonances of the air-war on Iraq. A more recent installation by Laura Kurgan explores the actual operation of several satellites in Global Positioning System or GPS by using them to trace the position of the New Museum gallery in New York. The installation was effectively demystifying, not only in revealing how this surveillance system works, but its material fallibility resulted in wavy deviations from geometric accuracy. Julia Scher has explored the psychical and cultural implications of electronic and computer surveillance in work spanning over a decade, including her 1993 installation, Predictive Engineering, at the San Francisco Museum of Modern Art, mixing live and recorded video on a two chiastically arranged and elegantly situated surveillance camera and monitor set-ups. The interest of art then may not be in the seamless operation of electronic culture nor in the production of realistic virtual worlds—like Icarus, that may be flying so low as to be dragged into the sea. The often mentioned desire for photographic resolution in virtual displays may also have as much to do with the goal of controlling physical objects and events as it does with aesthetics. An art of virtual spaces which simply aims toward realism of fit or of appearance with a physical landscape may then risk merely serving the instrumental or hegemonic purposes of military and business interests in an information society. On the other hand, art that surrenders to the allure of the mysterious or that seems to offer transcendence may find the wax that holds its feathers together melted by the sun. Exploiting the magical aura of virtual spaces risks satisfying the commodity and entertainment functions of information and nothing more. For, unlike prior illusion- producing modes, cyberspace is a means of enchanting not only liminal realms, but everyday reality. Even though it is has been discredited as a popular rather than scientific term, 'cyberspace' is appropriately built on the analogy of Norbert Wiener's cybernetics, or the study of feedback systems. In computers, feedback is elaborated into a programmed responsiveness which Sherry Turkle has noted, can captivate the user as a kind of 'second self'.2 But feedback is not restricted to the space of the monitor, for material artifacts and even a physical space itself can be 'cyberized,' or granted agency by programming it to simulate some form of human interaction, in the process ultimately lending it qualities associated with human personality. As Jay David Bolter, explains in Writing Space: The Computer, Hypertext and the History of Writing,' 'Artificial intelligence leads almost inexorably to a kind of animism, in which every technological device (computers, telephones, wristwatches, automobiles, washing machines) writes and in which everything that reads and writes also has a mind.' One futuristic vision of the personified or smart home proclaims, 'Once your house can talk to you, you may never feel alone again,14 suggesting this animism and a quasi-subjecthood can extend to even physical space, once it has been 'cyberized.' A utopia of ubiquitous computing would enchant the entire world, distributing magical powers to the most mundane aspects of existence.
Of course, business interests are far more concerned with 'information' as a resource and an exchange value, than with virtual environments, even 'smart houses' per se. 'Information' is knowledge decontextualized and stored as data (that is, as virtual objects.) In order to be retrieved and placed in a new context, that data must take on symbolic or metaphoric form in an interactive and to some degree immersive display. The value of information is realized not just in any one state, but as a passage from the conceptual to the virtual to the material and back again, crossing through a variety of reality statuses. For instance, virtual money or credit demands a passage through material objects in order to increase itself as interest. Jeff Schulz, for instance, has made the credit system the material of his performance art and of commentary in his essay, 'Virtu-Real Space: Information Technologies and the Politics of Consciousness:15 If virtual environments are best understood in connection with other social and cultural processes, as one stage in the unfolding of metaphors across a variety of reality-statuses and degrees of materiality, this suggests that the electronic arts are themselves part of a range or spectrum of interactive and immersive media and are not well-served by isolating them from art using other media, that is also concerned with the transformation of information societies into electronic culture. Artists from the ex-Eastern block or what was once the Third World are likely to suffer the consequences of this global change, even if they are excluded from its benefits. That is, there are artistic issues and perspectives which have a bearing on the global economic and cultural transformation we are undergoing that may be posed by those who have little access to computers or even to electricity —they must be welcomed into the discourse as full partners. Artists and cultural activists—for instance, Paper Tiger/Deep Dish and Ponton—have also not forgotten the issue of public access to the material and technical level where information is processed, stored and transmitted. It is real estate in terms of data space on computer disks and in main-frames, personal space in seats in front of computer work- stations, frequencies on the broadcast spectrum, satellite space off which to bounce signals, and room in the bandwidth of fiberoptic cables that global corporations struggle among themselves to own and control. The scarcity and costliness of these material gates of entry limit the number and types of subjects we can find in the virtual gathering spaces of an electronic culture. What we to some extent have and need more of is art which figures relationships between the virtual and the physical world, which demystifies the relation of the body to the virtual environment, and which is both a meta-commentary and an aesthetic statement. On the other hand, the technological ability to recreate the acoustic space of a medieval cathedral in one's living room, or to merge movie stars and tourists into the same image and have them interact, merely exploits the ability to superimpose the virtual over physical space: it is entertainment. The following section concludes by making some generalizations about virtual environments as virtual space, based in reflections on experience in cyberspace, from virtual realities to CD Rom work-stations to electronic networks.
The very idea of space becomes self-contradictory, when it is applied to virtual realms, especially the maze-like vectors and links which compose the paradoxical 'space' of networks. Virtual space is not so much space as 'nonspace,' for it need not occupy ground, nor be a continuous linear extension, area or void, nor even constitute the interval between things; and, unlike the material Lebensraum of earth, it not be perceived as limited or scarce. If the virtual space in question is the discontinuous, yet communal space of isolated computer network users, it can expand ad infinitum, like the text-based 'rooms' which make up a M.U.D. or multi-user dimension. But where is that noplace in which, for instance, two people talking by telephone meet? Where is the room and where is the display in which the hundreds who belong to the same M.U.D. (Multi-User Dungeon or Dimension) or M.O.O. (M.U.D., Object- Oriented) may gather? The reality-status of any one virtual environment is also unclear, seemingly in-between an exteriorized mental space, the apparatus of the image-display and the material world. The many different levels and degrees of virtuality in an information society add complexity to mystery. What, for instance, is the 'space' of a virtual object in a computer program? Even if it can be quantified as data in megabytes or ultimately in bandwidth or pixels, a virtual object itself remains an imperceptible potentiality, which occupies no space at all until it is accessed and displayed. Can one even say the object is 'inside' the opaque casing of the computer or hidden under the obscure machine language of programming?' Even if one could break into the black box or extract and analyze the program, one wouldnUt expose the virtual object, only the mechanism that has the potential to produce it. Yet, the virtual space on display is still a realm of cause-and-effect, though the consequences of any one action may seem more magical than logical, for they need not be proportionate to the results to which we are accustomed in physical space. Space is ordinarily conceived of as continuous or at least, at its most abstract, as a homogenous void. Yet, virtual non-space or cyberspace can be distributed discontinuously over physical space (in a way that is usually imagined as supported by ubiquitous computing.) Furthermore, physical separation between the users and objects of physical space need has little bearing on the seams which separate and link virtual spaces. What remains somewhat clumsy are the figural conventions which ease the passages between virtual 'worlds': the vortex, the window and the door are given too much work to do as metaphorical thresholds and passageways. The additive aesthetic principle of the Internet, the global network of networks, is an extremely elegant, non-hierarchical, rhizomatic global web of relatively independent yet connecting nodes. Though it was conceived out of militaristic considerations, it might be compared with Panofsky's analysis of the gothic cathedral. This comparison is not trivial, for combined as an infrastructural and virtual entity, the Internet is among the greatest architecture the world has every known, far greater than the material reference point of the information highway metaphor, the freeway system.
Such compression of space and time finds an exponent in Jeffrey ShawUs interactive installation, Revolution. The user's effort turns a grindstone interface, which churns out pictorial representations of hundreds of social revolutions in the historical record onto a video monitor. Revolution is then not a representational space of linear histories or of geographical areas but the presentational space of a metaphor and its recurring metahistorical patterns. The visitor to the installation stands for the protagonist and motive force of this social phenomenon, a spontaneously acting group called at times the 'mass,' the 'crowd' or the 'people.' Then the vocation of an art of the kind that reflects on electronic crowds and networks is not the representation of the visible world, but the visualization of what is otherwise inaccessible to perception and is difficult to imagine because of its scale, its discontinuity in space and or time or its impenetrability—from the insides of the body, the atom, or the black box to the outside of our galaxy and our universe. All the linking devices which create virtual spaces of greater and greater, albeit ephemeral unities—text-based networks, MUDUs and MOOUs, telecommunication satellite links and cables, but also protocyberspace like the nets which unify physical space—railroads and highways are understood, paradoxically enough as 'spaces.' Such virtual environments of discontinuous and overlapping jurisdictions would tax any political imagination capable of ethnic cleansing or of resolving ethnic conflict by dedicating bounded areas to one homogeneous culture. If virtual space were our model of political space, there would be no struggle for nationhood as a geographical entity. What would remain a nagging material problem is opening the gateway of induction into the virtual realm wide. The concept of 'space' applied to computer- and other machine- generated virtual realms is a metaphor that invokes something quite different than the fundamental experience of being in the space of the physical world in a body rooted to the ground by gravity, in view of a horizon. Cyberspace is heterogeneous and dispersed, it can be experienced in various degrees of person and immersion and in different symbolic modes as a virtually embodied metaphor where the flesh (or meat body) can't go, but into which disengaged spectral bodies and multiple personas be inducted, fly and interact, alone in an electronic crowd. The scene itself can move and is responsive to the user in ways which promote performative and/or magical experiences, loosely covered in scientific and socio-economic alibis. That is, electronically produced liminal realms and induced experiences are only superficially about technology, they are about transcendence (even when in degraded forms of sex, shopping, high-speed driving, mortal combat, etc.) Some of the organizing metaphors of cyberspace (frontier, highway, spaceflight, cave, net, theater, game, etc.) are propositions which should be scrutinized carefully as to the way they define the control, access, reality status and experiences assigned to the virtual and symbolic realm which is increasingly our everyday world.
Post-biological technologies enable us to become directly involved, body and mind, in our own transformation, and they are bringing about a qualitative change in our being. The emergent faculty of cyberception, our artificially enhanced interactions of perception and cognition, involves the transpersonal technology of global networks and cybermedia. We are learning to see afresh the processes of emergence in nature, the planetary media-flow, the invisible forces and fields of our many realities, while at the same time re-thinking possibilities for the architecture of new worlds. Cyberception not only implies a new body and a new consciousness but a redefinition of how we might live together in the interspace between the virtual and the real, calling for a wholly new social environment and a reconsideration of every aspect of our ways of being. Western architecture shows too much concern with surface and structures - an arrogant "edificiality" - and is too little aware of the human need for transformative systems. There is no biology of building. A city should offer its citizens the opportunity to participate in the process of cultural emergence. Its infrastructure, like its buildings, must be both intelligent and publicly intelligible, comprising systems that anticipate and react to our individual desires and needs as much as we interact with them. A "grow bag" culture is required in which seeding replaces designing, and where architecture finds its guiding metaphors in microsystems and horticulture rather than in monumentality and warfare. Currently, architecture has no response to the realities of cyborg living, or the distributed self, or to the ecology of digital interfaces and network nodes. It has produced a shopping cart world of pre-packed products wheeled around the sterile post-modernity of a mall culture. Buildings, like cities, should grow. As products of creative cyberception, they must become the matrix of new forms of consciousness and of the rhythms and realisations of post-biological life.

Techno music is an aggressive, technology and future oriented genre of youth culture and popular music. The historical background of this musical form lies in the avantgarde groups of 60's and 70's; especially Fluxus and Kraftwerk. From a philosophical point of view, techno can also be seen as a continuation to the modernist avantgarde movements such as futurism, surrealism and dadaism of the early 20th century. Techno music is especially popular in Europe. What used to be pure underground five years ago has become evidently mainstream. The recent commercial success of artists like Sven Vaeth, Westbam, LFO, Orbital, The Orb and Aphex Twin has proved techno to be a fast growing youth movement. Pop journalists and music experts have claimed techno to be "rock of the 90's". Concerning this, its is not surprising that the massive party concepts of Mayday and Love Parade have been called "Woodstocks of the 90's". In my paper I will introduce and analyse the latest developments of techno music and aesthetics. During the recent years techno has divided into several sub-genres such as ambient, trance, hardcore and gabber. A clear turning point can be seen. At its current status quo, techno seems to be a cultural phenomenon with a fascinating mixture of experimental avantgarde music and transnational pop culture. Techno music has been said to be "a soundtrack of the information age". Juergen Laarmann, the editorin-chief of German Frontpage techno magazine, has also written that techno music is only a small part of a broader concept of techno culture. In this case, we have to ask what is techno culture? In Laarmann's opinion all the computer based technologies from computer networks to video games and hypermedia programs represent techno culture. Concerning this point of view, it is interesting to bring up a citation from Bill Nichols' remarkable article "The Work of Culture in the Age of Cybernetic Systems": The Computer is more than an object; it is also an icon and a metaphor that suggests new ways of thinking about ourselves and our environment, new ways of constructing images of what it means to be human and to live in a humanoid world. Cybernetic systems include an entire array of machines and apparatuses that exhibit computational power. Such systems contain a dynamic, even if limited, quotient of intelligence. Telephone networks, communication satellites, radar systems, programmable laser videodiscs, robots, biogenetically engineered cells, rocket guidance systems, videotex networks - all exhibit a capacity to process information and execute actions. They are all "cybernetic" in that they are self-regulating mechanisms or systems within predefined limits and in relation to predefines tasks. Just as the camera has come to symbolise the entirety of the photographic and cinematic processes, the computer has come to symbolise the entire spectrum of networks, systems and devices that exemplify cybernetic of "a utomated but intelligent" behaviour.
Electronic artists rely on technologies developed by disciplines which did not exist just a few decades ago: computer graphics, image processing, computer vision, human-computer interface design, virtual reality and so on. The paper traces the history of these currently prominent image disciplines. My analysis begins in the 1920s when avant-garde artists, inspired by modern engineering, tried to systematically apply its principles to visual communication. To engineer vision meant to be able to affect the viewer with engineering precision, predictability, and effectiveness. Thus, Dziga Vertov championed montage as the most economical kind of communication while Sergei' Eisenstein searched for units to measure communication's efficiency. In its desire to engineer vision, the avant-garde was ahead of its time. The systematic engineering of vision took place only after World War II with the shift to post-industrial society. In post-industrial society, the mental labor of information processing is more important than manual labor. In contrast to a manual worker of the industrial age an operator in a humanmachine system is primarily engaged in the observation of displays which present information in real time about the changing status of a system or an environment, real or virtual: a radar screen tracking a surrounding space; a computer screen updating the prices of stocks; a video screen of a computer game presenting an imaginary battlefield, etc. In short, vision becomes the major instrument of labor, the most productive organ of a worker in a human-machine system. The research into human- machine interfaces — from first computer graphics displays of the late 1940s to today's VR — can be seen as attempts to make the use of vision in this new role as efficient as possible. The importance of information processing for post-industrial society also leads to the necessity to automate as much of it as possible. The ultimate aim is the complete replacement of human cognitive functions by a computer, including the substitution of human vision by computer vision. This is the second trajectory of image research in post-industrial society; from pattern recognition systems of the 1950s to today's computer vision systems. In summary, most of the new research into imaging and vision after World War II can be understood as following two directions: on the one hand, making human vision in its new role of human-machine interface as efficient and as productive as possible; on the other hand, transferring vision from a human to a computer. Why should this historical analysis be of concern to electronic artists? The notion that the artist functions outside of society, history, and industry is a modernist myth. Modernist artists were not only the pioneers of the utilitarian aesthetics of modern industrial design and the techniques of modern advertisement and political propaganda, but they have also pioneered the post-modern engineering of vision, the integration of human and machine in human-machine systems, and the replacement of human vision by computer vision. Today, computer graphics industry is one of the sites of this engineering. Whether computer artists acknowledge or ignore their relationship to this industry, it exists. Acknowledging rather than ignoring this relationship is the first step toward a critical computer art practice.
Art and Technology as the New Avant-garde Machine Vertebral Animal The status and understanding of technology in the computer epoch is very different from 'optic-engine based technology'. As opposed to the human body, an engine is a different body/construction. The human body was understood as a unit, as an undebatable organism. Computer technology can't be separated in reflective categories from the subject and, in a way, from the body. A machine now is not a structure that is alienated from the subject. (Structures don't go on the streets! – slogan of 1968). Technology is intermingled with intimate human life as a part of the 'molecular structure'. Technology seems to be saturated with desire, seduction, 'automata of the body'. It is supposed to be combined with desire with functions of the body and the filters of perception. Assemblage of Representations, Body When making a comparison between the body and the subject we work on the side of the subject. Making analyzable the bodily practices and the unconscious, we are continuing to dissociate the body, from one side and to incorporate it into forms of representations from the other side. The only territory of the body is the terrain of transgression, affect, death, sex. The body is incorporated into language and viewed through a multitude of practices. The practices could be understood as an assemblage of verbal and visual possibilities taken from past and present (marginal and dominant) culture. The Art of the Disembodied Subject a) dislocation. A virtual portrait (in VR games, for example) could include the following: mind, age , character, temperament, style, design, sex. Everything that was articulated, analyzed inside the subject could be terminated and artificially used. A subject is a landscape, open for a multitude of subjects, that can be recombined or segmented for different needs and functions. A subject can't live beyond the cultural media: literature, film, TV. It needs to be disembodied, moved to interfere with other life forms and to be dislocated from the automatic 'natural' body. It has a multitude of images and a freedom of recombining and choosing itself. b) Segmentation and interactivity. Interactivity is different from communication and information. An interactive technology needs a special subject and atactics, that avoids stable codes and emphasizes the process of collaborative acting. The paper will be illustrated with conceptual and video installations by Russian artists and by experts from experimental TV in Russia and the Piazza Virtuale in Kassel.
In his report on the study of pictorial perception among African subjects, William Hudson (1967) says that we take it very much for granted that methods which are only moderately successful in our own cultures will prove equally, if not highly, successful in an alien culture: "We fall into the error of thinking of the black man's mind as a tabula rasa, which we have only to fill with the benefits of our own cultural experience in order to promote whatever objectives we may have in mind. We forget or ignore the fact that the black man possesses his own indigenous culture." During recent years, many artists have addressed the issue of cultural diversity as part of their discussions on Electronic Art. Although the vast majority of artists claim the need for a transcultural approach, most of them have taken a superficial look at this complex problem, turning attention away from some of its more crucial points. Their discourse focuses on the possibilities for providing artistic bridges across different cultures, while their attitudes and works reflect, in many cases, a typical ethnocentric view. The discussion aims at promoting a debate on transcultural issues, as one of the major challenges electronic artists face today. In a world of social, cultural and economic disparities, how can technology meet basic human needs in both developed and developing countries? Which are the dominant cultural values that underlie computer-related technologies today? What is the impact of new electronic technologies on Third World nations? How can we minimize technological dependence and cultural domination, when 30 developed countries – with less than 30% of the world's population – account for approximately 95% of the world's scientific and technological production?
Some artists using electronics take for granted that the art that will be significantly new is going to emerge through new technology. If they look at a painting, they see a medium that doesn't do very much except sit on the wall. Old medium, old ideas. The new media involve intelligent and ambitious systems, radical shifts in our thinking. So it's natural to expect radical and impressive art, too. Working as a painter who also uses computers, I am more sceptical. The art of painting is built on asking questions about what you see, and the process has the feel of a stumbling search. Obsolete? During the sixties and seventies we had exhibitions with "beyond painting" in the title. Kinetic, Op, Minimal, Conceptual, all mixed make-believe and pseudo-science to suggest a future where only "de-materialized" art would be possible. In fact what evaporated wasn't the "art object" but the credibility of this way of thinking, discredited and soon forgotten because the work with real punch and ambition proved to come from painting. As well as finding another country for art – albeit a virtual country – the visual creativity of computing can function just as well within traditional media. The given technology of a painting – flat surface, nothing moving, no sound, no buttons or head-set, not even a plug required – is unimpressive, but it can whirl into life through the touch of colour, the dance of line, the stare of a face. At the Minneapolis conference last year the neighbouring museum held a small exhibition of Matisse's graphic work, its vitality and simplicity a reminder of how far the computer graphic exhibits (mine included) fell short. The technophobia of the mainstream art world is the routine excuse for the failures of computer works to be as impressive as they should be. But on exciting, sophisticated technology is just the starting point. Picasso on an Apple II might still be interesting. Whatever else is possible, a fusion of computer techniques and painterly sensibility shouldn't be discounted too hostily. If there are frontiers in art they certainly aren't where you expect them to be.
Remember Vincent Van Gogh's Painter on His Way to Work, carrying it all on his back? That's where art education is heading. I don't mean the canvas and easel. I mean carrying it all on your back, in the clothes that you wear and in the headband in your hair. 50% pure natural wool 50% optical fibre. I am talking about the interface moving onto and, eventually, into the body. That's your electronic media artist on her way to school. She's wearing the university on her sleeve. We're not talking about a few curriculum changes here. We're not talking about the gradual replacement of some of the library stacks with a few computers. We are talking about the total dissolution, disintegration, and dispersal of Higher Education. From real estate to cyber estate. The university is becoming the interversity. Ask the students. Hundreds of thousands use the Internet daily. When Larry Smart first issued NCSA Mosaic, the network interface to hypermedia browsing, there were ten thousand users in the first three weeks. Now there are over two million. Students are half in school and half in cyberspace. They live between the virtual and the real. They are in the Net more often than out of it. This is the advent of Inter Reality, the space we are most likely to inhabit for the next many years. The ethics of the net, its integrity and inclusiveness, are creating a social behaviour, a morality, which will bring huge bonuses to the real world. I am with Esther Dyson of the Electronic Frontier Foundation when she says that organised political parties won't be needed if open networks "enable people to organise ad hoc, rather than get stuck in some rigid group". The end is to reverse-engineer government, to hack Politics down to its component parts and fix it. She echoes the words of Hazel Henderson writing twenty years before her: "Networks are a combination of invisible college and a modern version of the Committees of Correspondence that our revolutionary forefathers used as vehicles for political change". This post-political process also involves the student in learning to browse, to graze, to hunt for ideas, projects, data, as well as intellectual and artistic collaboration and friendship in all kinds of electronic places, virtual libraries, telecommon rooms and cybercolleges. The students' time in telepresence and virtual learning mode is increasing rapidly. Have you noticed in the studios, libraries and computer suites how every terminal, every interface is occupied, all the time. There are 50 billion adults in the world seeking education in one form or another. That form will be on-line. CD ROM is migrating to big disks at a server near you. The future of education lies in the function of integrated multimedia telecommunication services. But that future could be solely in the hands of big business who simply see "content" as the "value-added" they've got to include to get "market share". I foresee a completely crazy take- over of education by these commercial telecommunications industries unless we can provide models of on-line collaborative creativity and learning whose originality, effectiveness and appeal outshine the more cynical manipulations of the market. We cannot hope to do that in isolation, in our separate colleges, just meeting occasionally, even at conferences as dynamic as this. I want to invoke the sense of a group in which each member has more or less equal power and authority in both access to knowledge and in the means of its reconfiguration and distribution; a group concerned with art and the advancement of learning through collaborative inquiry and shared experience. I want to propose the creation of a Planetary Collegium: non hierarchical , non-linear, and intrinsically interactive; a gathering together, a connecting, an integration of people and ideas. Combining cognition and connectivity, what better creative learning organism could serve our unfolding telematic culture. But by definition such an organism cannot be planned and implemented top down. In fact it is already emerging, bottom-up from the infinity of interactions within the net.
What will be the consequences for art and for education when the digital image is no longer box-dependant? When we no longer have to sit up and beg for information with a typewriter keyboard and TV set? When whole walls of building inside and out can be digitally flooded with sound, colour and light, images and texts flowing in endless transformations, when whole environments respond to our body movements and the articulations of our voices?. When the printed page no longer regiments our thinking into orderly rows of linear data? If the poets, artists and musicians of the world are not ready with strategies to effect this environmental and ecological digitalisation, the politicians, merchants and entrepreneurs will. In this context, Art schools have a clear necessity to put up or shut down. But college is a place for social experiment as much as artistic and intellectual growth. Nothing is more human, warm and convivial than a bunch of kids hanging out on the Internet. As networked virtual reality transports our telepresence, and gives us the tools to reconfigure our own identities, social life will become not only more complex but more imaginative, the scenarios of conviviality outstripping no doubt those of the most fecund scriptwriters of the old movie era. I am happy to admit to possessing a butterfly mind. I'm constantly on the move, physically and virtually, between nodes, between people, between data, between cities, between images, between channels, between texts. I have a psychic restlessness called connectivity. I blame the technology! But then, everyone blames the technology whilst everybody knows that technology has imposed nothing upon us that we did not first desire. Technology arises from our longing to be out-of-body, to see beneath the surface of things and events, to break the bounds of authorised perception, to exceed language, to transform the material world, to recreate ourselves. Technology represents a further embodiment of mind. Minds of course can be vacuous, coldly calculating and analytical, mean and narrow. The same is the case for technology. Minds also can be open, inclusive, loving, spiritual, transcendent. The hope for a cognitive technology lies in this , indeed the hope for a truly human electronic, multimedia and interactive art precisely lies here. The context of this embodiment, the ecology of mind, is at the root of all our considerations about art in the era of interactivity and transformation. With the bionic revelation of our cyborg nature now well rehearsed and understood, it is clear that our art is post-biological also. Again, the educational provision for the development of this post-biological culture must form the overarching agenda of a planetary collegium.
For me, Gelernter can be usefully triangulated with Varela and Stafford because of his project to build a spiritual computer. An emotional computer. Gelernter has valuable things to say about computers and creative thought. "A computer that can't feel can't think". His new book speaks our language. And from his vantage point at Yale, he "rejects the traditional academic subject divisions", and feels "especially at home in the no man's land between art and science". Professor Stafford must feel the same, I would guess, navigating between aesthetics and medicine to chart the emergent revolution in seeing and imaging. Equally, Francisco Varela is not fettered by academic boundaries and roams over an extensive cultural terrain, combining neuroscience with Buddhist theory, and speaking to issues in art as much as society. From these three vantage points, a map of consciousness and the reality we actively construct can be defined. Such vantage points and the new perspectives they cast upon our understanding of ourselves, must become endemic of the learning landscape our Collegium will provide. Not only are students redefining who they are and what they may become, but we too must redefine our identity as teachers, collaborators and guides relative to them. Similarly our tools are changing. While the printed book will continue to be employed, the question becomes how and for what purpose, since it is clear that hypermedia is in many areas set to replace it. The book has come to be the embodiment of authority and its obsolescence as a primary academic tool will cause considerable problems in the academic world. The book is a medium which is fixed and frozen while interactive media are fluid. Post-modernism with its relativist doctrine of layered realities and the slippage of codes has prepared us for the shifting uncertainties of authority, indeed of authorship and ownership of ideas whatever they might constitute, either in science or in art. But the scripting, negotiation and critical evaluation of a hypertext present demands for revolutionary pedagogical change . It's not simply that many colleges are haunted by the ghosts of culture past but that apparitions of the future are emerging on every screen, from every disk, in every network. These apparitions are the constructions of distributed mind, the coming-into-being of new forms of human presence, half real, half virtual, new forms of social relationships, realised in telepresence set in cyberspace. They are challenging the old discarded forms of representation and hermeneutics which still haunt the lecture halls. The students are beginning to treat the university as an interface to Inter Reality as a doorway to a radical constructivism, the way into building their own world. What could be more hopeful than a world designed by the young tested against the on-line wisdom of a global community. This is education in its hyper-Socratic form. There will be no easy transition from the past stability of tradition to the dynamic uncertainty of the immediate future. New priorities must be set in the fiscal affairs of universities. In academic networking and on-line research, change is imminent and difficult times are ahead. "As the Internet expands something will have to give: either the government will stop paying, or politicians will notice that the government is paying and will impose controls, like those imposed by school boards on textbook content or by the FCC on radio and TV broadcasts". The Clipper chip, the cryptography issue, poses serious problems for academic freedom. As Bruce Sterling recently reported from the Conference on Computers, Freedom and Privacy: when the audience was asked by a White House representative who they feared would abuse cryptography more, the US government or criminals, three quarters voted against the government.
Cyberspace: A word from the pen of William Gibson, science fiction writer, circa 1984. An unhappy word, perhaps, if it remains tied to the desperate, dystopic vision of the near future found in the pages of Neuromancer (1984) and Count Zero (1987)—visions of corporate hegemony and urban decay, of neural implants, of a life in paranoia and pain—but a word, in fact, that gives a name to a new stage, a new and irresistible development in the elaboration of human culture and business under the sign of technology. Cyberspace: A new universe, a parallel universe created and sustained by the world’s computers and communication lines. A world in which the global traffic of knowledge, secrets, measurements, indicators, entertainments, and alter-human agency takes on form: sights, sounds, presences never seen on the surface of the earth blossoming in a vast electronic night. Cyberspace: Accessed through any computer linked into the system; a place, one place, limitless; entered equally from a basement in Vancouver, a boat in Portau-Prince, a cab in New York, a garage in Texas City, an apartment in Rome, an office in Hong Kong, a bar in Kyoto, a cafe in Kinshasa, a laboratory on the Moon. Cyberspace: The tablet become a page become a screen become a world, a virtual world. Everywhere and nowhere, a place where nothing is forgotten and yet everything changes. 1 2 Michael Benedikt Cyberspace: A common mental geography, built, in turn, by consensus and revolution, canon and experiment; a territory swarming with data and lies, with mind stuff and memories of nature, with a million voices and two million eyes in a silent, invisible concert of enquiry, dealmaking, dream sharing, and simple beholding. Cyberspace: Its corridors form wherever electricity runs with intelligence. Its chambers bloom wherever data gathers and is stored. Its depths increase with every image or word or number, with every addition, every contribution, of fact or thought. Its horizons recede in every direction; it breathes larger, it complexifies, it embraces and involves. Billowing, glittering, humming, coursing, a Borgesian library, a city; intimate, immense, firm, liquid, recognizable and unrecognizable at once. Cyberspace: Through its myriad, unblinking video eyes, distant places and faces, real or unreal, actual or long gone, can be summoned to presence. From vast databases that constitute the culture’s deposited wealth, every document is available, every recording is playable, and every picture is viewable. Around every participant, this: a laboratory, an instrumented bridge; taking no space, a home presiding over a world . . . and a dog under the table. Cyberspace: Beneath their plaster shells on the city streets, behind their potted plants and easy smiles, organizations are seen as the organisms they are—or as they would have us believe them be: money flowing in rivers and capillaries; obligations, contracts, accumulating (and the shadow of the IRS passes over). On the surface, small meetings are held in rooms, but they proceed in virtual rooms, larger, face to electronic face. On the surface, the building knows where you are. And who. Cyberspace: From simple economic survival through the establishment of security and legitimacy, from trade in tokens of approval and confidence and liberty to the pursuit of influence, knowledge, and entertainment for their own sakes, everything informational and important to the life of individuals—and organizations— will be found for sale, or for the taking, in cyberspace.
Cyberspace: The realm of pure information, filling like a lake, siphoning the jangle of messages transfiguring the physical world, decontaminating the natural and urban landscapes, redeeming them, saving them from the chain-dragging bulldozers of the paper industry, from the dieselsmoke of courier and post office trucks, from jet fuel fumes and clogged airports, from billboards, trashy and pretentious architecture, hour-long freeway commutes, ticket lines, and choked subways. . .from all the inefficiencies, pollutions (chemical and informational), and corruptions attendant to the process of moving information attached to things—from paper to brains—across, over, and under the vast and bumpy surface of the earth rather than letting it fly free in the soft hail of electrons that is cyberspace. Cyberspace as just described—and, for the most part, as described in this book—does not exist. But this states a truth too simply. Like Shangri-la, like mathematics, like every story ever told or sung, a mental geography of sorts has existed in the living mind of every culture, a collective memory or hallucination, an agreed-upon territory of mythical figures, symbols, rules, and truths, owned and traversable by all who learned its ways, and yet free of the bounds of physical space and time. What is so galvanizing today is that technologically advanced cultures—such as those of Japan, Western Europe, and North America—stand at the threshold of making that ancient space both uniquely visible and the object of interactive democracy. Sir Karl Popper, one of this century’s greatest philosophers of science, sketched the framework in 1972. The world as a whole, he wrote, consists of three, interconnected worlds. World 1, he identified with the objective world of material, natural things and their physical properties—with their energy and weight and motion and rest; World 2 he identified with the subjective world of consciousness—with intentions, calculations, feelings, thoughts, dreams, memories, and so on, in individual minds. World 3, he said, is the world of objective, real, and public structures which are the not-necessarily-intentional products of the minds of living creatures, interacting with each other and with the natural World 1. Anthills, birds’ nests, beavers’ dams, and similar, highly complicated structures built by animals to deal with the environment, are forerunners. But many World 3 structures, Popper noted, 4 Michael Benedikt are abstract; that is, they are purely informational: forms of social organization, for example, or patterns of communication. These abstract structures have always equaled, and often surpassed, the World 3 physical structures in their complexity, beauty, and importance to life. Language, mathematics, law, religion, philosophy, arts, the sciences, and institutions of all kinds, these are all edifices of a sort, like the libraries we build, physically, to store their operating instructions, their “programs.” Man’s developing belief in, and effective behavior with respect to, the objective existence of World 3 entities and spaces meant that he could examine them, evaluate, criticize, extend, explore, and indeed make discoveries in them, in public, and in ways that could be expected to bear on the lives of all. They could evolve just as natural things do, or in ways closely analogous. Man’s creations in this abstract realm create their own, autonomous problems too, said Popper: witness the continual evolution of the legal system, scientific and medical practice, the art world, or for that matter, the computer and entertainment industries. And always these World 3 structures feed back into and guide happenings in Worlds 1 and 2. For Popper, in short, temples, cathedrals, marketplaces, courts, libraries, theatres or amphitheaters, letters, book pages, movie reels, videotapes, CDs, newspapers, hard discs, performances, art shows. . . are all physical manifestations—or, should one say, the physical components of—objects that exist more wholly in World 3. They are “objects,” that is, which are patterns of ideas, images, sounds, stories, data. . . patterns of pure information. And cyberspace, we might now see, is nothing more, or less, than the latest stage in the evolution of World 3, with the ballast of materiality cast away—cast away again, and perhaps finally. This book explores the consequences and limits of such a development. But let it be said that, in accordance with the laws of evolution, and no matter how far it is developed, cyberspace will not replace the earlier elements of World 3. It will not replace but displace them, finding, defining, its own niche and causing the earlier elements more closely to define theirs too. This has been the history of World 3 thus far. Nor will virtual reality replace “real reality.” Indeed, real reality—the air, the human body, nature, books, streets. . . who could finish such a list?—in all its exquisite design, history, quiddity, and meaningfulness may benefit from both our renewed appreciation and our no longer asking it to do what is better done “elsewhere.”
I have introduced Popper’s rather broad analysis to set the stage for a closer examination of the origins and nature of our subject, cyberspace. I discern four threads within the evolution of World3. These intertwine. Thread One This, the oldest thread, begins in language, and perhaps before language, with a commonness-of-mind among members of a tribe or social group. Untested by dialogue—not yet brought out “into the open” in this way—this commonnessof-mind is tested and effective nonetheless in the coordinated behavior of the group around a set of beliefs held simply to be “the case:” beliefs about the environment, about the magnitude and location of its dangers and rewards, what is wise and foolhardy, and about what lies beyond; about the past, the future, about what lies within opaque things, over the horizon, under the earth, or above the sky. The answers to all these questions, always “wrong,” and always pictured in some way, are common property before they are privately internalized and critiqued. (The group mind, one might say, precedes the individual mind, and consensus precedes critical exception, as Mead and Vygotsky pointed out.) With language and pictorial representation, established some ten to twenty thousand years ago, fully entering the artifactual world, World 3, these ideas blossom and elaborate at a rapid pace. Variations develop on the common themes of life and death, the whys and wherefores, origins and ends of all things, and these coalesce ecologically into the more or less coherent systems of narratives, characters, scenes, laws, and lessons that we now recognize, and sometimes disparage, as myth. One does not need to be a student of Carl Jung or Joseph Campbell to acknowledge how vital ancient mythological themes continue to be in our advanced technological cultures. They inform not only our arts of fantasy, but, in a very real way, the way we understand each other, test ourselves, and shape our lives. Myths both reflect the “human condition” and create it. Now, the segment of our population most visibly susceptible to myth and most productive in this regard are those who are “coming of age,” the young. Thrust inexorably into a complex and rule-bound world that, it begins to dawn on them, they did not make and that, further, they do not understand, adolescents are apt to reach with some anger and some confusion into their culture’s “collective unconscious”—a world they already possess—for anchorage, guidance, and a base for 6 Michael Benedikt resistance. The boundary between fiction and fact, between wish and reality, between possibility and probability, seems to them forceable; and the archetypes of the pure, the ideal, the just, the good, and the evil, archetypes delivered to them in children’s books and movies, become now, in their struggle towards adulthood, both magnified and twisted. It is no surprise that adolescents, and in particular adolescent males, almost solely support the comic book, science fiction, and videogame industries (and, to a significant extent, the music and movie industries too). These “media” are alive with myth and lore and objectified transcriptions of life’s more complex and invisible dynamics. And it is no surprise that young males, with their cultural bentindeed mission-to master new technology, are today’s computer hackers and so populate the on-line communities and newsgroups. Indeed, just as “cyberspace” was announced in the pages of a science fiction novel, so the young programmers of on-line “MUDS” (Multi-User Dungeons) and their slightly older cousins hacking networked videogames after midnight in the laboratories of MIT’s Media Lab, NASA, computer science departments, and a hundred tiny software companies are, in a very real sense, by their very activity, creating cyberspace. This is not to say that cyberspace is for kids, even less is it to say that it is for boys: only that cyberspace’s inherent immateriality and malleability of content provides the most tempting stage for the acting out of mythic realities, realities once “confined” to drug-enhanced ritual, to theater, painting, books, and to such media that are always, in themselves, somehow less than what they reach for, mere gateways. Cyberspace can be seen as an extension, some might say an inevitable extension, of our age-old capacity and need to dwell in fiction, to dwell empowered or enlightened on other, mythic planes, if only periodically, as well as this earthly one. Even without appeal to sorcery and otherworldy virtual worlds, it is not too farfetched to claim that already a great deal of the attraction of the networked personal computer in general-once it is no longer feared as a usurper of consciousness on the one hand, nor denigrated as a toy or adding machine on the otheris due to its lightning-fast enactment of special “magical” words, instruments, and acts, including those of induction and mastery, and the instant connection they provide to distant realms and buried resources. For the mature as well as the young, then, and for the purposes of art and self-definition as well as rational communications and business, it is likely that cyberspace will retain a good measure of mytho-logic, the exact manifestations of which, at this point, no one can predict. Three of the authors in this book—Michael Heim, Allucquere Rosanne Stone, and David Tomas—take up the cultural-anthropological theme, the latter two with special reference to the changing meaning of the “technophilic” physical body. Chip Morningstar and F. Randall Farmer describe their experiences with on-line games, in particular, LucasFilm’s Habitat. William Gibson’s short piece also makes its contribution at this level, if more directly, as an allegorical work of fiction itself. Thread Two Convolved with the history of myth is the thread of the history of media technology as such, that is, the history of the technical means by which absent and/or abstract entities—events, experiences, ideas—become symbolically represented, “fixed” into an accepting material, and thus conserved through time as well as space. Again, this a fairly familiar story, one whose detailed treatment is far beyond the scope of this introduction and this book. Nevertheless it is one worth rehearsing. It is also a topic that is extremely deep, for the secret of life itself is wrapped up in the mystery of genetic encoding and the replication and motility of molecules that orchestrate each other’s activity. Genes are information; molecules are media as well as motors, so to speak ... But we cannot begin here, where the interests of computation theorists and biologists coincide. Our story best begins with evolved man’s conscious co-option of the physical environment, specifically those parts, blank themselves, that best receive markings-such as sand, wood, bark, bone, stone, and the human body-for the purpose of preserving and delivering messages: signs, not unlike spoors, tracks, or tell-tale colors of vegetation or sky, but now intentional, between man and man, and man and his descendants. What a graceful and inspired step it was, then, to begin to produce the medium, to create smooth plastered walls, thin tablets, and papyrus, and to reduce the labor of marking-carving, chiseling—to the deft movement of a pigmented brush or stylus. As society elaborated itself and as the need to keep records and to educate grew, how much more efficient it was to shrink and conventionalize the symbols themselves, then to crowd them into rows and layers, "paper-thin" scrolls and stacks. 8 Michael Benedikt At this early stage already, the double movement towards the dematerialization of media on the one hand and the reification of meanings on the other is well underway. Against the ravages of time, nonetheless, and to impress the illiterate masses, only massive sculptures, friezes, and reliefs in stone would do. These are what we see today; these are what survive of ancient cultures and impress us still. But it would be wrong therefore to underestimate the traffic of information in more ephemeral media that must have sustained day-to-day life: the scratched clay tablets, the bark shards, graffitied walls, counters, papyri, diagrams in the sand, banners in the wind, gestures, demonstrations, performances, and, of course, the babble of song, gossip, rumor, and instruction that continuously filled the air. Every designed and every made thing was also the story of its use and its ownership, of its making and its maker. This world sounds strangely idyllic. Many of its components, in only slightly updated forms, survive today. It was a period perhaps four thousand years long when objects, even pure icons and symbols, were not empty or ignorable but were real and meaningful, when craftsmanship, consensus, and time were involved in every thing and its physical passage through society. But first, with the development of writing and counting and modes of graphic representation, and then, centuries later, with the invention of the printing press and the spread of literacy beyond the communities of religious scholars and noblemen, the din of ephemeral communications came to be recorded at an unprecedented scale. More important for our story, these “records” came to be easily duplicable, transportable, and broadcastable. Life would never be the same. The implications of the print revolution and the establishment of what Marshall McLuhan called the “Gutenberg galaxy” (in his book of the same name) for the structure and function of technologically advancing societies can hardly be overestimated. Not the least of these implications were (1) the steady, de facto, democratization of the means of idea production and dissemination, (2) the exponential growth of that objective body of scientific knowledge, diverse cultural practices, dreams, arguments, and documented histories called World 3, and (3) the fact that this body, containing both orthodoxies and heresies, could neither be located at any one place, nor be entirely controlled.
However, our double movement did not stop there, as we are all witness today. Although “printed matter” from proclamations to bibles to newspapers could, in principle, be taken everywhere a donkey, a truck, a boat, or an airplane could physically go, there was a limit, namely, time. No news could be fresh days or weeks later. The coordination of goods transportation in particular was a limiting case, for if no message could travel faster than that whose imminent arrival it was to announce.. . then of what value the message? Hence the telegraph, that first “medium” after semaphore, smoke signals, and light-flashing, to connect distant “stations” on the notion of a permanent network. Another related limit was expense: the sheer expenditure of energy required to convey even paper across substantial terrain. The kind of flexible common-mindedness made possible in small communities by the near-simultaneity and zero-expense of natural voice communications, or even rumor and leaflets, collapses at the larger scale. Social cohesion is a function of ideational consensus, and without constant update and interaction, such cohesion depends crucially on early, and strict, education in—and memory of—the architectures, as it were, of World 3. With the introduction of the telephone, both the problem of speed and the problem of expense were largely eliminated. Once wired, energy expenditure was trivial to relay a message, and it was soon widely realized (interestingly only in the 1930s and 40s) that the telephone need not be used like a “voice-telegraph,” which is to say, sparingly and for serious matters only. Rather, it could be used also as an open channel for constant, meaningful, community-creating and business-running interchanges; “one-on-one” interchanges, to be sure, but “many-to-many” over a period of time. Here was a medium, here is a medium, whose communicational limits are still being tested, and these quite apart from what can be accomplished using the telephone system for computer networks. Of course, the major step being taken here, technologically, is the transition, wherever advantageous, from information transported physically, and thus against inertia and friction, to information transported electrically along wires, and thus effectively without resistance or delay. Add to this the ability to store information electromagnetically (the first tape recorder was demonstrated commercially in 1935), and we see yet another significant and evolutionary step in dematerializing the medium and conquering—as they say—space and time. 10 Michael Benedikt But this was paralleled by a perhaps more significant development: wire-less broadcasting, that is, radio and television. Soon, encoded words, sounds, and pictures from tens of thousands of sources could invisibly saturate the world’s “airwaves,” every square millimeter and without barrier. What poured forth from every radio was the very sound of life itself, and from every television set the very sight of it: car chases, wars, laughing faces, oceans, volcanos, crying faces, tennis matches, perfume bottles, singing faces, accidents, diamond rings, faces, steaming food, more faces. . . images, ultimately, of a life not really lived anywhere but arranged for the viewing. Critic and writer Horace Newcomb (1976) calls television less a medium of communication than a medium of communion, a place and occasion where nightly the British, the French, the Germans, the Americans, the Russians, the Japanese.. .settle down by the million to watch and ratify their respective national mythologies: nightly variations on a handful of dreams being played out, over and over, with addicting, tireless intensity. Here are McLuhan’s acoustically structured global villages (though he wished there to be only one), and support for the notion that the electronic media, and in particular television, provide a medium not unlike the air itself—surrounding, permeating, cycling, invisible, without memory or the demand for it, conciliating otherwise disparate and perhaps antagonistic individuals and regional cultures. With cordless and then private cellular telephones, and “remote controls” and then hand-held computers communicating across the airwaves too, the very significance of geographical location at all scales begins to be questioned. We are turned into nomads . . . who are always in touch. All the while, material, print-based media were and are growing more sophisticated too: “vinyl” sound recording (a kind of micro-embossing), color photography, offset lithography, cinematography, and so on. . . the list is long. They became not only more sophisticated but more egalitarian as the general public not only “consumed” ever greater quantities of magazines, billboards, comic books, newspapers, and movies but also gained access to the means of production: to copying machines, cameras, movie cameras, record players, and the rest, each of which soon had its electronic/digital counterpart as well as a variety of hybrids, extensions, and cross-marriages: national newspapers printed regionally from satellite-transmitted electronic data, facsimile transmission, digital preprint and recording, and so on. The end of our second narrative thread is almost at hand. With the advent of fast personal computers, digital television, and high bandwidth cable and radio-frequency networks, so-called postindustrial societies stand ready for a yet deeper voyage into the “permanently ephemeral” (by which I mean, as the reader is well aware, cyberspace). As a number of chapters in this book observe, so-called online community, electronic mail, and information services (USENET, the Well, Compuserve, and scores of others) already form a technological and behavioral beginning. But the significance of this voyage is perhaps best gauged by the almost irrational enthusiasm that today surrounds the topic of virtual reality. Envisaged by science fiction writer/promoter Hugo Gernsback as long ago as 1963 (see Stashower 1990) and explored experimentally by Ivan Sutherland (1968), the technology of virtual reality (VR) stands at the edge of practicality and at the current limit of the effort to create a communication/communion medium that is both phenomenologically engulfing and yet all but invisible. By mounting a pair of small video monitors with the appropriate optics directly to the head, a stereoscopic image is formed before the “user’s” eyes. This image is continuously updated and adjusted by a computer to respond to head movements. Thus, the user finds himself entirely surrounded by a stable, three-dimensional visual world. Wherever he looks he sees what he would see were the world real and around him. This virtual world is either generated in real time by the computer, or it is preprocessed and stored, or it exists physically elsewhere and is “videographed” and transmitted in stereo, digital form. (In the last two cases the technique is apt to be named telepresence rather than virtual reality.) In addition, the user may be wearing stereo headphones. Tracked for head movements, a complete acoustic sensorium is thus added to the visual one. Finally, the user may wear special gloves, and even a whole body suit, wired with position and motion transducers to transmit to others—and to represent to himself—the shape and activity of his body in the virtual world. There is work underway also to provide some form of forcefeedback to the glove or suit so that the user will actually feel the presence of virtual “solid” objects—their weight, texture, and perhaps 12 Michael Benedikt even temperature (see Stewart 1991a for a recent survey, and Rheingold 1991). With a wishful eye cast towards such fictional technologies as the Holodeck, portrayed in the television series “Star Trek, the Next Generation,” devices sketched in such films as Total Recall and Brainstorm, and, certainly, the direct neural connections spoken of in Gibson’s novels, virtual reality/telepresence technology is as close as one can come in reality to entering a totally synthetic sensorium, to immersion in a totally artificial and/or remote world. Much turns on the question of whether this is done alone or in the company of others; and if the latter, of how many, and how. Most of the chapters in this book tackle the question in one form or another. For, engineering questions aside, as the population of a virtual world increases, with it comes the need for consensus of behavior, iconic language, modes of representation, object “physics,” protocols, and design—in a word, the need for cyberspace as such, seen as a general, singularat-some-level, public, consistent, and democratic “virtual world.” Herein lies the very power of the concept. In this volume, the chapters by Wendy A. Kellogg, John M. Carroll, and John T. Richards, by Steve Pruitt and Tom Barrett, by Meredith Bricken, and, again, by Michael Heim look specifically at the remarkable phenomenon of telepresence or “virtuality” as a prime component of the experience of cyberspace. Other authors in this volume imagine a viable cyberspace operating with less completely immersive techniques, although these nonetheless are thought of as considerably advanced over today’s rather simple, low-resolution, two-dimensional graphical and textual interfaces. Thread Two, then, is drawn from the history of communication media. The broad historical movement from a universal, preliterate actuality of physical doing, to an education-stratified, literate reality of symbolic doing loops back, we find. With movies, television, multimedia computing, and now VR, it loops back to the beginning with the promise of a postliterate era, if such can be said; the promise, that is, of “post-symbolic communication” to put it in VR pioneer Jaron Lanier’s words (Lanier 1989, Stewart 1991b). In such an era, languagebound descriptions and semantic games will no longer be required to communicate personal viewpoints, historical events, or technical information. Rather, direct—if “virtual”— demonstration and interactive experience of the “original” material will prevail, or at least be a universal possibility. We would become again “as children,” but this time with the power of summoning worlds at will and impressing speedily upon others the particulars of our experience. In future computer-mediated environments, whether or not this kind of literal, experiential sharing of worlds will supersede the symbolic, ideational, and implicit sharing of worlds embodied in the traditional mechanisms of text and representation remains to be seen. While pure VR will find its unique uses, it seems likely that cyberspace, in full flower, will employ all modes. Thread Three Another narrative, this one is spun out of the history of architecture. The reader may remember that Popper saw architecture as belonging to World 3. This it surely does, for although shelter, beauty, and meaning can be found in “unspoiled” nature, it is only with architecture that nature, as habitat, becomes co-opted, modified, and codified. Architecture, in fact, begins with displacement and exile: exile from the temperate and fertile plains of Africa two million years ago—from Eden, if you will, where neither architecture nor clothing was required—and displacment through emigration from a world of plentiful food, few competitors, and no more kin than the earth would provide for. Rapid climatic change, increasing competition, and exponential population growth was to change early man’s condition irreversibly. To this day, architecture is thus steeped in nostalgia, one might say; or in defiance. Architecture begins with the creative response to climatic stress, with the choosing of advantageous sites for settlements (and the need to defend these), and the internal development of social structures to meet population and resource pressure, to wit: with the mechanisms of privacy, property, legitimation, task specialization, ceremony, and so on. All this had to be carried out in terms of the availability of time, materials, and design and construction expertise. Added to these were the constraints and conventions manufactured by the culture up to that point. These were often arbitrary and inefficient. But always, even as conventions and constraints transformed, and as man passed from hunting and gathering to agrarianism to urbanism, the theme of return to Eden endured, the idea of return to a time of (presumptive) innocence and tribal/familial/national oneness, with each other and with nature. 14 Michael Benedikt I bring up this theme not because it “explains” architecture, but because it is a principle theme driving architecture’s self-dematerialization. Dematerialization? The reader may be surprised. What is architecture, after all, if not the creation of durable physical worlds that can orient generations of men, women, and children, that can locate them in their own history, protect them always from prying eyes, rain, wind, hail, and projectiles.. . durable worlds, and in them, permanent monuments to everything that should last or be remembered? Indeed these are some of architecture’s most fundamental charges; and most sacred among them, as I have argued elsewhere (Benedikt 1987), is architecture’s standard bearing, along with nature, for our sense of what we mean by “reality.” But this should not blind us to a significant countercurrent, one fed by a resentment of quotidian architecture’s bruteness and claustrophobia, which itself is a spilling over of the resentment we feel for our own bodies’ cloddishness, limitations, and final treachery: their mortality. Reality is death. If only we could, we would wander the earth and never leave home; we would enjoy triumphs without risks, eat of the Tree and not be punished, consort daily with angels, enter heaven now and not die. In the name of these unreasonable desires we revere finery and illumination, and reward bravery, goodness, and learning with the assurance of eternal life. As though we could grow wings! As though we could grow wings, we erect gravity-defying cathedrals resplendent with colored windows and niches crowded with allegorical life, create paradisiacal gardens such as those at Alhambra, Versailles, the Taj Mahal, Roan-Ji, erect stadia for games, create magnificent libraries, labyrinths, and observatories, build on sacred mountain tops, make enormous, air conditioned greenhouses with amazing flying-saucer elevators, leap from hillsides strapped to kites, dazzle with gold, chandeliers, and eternally running streams; we scrub and polish and whiten. . . all in a universal, crosscultural act of reaching beyond brute nature’s grip in the here and now. And this with the very materials nature offers us. In counterpoint to the earthly garden Eden (and even to that walled garden, Paradise) then, floatsthe image of the Heavenly City, the new Jerusalem of the book of Revelation. Like a bejeweled, weightless palace it comes down out of heaven itself “its radiance like a most rare jewel, like jasper, transparent” (Revelation 21:9). Never seen, we know its geometry to be wonderfully complex and clear, its twelves and fours and sevens each assigned a set of complementary cosmic meanings. A city with streets of crystalline gold, gates of solid pearl, and no need for sunlight or moonlight to shine upon it for “the glory of God is its light.” In fact, all images of the Heavenly City-East and West-have common features: weightlessness, radiance, numerological complexity, palaces upon palaces, peace and harmony through rule by the good and wise, utter cleanliness, transcendence of nature and of crude beginnings, the availability of all things pleasurable and cultured. And the effort at describing these places, far from a mere exercise in superlatives by medieval monks and painters, continues to this day on the covers and in the pages of innumerable science fiction novels and films. (Think of the mother ship in Close Encounters of the Third Kind.) Here is what it means to be “advanced,” they all say. From Hollywood Hills to Tibet, one could hardly begin to list the buildings actually built and projects begun in serious pursuit of realizing the dream of the Heavenly City. If the history of architecture is replete with visionary projects of this kind, however, these should be seen not as naive products of the fevered imagination, but as hopeful fragments. They are attempts at physically realizing what is properly a cultural archetype, something belonging to no one and yet everyone, an image of what would adequately compensate for, and in some way ultimately justify, our symbolic and collective expulsion from Eden. They represent the creation of a place where we might re-enter God’s graces. Consider: Where Eden (before the Fall) stands for our state of innocence, indeed ignorance, the Heavenly City stands for our state of wisdom, and knowledge; where Eden stands for our intimate contact with material nature, the Heavenly City stands for our transcendence of both materiality and nature; where Eden stands for the world of unsymbolized, asocial reality, the Heavenly City stands for the world of enlightened human interaction, form and information. In Eden the sun rose and set, there were days and nights, wind and shadow, leaf and stone, and all perfumed. The Heavenly City, though it may contain gardens, breathes the crystalline gleam of its own lights, sparkling, insubstantial, laid out like a beautiful equation. Thus, while the biblical Eden may be imaginary, the Heavenly City is doubly imaginary: once, in the conventional sense, because it is not actual, but once again because even if it became actual, because it is information, it could come 16 Michael Benedikt into existence only as a virtual reality, which is to say, fully, only “in the imagination.” The image of The Heavenly City, in fact, is an image of World 3 become whole and holy. And a religious vision of cyberspace. I must now return briefly to the history of architecture, specifically in modern times. After a century of the Industrial Revolution, the turn of the twentieth century saw the invention of hightensile steels, ofsteel-reinforced concrete, and of high-strength glass. Very quickly, and under economic pressure to do more with less, architects seized and celebrated the new vocabulary of lightness. Gone were to be the ponderous piers, the small wooden windows, the painstaking ornament, the draughty chimneys and lanes, the chipping and smoothing and laying! Instead: daring cantilevers, walls reduced to reflective skins, openness, light, swiftness of assembly, chromium. Gone the stairs, the horse-droppings in the street, and the cobbles. Instead, the highway, the bulletlike car, the elevator, the escalator. Gone the immovable monument, instead the demountable exhibition; gone the Acropolis, instead the World’s Fair. In 1924, the great architect Le Corbusier proposed razing half of Paris and replacing it with La Ville Radieuse, the Radiant City, an exercise in soaring geometry, rationality, and enlightened planning, unequaled since. A Heavenly City. By the late 1960s, however, it was clear that the modern city was more than a collection of buildings and streets, no matter how clearly laid out, no matter how lofty its structures or green its parks. The city became seen as an immense node of communications, a messy nexus of messages, storage and transportation facilities, a massive education machine of its own complexity, involving equally all media, including buildings. To no one was this more apparent than to a group of architects in England calling themselves Archigram. Their dream was of a city that built itself unpredictably, cybernetically, and of buildings that did riot resist television and telephones and air conditioning and cars and advertising but accommodated and played with them; inflatable buildings, buildings on rails, buildings like giant experimental theaters with video cameras gliding like sharks through a sea of information, buildings bedecked in neon, projections, lasers beams. . . . These were described in a series of poster-sized drawings called architectural telegrams, which were themselves, perhaps not incidentally, early examples of what multimedia computer screens might look like tomorrow (Cook 1973). Although the group built nothing themselves, they were and are, nonetheless, very influential in the world of architecture.
Now, a complete treatment of the signs of the ephemeralization of architecture and its continuing capitulation to media is outside the scope of this introduction. It occurs on many fronts, from the wild “Disneyfication” of form, to the overly meek accommodation of services. Most interesting, however, is a thread that arises from thinking of architecture itself as an abstraction, a thread that has a tradition reaching back to ancient Egypt and Greece and the coincidence of mathematical knowledge with geometry and hence correct architecture. As late as the eighteenth century, architects were also scientists and mathematicians; witness Andrea Palladio, Sir Christopher Wren, and before them, of course, Leonardo da Vinci and Leon Battista Alberti. From the 1920s till the 1960s, the whole notion that architecture is about the experiential modulation of space and time—that it is “four dimensional”—captivated architectural theory, just as it had captivated a generation of artists in the 20s and 30s (Henderson 1983). This was something conceptually far beyond the simple mathematics of good proportions, even of structural engineering. It is an idea that still has force. Then too there is the tradition of architecture seen for its symbolic content; that is, for not only the way it shapes and paces information fields in general (the emanations of faces, voices, paintings, exit signs, etc.) but the way buildings carry meaning in their anatomy, so to speak, and in what they “look like.” After five thousand years, the tradition is very much alive as part of society’s internal message system. In recent years, however, the architectural “message system” has taken on a life of its own. Not only have architectural drawings generated an art market in their own right—as illustrated conceptual art, if you will—but buildings themselves have begun to be considered as arguments in an architectural discourse about architecture, as propositions, narratives, and inquiries that happen, also, to be inhabitable. In its most current avant-garde guise, the movement goes by the name of Deconstructivism, or Post-Structuralism (quite explicitly related to the similarly named movements in philosophy and literary criticism). Its interests are neither in the building as an object of inhabitation nor as an object of beauty, but as an object of information, a collection of ciphers and “moves,” junctions and disjunctions, reversals and iterations, metaphorical woundings and healings, and so on, all to be “read.” This would be of little interest to us here were it not an 18 Michael Benedikt indication of how far architecture can go towards attempting to become pure demonstration, and intellectual process, and were it not fully a part of the larger movement I have been describing. (And we should remember that, as a rule, today’s avantgarde informs tomorrow’s practice. See Betsky 1990.) But there is a limit to how far notions of dematerialization and abstraction can go and still help produce useful and interesting, real architecture. That limit has probably been reached, if not overshot (Benedikt 1987). And yet the impetus toward the Heavenly City remains. It is to be respected; indeed, it can usefully flourish. . . in cyberspace. The door to cyberspace is open, and I believe that poetically and scientifically minded architects can and will step through it in significant numbers. For cyberspace will require constant planning and organization. The structures proliferating within it will require design, and the people who design these structures will be called cyberspace architects. Schooled in computer science and programming (the equivalent of “construction”), in graphics, and in abstract design, schooled also along with their brethren “real-space” architects, cyberspace architects will design electronic edifices that are fully as complex, functional, unique, involving, and beautiful as their physical counterparts if not more so. Theirs will be the task of visualizing the intrinsically nonphysical and giving inhabitable visible form to society’s most intricate abstractions, processes, and organisms of information. And all the while such designers will be rerealizing in a virtual world many vital aspects of the physical world, in particular those orderings and pleasures that have always belonged to architecture. Two chapters in this volume “come out of” architecture, my own and Marcos Novak’s. My chapter attempts to discuss cyberspace in terms of certain basic design principles and then show some visualized examples; Novak discusses the idea of cyberspace as a poetic mediuim that, among other things, creates a “liquid architecture,” an architecture of information, being less a proposition about designing buildings, of course, than a prelude as to how we might evolve legible forms in the context of a user-driven and self-organizing cyberspace system. Thread Four This thread is drawn from the larger history of mathematics. It is the line of arguments and insights that revolve around (1) the propositions of geometry and space, (2) the spatialization of arithmetical/algebraic operations, and (3) reconsideration of the nature of space in the light of (2). Since Artistotle, operating alongside this “spatial-geometrical” thread in mathematics has been a complementary one, that is, the development of symbolic logic, algebraic notation, calculus, finite mathematics, and so on, to modern programming languages. I say “complementary” because these last-named subjects could (and can still) proceed purely symbolically, with little or no geometrical, spatial interpretation; algebra, number theory, computation theory, logic. . . these are symbolic operations upon symbolic operations and have a life of their own. In practice, of course, diagrams, which are spatial and geometrical, and symbol strings (mathematical notation, language) are accepted as mutually illuminating representations and are considered together. But the distinction between them, and the tension, still remain. There are those who think most easily and naturally in symbolic sequences, and linear operations upon them; there are those who think most easily and naturally in shapes, actions, and spaces. Apparently more than one type of intelligence is involved here (West 1991, Gardner 1983, Hadamard 1945). Be this as it may, cyberspace clearly is premised upon the desirability of spatialization per se for the understanding of information. Certainly, it extends the current paradigm in computing of “graphic user interfaces” into higher dimensions and more involving experiences, and it extends current interest, as evidenced by the popularity of Edward Tufte’s books (1983, 1990), in “data cartography” in general and in the field of scientific visualization. But, more fundamentally, cyberspace revivifies and then extends some of the more basic techniques and questions having to do with the spatial nature of mathematical entities, and the mathematical nature ofspatial entities, that lie at the heart of what we consider both real and measurable. Rigorous reasoning with shape—deductive geometry—began, as we all know, in ancient Greece with Thales around 600 B.C., continuing through 225 B.C. with Pythagoras, Euclid, and Apollonius. The subject was twin: (1) the nature (and methods of construction) of the idealized forms studied—basically lines, circles, regular polygons and polyhedra, although Apollonius began work on conic sections—and (2) the nature of perfect reasoning itself, which the specifiability and universality of 20 Michael Benedikt geometrical operations seemed to exemplify. The results of geometrical study had practical use in building and road construction, land surveying, and what we today call mechanical engineering. Its perfection and universality also supported the casting of astrological/cosmological models along geometrical lines. The science and art of geometry has developed sporadically since, receiving its last major “boost” of renewed interest—after Kepler and Newton—in the late nineteenth century, with Bolyai and Lobatchevsky’s discovery of non-Euclidean geometry. Soon, however, with the concept of pure topology and the discovery of consistent geometries of higher dimensionality than three, first Euclidean geometry and then geometry in general began to lose something of its luster as a science wherein significant new discoveries could be made. All statements of visual geometrical insight, it seemed, could be studied more generally and accurately in the symbolic/algebraic language of analytical mathematics—final fruit of Descartes’ project in La Géométrie, which was precisely to show how the theorems of geometry could be transcribed into analytical (algebraic) form. Of course the linkage, once made, between geometry and algebra, space and symbol, form and argument, is a two-way one. Descartes had both “algebraized” geometry and “geometrized” algebra. (And it is this second movement that is of most interest to us here.) With one profound invention, he had built the conceptual bridge we today call the Cartesian coordinate system. Here was the insight: just as the positions of points in natural, physical space could be encoded, specified, by a trio of numbers, each referring to a distance from a common but arbitrary origin in three mutually orthogonal directions, so too could the positions of points in a “mathematical space” where the “distances” are not physical distances but numerical values, derived algebraically, of the solution of equations of (up to) three variables. In this way, thousands of functions could accurately be “graphed” and made visible. Today, procedures based on Descartes’ insight are a commonplace, taught even at good elementary schools. But this should not mask the power of the implicit notion that space itself is something not necessarily physical: rather that it is a “field of play” for all information, only one of whose manifestations is the gravitational and electromagnetic field of play that we live in, and that we call the real world. Perhaps no examples are more vivid than the beautiful forms that emerge from simple recursive equations—the new science of “fractals”—and recent discoveries of “strange attractors,” objects of coherent geometry and behavior that “exist” only in mathematical spaces (coordinate systems with specially chosen coordinates) and that economically map/describe/prescribe the behavior of complex, chaotic, physical systems. Which reality is the primary one? we might fairly ask. Actually, why choose? Modern physicists are sanguine: Minkowski had shown the utility of mapping time together with space, Hamiltonian mechanics lent themselves beautifully to visualizing the dynamics of a physical system in n-dimensional state or phase space where a single point represents the entire state of the system, and quantum mechanics seems to play itself out in the geometrical behavior of vectors in Hilbert space, in which one or more of the coordinates are “imaginary” (see Penrose 1989 for a recent explication). In the meantime, the more common art of diagrams and charts proliferatedfrom old maps, schedules, and scientific treatises, to the pages of modern economics primers, advertisements, and boardroom “business graphics.” Many of these representations are in fact hybrids, mixing physical, energic or spatiotemporal, coordinates with abstract, mathematical ones, mixing histories with geographies, simple intervallic scales with exponential ones, and so on. The practice of diagramming (surely one whose origins are earlier than writing) continues too, today enhanced by the mathematics of graph theory with its combinatorial and network techniques to analyze and optimize complex processes. What, we may ask, is the ontological status of such representations? All of them—from simple bar charts and organizational “trees” through matrices, networks, and “spreadsheets” to elaborate, multidimensional, computer-generated visualizations of invisible physical processes—all of these, and all abstract phase-, state-, and Hilbert-space entities, seem to exist in a geography, a space, borrowed from, but not identical with, the space of the piece of paper or computer screen on which we see them. All have a reality that is no mere picture of the natural, phenomenal world, and all display a physics, as it were, from elsewhere. What are they, indeed? Neither discoveries nor inventions, they are of World 3, entities themselves evolved by our intelligence in the world of things and of each other. They represent first evidence of a continent about which we have hitherto communicated only in sign language, 22 Michael Benedikt a continent “materializing,” in a way. And at the same time they express a new etherealization of geography. It is as though, in becoming electronic, our beautiful old astrolabes, sextants, surveyor’s compasses, observatories, orreries, slide rules, mechanical clocks, drawing instruments and formwork, maps and plans—physical things all, embodiments of the purest geometry, their sole work to make us at home in space—become environments themselves, the very framework of what they once only measured. The contributions by Tim McFadden, Carl Tollander, and Alan Wexelblat are partially woven from this thread, as is a good part of my own. McFadden examines the idea of cyberspace as an informational Indra’s Net, a universe of pointlike beads, infinite in number, each of which reflects all the others. Here cyberspace is an evolving, fourdimensional hologram of itself. (It was this ancient Hindu image of Indra’s Net that also informed Leibniz’s Monadology, as Heim discusses.) Tollander introduces Edelman’s Neuronal Group Selection theory into the design of a noncentralized system of computational “engines” to create cyberspaces that can evolve in a “natural” way. (Novak also discusses this notion). Wexelblat examines the nature of coordinates in abstract spaces in general, in modern personal computing, and then, extrapolated, in terms of cyberspace specifically considered as an outgrowth of these. My account of the intertwining “threads” that seem to lead to cyberspace is, of course, impressionistic and incomplete, and not just for lack of space in this introduction. Cyberspace itself is an elusive and future thing, and one can hardly be definitive at this early stage. But it is also clear that the “threads” themselves are made of threads, and that there are others. For example, the history of art into modern times tells a related story, fully involving mythology, changing media, a relationship to architecture, logic, and so on. It is a thread I have not described, and yet the contribution of artists—visual, musical, cinematic—to the design of virtual worlds and cyberspace promises to be considerable, as Nicole Stenger, poet and animation artist, attests in this volume. Similarly, the story of progress in telecommunications and computing technology—the miniaturizations, speeds, and economies, the new materials, processes, interfaces and architectures—is a thread in its own right, with its own thrusts and interests in the coming-to-be of cyberspace. This story is well chronicled elsewhere (Rheingold 1985, Gilder 1988). Then there is the sociological story, and the economic one, the linguistic one, even the biological one . . . and one begins to realize that every discipline can have an interest in the enterprise of creating cyberspace, a contribution to make, and a historical narrative to justify both. How could it be otherwise? We are contemplating the arising shape of a new world, a world that must, in a multitude of ways, begin, at least, as both an extension and a transcription of the world as we know it and have built it thus far. Another reason that my account is impressionistic and incomplete, however, is that the very metaphor of threads is too tidy and cannot support all that needs to be said. Scale aside, something deeper and more formless is going on. Consider: if information is the very stuff of space and time, what does it mean to manufacture information, and what does it mean to transfer it at ever higher rates between spatiotemporally distinct points, and thus dissolve their very distinctness? With mature cyberspaces and virtual reality technology, this kind of warpage, tunneling, and lesioning of the fabric of reality will become a perceptual, phenomenal fact at hundreds of thousands of locations, even as it falls short of complete, quantum level, physical achievement. Today intellectual, tomorrow practical, one can only guess at the implications. Finally, my “narrative of threads” has not done justice to the authors represented in this volume. Each has their own perspective, expertise, and interest, and each draws inspiration from matters I have not mentioned, and stories I have not sketched or have only touched upon. Rather than extend this introduction with fuller discussion of each chapter, however, I recommend that the reader turn to them forthwith! Many are expanded and revised versions of presentations made at The First Conference on Cyberspace.1 Others are written especially for the present collection.2 All the authors address themselves to the topic with extraordinary seriousness, acumen, and enthusiasm, even though—and perhaps because—the varieties of cyberspace they imagine, describe, and sometimes criticize, do not yet exist. Indeed, the very definition of cyberspace may well be in their hands (or yours, dear reader). Of this much, one can be sure: the advent of cyberspace will have profound effects on so-called postindustrial culture, and the 1Held on May 4 and 5, 1990, at The University of Texas at Austin. The Second (International) Conference on Cyberspace was held April 18-19, 1991, at The University of California at Santa Cruz. 2Gibson, Tomas, Stone, and Wexelblat. 24 Michael Benedikt material and economic rewards for those who first and most properly conceive and implement cyberspace systems will be enormous. But let usset aside talk of rewards. With this volume, with these “firststeps,” let us begin to face the perplexities involved in making the unimaginable imaginable and the imaginable real. Let the ancient project that is cyberspace continue.

Codes—by name and by matter—are what determine us today, and what we must articulate if only to avoid disappearing under them completely. They are the language of our time precisely because the word and the matter code are much older, as I will demonstrate with a brief historical regression. And have no fear: I promise to arrive back at the present. Imperium Romanum Codes materialize in processes of encryption, which is, according to Wolfgang Coy’s elegant defi nition, “from a mathematical perspective a mapping of a fi - nite set of symbols of an alphabet onto a suitable signal sequence.”1 This defi nition clarifi es two facts. Contrary to current opinion, codes are not a peculiarity of computer technology or genetic engineering; as sequences of signals over time they are part of every communications technology, every transmission medium. On the other hand, much evidence suggests that codes became conceivable and feasible only after true alphabets, as opposed to mere ideograms or logograms, had become available for the codifi cation of natural languages. Those alphabets are systems of identically recurring signs of a countable quantity, which map speech sounds onto letters more or less one- to- one and, hopefully, completely. A vocalic alphabet of a type such as Greek,2 justly praised for being the “fi rst total analysis of a language,”3 does appear to be a prerequisite for the emergence of codes, and yet, not a suffi cient one. For what the Greeks lacked (leaving out of consideration sporadic allusions in the work of Aischylos, Aenas, Tacticus, and Plutarch to the use of secret writing4 was that second prerequisite of all coding, namely, developed communications technology. It is anything but coincidental that our reports of the fi rst secret message systems coincide with the rise of the Roman Empire. In his Lives of the Caesars, Suetonius—who himself served as secret scribe to a great emperor—recounts discovering encrypted letters among the personal fi les left behind by both the divine Caesar and the divine Augustus. Caesar contented himself with moving all the letters of the Latin alphabet by four places, thus writing D instead of A, E instead of B, and so forth. His adoptive son Augustus, by contrast, is reported to have merely skipped one letter, but a lack of mathematical discernment led him to replace the letter X, the last in his alphabet, by a double A.5 The purpose was obvious: When read aloud by those not called upon to do so (and Romans were hardly the most literate of people), a stodgy jumble of consonants resulted. And as if such innovations in matters of encryption were not suffi cient, Suetonius attributes to Caesar another invention immediately beforehand—that of having written in several columns, or even separate pages, reports to the Roman Senate on the Gallic campaign. Augustus is credited with the illustrious deed of creating, with riders and relay posts, Europe’s fi rst strictly military express- mail system.6 In other words, the basis on which command, code, and communications technology coincided was the Empire, as opposed to merely the Roman Republic or shorthand writers like Cicero. Imperium is the name of both the command and its effect: the world empire. “Command, control, communications, intelligence” was also the Pentagon’s imperial motto until very recently, when, due to the coincidence of communication technologies and Turing machines it was swapped for C4 —“command, control, communication, computers”—from Orontes to the Scottish headland, from Baghdad to Kabul. It was the case, however, that imperia, the orders of the Emperor, were also known as codicilla, the word referring to the small tablets of stripped wood coated with wax in which letters could be inscribed. The etymon codex for its part—caudex in Old Latin and related to the German verb hauen (to hew)—in the early days of the Empire assumed the meaning of “book,” whose pages could, unlike papyrus scrolls, for the fi rst time be leafed through. And that was how the word that interests us here embarked on its winding journey to the French and English languages. From Imperator Theodosius to Empereur Napoleon, “code” was simply the name of the bound book of law, and codi- fi cation became the word for the judicial- bureaucratic act needed to arrest in a single collection of laws the torrents of imperial dispatches or commands that for centuries had rushed along the express routes of the Empire. Message transmission turned into data storage,7 pure events into serial order. And even today the Codex Theodosius and Codex Iustinianus continue to bear a code of ancient European rights and obligations in those countries where Anglo- American common law does not happen to be sweeping the board. In the Corpus Iuris, after all, copyrsights and trademarks are simply meaningless, regardless of whether they protect a codex or a code. Nation- States The question that remains is why the technical meaning of the word “code” was able to obscure the legal meaning to such a degree. As we know, contemporary legal systems regularly fail to grasp codes in the fi rst place and, in consequence, to protect them, be it from robbers and purchasers or, conversely, from their discoverers and writers. The answer seems to be simple. What we have been calling a code since the secret writings of Roman emperors to the arcana imperii of the modern age was known as a “cipher” from the late Middle Ages onward. For a long time the term code was understood to refer to very different cryptographic methods whereby words could still be pronounced, but obscure or innocuous words simply replaced the secret ones. Cipher, by contrast, was another name for the zero, which at that time reached Europe from India via Baghdad and put sifr (Arabic: “emptiness”) into mathematical- technical power. Since that time, completely different sets of characters have been devised (in sharp contrast to the invention of Greek for speech sounds and numbers: on one side of language the alphabet of the people, on the other the numbers of the bearers of secrets—the name of which spelled the Arabic sifr once again. Separate character sets, however, are productive. Together they brew wondrous creatures that would never have occurred to the Greeks or Romans. Without modern algebra there would be no encoding; without Gutenberg’s printing press, no modern cryptology. In 1462 or 1463, Battista Leone Alberti, the inventor of linear perspective, was struck by two plain facts. First, that the frequency of occurrence of phonemes or letters varies from language to language, a fact which is proved, according to Alberti, by Gutenberg’s letter case. From the frequency of shifted letters as they were written by Caesar and Augustus, cryptanalysis can heuristically derive the clear text of the encrypted message. Second, it is therefore insuffi cient to encrypt a message by shifting all the letters by the same number of places. Alberti’s proposal that every new letter in the clear text be accompanied by an additional place- shift in the secret alphabet was followed up until World War II.8 One century after Alberti, François Viète, the founder of modern algebra, and also a cryptologist in the service of Henry IV, intertwined number and letter more closely still. Only since Viète have there been equations containing unknowns and universal coef- fi cients written with numbers encoded as letters.9 This is still the work method of anybody who writes in a high- level programming language that likewise allocates variables (in a mathematically more or less correct manner) to alpha numeric signs, as in equations. On this basis—Alberti’s polyalphabetic code, Viète’s algebra, and Leibniz’ differential calculus—the nation- states of the modern age were able to technically approach modernity. Global Message Traffi c Modernity began, however, with Napoleon. As of 1794, messengers on horseback were replaced by an optical telegraph which remote- controlled France’s armies with secret codes. In 1806, the laws and privileges surviving from the old days were replaced by the cohesive Code Napoléon. In 1838, Samuel Morse is said to have inspected a printing plant in New York in order—taking a leaf from Alberti’s book—to learn from the letter case which letters occurred most frequently and therefore required the shortest Morse signals.10 For the fi rst time a system of writing had been optimized according to technical criteria—that is, with no regard to semantics—but the product was not yet known as Morse code. The name was bestowed subsequently in books known as Universal Code Condensers, which offered lists of words that could be abbreviated for global cable communications, thus reducing the length, and cost, of telegrams, and thereby encrypting the sender’s clear text for a second time. What used to be called deciphering and enciphering has since then been referred to as decoding and encoding. All code processed by computers nowadays is therefore subject to Kolmogorov’s test: Input is bad if it is longer than its output; both are equally long in the case of white noise; and a code is called elegant if its output is much longer than itself. The twentieth century thus turned a thoroughly capitalist money- saving device called “code condenser” into highest mathematical stringency. The Present Day—Turing All that remains to ask is how the status quo came about or, in other words, how mathematics and encryption entered that inseparable union that rules our lives. That the answer is Alan Turing should be well known today. The Turing machine of 1936, as the principle controller of any computer, solved a basic problem of the modern age: how to note with fi nitely long and ultimately whole numbers the real, and therefore typically infi nitely long, numbers on which technology and engineering have been based since Viète’s time. Turing’s machine proved that although this task could not be accomplished for all real numbers, it was achievable for a crucial subset, which he dubbed computable numbers.11 Since then a fi nite quantity of signs belonging to a numbered alphabet which can, as we know, be reduced to zero and one, has banished the infi nity of numbers. No sooner had Turing found his solution than war demanded its cryptanalytical application. As of spring 1941 in Britannia’s Code and Cipher School, Turing’s proto- computers almost decided the outcome of the war by successfully cracking the secret codes of the German Wehrmacht, which, to its own detriment, had remained faithful to Alberti. Today, at a time when computers are not far short of unravelling the secrets of the weather or the genome—physical secrets, that is to say, and increasingly often biological ones, too—we all too often forget that their primary task is something different. Turing himself raised the question of the purpose for which computers were actually created, and initially stated as the primary goal the decoding of plain human language: Of the above possible fi elds the learning of languages would be the most impressive, since it is the most human of these activities. This fi eld seems, however, to depend rather too much on sense organs and locomotion to be feasible. The fi eld of cryptography will perhaps be the most rewarding. There is a remarkably close parallel between the problems of the physicist and those of the cryptographer. The system on which a message is enciphered corresponds to the laws of the universe, the intercepted messages to the evidence available, the keys for a day or a message to important constants which have to be determined. The correspondence is very close, but the subject matter of cryptography is very easily dealt with by discrete machinery, physics not so easily.12 Conclusions Condensed into telegraphic style, Turing’s statement thus reads: Whether everything in the world can be encoded is written in the stars. The fact that computers, since they too run on codes, can decipher alien codes is seemingly guaranteed from the outset. For the past three- and- a- half millennia, alphabets have been the prototype of everything that is discrete. But it has by no means been proven that physics, despite its quantum theory, is to be computed solely as a quantity of particles and not as a layering of waves. And the question remains whether it is possible to model as codes, down to syntax and semantics, all the languages that make us human and from which our alphabet once emerged in the land of the Greeks. This means that the notion of code is as overused as it is questionable. If every historical epoch is governed by a leading philosophy, then the philosophy of code is what governs our own, and so code—harking back to its root, “codex”—lays down the law for one and all, thus aspiring to a function that was, according to the leading philosophy of the Greeks, exercised exclusively by Aphrodite.13 But perhaps code means nothing more than codex did at one time: the law of precisely that empire which holds us in subjection and forbids us even to articulate this sentence. At all events, the major research institutions that stand to profi t most from such announcements proclaim with triumphant certainty that there is nothing in the universe, from the virus to the Big Bang, which is not code. One should therefore be wary of metaphors that dilute the legitimate concept of code, such as when, for instance, in the case of DNS, it was not possible to fi nd a one- to- one correspondence between material elements and information units as Lily Ray discovered in the case of bioengineering. As a word that in its early history meant “displacement” or “transferral”—from letter to letter, from digit to letters, or vice versa—code is the most susceptible of all to faulty communication. Shining in the aura of the word code one now fi nds sciences that do not even master their basic arithmetic or alphabet, let alone cause something to turn into something different as opposed to merely, as in the case of metaphors, go by a different name. Therefore, only alphabets in the literal sense of modern mathematics should be known as codes, namely one- to- one, fi nite sequences of symbols, kept as short as possible but gifted, thanks to a grammar, with the incredible ability to infi nitely reproduce themselves: Semi- Thue groups, Markov chains,14 Backus- Naur forms, and so forth. That, and that alone, distinguishes such modern alphabets from the familiar one that admittedly spelled out our languages and gave us Homer’s poetry15 but cannot get the technological world up and running the way computer code now does. For while Turing’s machine was able to generate real numbers from whole numbers as required, its successors have—in line with Turing’s daring prediction—taken command.16 Today, technology puts code into the practice of realities, that is to say: it encodes the world. I cannot say whether this means that language has already been vacated as the House of Existence. Turing himself, when he explored the technical feasibility of machines learning to speak, assumed that this highest art, speech, would be learned not by mere computers but by robots equipped with sensors, effectors, that is to say, with some knowledge of the environment. However, this new and adaptable environmental knowledge in robots would remain This means that the notion of code is as overused as it is questionable. If every historical epoch is governed by a leading philosophy, then the philosophy of code is what governs our own, and so code—harking back to its root, “codex”—lays down the law for one and all, thus aspiring to a function that was, according to the leading philosophy of the Greeks, exercised exclusively by Aphrodite.13 But perhaps code means nothing more than codex did at one time: the law of precisely that empire which holds us in subjection and forbids us even to articulate this sentence. At all events, the major research institutions that stand to profi t most from such announcements proclaim with triumphant certainty that there is nothing in the universe, from the virus to the Big Bang, which is not code. One should therefore be wary of metaphors that dilute the legitimate concept of code, such as when, for instance, in the case of DNS, it was not possible to fi nd a one- to- one correspondence between material elements and information units as Lily Ray discovered in the case of bioengineering. As a word that in its early history meant “displacement” or “transferral”—from letter to letter, from digit to letters, or vice versa—code is the most susceptible of all to faulty communication. Shining in the aura of the word code one now fi nds sciences that do not even master their basic arithmetic or alphabet, let alone cause something to turn into something different as opposed to merely, as in the case of metaphors, go by a different name. Therefore, only alphabets in the literal sense of modern mathematics should be known as codes, namely one- to- one, fi nite sequences of symbols, kept as short as possible but gifted, thanks to a grammar, with the incredible ability to infi nitely reproduce themselves: Semi- Thue groups, Markov chains,14 Backus- Naur forms, and so forth. That, and that alone, distinguishes such modern alphabets from the familiar one that admittedly spelled out our languages and gave us Homer’s poetry15 but cannot get the technological world up and running the way computer code now does. For while Turing’s machine was able to generate real numbers from whole numbers as required, its successors have—in line with Turing’s daring prediction—taken command.16 Today, technology puts code into the practice of realities, that is to say: it encodes the world. I cannot say whether this means that language has already been vacated as the House of Existence. Turing himself, when he explored the technical feasibility of machines learning to speak, assumed that this highest art, speech, would be learned not by mere computers but by robots equipped with sensors, effectors, that is to say, with some knowledge of the environment. However, this new and adaptable environmental knowledge in robots would remain obscure and hidden to the programmers who started them up with initial codes. The so- called “hidden layers” in today’s neuronal networks present a good, if still trifl ing, example of how far computing procedures can stray from their design engineers, even if everything works out well in the end. Thus, either we write code that in the manner of natural constants reveals the determinations of the matter itself, but at the same time pay the price of millions of lines of code and billions of dollars for digital hardware; or else we leave the task up to machines that derive code from their own environment, although we then cannot read—that is to say: articulate—this code. Ultimately, the dilemma between code and language seems insoluble. And anybody who has written code even only once, be it in a high- level programming language or assembly, knows two very simple things from personal experience. For one, all words from which the program was by necessity produced and developed only lead to copious errors and bugs; for another, the program will suddenly run properly when the programmer’s head is emptied of words. And in regard to interpersonal communications, that can only mean that self- written code can scarcely be passed on with spoken words. May myself and my audience have been spared such a fate in the course of this essay.


My semi-technical introduction to computer graphics will, however, provide only a half-answer, one that, in particular, cannot address the necessary comparison between paintings and computer images or between subtractive and additive color mixing. Simplified accordingly, a computer image is a two-dimensional additive mixture of three base colors shown in the frame, or parergon, of the monitor housing. Sometimes the computer image as such is less apparent, as in the graphic interface of the newfangled operating systems, sometimes rather more, as in "images" in the literal sense of the word. At any rate, the generation of 2000 likely subscribes to the fallacy-backed by billions of dollars-that computers and computer graphics are one and the same. Only aging hackers harbor the trace of a memory that it wasn't always so. There was a time when the computer screen's display consisted of white dots on an amber or green background, as if to remind us that the techno-historical roots of computers lie not in television, but in radar, a medium of war.
The computer image derives precisely this addressability from early-warning systems, even if it has replaced the polar coordinates _ I of the radar screen with Cartesian coordinates. In contrast to the semi-analog medium of television, not only the horizontal lines but also the vertical columns are resolved | lI . . into basic units. The mass of these so-called- 1 I "pixels" forms a two-dimensional matrix - that assigns each individual point of the " i* ] image a certain mixture of the three base colors: red, green, and blue. The discrete, or digital, nature of both the geometric coordinates and their chromatic values makes possible the magical artifice that separates computer graphics from film and television. Now, for the first time in the history of optical media, it is possible to address a single pixel in the 849th row and 720th column directly without having to run through everything before and after it. The computer image is thus prone to falsification to a degree that already gives television producers and ethics watchdogs the shivers; indeed, it is forgery incarnate. It deceives the eye, which is meant to be unable to differentiate between individual pixels, with the illusion or image of an image, while in truth the mass of pixels, because of its thorough addressability, proves to be structured more like a text composed entirely of individual letters. For this reason-and for this reason only-it is no problem for a computer monitor to switch between text and graphics modes. The twofold digitality of coordinates and color value, however, creates certain problem areas, of which at least three should be mentioned.
Third, the digitality of computer graphics creates a problem unknown to computer music. In an essay on time axis manipulation, I have previously tried to show the leeway produced by the fact that the digital sampling of any given musical sequence falls into three elements (a triad is familiar to us through Giuseppe Peano's theory of natural numbers): an event or state of a millisecond's duration, its predecessor, and its successor.2 These three can be integrated or differentiated, exchanged or scrambled until the limits of modern academic and popular music are truly explored. In principle-and that means, unfortunately, given an exponentially higher processing time-these tricks could be adapted from digital music's single dimension to the two dimensions of digital images. The result, however, tends to be so chaotic that it is as if perception were regressing to pure sensation a la David Hume or Kaspar Hauser. The reason for this is as fundamental as it is non-trivial. Every image (in the sense of art, not of mathematics) has a top and a bottom, a left and a right. Pixels, insofar as they are constructed algebraically as two-dimensional matrices and geometrically as orthogonal grids, necessarily have more than one neighbor. In the heroic beginnings of computer science, great mathematicians had to begin by formulating truisms, whence arose W. Ross Ashby's and John von Neumann's concepts of neighboring elements. In the former, a given element is considered to be surrounded only by a cross of neighbors: above, below, left, and right; in the latter, it is surrounded by a square of the above-mentioned orthogonal elements plus four additional diagonal neighbors. A difference that could perfectly describe, if you like, the difference between the urban fabrics of Manhattan and Tokyo, respectively.
Heidegger posed the riddle of perception thus: "in the appearing of things, never do we, either preliminarily or essentially, perceive an onrush of sensations."3 For beings that dwell in language, anything seen or heard shows itself always already as something. For computer-supported image analysis, however, this something-assomething remains a distant theoretical goal, the achievement of which is not even assured. Therefore I would postpone the question of automatic image analysis for symposia on perception to take place not sooner than a decade from now, and limit myself in the following to the problem of automatic image synthesis. I am not concerned, then, with how computers simulate optical perception, but rather only with how they deceive us. For it seems to be precisely this exorbitant capacity that elevates the medium of the computer above all optical media in Western history.
Computer graphics are to these optical media what the optical media are to the eye. Just as the camera lens, literally as hardware, simulates the eye, which is literally wetware. so does software, as computer graphics, simulate hardware. The optical laws of reflection and refraction remain in effect for output devices such as monitors or LCD screens, but the program whose data directs these devices transposes such optical laws as it obeys into algebraically pure logic. These laws are generally, it should be noted from the outset, by no means all the optical laws valid for fields of vision and surfaces, shadows and effects of light; what is played out are these selected laws themselves and not, as in the optical media, just the effects they produce. It's no wonder, then, that art historian Michael Baxandall can go so far as to suggest that computer graphics provide the logical space of which any given perspective painting forms a more or less rich subset.
Conversely, computer graphics, because it is software, consists of algorithms and only of algorithms. The optimal algorithm for automatic image synthesis can be determined just as easily as non-algorithmic image synthesis. It would merely have to calculate all optical, i.e. electromagnetic, equivalencies that quantum electrodynamics recognizes for measurable spaces, for virtual spaces as well; or, to put it more simply, it would have to convert Richard Feynman's three-volume Lectures on Physics into software. Then a cat's fur, because it creates anisotropic surfaces, would shimmer like cat's fur; then streaks in a wine glass, because they change their refraction index at each point, would turn the lights and things behind them into complete color spectra. Theoretically, nothing stands in the way of such miracles. Universal discrete machines, which is to say, computers, can do anything so long as it is programmable. But it is not just in Rilke's Malte Laurids Brigge but also in quantum electrodynamics that "realities are slow and indescribably detailed."7 The perfect optics could be programmed just barely within a finite time, but, because of infinite monitor waiting times, would have to put off rendering the perfect image. Computer graphics are differentiated from the cheap real-time effects of the visual entertainment media by a capacity to waste time that would rival that of good old painters if its users were just more patient. It is only in the name of impatience that all existing computer graphics are based on idealizations-a term that functions here, unlike in philosophy, as a pejorative.

In all historical accuracy I shall begin with raytracing, if only because it, for the best or worst reasons in the world, is much older than the radiosity algorithm. As Axel Roch will soon make public, the concept of raytracing derives not at all from computer graphics, but rather from its military predecessor: the tracking of enemy airplanes with radar. And as the computer graphics expert Alan Watt has recently shown, raytracing is in fact even more venerable. The first light ray whose refraction and reflections generated a virtual image was constructed in the year of our Lord 1637 by a certain Rene Descartes.9 Eighteen years earlier, in the wartime of November 1619, Descartes had received one illumination and three dreams. The illumination was about a wondrous science-perhaps the analytic geometry he would go on to develop later. The dreams, however, began with a storm that spun Descartes, who was lame on his right side, around his own left leg three or four times. I suspect, however, that the dream and the science are one and the same. In the dream the subject becomes an unextendable point or, better, midpoint, around which one's own body, as a three-dimensional res extensa, describes the geometric figure of a circle. Cartesian philosophy, as is well known, deals with the res cogitans and the res extensa; as is far less well known, analytic geometry deals with algebraically describable movements or surface areas. Descartes made it possible, for the first time in the history of mathematics, not to produce figures like the circle as the drawn likeness of a celestial-geometrical given but rather to construct them as functions of an algebraic variable. The subject as res cogitans took a wild ride, so to speak, through all the functional values of an equation, until in Descartes's initial dream of 1619 the circle (or, in Miinchhausen's ride on the cannonball, the parabola) was described.

To be sure, Heron of Alexandria had already formulated the law of reflection, Willibrord Snell the law of refraction. It remained to Descartes, however, to piece together the path of a single ray of light through the repeated application of both laws. The Cartesian subject comes about through self-application, or, to put it in the terms of computer science, through recursion. Precisely for this reason, Cartesian raytracing never inspired any painter, let alone any optical analog medium. Only computers and, more precisely, computer languages that allow for recursive functions have the processing power to even trace the countless alternative cases or fates of a single light ray in a virtual space full of virtual surfaces. Raytracing programs begin, in the most elementary case, by defining the computer screen as a two-dimensional window onto a virtual three-dimensionality. Then, two iteration loops follow all the lines and columns of this screen until the ray of vision of a virtual eye situated in front of the screen has reached all the pixels. These virtual rays, though, keep wandering behind the pixels in order to explore the various different outcomes. Most of these have the fortune not to collide with a surface, and thus can quickly execute their task of rendering a mere background color such as that of the sky. Other rays, however, find themselves trapped in a transparent glass globe like Descartes's, where they would be subject to an endless series of refractions and reflections if the impatience of computer graphics programs did not limit the maximum allowable recursions. This is necessary if only because a light ray, should it play between two parallel and perfect mirrors, would never stop, while algorithms are all but defined by a finite use of time.

Radiosity is consequently, in contrast to raytracing, an algorithm born of necessity. Only when seen in its formal elegance can integration be defined as the reverse function of differentiation, for the bitter empirical and numerical truth is that it consumes dramatically higher processing time. Radiosity programs have only become feasible since they have stopped promising to solve their linear equation system in a single run-through.1 In more prosaic terms: one starts up the algorithm, contemplates the as yet completely black screen, takes one of the coffee breaks so famous among computer programmers, then returns after one or two hours to have a look at the first passable results of the global light energy distribution. What so-called nature can accomplish in nanoseconds with its parallel calculation drives its alleged digital equivalent to overload.

Had I promised mere recipes instead of a semi-technical introduction to computer graphics, this short text could end here. Fans of interiors would download some radiosity programs, while fans of the open horizon would surf the Net for some raytracing programs. And now that, at least with LINUX, we have the Blue Moon Rendering Tools, the very decision has become moot. This software, no less wondrous than a blue moon, calculates virtual image worlds in the first run-through following global dependencies in the sense of radiosity, but in the second runthrough follows local singularities in the sense of raytracing. It thus promises a coincidentia oppositorum, which cannot be a matter of simple addition given all that has been said above. It would be going too far afield if I were to try to explain why, in the case of such two-step processes, not only the second step must orient itself to the first but, what is nearly impossible, the first must already orient itself to the second. Otherwise, the four possible cases of optical energy transmission couldn't possibly all be taken into consideration. 

The partnership between computers and the visual arts is now at a singular point in its development, emerging from a state of infancy into a first level of maturity. Many artists have used the new technology to enhance or facilitate work on lines established by other media, primarily paint and photographic film. Many are working away from these preexisting genres into modes that could only be created with computers. Though it's always fun to speculate, it's impossible to say with even a slight bit of certainty what this new alliance between art and technics may bring. We may see massive changes in all the arts, perhaps coming on us so rapidly that we won't know what hit us. Perhaps the alliance will result primarily in techniques that will allow artists to do what they'd be doing anyway, but with greater ease and speed. We may see bio chips and neural interfaces allowing us to experience all art simultaneously and internally, and take it from there to wherever our own personal capabilities allow. Or maybe we'll just see a few good pieces and some snappier special effects in the movies. Whatever the case, a lot is going on right now, and it would be a shame to miss out on the marvelous advent of computer art — this coming of age will not happen again.

Cynthia Goodman's Digital Visions is an excellent survey of the state of the art at the time of publication. Despite the rapid changes in computer technology, this book will probably be the best survey available for several years and remain a landmark after it has been superseded. The book includes about 150 samples of computer related art, reproduced as well as images often meant to be seen on a different scale or in a different context or illuminated from behind can be printed in an affordable edition. Goodman's commentary is just what a survey should be: descriptive, impersonal, nonjudgemental and pluralistic. Her documentation is sufficiently detailed in her listing of hardware and software used in samples to satisfy those who are knowledgeable, but her commentaries are free from the technical argot that would make it difficult reading for those unfamiliar with computers.

One of the fascinating phenomena of the present state of the alliance is the way that computers can be used to make standard functions easier and quicker. Using a keyboard or any one of a number of input devices, including light pens that can be used directly on a computer terminal, an artist can create a basic design, save the original on a magnetic disk, and then rework it, changing existing forms, adding new ones, deleting others, and shifting color around. If one color doesn't work, it can be dropped and substituted by another by pressing a few keys. At the present state of the art, this need not produce the clunky images and lifeless colors often associated with computers -- resolutions so fine that disjunctions are imperceptible to the human eye allow a delicacy of shading fully comparable with anything a brush can achieve; and with a palette of some sixteen million colors (about all a human eye can discern) available on some of the most powerful units, it could be argued that computers offer more color options than any other medium. At present, some artists use this sort of technique as a means of making sketches for work to be completed in other media. Others print out their work directly from the images composed on their computer monitors.

In many cases the results are so much like easel paintings or photographs that the use of the computer seems comic, a great hooplah made over nothing. Used in this mimetic way, the value of computers can only be assessed by the artists using them. With the advent of inexpensive micro-computers we can assume that more artists will try these convenience functions and accept or reject them. If this usage becomes common practice, it probably won't make much difference to viewers -- it will simply become part of the professional bag of tricks. The majority of the works in Goodman's book use techniques of this sort. Whether the works are interesting or not, the many elaborate techniques are fascinating and, again, now is the time to be enthralled by them — the magic won't last.

Emerging from these convenience functions are some interesting shifts that move away from computer assistance to possibilities unattainable with traditional techniques. Perhaps the most promising is a shift from printing out the final work to creating art meant to be seen on computer terminals or other illuminated devices. These works, seen by radiant rather than reflected light may be the stained glass windows of a future age.

Among the artists who've gone beyond the level of simple convenience, I'd like to bring special attention to two who represent computer art's first level of maturity. Their work goes in different directions, suggesting the versatility of computer usage. In both we see a strong basis in techniques and aesthetics that have nothing to do with computers, and at the same time move the state of computer art beyond simple housekeeping.

Manfred Mohr has for some fifteen years been exploring the possibilities of restructuring the twelve sides of the most basic of forms, the cube, in two dimensional, black and white images. Mohr begins by designing a non-visual program based on algorithms (calculations with cyclic regularities) which are transformed into signs by the computer. Mohr then reworks the signs to his satisfaction and has a plotter (a computer driven drawing device) produce the final image on canvas or paper. The result is a large opus of dynamic images and sequences that can be read as narrative or analyzed by semiotic method. Both the program and the plotter put some distance between Mohr and the finished work, allowing geometry, mathematics, and chance to play an independent role in the work, and minimizing personal or idiosyncratic elements. Mohr's art seems to have raised Constructivism to a level unattainable by his predecessors, Malevich and Mondrian.

Harold Cohen has designed an artificial intelligence program called AARON and he has been able to teach this program to draw clearly legible human figures, plant forms, and other objects, as well as clearly conceived abstractions. AARON produces lively, fluid, energetic drawings with much of the expressiveness you would expect from an artist coming out of a tradition that emphasizes human individualism and prizes natural mysticism. The program has a capacity to learn, it is not simply repeating preexisting drawings but making drawings that could not have been anticipated by Cohen when he wrote or refined the program. Cohen's interactions with AARON occur on several levels: he refines his program as he goes along, taking cues and challenges from what the program has accomplished. AARON is limited to monochrome productions and Cohen often radically alters the program's drawings by adding color. The artificial intelligence of this program is a far cry from the advanced sort of A.I. that technocrats and sci fi buffs forecast, but here we have the first real example of man and computer communicating and interacting constructively, producing art that goes beyond simple mechanical gimicry.

Conventional wisdom has it that computers are inherently dehumanizing devices, the product of mad scientists working in isolation, unaware that their machines are foisting their alienation and solipsism on everyone else. That's more a product of the movies than of computers. Though a new generation of artists turned hackers and scientists turned artist is now emerging, most computer artists have had to form alliances with the scientists who are often perceived as their polar opposites in temperament and personality — and often enough both have had to use equipment owned by great corporate beasts like IBM, Phillips and the pentagon. This collaboration sometimes functions on an intimate level: many computer art producers are married couples or lovers working in tandem, as often as not initially brought together by their need to share skills. (Maybe we could think of this as a form of computer dating that actually works!) Among producers, the computer has encouraged community rather than alienation, perhaps beginning to exorcise the "two cultures" boogie man still seen by many as part of our collective schizophrenia.

Perhaps participatory works will also bring viewers together and encourage community. In popular culture, their cognates are already doing so -- how many kids have met each other for the first time in video arcades since you started reading this article? On the level of self conscious art, computers tend to encourage participatory work, and it's my hunch that computer art will most distinguish itself in this area. A few of the many examples in Goodman's book illustrate directions in which this trend is going.

Wen-Ying Tsai's compositions of moving fiberglass rods illuminated by strobes are simple and elegant examples. Audio feedback devices speed up or slow down the movement of the strobes in response to sounds made by people around them, moving slowly when the environment is quiet, frantically when it is noisy. People around these pieces can control the apparent movement of the rods by making noises ranging from whispers to speech to laughter to clapping, or the units may simply reflect the sounds of people who are not trying to interact with the sculptures.

In the collaborations between Otto Piene and Paul Earls, the frequencies of Earls's electronic music guide the images of Piene's computer drawing program. The images are created by a laser which can project them in all sorts of environments, including projections into the sky, where their three dimensional quality takes on the character of constantly changing monumental sculpture. In work like this, what you see could only be created by computer and laser. The maximum so far attained in collaboration between media and artists is in dance performances such as Phosphones, which use the CORTLI system designed by computer sculptor James Seawright, electronic music composer Emmanuel Ghent, programer William Hemsath, and choreographer Mimi Garrard. This system presents complex interactions between music and lighting, which in turn interact with the movements of the dancers in Ms. Garrard's company. This is just a few steps away from a total art form in which everyone dances and the audience and the work are reintegrated. And it's not far from massive works in which thousands of people participate, and a final "product" is never achieved or desired.

The basis of the partnership between computers and the arts is a human partnership. How much it can grow through its interaction with the nonhuman may be a partial test of its value. But ultimately this new technology will be a test of our cooperative and conceptual capacities and of our imagination and courage.

This volume of essays is the happy result of contacts and collaborations established during the three years devoted to the preparation of 'Cybernetic Serendipity'. Cybernetic Serendipity was an exhibition mounted at the Institute of Contemporary Arts in the summer of 1968, which dealt with the relationship of the computer and the arts. The exhibition, like this book, was concerned with the exploration and demonstration of connexions between creativity and technology (and cybernetics in particular), the links between scientific or mathematical approaches, intuitions, and the more irrational and oblique urges associated with the making of music, art and poetry. The title itself was intended to convey the fact that through the use of cybernetic devices we have made many fortunate discoveries for the arts. The exhibition Cybernetic Serendipity was mounted in a gallery of 6500 square feet, involved 325 participants and was seen by 60,000 people. The exhibits showed how man can use the computer and new technology to extend his creativity and inventiveness. These consisted of computer graphics, computercomposed and -played music, computer-animated films, computertexts, and among other computergenerated material, the first computer sculpture. There were also cybernetic machines such as Gordon Pask's 'colloquy of mobiles', television sets converting sound into visual patterns, Peter Zinovieff's electronic music studio with a computer which improvised on tunes whistled into a microphone by the visitors; there were robots, drawing machines and numerous constructions which responded to ambient sound and light. Six IBM machines demonstrated the uses of computers, and a visual display provided information on the history of cybernetics. Two aspects of this whole project are particularly significant. The first is that at no point was it clear to any of the visitors walking around the exhibition, which of the various drawings, objects and machines were made by artists and which were made by engineers; or, whether the photographic blow-ups of texts mounted on the walls were the work of poets or scientists. There was nothing intrinsic in the works themselves to provide information as to who made them. Among the contributors to the exhibition there were forty-three composers, artists and poets, and eighty-seven engineers, doctors, computer systems designers and philosophers. The second significant fact is that whereas new media inevitably contribute to the changing forms of the arts, it is unprecedented that a new tool should bring in its wake new people to become involved in creative activity, whether composing music, painting or writing. Graphic plotters, cathode-ray tube displays and teleprinters have enabled engineers, and others, who would never even have thought of putting pen to paper, to make images for the sheer pleasure of seeing them materialize. Many of the computer graphics made by engineers in Europe, Japan and the USA, approximate very closely to what we have learned to call art and put in our public galleries. This raises a very real question - should these computer graphics hang side by side with drawings by artists in museums and art galleries, or should they belong to another, as yet unspecified, category of creative achievement? There are certain classifications to which we are all assigned according to what we do. These categories which relate solely to our work, or our professional titles, inform the outside world about our way of life, our abilities and creative propensities. The deductions based on these classifications are not necessarily accurate but they suffice to colour the picture of an individual sufficiently for him to be irrevocably labelled. These labels provide information which is accepted without question and without protest. Thus it is assumed that the electronic engineers represent a clever but an uncreative branch of society, whereas artists are exceptionally creative but it is unlikely that they should possess any technological skills. It is also widely assumed that to the engineer, scientist and mathematician, art is magic, and to the composer, painter and poet, technology is a mystery. These rough assumptions are very broadly true but not altogether true. Since the middle 1950s the relationship between art and technology has been increasingly in evidence through the advent of computer-aided creative design. Today these categorical assumptions about our various talents, functions and possibilities are less accurate than ever. Thus Cybernetic Serendipity was not an art exhibition as such, nor a technological fun fair, nor a programmatic manifesto - it was primarily a demonstration of contemporary ideas, acts and objects, linking cybernetics and the creative process.
The question what is life, says Norman O. Brown, turns out to be the question what is sleep. We perceive that the sky exists only on earth. Evolution and human nature are mutually exclusive concepts. We're in transition from the Industrial Age to the Cybernetic Age, characterized by many as the post-Industrial Age. But I've found the term Paleocybernetic valuable as a conceptual tool with which to grasp the significance of our present environment: combining the primitive potential associated with Paleolithic and the transcendental integrities of "practical utopianism" associated with Cybernetic. So I call it the Paleocybernetic Age: an image of a hairy, buckskinned, barefooted atomic physicist with a brain full of mescaline and logarithms, working out the heuristics of computer-generated holograms or krypton laser interferometry. It's the dawn of man: for the first time in history we'll soon be free enough to discover who we are.

Radical Evolution and Future Shock
in the Paleocybernetic Age It is perhaps not coincidental that Western youth has discovered the
I Ching, or Book of Changes, on a somewhat popular level as wemove into the final third of the twentieth century. Change is now ouronly constant, a global institution. The human ecological biosphere isundergoing its second great transition, destined to be even moreprofound than the invention of agriculture in the Neolithic Age. If wecan't see the change, at least we can feel it. Future shock affects ourpsyche and our economy just as culture shock disorients the PeaceCorps worker in Borneo.It is said that we are living in a period of revolution. But nothingsells like freedom: Revolution is big business. As the physicist P. W.Bridgman once said, the true meaning of a term is found byobserving what a man does with it, not what he says about it. Sincethe phenomenon we call revolution is worldwide, and since it's felt inevery human experience, perhaps we might think of it not asrevolution but as radical evolution. Revolution is basically the samewhether defined by Marx or the I Ching: removal of the antiquated.But revolution replaces one status quo with another. Radicalevolution is never static; it's a perpetual state of polarization. Wecould think of it as involuntary revolution, but whatever terminologywe apply that's the condition of the world today, the environment withwhich the artist must work. Radical evolution would be kinder if itwere better understood; but it won't be so long as commercialentertainment cinema continues to representa "reality" that doesn'texist.Sociologist Alvin Toffler has stressed ephemerality as a chiefaspect of radical evolution: "Smith Brothers Cough Drops, CalumetBaking Soda, Ivory Soap, have become institutions by virtue of theirlong reign in the marketplace. In the days ahead, few products willenjoy such longevity. Corporations may create new productsknowing full well they'll remain on the market for only a matter of afew weeks or months. By extension, the corporations themselves—as well as unions, government agencies and all other organizations—may either have shorter life-spans or be forced to undergoincessant and radical reorganization. Rapid decay and regenerationwill be the watchwords of tomorrow."8 Toffler observes that noreasonable man should plan his life beyond ten years; even that, hesays, is risky. When parents speak of their sons becoming lawyersthey are deceiving themselves and their sons, according to thesociologist, "Because we have no conception of what being a lawyerwill mean twenty years hence. Most probably, lawyers will becomputers." In fact, we can't be sure that some occupations willeven exist when our children come of age. For example, thecomputer programmer, a job first created in the 1950's, will be asobsolete as the blacksmith within a decade; computers will reprogram and even regenerate themselves (IBM recently announceda new computer that repairs itself).John McHale, coauthor of the World Design Science Decadedocuments with Buckminster Fuller, emphasizes expendability andimpermanence in radical evolution: "Use value is replacingownership value. For example, the growth of rental and services—not only in automobiles and houses, but from skisto bridal gowns toheirloom silver, castles and works of art... our personal and household objects, when destroyed physically or outmoded symbolically,may be replaced by others exactly similar. A paper napkin, a suit, achair, an automobile, are items with identical replacement value.Metals in a cigarette lighter today may be, within a month or year,part of an auto, lipstick case or orbiting satellite... the concept ofpermanence in no way enables one to relate adequately to ourpresent situation." 9McHale has seen the need for a totally new world view as radicalevolution speeds farther from our grasp. "There's a mythologyabroad which equates the discovery and publication of newfacts with new knowledge. Knowledge is not simply accumulated facts butthe reduction of unrelated and often apparently irrelevant facts intonew conceptual wholes."10 He's talking about completely new ways of looking at the world and everything in it. This is proposition farmore profound than mere political revolution, which Krishnamurti hascharacterized as "The modification of the right according to the ideasof the left.''11 The new consciousness transcends both right and left.We must redefine everything.What happens to our definition of "intelligence" when computers,as an extension of the human brain, are the same size, weight, andcost as transistor radios? They're being developed through themicroelectronics process of Large-Scale Integration.What happens to our definition of "morality" when biochemists areabout to unravel the secrets of the DNA/RNA interaction mechanismto create human life?What happens to our definition of "man" when our next doorneighbor is a cyborg (a human with inorganic parts)? There areseveral crude cyborgs in the world today.What happens to our definition of "environment" when our videoextensions bring us the reality of the solar system daily? What do wemean by "nature" under these circumstances? (McLuhan: "The firstsatellite ended nature in the conventional sense.")What happens to our definition of "creativity" when a computerasks itself an original question without being programmed to do so?This has occurred several times.What happens to our definition of "family" when the intermedianetwork brings the behavior of the world into our home, and whenwe can be anywhere in the world in a few hours?What happens to our definition of "progress" when, according toLouis Pauwels: "For the really attentive observer the problems facingcontemporary intelligence are no longer problems of progress. Theconcept of progress has been dead for some years now. Today it is aquestion of a change of state, a transmutation.''12 Or Norbert Wiener:"Simple faith in progress is not a conviction belonging to strength butone belonging to acquiescence and hence to weakness.'' What happens to our definitions of "material" and "spiritual" whenscience has found no boundary between the two? Although it is stillpopularly assumed that the world is divided into animate andinanimate phenomena, virologists working at the supposed thresholdbetween life and nonlife at the virus level have in fact discovered nosuch boundary. "Both animate and inanimate have persisted rightacross yesterday's supposed threshold in both directions...subsequently what was animate has become foggier and foggier...no life, per se, has been isolated.''14 Indeed, what becomes of "reality" itself as science expands itsmastery of the forces of the universe? "The paradox of twentiethcentury science consists of its unreality in terms of sense impressions. Dealing as it does in energy transformation and submicroscopic particles, it has become a kind of metaphysics practiced by adevoted priestly cult—totally as divorced from the common-sensenotions of reality as was the metaphysics practiced by witch doctorsand alchemists. It is not at all odd, then, to discover that the closerwe come via the scientific method to 'truth,' the closer we come tounderstanding the 'truth' symbolized in myths.''15 This, then, is merely a superficial glimpse at some of the phenomena that characterize the Paleocybernetic Age. Quite clearly manis in the paradoxical position of existing in a state of consciousnesswithout being able to understand it. Man does not comprehend hisrelationship to the universe, either physical or metaphysical. Heinsists on "doing his thing" without the slightest notion of what his"thing" might be. This cosmic credibility gap exists primarily betweenthe facts of scientific experience and the illusions of environmentalconditioning as manifested in the global intermedia network.
The Intermedia Network as Nature The point I wish to make here is obvious yet vital to anunderstanding of the function of art in the environment, even thoughit is consistently ignored by the majority of film critics. It's the ideathat man is conditioned by his environment and that "enviromnent"for contemporary man is the intermedia network. We are conditionedmore by cinema and television than by nature. Once we've agreedupon this, it becomes immediately obvious that the structure andcontent of popular cinema is a matter of cardinal importance, at leastas serious as most political issues, and thus calls for comment notfrom journalists but from those who work at the matter, artiststhemselves.The cinema isn't just something inside the environment; theintermedia network of cinema, television, radio, magazines, books,and newspapers is our environment, a service environment thatcarries the messages of the social organism. It establishes meaningin life, creates mediating channels between man and man, man andsociety. "In earlier periods such traditional meaning and valuecommunication was carried mainly in the fine and folk arts. But todaythese are subsumed amongst many communicating modes. Theterm 'arts' requires expansion to include those advanced technological media which are neither fine nor folk.''16 We've seen the need for new concepts regarding the nature ofexistence; yet concepts are expanded or constricted in direct relationto the relevancy of prevailing languages. In a world where change isthe only constant, it's obvious we can't afford to rely on traditionalcinematic language. The world has changed immeasurably in theseventy years since the birth of cinema: for one thing "world" nowincludes the microcosm of the atom and the macrocosm of the universe in one spectrum. Still popular films speak a languagedeveloped by Griffith, Lumière, Méliès, derived from traditions ofvaudeville and literature.In the Agricultural Age man was totally passive, conditioned andvictimized by the environment. In the Industrial Age man's role was participatory; he became more aggressive and successful in hisattempts to control his environment. We're now moving into theCybernetic Age in which man learns that to control his environmenthe must cooperate with it; he not only participates but actuallyrecreates his environment both physical and metaphysical, and inturn is conditioned by it.To be free of the toil of old relationships we must first be free of theconditioning that instills it within us. As radical evolution gainsmomentum the need to unlearn our past becomes increasingly clear:contemporary life is a process of miseducation/uneducation/reeducation, at a cost of much precious time. McLuhan has noted thatthe true significance of Pavlov's experiments was that any controlledman-made environment is a conditioner that creates "non-perceptivesomnambulists." Since then science has discovered that "molecularmemory" is operative in single-celled and some multi-celledorganisms, and there's evidence that memory-in-the-flesh exists inhumans as well. Biochemists have proven that learned responses toenvironmental stimuli are passed on phylogenetically fromgeneration to generation, encoded in the RNA of the organism'sphysical molecular structure.17 And what could be a more powerfulconditioning force than the intermedia network, which functions toestablish meaning in life?Science has proven that there's no such thing as "human nature."Just as water takes the shape of its container, so human nature isrelative to its past and present conditioning. Optimum freedom ofbehavior and increased self-awareness are implicit in the industrialequation that is trending toward physical success for all men;Paleocybernetic man, however, has not learned to control the environment he creates. "The content of what is available for emulationon the part of the young in each society is itself culturally shapedand limited... the individual typically remains, throughout his lifetime,unaware of how his own habits, which to him appear 'only natural,' infact result from a learning process in which he never had anopportunity to attempt alternative responses.''18 This process is fortunate to have a tool that makes him awareof his own enculturation and thus he enjoys greater psychic freedomthan his ancestors. This tool is what Teilhard de Chardin has calledthe noosphere, the film of organized intelligence that encircles theplanet, superposed on the living layer of the biosphere and thelifeless layer of inorganic material, the lithosphere. The minds ofthree-and-a-half-billion humans—twenty-five percent of all humanswho ever lived—currently nourish the noosphere; distributed aroundthe globe by the intermedia network, it becomes a new "technology"that may prove to be one of the most powerful tools in man's history.John McHale: "World communications... diffuse and interpenetratelocal cultural tradition, providing commonly-shared culturalexperience in a manner unparalleled in human history. Within thisglobal network the related media share and transmit man's symbolicneeds and their expression on a world scale. Besides theenlargement of the physical world, these media virtually extend ourpsychical environment, providing a constant stream of moving,fleeting images of the world for our daily appraisal. They provide
psychic mobility for the greater mass of our citizens. Through thesedevices we can telescope time, move through history, and span theworld in a great variety of unprecedented ways.'' 19Like all energy sources the noosphere can be used for negativepurposes. Its resources can be manipulated to disguise craft ascreativity, especially in these Paleocybernetic days when we're stillimpressed by the sudden influx of information. Fuller hasdifferentiated craft from industry by demonstrating that craft isinherently local in technique and effect whereas industry is inherentlycomprehensive and universal in technique and effect. One mightmake a similar analogy between entertainment and art: entertainment is inherently "local," that is, of limited significance, whereasart is inherently universal and of unlimited significance. Too oftentoday we find that so-called artists working in the intermedia network are little more than adroit imitators, collectors of data andphenomena, which they glean from the noosphere and amalgamateinto packages that are far from whole. They're clever and glib;they've made an art of selling themselves, but they know only effect,not cause; they are merchants of mannerisms.It is precisely this confusion that clouds critical appraisal of"content" in the popular arts. All too frequently eclectic thinking isconfused with creative thinking. The distinction is subtle to be sure:integrative thinking can be the highest form of creativity. Indeed bothart and science function to reveal similarities within an a prioriuniverse of apparent dissimilarities. As with all else, however, there'san art and a craft to thinking, and the popular entertainments remainat the craft level by the very nature of their purpose.The intermedia network has made all of us artists by proxy. Adecade of television-watching is equal to a comprehensive course indramatic acting, writing, and filming. Compressed in such constantand massive dosage, we begin to see the methods and clichés moreclearly; the mystique is gone—we could almost do it ourselves.Unfortunately too many of us do just that: hence the glut of submediocre talent in the entertainment industry. Paradoxically thisphenomenon carries with it the potential of finally liberating cinemafrom its umbilical to theatre and literature, since it forces the moviesto expand into ever more complex areas of language andexperience. Evidence of television's effect on the cinema is alreadyapparent, as we shall see in our discussion of synaesthetic cinema.From another more immediate perspective, however, it is quiteunfortunate. We live in an age of hyperawareness, our sensesextended around the globe, but it's a case of aesthetic overload: ourtechnological zeal has outstripped our psychic capacity to cope withthe influx of information. We are adrift on the surface of radicalevolution unable to plumb the depths of its swift and turbulentcurrent.
The current generation is engaged in an unprecedented questioningof all that has been held essential. We question traditional conceptsof authority, ownership, justice, love, sex, freedom, politics, eventradition itself. But it's significant that we don't question ourentertainment. The disenfranchised young man who dropped out ofcollege, burned his draft card, braids his hair, smokes pot, and digsDylan is standing in line with his girl, who takes the pill, waiting tosee The Graduate or Bonnie and Clyde or Easy Rider— and they'rereacting to the same formulas of conditioned response that lulledtheir parents to sleep in the 1930's.We've seen the urgent need for an expanded cinematic language. Ihope to illustrate that profit-motivated commercial entertainment, byits very nature, cannot supply this new vision. Commercialentertainment works against art, exploits the alienation and boredomof the public, by perpetuating a system of conditioned response toformulas. Commercial entertainment not only isn't creative, it actuallydestroys the audience's ability to appreciate and participate in thecreative process. The implications become apparent when werealize that, as leisure time increases, each human will be forced tobecome a creative, self-sufficient, empirical energy laboratory.D. H. Lawrence has written: "The business of art is to reveal therelation between man and his circumambient universe at this livingmoment. As mankind is always struggling in the toil of oldrelationships, art is always ahead of its 'times,' which themselves arealways far in the rear of the living present." Jean-Jacques Lebelstated the same idea in different terms when he described art as "thecreation of a new world, never seen before, imperceptibly gaining onreality."
We've seen that man is conditioned by, and reacts to, certainstimuli in the man-made environment. The commercial entertainer isa manipulator of these stimuli. If he employs a certain triggermechanism, we're guaranteed to react accordingly, like puppets,providing he manipulates the trigger properly. I'm not saying theartist doesn't resort to audience manipulation; we know he oftendoes. The point, however, is the motivation in doing so. If the artistmust resort to trigger mechanisms to make himself clear, he will; butit's only a means to his end. In the case of the commercialentertainer, however, it's the end in itself.Plot, story, and what commonly is known as "drama" are thedevices that enable the commercial entertainer to manipulate hisaudience. The very act of this manipulation, gratifying conditionedneeds, is what the films actually are about. The viewer purchases itwith his ticket and is understandably annoyed if the film asks him tomanipulate himself, to engage in the creative process along with theartist. Our word poetry derives from the Greek root poiein meaning"to make" or "to work." The viewer of commercial entertainmentcinema does not want to work; he wants to be an object, to be actedupon, to be manipulated. The true subject of commercialentertainment is this little game it plays with its audience.By perpetuating a destructive habit of unthinking response toformulas, by forcing us to rely ever more frequently on memory, thecommercial entertainer encourages an unthinking response to dailylife, inhibiting self-awareness. Driven by the profit motive, thecommercial entertainer dares not risk alienating us by attemptingnew language even if he were capable of it. He seeks only to gratifypreconditioned needs for formula stimulus. He offers nothing wehaven't already conceived, nothing we don't already expect. Artexplains; entertainment exploits. Art is freedom from the conditionsof memory; entertainment is conditional on a present that isconditioned by the past. Entertainment gives us what we want; artgives us what we don't know we want. To confront a work of art is toconfront oneself—but aspects of oneself previously unrecognized.The extent to which blatant audience manipulation not only istolerated but extolled is alarming. Alfred Hitchcock, for example, inhis interview with François Truffaut, finds merit in his ability tomanipulate preconditioned needs for formula stimulus. Speaking of
Psycho, Hitchcock frankly admits: "It wasn't a message that stirred them, nor was it a great performance, or their enjoyment of thenovel... they were aroused by the construction of the story, and theway in which it was told caused audiences all over the world to reactand become emotional.''21 It is essential to understand that Hitchcock openly admits that hedidn't even try to expand awareness or to communicate somesignificant message, but only exploited a universal tradition ofdramatic manipulation in order to supply his audience with thegratification it paid for. The audience sees itself and its dreamsreflected in the film and reacts according to memory, whichKrishnamurti has characterized as being always conditioned."Memory," says Krishnamurti, "is always in the past and is given lifein the present by a challenge. Memory has no life in itself; it comesto life in the challenge [preconditioned formula stimulus]. And allmemory, whether dormant or active, is conditioned."22 It is thisprocess that the entertainment industry calls audience identification.To a healthy mind, anything that is primarily art is also immenselyentertaining. It seems obvious that the most important things shouldbe the most entertaining. Where there's a difference between whatwe "like" and what we know to be vital, we have a condition ofschizophrenia, an unnatural and destructive situation. I speakdeliberately of a "healthy" mind as one capable of creative thinking.Filmmaker Ken Kelman: "The old cinema removes experience,making us see things along with (or through) a protagonist withwhom we identify, and a plot in which we are caught. Such anapproach tends toward not only a lack of viewpoint, of definition of
whose experience it is, but also filters the power of sight into merehabit, dissolves insight into vicariousness. The spectator is reducedto a voyeur—which is, increasingly, the individual's role in society atlarge."23 Minimalist painter David Lee: "When people do not trust theirsenses they lack confidence in themselves. For the last fewcenturies people have lacked confidence. They have not trusted their experience to provide a standard for knowing how to act." 24 It isquite obvious that most of us not only don't know much about art, wedon't even know what we like. Krishnamurti: "One of the fundamentalcauses of the disintegration of society is copying, which is theworship of authority."25 Imitation is the result of inadequate information. Information resultsin change. Change requires energy. Energy is the result of adequateinformation Energy is directly proportional to the amount ofinformation about the structure of a system. Norbert Wiener: "Information is a name for the content of what is exchanged with theouter world as we adjust to it and make our adjustment felt upon it …to live effectively is to live with adequate information."26 From thecinema we receive conceptual information (ideas) and designinformation (experiences). In concert they become one phenomenon, which I've described as the experiential information of aestheticconceptual design. This information is either useful (additive) or redundant. Useful information accelerates change. Redundant information restricts change. If sustained long enough redundant information finally becomes misinformation, which results in negativechange.In communication theory and the laws of thermodynamics thequantity called entropy is the amount of energy reversiblyexchanged from one system in the universe to another. Entropy alsois the measure of disorder within those systems. It measures thelack of information about the structure of the system. For ourpurposes "structure of the system" should be taken to mean "thehuman condition," the universal subject of aesthetic activity. Entropyshould be understood as the degree of our ignorance about thatcondition. Ignorance always increases when a system's messagesare redundant. Ignorance is not a state of limbo in which noinformation exists, but rather a state of increasing chaos due to
misinformation about the structure of the system.The First Law of Thermodynamics states that energy is constant: itcannot be created or destroyed; its form can change, but not its quantity.
The Second Law states that the amount of energy within a localsystem is naturally entropic—it tends toward disorder, dissipation,incoherence. And since energy is defined as "a capacity to rearrange elemental order," entropy, which runs counter to thatcapacity, means less potential for change. We've learned fromphysics that the only anti-entropic force in the universe, or what iscalled negentropy (negative entropy), results from the process offeedback. Feedback exists between systems that are not closed butrather open and contingent upon other systems. In the strictestsense there are no truly "closed" systems anywhere in the universe;all processes impinge upon and are affected by other processes insome way. However, for most practical purposes, it is enough to saythat a system is "closed" when entropy dominates the feedbackprocess, that is, when the measure of energy lost is greater than themeasure of energy gained.The phenomenon of man, or of biological life on earth taken as aprocess, is negentropic because its subsystems feed energy backinto one another and thus are self-enriching, regenerative. Thusenergy is wealth, and wealth according to Buckminster Fuller is "thenumber of forward days a given system is sustainable." BiologistJohn Bleibtreu arrived at a similar conclusion when he noted that theconcept of time can best be viewed as a function of the Second Lawof Thermodynamics—that the measure of entropy in a system is ameasure of its age, or the passage of time since the systemoriginated.27 In other words the degree of a system's entropy isequal to redundancy or stasis whereas its negentropy is equal tokinesis or change. So information becomes energy when itcontributes to the self-enriching omni-regenerative wealth of thesystem. When it's not contributing (i.e., redundant) it is allowing thenatural entropy to increase."It is possible to treat sets of messages as having an entropy likesets of states of the external world... in fact, it is possible to interpretthe information carried by a message as essentially the negative ofits entropy... that is, the more probable the message the lessinformation it gives. Clichés, for example, are less illuminating thangreat poems." 28 Thus the more information concerning the human condition that the artist is able to give us, the more energy we havewith which to modify ourselves and grow in accord with theaccelerating accelerations of the living present.Commercial entertainment may be considered a closed systemsince entropy dominates the feedback process. To satisfy the profitmotive the commercial entertainer must give the audience what itexpects, which is conditional on what it has been getting, which isconditional on what it previously received, ad infinitum. Inherent inthe term "genre," which applies to all entertainment, is that it must beprobable. The content of westerns, gangster movies, romances, etc.,is probable in that it can be identified and comprehended simply byclassification. The phenomenon of drama itself usually is notconsidered a genre, but is in fact the most universal and archetypicalof all genres. Drama, by definition, means conflict, which in turnmeans suspense. Suspense is requisite on the expectation of knownalternatives. One cannot expect the unknown. Thereforeexpectation, suspense, and drama are all redundant probablequalities and thus are noninformative.Drama requires a plot that forces the viewer to move from point Ato point B to point C along predetermined lines. Plot does not mean"story" (beginning-middle-end). It simply indicates a relatively closedstructure in which free association and conscious participation arerestricted. Since the viewer remains passive and is acted upon bythe experience rather than participating in it with volition, there's nofeedback, that vital source of negentropy. Norbert Wiener:"Feedback is a method of controlling a system by reinserting into itthe results of its past performance... if the information whichproceeds backward from the performance is able to change thegeneral method and pattern of performance, we have a processwhich may well be called learning."29 Fuller: "Every time man makesa new experiment he always learns more. He cannot learn less.”30 In the cinema, feedback is possible almost exclusively in what I callthe synaesthetic mode, which we'll discuss presently. Because it isentirely personal it rests on no identifiable plot and is not probable.The viewer is forced to create along with the film, to interpret forhimself what he is experiencing. If the information (either concept or design) reveals some previously unrecognized aspect of the viewer'srelation to the circumambient universe—or provides language withwhich to conceptualize old realities more effectively— the viewerrecreates that discovery along with the artist, thus feeding back intothe environment the existence of more creative potential, which mayin turn be used by the artist for messages of still greater eloquenceand perception. If the information is redundant, as it must be incommercial entertainment, nothing is learned and change becomesunlikely. The noted authority on communication theory, J. R. Pierce,has demonstrated that an increase in entropy means a decrease inthe ability to change. 31 And we have seen that the ability to changeis the most urgent need facing twentieth-century man.The notion of experimental art, therefore, is meaningless. All art isexperimental or it isn't art. Art is research, whereas entertainment isa game or conflict. We have learned from cybernetics that inresearch one's work is governed by one's strongest points, whereasin conflicts or games one's work is governed by its weakestmoments. We have defined the difference between art and entertainment in scientific terms and have found entertainment to beinherently entropic, opposed to change, and art to be inherentlynegentropic, a catalyst to change. The artist is always an anarchist,a revolutionary, a creator of new worlds imperceptibly gaining onreality. He can do this because we live in a cosmos in which there'salways something more to be seen. When finally we erase thedifference between art and entertainment—as we must to survive—we shall find that our community is no longer a community, and weshall begin to understand radical evolution.
The image I would offer as representative of the PaleocyberneticAge is that of the dying man whose life passes before him: a Retrospective Man who discovers the truth about himself too late to makeuse of it. The information explosion is not a window on the future somuch as a mirror of the past catching up with the present. Theintermedia network, or global communications grid, taps knowledgeresources that always have existed in discrete social enclavesaround the planet and saturates them into the collective consciousness. Suddenly the mass public "discovers" African culture,East Indian and American Indian cultures, folk music, politics.Knowledge previously the domain of scholars becomes commonknowledge, and precisely at that point when the old order is about tofade it sees itself clearly for the first time. William Burroughs hascalled it the Age of Total Confront, noting that all the heretoforeinvisible aspects of our condition have quite suddenly becomevisible.Through Duchamp, Cage, and Warhol, for example, we haverediscovered art in the ancient Platonic sense in which there's nodifference between the aesthetic and the mundane. Although thesemen certainly fulfill an avant-garde function in present society, theyin fact conform to the most universal and enduring definition of art. Ifthey've been rejected as artists by the majority of our citizens it'sbecause we've been conditioned by an economic system in whichaesthetic concerns must assume a secondary position if the systemis to survive. Thus art is separated from common experience and anelite hierarchy is established, which seems only natural to everyonecaught up in the economic struggle. John Dewey: "When art attainsclassic status it becomes isolated from the human conditions underwhich it was brought into being and from the human consequences itengenders in actual life experience... when, because of their remoteness, the objects acknowledged by the cultivated to be works offine art seem anemic to the mass of people, aesthetic hunger islikely to seek the cheap and the vulgar."32 Twentieth-century man is retrospective also because the symbolic and value content of his messages—most of which take the form ofcommercial entertainment—is predominantly redundant. NorbertWiener: "Society can only be understood through a study of themessages and the communication facilities which belong to it."33 Almost without exception, these messages tend to be concernedwith what is known as the "human condition." The history of popularentertainment, in terms of its conceptual content, can be divided intothree general categories: (1) idealization, which corresponds tostates of happiness in which life is seen as a heavenly experienceand man is characterized by his most noble deeds; (2)frustration, anexpression of the conflict between inner and outer realities, whenwhat is is not what should be; (3) demoralization, generallyexpressed as "the blues." In commercial entertainment cinema thesethree formulas are followed religiously, almost without exception, andusually comprise the nature of the message. They are the humancondition, that which is taken for granted, the given, the facts of life.Everyone has ideals, everyone is frustrated, everyone gets theblues. But this information is redundant; we must go on from there.Commercial entertainment is "popular" and not what we call artbecause it doesn't go on from there. To insure the widest possibleacceptance of his message, the commercial entertainer must speaka common language. He copies, repeats, or imitates that whichalready exists within the grasp of the so-called average man. Andthe majority of us embrace it because it offers security, a crutch, inthe knowledge that the miseries we suffer are shared by others. Butart transcends the human condition. The artist doesn't want to hearour problems and our dreams—he already knows them. Instead hewants to know what we're doing about them, and he gives us theinstruments we need for the task. The symbol is the basic instrumentof thought; those who create new symbols—artists, scientists, poets,philosophers—are those who, by giving us new instruments to thinkwith, give us new areas to explore in our thinking.A rather indignant woman once asked me how I could have thenerve to suggest that an "abstract" film like Brakhage's Dog Star
Man could be more important than an immortal classic like Renoir's  like Renoir's do not contain one single insight into the nature of thehuman condition that has not already been absorbed by the collective consciousness. Bob Dylan: "How many times must a man lookup before he can see the sky? How many ears must one man havebefore he can hear people cry?" And my own question: how manytimes must we acknowledge the human condition before it becomesredundant? How long must we tolerate the same facts of life beforewe begin seeking new facts? We intuit that the human condition hasexpanded since yesterday, but the popular arts aren't telling us. Thehuman condition does not stop with what we know about ourselves.Each genuinely new experience expands the definition of the humancondition that much more. Some are seeking those new facts, thosenew experiences, through the synaesthetic research of expandedcinema.Barbara Rose: "The new art... posits an entirely new world viewwhich shifts cultural values from a death-oriented, commemorative,past-enshrining culture to a life-oriented, present-oriented civilization... In this sense [Claes] Oldenburg's monuments represent, as hecontended, not the appearance of something, but its disappearance... the tomb, the memorial, the shrine, the monument, all belongto cultures that commemorate."34 John McHale: "The problem now is that those areas of our formaleducation which deal with the symbolic and value content of ourculture do so almost entirely in terms of the past35 ... The neweducational technologies are largely being used as twentieth-centurychannels to convey a conceptual context which is still nineteenthcentury or earlier. The most recent example was mathematics,where the Sputnik-inspired 'second look' revealed that mathematicsas generally taught was quite out of date. Science has begun to takea second look at its contents as currently taught. But the arts andhumanities remain relatively unaware of any need to revise theconceptual framework of studies little removed from the politeeducation of eighteenth-century gentry."
The entropy of commercial entertainment is the chaos that resultsfrom its retrospective nature, forever commemorating past events,historical figures, social eras, life-styles, or the memory of the viewer,while the living present speeds farther from our grasp. Alvin Toffler:"We offer children courses in history; why not also make a course in'future' a prerequisite for every student? A course in which thepossibilities and probabilities of the future are systematically explored exactly as we now explore the social system of the Romansor the rise of the feudal manor?
Our discussion obviously has excluded many important works of artthat function completely within the genres of drama, plot, and story.
Citizen Kane, L'Avventura, Pierrot le Fou, and 8½ are dramatic, plotfilms, yet no one denies their greatness. We know also that most ofthe truly significant films such as Beauty and the Beast or Pather
Panchali operate entirely within parameters of the human conditionas generally recognized. Moreover, common sense tells us that theartist must work with what exists, with the given, the humancondition; he could produce no art at all if he relied exclusively oninformation that is totally new.Yet the undeniable aesthetic value of these works does notcontradict what I have said about art and entertainment. These filmstranscend their genres. They are not important for their plots orstories but rather for their design. Susan Sontag: "If there is any'knowledge' to be gained through art, it is the experience of the formor style of knowing the subject, rather than a knowledge of thesubject itself."38 To perceive that the artist functions as design scientist we mustfirst understand that in their broadest implications art and scienceare the same. Eddington's classic definition of science, "The earnestattempt to set in order the facts of experience," corresponds withBronowski's view of science as "The organization of knowledge insuch a way that it commands more of the hidden potential innature…all science is the search for unity in hidden likenesses."39 It'sthe same in art: to set in order the facts of experience is to reveal therelation between man and his circumambient universe with all itshidden potential.Herbert Read: "Only in so far as the artist establishes symbols forthe representation of reality can mind, as a structure of thought, takeshape. The artist establishes these symbols by becoming conscious of new aspects of reality and by representing his consciousness inplastic or poetic form... it follows that any extension ofawareness ofreality, any groping beyond the threshold of present knowledge,must first establish its sensuous imagery." 40Our word "design" is composed of "de" and "sign," indicating that itmeans "to remove the symbol of." In this context "symbol" signifiesideas distinct from experiences. As design scientist the artistdiscovers and perfects language that corresponds more directly toexperience; he develops hardware that embodies its own softwareas a conceptual tool for coping with reality. He separates the imagefrom its official symbolic meaning and reveals its hidden potential, itsprocess, its actual reality, the experience of the thing. (A. N.Whitehead: "Process and existence pre-suppose each other.") Heestablishes certain parameters that define a discrete "special case"phenomenon, principle, or concept known as the subject. The work,in effect, poses this "problem" of perception and we as viewers mustdraw from this special case all the "general case" metaphysicalrelationships that are encoded within the language of the piece.This language is the experiential information of aestheticconceptual design; it is addressed to what Wittgenstein termed the"inarticulate conscious," the domain between the subconscious andthe conscious that can't be expressed in words but of which weconstantly are aware. The artist does not point out new facts somuch as he creates a new language of conceptual design information with which we arrive at a new and more completeunderstanding of old facts, thus expanding our control over theinterior and exterior environments.The auteur theory of personal cinema indicates those instanceswhen the filmmaker's design science transcends the parameters ofhis genre; our comprehension of that genre, that human condition isthus expanded. But cybernetics has demonstrated that the structureof a system is an index of the performance which may be expectedfrom it.41 That is, the conceptual design of a movie determines thevariety and amount of information we're likely to obtain from it. Andsince we've seen that the amount of information is directly proportional to the degree of available choices we can seethat drama, story, and plot, which restrict choice, also restrict information. So the auteur is limited to developing new designs for oldinformation, which we all know can be immensely enjoyable andinstructive. There are no "new" ideas in L'Avventura, for example,but Antonioni voiced the inarticulate conscious of an entiregeneration through the conceptual and structural integrity of histranscendental design science, merging sense and symbol, form andcontent.Rudolph Arnheim: "Perceiving achieves at the sensory level whatin the realm of reasoning is known as understanding... eyesight isinsight."42 If we realize that insight means to see intuitively, weacknowledge that Arnheim's assertion is true only when ordinaryvision—conditioned and enculturated by the most vulgar of environments—is liberated through aesthetic conceptual design information. Film is a way of seeing. We see through the filmmaker's eyes.If he's an artist we become artists along with him. If he's not,information tends toward misinformation.The artist's intuitive sense of proportion corresponds to thephenomenon of absolute pitch in musicians and answers a fundamental need in comprehending what we apprehend. In the finalanalysis our aptitudes and our psychological balance are a result ofour relation to images. The image precedes the idea in the development of consciousness: an infant doesn't think "green" when itlooks at a blade of grass. It follows that the more "beautiful" theimage the more beautiful our consciousness.The design of commercial entertainment is neither a science noran art; it answers only to the common taste, the accepted vision, forfear of disturbing the viewer's reaction to the formula. The viewer'staste is conditioned by a profit-motivated architecture, which hasforgotten that a house is a machine to live in, a service environment.He leaves the theatre after three hours of redundancy and returnshome to a symbol, not a natural environment in which beauty andfunctionality are one. Little wonder that praise is heaped on filmswhose imagery is on the level of calendar art. Global man stands onthe moon casually regarding the entire spaceship earth in a glance, yet humanity still is impressed that a rich Hollywood studio can lugits Panavision cameras over the Alps and come back with prettypictures. "Surpassing visual majesty!" gasp the critics over A Man
and a Woman or Dr. Zhivago. But with today's technology andunlimited wealth who couldn't compile a picturesque movie? In factit's a disgrace when a film is not of surpassing visual majesty because there's a lot of that in our world. The new cinema, however,takes us to another world entirely. John Cage: "Where beauty endsis where the artist begins."
"The final poem will be the poem of fact in the language of fact. But it will be the poem
of fact not realized before." WALLACE STEVENSExpanded cinema has been expanding for a long time. Since it leftthe underground and became a popular avant-garde form in the late1950's the new cinema primarily has been an exercise in technique,the gradual development of a truly cinematic language with which toexpand further man's communicative powers and thus his awareness. If expanded cinema has had anything to say, the message hasbeen the medium.1 Slavko Vorkapich: "Most of the films made so farare examples not of creative use of motion-picture devices andtechniques, but of their use as recording instruments only. There areextremely few motion pictures that may be cited as instances ofcreative use of the medium, and from these only fragments and shortpassages may be compared to the best achievements in the otherarts."2 It has taken more than seventy years for global man to come toterms with the cinematic medium, to liberate it from theatre andliterature. We had to wait until our consciousness caught up with ourtechnology. But although the new cinema is the first and only truecinematic language, it still is used as a recording instrument. Therecorded subject, however, is not the objective external human condition but the filmmaker's consciousness, his perception and its process. If we've tolerated a certain absence of discipline, it has been infavor of a freedom through which new language hopefully would bedeveloped. With a fusion of aesthetic sensibilities and technologicalinnovation that language finally has been achieved. The new cinemahas emerged as the only aesthetic language to match theenvironment in which we live.Emerging with it is a major paradigm: a conception of the nature ofcinema so encompassing and persuasive that it promises todominate all image-making in much the same way as the theory ofgeneral relativity dominates all physics today. I call it synaesthetic
cinema. In relation to traditional cinema it's like the science of bionicsin relation to previous notions of biology and chemistry: that is, itmodels itself after the patterns of nature rather than attempting to"explain" or conform nature in terms of its own structure. The newartist, like the new scientist, does not "wrest order our of chaos."Both realize that supreme order lies in nature and traditionally wehave only made chaos out of it. The new artist and the new scientistrecognize that chaos is order on another level, and they set about tofind the rules of structuring by which nature has achieved it. That'swhy the scientist has abandoned absolutes and the filmmaker hasabandoned montage.Herbert Read: "Art never has been an attempt to grasp reality as awhole—that is beyond our human capacity; it was never even anattempt to represent the totality of appearances; but rather it hasbeen the piecemeal recognition and patient fixation of what is significant in human experience."3 We're beginning to understand that"what is significant in human experience” for contemporary man isthe awareness of consciousness, the recognition of the process ofperception. (I define perception both as "sensation" and "conceptualization," the process of forming concepts, usually classified as"cognition." Because we're enculturated, to perceive is to interpret.)Through synaesthetic cinema man attempts to express a total phenomenon—his own consciousness.
Synaesthetic cinema is the only aesthetic language suited to thepost-industrial, post-literate, man-made environment with its multidimensional simulsensory network of information sources. It's theonly aesthetic tool that even approaches the reality continuum ofconscious existence in the nonuniform, nonlinear, nonconnectedelectronic atmosphere of the Paleocybernetic Age. "As visual spaceis superseded," McLuhan observes, "we discover that there is nocontinuity or connectedness, let alone depth and perspective, in anyof the other senses. The modern artist—in music, in painting, inpoetry—has been patiently expounding this fact for decades."5 Themodern synaesthetic filmmaker has been patiently expounding thisfact for decades as well, and with far more success than painters orpoets.Finally, I propose to show that synaesthetic cinema transcends therestrictions of drama, story, and plot and therefore cannot be called agenre. In addition to matching McLuhan's view of contemporaryexistence, it also corresponds to Buckminster Fuller's observationson natural synergetics and consequently is negentropic. Beforediscussing specifics, however, we must first understand why synaesthetic cinema is just now being developed into a universallanguage, more than seventy years after the birth of the medium.Like most everything else, it's because of television.
Just as every fact is also metaphysical, every piece of hardwareimplies software: information about its existence. Television is thesoftware of the earth. Television is invisible. It's not an object. It's nota piece of furniture. The television set is irrelevant to the phenomenon of television. The videosphere is the noosphere transformed into a perceivable state. "Television," says video artist LesLevine, "is the most obvious realization of software in the generalenvironment. It shows the human race itself as a working model ofitself. It renders the social and psychological condition of the environment visible to the environment."A culture is dead when its myths have been exposed. Television isexposing the myths of the republic. Television reveals the observed,the observer, the process of observing. There can be no secrets inthe Paleocybernetic Age. On the macrostructural level all televisionis a closed circuit that constantly turns us back upon ourselves.Humanity extends its video Third Eye to the moon and feeds its ownimage back into its monitors. "Monitor" is the electronic manifestationof superego. Television is the earth's superego. We become awareof our individual behavior by observing the collective behavior asmanifested in the global videosphere. We identify with persons innews events as once we identified with actors or events in fictionfilms. Before television we saw little of the human condition. Now wesee and hear it daily. The world's not a stage, it's a TV documentary.Television extends global man throughout the ecological biospheretwenty-four hours a day. By moving into outer space, televisionreveals new dimensions of inner space, new aspects of man'sperception and the results of that perception.This implosive, self-revealing, consciousness-expanding process isirreversible. Global information is the natural enemy of local government, for it reveals the true context in which that government is operating. Global television is directly responsible for the politicalturmoil that is increasing around the world today. The politicalestablishments sense this and are beginning to react. But it's toolate. Television makes it impossible for governments to maintain theillusion of sovereignty and separatism which is essential for theirexistence. Television is one of the most revolutionary tools in theentire spectrum of technoanarchy.We recognize television's negative effect on the popular arts: that itinduces a kind of sedentary uniformity of expression and generates afalse sense of creativity. In its broader consequences, however,television releases cinema from the umbilical of theatre andliterature. It renders cinema obsolete as communicator of the objective human condition. It has affected cinema in much the same wayas the invention of photography affected sculpture and painting.Cubism and other means of abstracting the realistic image were bornwith the photographic plate because painting no longer provided themost realistic images. The plastic arts abandoned exterior reality forinterior reality. The same has happened to cinema as a result oftelevision: movies no longer provide the most realistic images sothey've turned inward.We're in direct contact with the human condition; there's no longerany need to represent it through art. Not only does this releasecinema; it virtually forces cinema to move beyond the objectivehuman condition into newer extra-objective territory. There aremanifold trends that indicate that virtually all cinema has felt theprofound impact of television and is moving inevitably towardsynaesthesis. The progression naturally includes intermediary stepsfirst toward greater "realism," then cinéma-vérité, before the final andtotal abandon of the notion of reality itself. The fact that we're nowapproaching the peak of the realism stage is demonstrated byWarhol, for example, whose recent work contrasts "reality" with"realism" as manifested in the spontaneous behavior of actors pretending to be acting. In addition there's virtually all of Godard's work,as well as John Cassavetes' Faces, James McBride's David
Holzman's Diary, Peter Watkins' The War Game, Gillo Pontecorvo's
The Battle of Algiers, Paul Morrissey's Flesh, and Stanton Kaye's
Georg and Brandy in the Wilderness.
Most of this work is characterized by an astute blending of scriptedand directed acting with spontaneous improvisation, in which theactor randomly fills in the parameters of a characterization predetermined and predestined by the director. Yet precisely becausethey attempt to approximate objective reality without actually beingreal, places them firmly in the tradition of conventional Hollywoodpretend movies, with the exception of camera presence or whatmight be called process-level perception.It's only natural that contemporary filmmakers should be moresuccessful at imitating reality since the intermedia network makes usmore familiar with it. But there's a curious and quite significantaspect to the nature of this new realism: by incorporating a kind ofbastardized cinéma-vérité or newsreel style of photography andbehavior, the filmmaker has not moved closer to actual unstylizedreality itself but rather a reality prestylized to approximate ourprimary mode of knowing natural events: television. We accept it asbeing more realistic because it more closely resembles the processlevel perception of TV watching, in which unstylized reality is filteredand shaped through the process of a given medium.The traditional dramatic structure of these films becomes moreeasily discernible in contrast with pure cinéma-vérité work such asJean Rouch's Chronicle of a Summer, Pennebaker's Don't Look
Back, or Chris Marker's brilliant Le Joli Mai. A comparison of Facesor David Holzman's Diary with Warhol's Nude Restaurant is evenmore revealing: the difference between prestylized and predestinedrealities on the one hand, and Warhol's totally random and onlypartially prestylized reality on the other, is brought into sharp focus.Warhol has expressed regret that a camera cannot simply beswitched on and left running for twenty-four hours, since the "important" (naturally-revealing) events seem to occur at that momentjust after it stops turning. Godard disclosed similar sentiments whenhe said: "The ideal for me is to obtain right away what will work. Ifretakes are necessary it falls short of the mark. The immediate ischance. At the same time it is definitive. What I want is the definitiveby chance."
Simultaneous Perception of Harmonic Opposites Time, said St. Augustine, is a threefold present: the present as weexperience it; the past as present memory; the future as presentexpectation. Hopi Indians, who thought of themselves as caretakersof the planet, used only the present tense in their language: past wasindicated as "present manifested," and the future was signified by"present manifesting.”6 Until approximately 800 B.C., few culturesthought in terms of past or future: all experience was synthesized inthe present. It seems that practically everyone but contemporaryman has intuitively understood the space-time continuum.Synaesthetic cinema is a space-time continuum. It is neither subjective, objective, nor nonobjective, but rather all of these combined:that is to say, extra-objective. Synaesthetic and psychedelic meanapproximately the same thing. Synaesthesis is the harmony ofdifferent or opposing impulses produced by a work of art. It meansthe simultaneous perception of harmonic opposites. Its sensorialeffect is known as synaesthesia, and it's as old as the ancientGreeks who coined the term. Under the influence of mindmanifesting hallucinogens one experiences synaesthesia in additionto what Dr. John Lilly calls "white noise," or random signals in thecontrol mechanism of the human bio-computer.7 Any dualism is composed of harmonic opposites: in/out, up/ down,off/on, yes/no, black/white, good/bad. Past aesthetic traditions,reflecting the consciousness of their period, have tended toconcentrate on one element at a time. But the Paleocyberneticexperience doesn't support that kind of logic. The emphasis of traditional logic might be expressed in terms of an either/or choice, whichin physics is known as bistable logic. But the logic of the CyberneticAge into which we're moving will be both/and, which in physics is called triadic logic. Physicists have found they can no longer describe phenomena with the binary yes/no formula but must operatewith yes/no/maybe.The accumulation of facts is no longer of top priority to humanity.The problem now is to apply existing facts to new conceptual wholes,new vistas of reality. By "reality" we mean relationships. PietMondrian: "As nature becomes more abstract, a relation is moreclearly felt. The new painting has clearly shown this. And that is whyit has come to the point of expressing nothing but relations."8 Synaesthetic cinema is an art of relations: the relations of the conceptual information and design information within the film itselfgraphically, and the relation between the film and the viewer at thatpoint where human perception (sensation and conceptualization)brings them together. As science gropes for new models to accommodate apparent inconsistencies and contradictions, the need forseeing incompatibles together is more easily discerned. For example, the phenomenon of light is conceived in both/and terms: bothcontinuous wave motions and discontinuous particles. And we havenoted our incapacity for observing both movement and position ofelectrons.This is but one of many reasons that synaestheticcinema is theonly aesthetic language suited to contemporary life. It can function asa conditioning force to unite us with the living present, not separateus from it. My use of the term synaesthetic is meant only as a way ofunderstanding the historical significance of a phenomenon withouthistorical precedent. Actually the most descriptive term for the newcinema is "personal" because it's only an extension of the filmmaker's central nervous system. The reader should not interpret"synaesthetic" as an attempt to categorize or label a phenomenonthat has no definition. There's no single film that could be calledtypical of the new cinema because it is defined anew by each individual filmmaker.I've selected about seven films that are particularly representativeof the various points I wish to make. I'm using them only to illuminatethe nature of synaesthetic cinema in general, not as specific archetypal examples. Sufficient literature exists on Brakhage's Dog Star
Man to preclude any major expository analysis here, but it is exemplary of virtually all concepts involved in the synaesthetic mode, inparticular syncretism and metamorphosis. Will Hindle's Chinese
Firedrill is an outstanding example of the evocative language ofsynaesthetic cinema as distinct from the expositional mode ofnarrative cinema. Pat O'Neill's 7362, John Schofill's XFilm, andRonald Nameth's Exploding Plastic Inevitable provide some insightinto kinaesthetics and kinetic empathy. Carolee Schneemann's
Fuses, in contrast with Warhol's Blue Movie and Paul Morrissey's
Flesh, illustrates the new polymorphous eroticism. And, finally,Michael Snow's Wavelength has been chosen for its qualities ofextra-objective constructivism.
Montage as Collage The harmonic opposites of synaesthetic cinema are apprehendedthrough syncretistic vision, which Anton Ehrenzweig has characterized as: "The child's capacity to comprehend a total structure ratherthan analyzing single elements... he does not differentiate the identityof a shape by watching its details one by one, but goes straight forthe whole."9 Syncretism is the combination of many different formsinto one whole form. Persian tapestries and tile domes aresyncretistic. Mandalas are syncretistic. Nature is syncre-tistic. Themajority of filmgoers, conditioned by a lifetime of conven-tionalnarrative cinema, make little sense of synaesthetic cinema becausetheir natural syncretistic faculty has suffered entropy and atrophy.Buckminster Fuller: "All universities have been progressivelyorganized for ever-finer specialization. Society assumes that specialization is natural, inevitable and desirable. Yet in observing a littlechild we find it is interested in everything and spontaneously apprehends, comprehends and coordinates an ever-expanding inventory of experience.''10 It has been demonstrated that all species of life on earth that havebecome extinct were doomed through overspecialization, whetheranatomical, biological, or geological. Therefore conventional narrative cinema, in which the filmmaker plays policeman guiding oureyes here and there in the picture plane, might be described as"specialized vision," which tends to decay our ability to comprehendthe more complex and diffuse visual field of living reality.The general impression that syncretism, and therefore synaestheticcinema, is empty of detail or content is an illusion: "… it is highlysensitive to the smallest of cues and proves more efficient in identifying individual objects. It impresses us as empty, vague and generalized only because the narrowly-focused surface consciousnesscannot grasp its wider more comprehensive structure. Its precise,concrete content has become inaccessible and ‘unconscious.’11 ''Synaesthetic cinema provides access to syncretistic contentthrough the inarticulate conscious. Similarly, it contradicts theteachings of Gestalt psychology, according to which we must makean either/or choice: we can choose either to see the "significant"figure or the "insignificant" ground. But when the "content" of themessage is the relationship between its parts, and when structureand content are synonymous, all elements are equally significant.Ehrenzweig has suggested that syncretism is "Gestalt-free perception," and indeed this must be the case if one expects any visual"meaning" from synaesthetic cinema.Paul Klee, whose syncretistic paintings closely resemble certainworks of synaesthetic cinema, spoke of the endotopic (inside) and
exotopic (outside) areas of a picture plane, stressing their equalimportance in the overall experience.12 Synaesthetic cinema, primarily through superimposition, fuses the endotopic and exotopic byreducing depth-of-field to a total field of nonfocused multiplicity.Moreover, it subsumes the conventional sense of time by interconnecting and interpenetrating the temporal dimension with images thatexist outside of time. The "action" of Dog Star Man, for example,could be an entire life-span or merely a split second in the inarticulate conscious of Stan Brakhage. I stress "action" as commonlyunderstood in the cinema because synaesthetic syncretism replacesmontage with collage and, as André Bazin has observed, "montageis the dramatic analysis of action." Bazin was perceptive enough torealize that "only an increased realism of the image can support theabstraction of montage.''13 Synaesthetic cinema subsumes Eisenstein's theory of montage-ascollision and Pudovkin's view of montage-as-linkage. It demonstratesthat they were certainly correct but didn't follow their own observations to their logical conclusions. They were restricted by the consciousness of their times. Synaesthetic cinema transcends the notionof reality. It doesn't "chop the world into little fragments," an effectBazin attributed to montage, because it's not concerned with theobjective world in the first place. The new filmmaker is showing ushis feelings. Montage is indeed an abstraction of objective reality;that's why, until recently, Warhol did not cut his films at all. Butsynaesthetic syncretism is the only mode in which the manifestationsof one's consciousness can be approximated without distortion.There's no conflict in harmonic opposites. Nor is there anything thatmight be called linkage. There is only a space-time continuum, amosaic simultaneity. Although composed of discrete elements it isconceived and edited as one continuous perceptual experience. Asynaesthetic film is, in effect, one image continually transforming intoother images: metamorphosis. It is the one unifying force in all ofsynaesthetic cinema. The notion of universal unity and cosmicsimultaneity is a logical result of the psychological effects of theglobal communications network.If montage is the dramatic analysis of action, a film without classicmontage thus avoids at least the structural element of dramainherent within the medium. All that remains to avoid drama entirelyis to exclude dramatic (i.e., theatrical) content by making content andstructure the same. Warhol's films are not dramatic, and neither arefilms at the extreme opposite end of the spectrum, synaesthesia. Theclassical tension of montage is dissolved through overlappingsuperimposition. For example: we have shots A, B. and C. First wesee A, then B is superimposed over it to produce AB. Then A fadesas C fades in. There's a brief transitional period in which we'reseeing ABC simultaneously, and finally we're looking only at BC. Butno sooner has this evolved than B begins to fade as D appears, andso on.This is a physical, structural equivalent of the Hopi "presentmanifested" and "present manifesting" space-time continuum. It's theonly style of cinema that directly corresponds to the theory of generalrelativity, a concept that has completely transformed all aspects ofcontemporary existence except traditional Hollywood cinema. Theeffects of metamorphosis described above become more apparent ifshots A, B. and C happen to be of the same image but from slightly different perspectives, or with varied inflections of tone and color. Itis through this process that a synaesthetic film becomes, in effect,one image constantly manifesting.And finally we're forced to admit that the pure art of cinema existsalmost exclusively in the use of superimposition. In traditionalcinema, superimposition usually gives the impression of two moviesoccurring at once in the same frame with their attendant psychological and physiological connotations coexisting separately. In synaesthetic cinema they are one total image in metamorphosis. This doesnot imply that we must relinquish what Eisenstein called "intellectualmontage." In fact, the conflict-juxtaposition of intellectual effects isincreased when they occur within the same image. Fiction, legend,parable, myth, traditionally have been employed to make comprehensible the paradoxes of that field of nonfocused multiplicity that islife. Synaesthetic cinema, whose very structure is paradox, makesparadox a language in itself, discovering the order (legend) hiddenwithin it.
Stan Brakhage: Dog Star Man
Dog Star Man is a silent, seventy-eight-minute film divided into
Prelude and Parts One through Four. It was shot in 1959-60 andedited during the next four years. Prelude is an extremely fastcollage of multiple-level superimpositions and compounded imagesthat emerge from a blurry diaphanous haze and slowly take form,only to be obscured by other images and countermotions. We beginto discern specific objects, patterns, and finally a motif or theme: theelements of Earth, Air, Fire, and Water; a childbirth; a man climbinga mountain with his dog; the moon; the sun throwing off huge solarprominences; lovemaking; photomicrography of blood vessels; abeating heart; a forest; clouds; the faces of a man and a woman; andliterally thousands of other images to appear in the rest of the film.These images exist essentially autonomously and are superimposed or compounded not for "dramatic" effect but rather as a kind ofmatrix for psychic exercise on the part of the viewers. For example,over an expanding solar prominence we might see Brakhage'sleonine face or a line of snow-covered fir trees in the mountains ofColorado. We are not asked to interpret or find "meaning" in these combinations, though vastly rich experiences arepossible. When theimages emerge from a hazy blur, for example, we are not asked tointerpret this as the creation of life or some similar dramatic notion,but rather as a perceptual experience for its own sake, in addition tothe contextual relationship of this image to the rest of the film, orwhat Eisenstein indicated by the term "intellectual montage."Whereas Prelude is a rapid barrage of multiple overlays, Part Oneis superimposed sparingly, concentrating on interface relationshipsbetween individual shots. However, every effort is made to subdueany effect that might be considered montage. The shots fade in andout very slowly, often fading into a color such as red or green. Thefragments of Prelude fall into place and an overwhelming sense ofoceanic consciousness evolves. We begin to realize that Brakhageis attempting to express the totality of consciousness, the realitycontinuum of the living present. As his solitary figure climbs thesnow-covered mountain, we see images of man's world from themicrospectrum of the bloodstream to the macrospectrum of the sun,moon, and universe. Both time and space are subsumed in thewholeness of the experience. Superimposition is not used as aneconomical substitute for "parallel montage"—indicating simultaneous but spatially separate events—for spatio-temporal dimensionsdo not exist in the consciousness. Brakhage is merely presenting uswith images orchestrated in such a way that a new reality arises outof them.When we see the sun superimposed over a lovemaking scene, it'snot an invitation to interpret a meaning such as cosmic regenerationor the smallness of man in the universe, but rather as an occasion toexperience our own involuntary and inarticulate associations. Theimages are not symbolic, as in The Seventh Seal, or artfully composed as in Last Year at Marienbad. Brakhage does not manipulateus emotionally, saying: "Now I want you to feel suspense" or "Now Iwant you to laugh" or "Now is the time to be fearful." This is the ployof the commercial entertainer: an arrogant degradation of cinema,using film as a tool for cheap sensationalism. This is not to say thatspatio-temporal experiences, or suspense, humor, or any emotioncannot be found in synaesthetic cinema. Quite the contrary: becausewe're dealing with our own personal associations, emotion is guaranteed. And it will be more genuinely profound than the formulatriggered gratification of conditioned response that we receive fromcommercial entertainment.Brakhage has spoken of "restructuring" vision through his films,and often refers to the "untutored" vision of the child before he'staught to think and see in symbols. In what he calls "closed-eyevision," Brakhage attempts to simulate, by painting and scratching onfilm, the flashes and patterns of color we perceive when our eyes areclosed. Approximately midway through Dog Star Man, otherwisemundane images take on wholly new meanings and in some casesnew appearances. We stop mentally labeling images and concentrate instead on the synaesthetic/kinaesthetic flow of color,shape, and motion.This is not to suggest a nonobjective experience. The imagesdevelop their own syntactical meaning and a "narrative" line isperceived, though the meaning of any given image may change inthe context of different sequences. This constitutes a creative use ofthe language itself, over and above any particular "content" conveyed by that language. (Wallace Stevens: "A new meaning isequivalent to a new word.") The effect of synaesthetic cinema is tobreak the hold that the medium has over us, to make us perceive itobjectively. Art is utter folly unless it frees us from the need of art asan experience separate from the ordinary.Wittgenstein has described art as a game whose rules are madeup as the game is in process. The exact meaning of words (images)becomes known only in the context of each new statement.14 E. H.Gombrich, on the other hand, demonstrates that objective realismalso is a game, but one whose schema is established prior to its useand is never altered. Artists and society thus learn to read theschema as though it were objective reality. But since the languageitself is not used creatively, the viewer is seduced beyond form intoan abstract content with an illusion of being externally objective.15 Thus the viewer is captive under the hold, or spell, of the mediumand is not free to analyze the process of experience.
14 Ludwig Wittgenstein, Philosophical Investigations (Oxford: Blackwell Press, 1963).
Brakhage expressed this concept with respect to his own work:"Imagine an eye unruled by man-made laws of perspective, an eyeunprejudiced by compositional logic, an eye which must know eachobject encountered in life through a new adventure of perception.Imagine a world alive with incomprehensible objects and shimmeringwith an endless variety of movement and gradations of color.Imagine a world before the beginning was the word.''16
16 Stan Brakhage, "Metaphors on Vision," ed. P. Adams Sitney, Film Culture (Fall, 1963).
Evocation and Exposition:
Toward Oceanic Consciousness There is an important distinction to be made betweenevocation, thelanguage of synaesthetic cinema, primarily poetic in structure andeffect, and exposition, the language of narrative cinema, whichchiefly conforms to traditional, literary narrative modes. Intermediaartist and filmmaker Carolee Schneemann has characterized evocation as the place between desire and experience, the interpenetrations and displacements which occur between various sense stimuli."Vision is not a fact," Miss Schneemann postulates, "but anaggregate of sensations. Vision creates its own efforts towardrealization; effort does not create vision.”17 Thus, by creating a new kind of vision, synaesthetic cinema createsa new kind of consciousness: oceanic consciousness. Freud spokeof oceanic consciousness as that in which we feel our individualexistence lost in mystic union with the universe. Nothing could bemore appropriate to contemporary experience, when for the first timeman has left the boundaries of this globe. The oceanic effect ofsynaesthetic cinema is similar to the mystical allure of the naturalelements: we stare in mindless wonder at the ocean or a lake orriver. We are drawn almost hypnotically to fire, gazing as thoughspellbound. We see cathedrals in clouds, not thinking anything inparticular but feeling somehow secure and content. It is similar to theconcept of no-mindedness in Zen, which also is the state of mantraand mandala consciousness, the widest range of consciousness.Miss Schneemann defines perception as eye-journey or empathydrawing. It is precisely through a kind of empathy-drawing that thecontent of synaesthetic cinema is created jointly by the film and theviewer. The very nature of evocation requires creative effort on thepart of the viewer, whereas expository modes do all the work and theviewer becomes passive. In expositional narrative, a story is being figure of Stan Brakhage in Dog Star Man actually moves through apsychic environment created by the viewer, whose deeply-hiddencreative resources and hungers have been evoked by the film.With typical poetic eloquence, Hermann Hesse has summarizedthe evocative effects of oceanic consciousness in this memorablepassage from Demian: "The surrender to nature's irrational, strangelyconfused formations produces in us a feeling of inner harmony withthe force responsible for these phenomena... the boundaries separating us from nature begin to quiver and dissolve... we are unable todecide whether the images on our retina are the result ofimpressions coming from without or from within... we discover towhat extent we are creative, to what extent oursoul partakes of theconstant creation of the world.'
Will Hindle: Chinese FiredrillThere have been essentially three generations of personal filmmakers in the United States. The first began with the invention of themedium and continued in various stages through the 1940's. Thesecond began approximately in the mid-1950's with the increasingavailability of inexpensive 8mm. and 16mm. equipment. It represented the first popular movement toward personal cinema as a wayof life. The third generation has evolved since the mid-1960's, primarily in the San Francisco area, where the latest trend is toward ablending of aesthetics and technology. One reason personal cinemais more eloquent than commercial cinema is that the filmmaker isforced into a closer interaction with his technology.Will Hindle is exemplary of this recent technological awareness, acombination of engineering and aesthetics. Trained in art, literature,and professional television filmmaking, Hindle has applied hisknowledge to personal cinema in a singularly spectacular fashion.His ability to invest a technical device with emotional or metaphysical content is truly impressive. He has, for example, developedthe technique of rear-projection rephotography to a high degree ofeloquence. He shoots original scenes with wide-angle lenses, then"crops" them by projecting and rephotographing this footage using aspecial single-frame projector. Thus extremely subtle effects are achieved that would be prohibitively expensive, if not impossible, ifdone through conventional laboratory optical printing.Although many synaesthetic films are wonderfully evocative,Hindle's recent works are especially notable for their ability to generate overwhelming emotional impact almost exclusively from cine matic technique, not thematic content. Hindle has an uncanny talentfor transforming spontaneous unstylized reality into unearthly poeticvisions, as in Billabong (1968), a wordless impressionistic "documentary" about a boy's camp in northern California, and Watersmith (1969), a spectacular visual fantasy created from footage of anOlympic swimming team at practice.
Chinese Firedrill, unique in Hindle's work, was prestylized and"performed" almost in the traditional sense of a scripted, directed,and acted movie. The difference is that Hindle used the images notfor their symbolic or theatrical content but as ingredients of an almosticonographic nature, to be compounded and manipulated through theprocess of the medium. Although there are "actors" (Hindle plays theprincipal role), there is no characterization. Although there are sets,we're not asked to suspend our disbelief.
Chinese Firedrill is a romantic, nostalgic film. Yet its nostalgia is ofthe unknown, of vague emotions, haunted dreams, unspoken words,silences between sounds. It's a nostalgia for the oceanic presentrather than a remembered past. It is total fantasy; yet like the bestfantasies—8½, Beauty and the Beast, The Children of Paradise— itseems more real than the coldest documentary. The "action" occursentirely within the mind of the protagonist, who never leaves thesmall room in which he lives. It's all rooms everywhere, all cubicleswherever we find man trapped within his dreams. Through thedoor/mirror is the beyond, the unreachable, the unattainable, thebeginning and the end. Not once in the film's twenty minutes can wepinpoint a sequence or action that might be called "dramatic" in theusual sense. Yet almost immediately an overwhelming atmosphereof pathos is generated. There are moments of excruciating emotionalimpact, not from audience manipulation but from Hindle's ability torealize metaphysical substance, stirring the inarticulate conscious.Every effort is made to distance the viewer, to keep us aware of ourperceptions, to emphasize the purely cinematic as opposed to thetheatrical.
We find Hindle kneeling on the floor of his surrealistic room stuffingthousands of IBM cards into boxes. Over this we hear a strangemonologue of fragmented words and sentences in an odd foreignaccent. This is punctuated by fierce thunderclaps and howling windthat evolve into ethereal music and tinkling bell sounds. Periodicallythe screen is slashed across with blinding white flashes while thecentral images constantly are transformed through lap-dissolves andmultiple superimpositions. There are flash-forwards of images to beencountered later, though we don't recognize them and thereforedon't interpret them. We see nude lovers, a small boy bathing, abeautiful woman with candles, a huge eyeball, a battery of glaringlights. These are noted for their inherent psychological connotationsand not as narrative devices.The most memorable sequence of Firedrill, possibly one of thegreat scenes in the history of film, involves Hindle lying in anguish onhis floor and slowly reaching out with one hand toward theglimmering void beyond his door. Suddenly a mirror-like reflection ofhis arm and hand appears on the opposite side of the mirror. Whenhe removes his hand we see the vague shadowy figure of a nudewoman silhouetted ghostlike, her skin sparkling. In slow motion thesilhouette of a nude man enters from an opposite direction and thetwo gossamer figures embrace in a weightless ballet of gracefulmotion in some dream of bliss. In the film's final image, the hauntedman has become a child once again, splashing in his bath in a seriesof freeze-frames that grow ever fainter until they vanish.
Synaesthetics and Kinaesthetics:
The Way of All Experience The term kinetic generally indicates motion of material bodies andthe forces and energies associated with it. Thus to isolate a certaintype of film as kinetic and therefore different from other films meanswe're talking more about forces and energies than about matter. Idefine aesthetic quite simply as: the manner of experiencing something. Kinaesthetic, therefore, is the manner of experiencing a thingthrough the forces and energies associated with its motion. This iscalled kinaesthesia, the experience of sensory perception. One whois keenly aware of kinetic qualities is said to possess a kinaestheticsense.The fundamental subject of synaesthetic cinema—forces andenergies—cannot be photographed. It's not what we're seeing somuch as the process and effect of seeing: that is, the phenomenon ofexperience itself, which exists only in the viewer. Synaestheticcinema abandons traditional narrative because events in reality donot move in linear fashion. It abandons common notions of "style"because there is no style in nature. It is concerned less with factsthan with metaphysics, and there is no fact that is not also metaphysical. One cannot photograph metaphysical forces. One cannoteven “represent" them. One can, however, actuallyevoke them in theinarticulate conscious of the viewer.The dynamic interaction of formal proportions in kinaestheticcinema evokes cognition in the inarticulate conscious, which I call kinetic empathy. In perceiving kinetic activity the mind's eye makesits empathy-drawing, translating the graphics into emotionalpsychological equivalents meaningful to the viewer, albeit meaningof an inarticulate nature. "Articulation" of this experience occurs inthe perception of it and is wholly nonverbal. It makes us aware offundamental realities beneath the surface of normal perception:forces and energies.

Social media content shared today in cities, such as Instagram images, their tags and descriptions, is the key form of contemporary city life. It tells people where activities and locations that interest them are and it allows them to share their urban experiences and selfrepresentations. Therefore, any analysis of urban structures and cultures needs to consider social media activity. In our paper, we introduce the novel concept of social media inequality. This concept allows us to quantitatively compare pattern in social media activities between parts of a city, a number of cities, or any other spatial areas. We define this concept using an analogy with the concept of economic inequality. Economic inequality indicates how some economic characteristics or material resources, such as income, wealth or consumption are distributed in a city, country or between countries. Accordingly, we define social media inequality as unequal spatial distribution of social media sharing in a particular geographic area or between areas. To quantify such distributions, we can use many characteristics of social media such as number of people sharing it, the number of photos they have shared, their content, and user assigned tags. We propose that the standard inequality measures used in other disciplines, such as the Gini coefficient, can also be used to characterize social media inequality. To test our ideas, we use a dataset of 7,442,454 public geo-coded Instagram images shared in Manhattan during five months (March-July) in 2014, and also selected data for 287 Census tracts in Manhattan. We compare patterns in Instagram sharing for locals and for visitors for all tracts, and also for hours in a 24 hour cycle. We also look at relations between social media inequality and socio-economic inequality using selected indicators for Census tracts. The inequality of Instagram sharing in Manhattan turns out to be bigger than inequalities in levels of income, rent, and unemployment.

Social media content shared today in cities, such as Instagram images, their tags and descriptions, is the key form of contemporary city life. It tells people where activities and locations that interest them are and it allows them to share their urban experiences and selfrepresentations. Social media also has become one of the most important representations of city life to both its residents and the outside world. One can argue that any city today is as much media content shared in that city on social networks as its infrastructure and economic activities. For these reasons, any analysis of urban structures and cultures needs to consider social media activity and content. While the industry developed many concepts and measurement tools to analyze social media, these concepts and tools were not developed with the view for the comparative urban analysis. Therefore, we need to develop our own concepts that bridge the perspectives of urban studies and design and quantitative analysis of social networks that uses computational methods and “big data.” In the last few years, one of the most frequently discussed public issues has been the rise in income inequality (Stiglitz 2012, Piketty 2014, Atkinson 2015). But inequality does not only refer to distribution of income. It is a more general concept, and it has been used for decades in a number of academic disciplines besides economics, such as urban planning, sociology, education, engineering, and ecology. The quantitative measurements of inequality allow researchers to characterize a set of numbers or compare multiple sets, regardless of what the data represents. In addition to income inequality, we can measure inequality in wealth, education levels, social well-being, and numerous other social characteristics. In our paper, we introduce the novel concept of social media inequality. We define this concept using an analogy with the concept of economic inequality. Economic inequality refers to how some economic characteristics or material resources, such as income, wealth or consumption are distributed in a city, country or between countries (Ray 1998, Milanovic 2007, OECD 2011). Accordingly, we can define social media inequality as the measures of distribution of characteristics of social media content shared in a particular geographic area or between areas. An example of such characteristics is the number of photos shared by all users of a social network such as Instagram in a given city or city area. Another example is the number of hashtags – how many hashtags users added to the photos, and how many of these hashtags are unique. Other examples include average number of tweets shared by a user in a particular period; numbers of tweets shared per month, per week or per hour of a day; the proportions of tweets that were retweeted, and so on. Of course, we can computer and analyze features of content itself - for example, how many different subjects appear the photos, and what are their proportions. In fact, any metric of social media can be used to compare inequality in social 5 media activity between areas - for example, number of likes, length of text messages, most frequent and least frequent words, number of unique topics, number of distinct photographic styles, image compositions, styles of video editing, and so on. We propose that the standard inequality measures used in other disciplines, such as the Gini coefficient, can also be used to characterize social media inequality. We can also compare these measures between content shared on various social networks (Instagram, Twitter, etc.) in the same area or areas. We can do these comparisons for social networks where the main content is text (e.g., Twitter, VK), images (e.g., Instagram, Tumblr), video (e.g., YouTube), or combination of different media (e.g. Facebook, QZone, Sina Weibo, Line, etc.). Finally, we can also compare characteristics of shared content with various social and economic characteristics in the same areas, such as income, rent, the level of education, or ethnic mix. The paper tests some of these ideas using a large dataset of Instagram images shared in Manhattan borough of New York City. This dataset, which we created for this study, contains 7,442,454 public geo-coded Instagram images shared in Manhattan during five months (February 1, 2014 July 31, 2014). Among these images, 1,524,046 were shared by 515,608 city visitors; the remaining 5,918,408 images were shared by 375,876 city residents. Our analysis of the images shared by two types of users in this paper is inspired by the pioneering project Locals and Tourists created by Eric Fischer (Fisher, 2010.) We used the following method to divide users into “visitors” and “locals.” If a user posted all her photos within a single 12-day period out of the total five months, we consider this person a “visitor”. If a user shared a minimum of two photos within any interval larger than 12 days, we consider this person a local. Although this very simple method is expected to produce some errors, we felt that they are acceptable given the size of our dataset. (Such method is also used a number of published papers that analyze patterns in social media activity.) Comparing the locations of images shared by visitors (figure 1) and locals (figure 2) gives us an intuition for social media inequality concept. We can immediately notice that in each case these locations are not distributed evenly. Some parts of the city have many more images than other parts. These figures also suggest that the big proportion of images by city visitors are shared only in a few areas, while the locals share images in most areas of the city. Note that we use the term “shared” rather than “captured” because Instagram allows sharing of any image from user’s phone and not only the ones captured within Instagram app. So users can upload images taken previously in other locations. However, since Instagram captured the geolocation and time when an image was shared (for users who allowed Instagram access to this data), the metadata of images in our dataset tells us about people’s presence at particular place in the city at a particular time.
While the U.S. Census collects data on individuals, it only reports the data aggregated by geographic areas at different scales. We follow a similar logic in our analysis of spatial social media inequality by dividing a city into hundreds of small areas and aggregating characteristics of social media content shared in each area - as opposed to comparing individuals to each other. The way we measure social media inequality is comparable to how Milanovic defines one of the measures of global economic inequality (Milanovic 2006, Concept 1). This measure uses countries as the units of observation. Milanovic does not directly compare the income of people worldwide. Instead he compares average income across different countries to calculate global inequality. In our case, the Census tracts are our units of observation. We aggregate social media characteristics at the tract level in order to analyze social media inequality across all of Manhattan. Social media content shared in a given area may combine contributions from different kinds of users: people who reside in this area, people who live in different parts of the city or in suburbs but spend significant time in this area for work during weekdays; international or domestic tourists visiting a city; companies located in this area, and so on. Together, the content shared by all these users create a collective “voice” of a particular area of a city. A city as a whole can be compared to an orchestra of all these voices (although, of course, they are not necessary performing the same composition.) Applying the concept of inequality to a collection of these urban voices can give us new ways of understanding a city, and provide an additional metric for comparing numerous cities around the world. Social media inequality as we define it refers to the unequal distribution of social media content and its metadata and their characteristics in any type of geographic area – a city, a region, a country, or any other type of area. However, as Fischer’s maps show visually, the density of social media contributions in larger cities is much higher than in non-urban areas, which makes these cities particularly convenient areas of study. We think that our proposed measurements of social media inequality can be useful for urbanism studies, urban planning, urban design, public administration, economics, and other professional and academic fields. While researchers in the fields of social computing, spatial analytics, and “science of cities” have published many quantitative studies analyzing urban data of many kinds (Batty 2013, Goldsmith and Crawford 2014, Townsend 2014, Pucci et al. 2015, Ratti et al. 2006), a significant portion of this analysis cannot be approached without having a degree in computer science. In contrast, social media inequality measurement is a concept that is easy to understand and also easy to calculate. The locations of social media contributions reflect the presence of people in a particular part of a city at a particular time. However, in comparison to pure location data captured by mobile phones or other body sensors, social media images are much more than simple coordinates and time stamps. The content of these contributions can also tell us what people find interesting and how they are spending their time. Therefore, mapping and measuring inequality in 9 characteristics of social media can help us understand how social, economic, and urban design characteristics of cities influence life patterns and the overall “dynamism” and “vitality” of a city. Researchers have never observed perfect equality in any natural, biological or social system or population. In using the term “social media inequality,” we are not suggesting that the goal of urban planners or city administration should be to reduce differences in social media use between various areas to a minimum, or to some optimal level. If people are sharing the same amount of social media in every area of the city, it means that this city does not have any centers or attractions that stand out, or places where many people gather. In terms of modern housing, large American-type suburbs with the same density of houses and same demographics of families and income would probably generate least amount of social media inequality. Today such suburbs are common around the world, from Mexico to China. Given the wide criticism of this classical suburb type, we can assume that some level of spatial social media inequality is desirable. In this case, inequality stands for variety and differentiation while complete equality stands for sameness and lack of variety. But is extreme social media inequality a good thing? For example, do we really want all people living in a city to spend their weekends in a single place? There are certain situations where reducing extreme spatial social media inequality would be very desirable. For example, if city authorities find that most tourists’ social media activity is concentrated in just a few areas surrounding only a few landmarks (like Times Square in New York City), they can change the way the city is promoted to visitors to diversify where tourists go, what they look at, and what they experience. Being able to quantify inequality of social media would allow for better planning and evaluation of such changes. Formulated as a type of spatial analysis, our study compares the parts of the city that attract more people and generate more content shared on social media networks and thus are “social media rich” with parts of the city that are “social media poor.” What are the relationships between such social media rich and social media poor areas? Is social media inequality larger or smaller than economic or social inequality in the same areas? Does social media inequality increase worldwide, similar to how economic inequality has been growing recently? Which parts of the world have the highest social media inequality and which are the most equal? Although our analysis is focusing on one part of a single megacity (i.e., Manhattan in New York City), it can be expanded to consider hundreds of cities around the world to consider such questions.

I present a number of core concepts from data science that are relevant to digital art history and the use of quantitative methods to study any cultural artifacts or processes in general. These concepts are objects, features, data, feature space, and dimension reduction. These concepts enable computational exploration of both large and small visual cultural data. We can analyze relations between works on a single artist, many artists, all digitized production from a whole historical period, holdings in museum collections, collection metadata, or writings about art. The same concepts allow us to study contemporary vernacular visual media using massive social media content. (In our lab, we analyzed works by van Gogh, Mondrian, and Rothko, 6000 paintings by French Impressionists, 20,000 photographs from MoMA photography collection, one million manga pages from manga books, one million artworks of contemporary non-professional artists, and over 13 million Instagram images from 16 global cities.) While data science techniques do not replace other art historical methods, they allow us to see familiar art historical material in new ways, and also to study contemporary digital visual culture. In addition to their relevance to art history and digital humanities, the concepts are also important by themselves. Anybody who wants to understand how our society “thinks with data” needs to understand these concepts. They are used in tens of thousands of quantitative studies of cultural patterns in social media carried out by computer scientists in the last few years. More generally, these concepts are behind data mining, predictive analytics and machine learning, and their numerous industry applications. In fact, they are as central to our “big data society” as other older cultural techniques we use to represent and reason about the world and each other – natural languages, material technologies for preserving and accessing information (paper, printing, digital media, etc.), counting, calculus, or lens-based photo and video imaging. In short, these concepts form the data society’s “mind” – the particular ways of encountering, understanding, and acting on the world and the humans specific to our era.

Will art history fully adapt quantitative and computational techniques as part of its methodology? While the use of computational analysis in literary studies and history has been growing slowly but systematically during 2000s and first part of 2010s, this has not yet happened in the fields that deal with the visual (art history, visual culture, film, and media studies). However, looking at the history of adoption of quantitative methods in the academy suggests that these fields sooner or later will also go through their own “quantitative turns.” Writing in 2001, Adrian Raftery points out that psychology was the first to adopt quantitative statistical methods in 1920s-1930s, followed by economics in 1930s-1940s, sociology in 1960s, and political science in 1990s.2 Now, in 2015, we also know that humanities fields dealing with texts and spatial information (i.e., already mentioned literary studies and history) are going through this process in 2000s- 2010s. So I expect that “humanities of the visual” will be the next to befriend numbers.  This adaption will not, however, simply mean figuring out what be counted, and then using classical statistical methods (developed by the 1930s and still taught today to countless undergraduate and graduate students pretty much in the same way) to analyze these numbers. Instead, it will take place in the context of a fundamental social and cultural development of the early 21st century – the rise of “big data,” and a new set of methods, conventions, and skills that came to be called “data science.” Data science includes classical statistical techniques from the 19th and early 20th century, additional techniques and concepts for data analysis that were developed starting in 1960s with the help of computers, and concepts from a number fields that also develop in the second part of the 20th century around computers: pattern recognition, information retrieval, artificial intelligence, computer science, machine learning, information visualization, data mining. Although the term "data science" is quite recent, it is quite useful as it acts as an umbrella for currently most frequently used methods of computational data analysis. (Alternatively, I could have chosen machine learning or data mining as the key term for this article, but since data science includes their methods, I decided that if I am to refer to all computational data analysis using a single term, data science is best right now.)
Data science includes many ideas developed over many decades, and hundreds of algorithms. This sounds like a lot, and it is. It is much more than can be learned in one or two graduate methods classes, or summarized in a single article, or presented in a single textbook. But rather than simply picking particular algorithms and techniques from a large arsenal of data science, or borrowing whatever technique happens to be the newest and therefore is currently in fashion (for example, “topic modeling” or “deep learning”) and trying to apply them to art history, it is more essential to fist understand the most fundamental as sumption of the field as a whole. That is, we in art history (or any other humanities field) need to learn the core concepts that underlie the use of data science in contemporary societies. These concepts do not require formulas to explain, and they can be presented in one article, which is what I will attempt here. (Once we define these core concepts, a variety of terms employed in data science today can also become less confusing for the novice.)
Surprisingly, after reading thousands of articles and various textbooks over last eight years, I have not found any short text that presents these core concepts together in one place. While many data science textbooks, of course, do talk about them, their presentation often takes place in the context of mathematically sophisticated techniques or particular applications which can make it hard to understand the generality of these ideas.3 These textbooks in general can be challenging to read without computer science background. Since my article is written for humanities audience, it is on purpose biased–my examples of the application of the core concepts of data science come from humanities as opposed to economics or sociology. And along with an exposition, I also have an argument. I will suggest that some parts of data science are more relevant to humanities research than others, and therefore beginning “quantitative humanists” should focus on learning and practicing these techniques first.
If we want to use data science to “understand” some phenomenon (i.e., something outside of a computer), how do we start? Like other approaches that work on data such as classical statistics and data visualization, data science starts with representing some phenomenon or a process in a particular way. This representation may include numbers, categories, digitized texts, images, audio, spatial locations, or connections between elements (i.e., network relations). Only after such a representation is constructed, we can use computers to work on it. In most general terms, creating such a representation involves making three crucial decisions: What are the boundaries of this phenomenon? For example, if we are interested to study “contemporary societies,” how can we make this manageable? Or, if we want to study “modern art,” how we will choose what time period(s), countries, artist(s), and artworks, or other information to include? In another example, let’s say that we are interested in contemporary “amateur photography.” Shall we focus on studying particular groups on Flickr that contain contributions of people who identify themselves as amateur or semi-pro photographers, or shall we sample widely from all of Flickr, Instagram, or other media sharing service since everybody today with a mobile phone with a built-in camera automatically becomes a photographer. What are the objects we will represent? For example, in modern art example, we may include the following “objects” (in data science they can be also called data points, records, samples, measurements, etc.): individual artists, individual artworks, correspondence between artists, reviews in art journals, passages in art book, auction prices. (For example, 2012 Inventing Abstraction exhibition in MoMA in NYC featured a network visualization showing connections between artists based on the number of letters they exchanged.4 In this representation, modernist abstract art was represented by a set of connections between artists, rather than any other kind of object I listed above.) In a “society” example, we can for instance choose a large set of randomly chosen people, and study social media they share, their demographic and economic characteristics, their connections to each other, and biological daily patterns as recorded by sensors they wear. If we want to understand patterns of work in a hospital, we may use as elements people (doctors, nurses, patients, and any others), also medical procedures to be performed, tests to be made, written documentation and medical images produced, etc. What characteristics of each object we will include? (These are also referred to as metadata, features, properties, or attributes.). In humanities, we usually refer to characteristics that are already available as part of the data (because somebody already recorded them) and characteristics we have added (for example, by tagging) as metadata. In social science, the process of manually adding descriptions of data is called coding. In data science, people typically use algorithms to automatically extract additional characteristics from the objects, and they are referred as features (this process is called “feature extraction”). For example, artists’ names is an example of metadata; average brightness and saturation of their paintings, or the length of words used in all titles of their works are examples of features that can be extracted by a computer. Typically features are numerical descriptions (whole or fractional numbers) but they can also take other form. For example, a computer can analyze an image and generate a few words describing content of the image. In general, both metadata and features can use various data types: numbers, categories, free text, network relations, spatial coordinates, dates, times, and so on.  Although it is logical to think of the three questions above as three stages in the process of creating a data representation– limiting the scope, choosing objects, and choosing their characteristics – it is not necessary to proceed in such linear order. At any point in the research, we can add new objects, new types of objects and new characteristics. Or we can find that characteristics we wanted to use are not practical to obtain, so we have to abandon our plans and try to work with other characteristics. In short, the processes of generating a representation and using computer techniques to work on it can proceed in parallel and drive each other.
The author and several colleagues studied cultural differences using these computerized patterns of Instagram postings—arranged by hue and brightness—from Tokyo, New York, Bangkok, and San Francisco.

In 2002, I was in Cologne, Germany, and I went into the best bookstore in the city devoted to humanities and arts titles. Its new-media section contained hundreds of books. However, not a single title was about the key driver of the "computer age": software. I started going through indexes of book after book: No "software."

Yet in the 1990s, software-based tools were adopted in all areas of professional media production and design. In the 2000s, those developments have made their way to the hundreds of millions of people writing blogs and tweeting, uploading photos and videos, reading texts on Scribd, and using free tools that 10 years earlier would have cost tens of thousands of dollars.

Thanks to practices pioneered by Google, the world now operates on web applications that remain forever in beta stage. They can be updated anytime on remote servers without consumers having to do anything—and in fact, Google is revising its search-algorithm code as often as 600 times a year. Welcome to the world of permanent change—a world defined not by heavy industrial machines that are modified infrequently, but by software that is always in flux.

Software has become a universal language, the interface to our imagination and the world. What electricity and the combustion engine were to the early 20th century, software is to the early 21st century. I think of it as a layer that permeates contemporary societies. If we want to understand today's techniques of communication, representation, simulation, analysis, decision making, memory, vision, writing, and interaction, we must understand software.

But while scholars and media and new-media theorists have covered all aspects of the IT revolution, creating fields like cyberculture studies, Internet studies, game studies, new-media theory, and the digital humanities, they have paid comparatively little attention to software, the engine that drives almost all they study.

It's time they did.

Consider the modern "atom" of cultural creation: a "document," i.e. content stored in a physical form delivered to consumers via physical copies (books, films, audio records) or electronic transmission (television). In software culture, we no longer have "documents." Instead, we have "software performances."

If you are a scholar working inside Google or Facebook, you have a major advantage over colleagues in academe.

I use the word "performance" because what we are experiencing is constructed by software in real time. Whether we are exploring a website, playing a video game, or using an app on a mobile phone to locate nearby friends or a place to eat, we are engaging with the dynamic outputs of computation.

Although static documents may be involved, a scholar cannot simply consult a single PDF or JPEG file the way 20th-century critics examined a novel, movie, or TV program. Software often has no finite boundaries. For instance, a user of Google Earth is likely to experience a different "earth" every time he or she uses the application. Google could have updated some of the satellite photographs or added new Street Views and 3D buildings. At any time, a user of the application can also load more geospatial data created by other users and companies.

Google Earth is not just a "message." It is a platform for users to build on. And while we can find some continuity here with users' creative reworking of commercial media in the 20th century—pop art and appropriation, music, slash fiction and video, and so on—the differences are larger than the similarities.

Even when a user is working only with a single local media file stored in his or her computer, the experience is still only partly defined by the file's content and organization. The user is free to navigate the document, choosing both what information to see and the sequence in which to see it. (In Google Earth, I can zoom in and out, switching between a bird's-eye view of the area, and its details; I can also switch between different kinds of maps.)

Most important, software is not hard-wired to any document or machine: New tools can be easily added without changing the documents themselves. With a single click, I can add sharing buttons to my blog, thus enabling new ways to circulate its content. When I open a text document in Mac OS Preview media viewer, I can highlight, add comments and links, draw and add thought bubbles. Photoshop allows me to save my edits on separate "adjustment layers," without modifying the original image. And so on.

All that requires a new way to analyze media and culture. Since the early 2000s, some of us (mostly from new-media studies and digital arts) have been working to meet that challenge. As far as I know, I was the first to use the terms "software studies" and "software theory" in 2001. The field of software studies gradually took shape in the mid-2000s. In 2006, Matthew Fuller, author of the pioneering Behind the Blip: Essays on the Culture of Software(Sagebrush Education Resources, 2003), organized the first Software Studies Workshop in Rotterdam. "Software is often a blind spot in the theorization and study of computational and networked digital media," Fuller wrote in introducing the workshop. "In a sense, all intellectual work is now 'software study,' in that software provides its media and its context, but there are very few places where the specific nature, the materiality, of software is studied except as a matter of engineering."

In 2007, we started the Software Studies Initiative at the University of California at San Diego, and in 2008 we held the second software-studies workshop. The MIT Press offers a software-studies series, and a growing number of books in other fields (media theory, platform studies, digital humanities, Internet studies, game studies) also help us better understand the roles software plays in our lives. In 2011, Fuller and other researchers in Britain began Computational Culture, an open-access peer-reviewed journal.

T

here is much more to do. One question that particularly interests me is how software studies can contribute to "big data"—analyzing vast data sets—in fields like the digital humanities, computational social science, and social computing. Here are some of the key questions related to big cultural data that software studies could help answer:

What are interactive-media "data"? Software code as it executes, the records of user interactions (for example, clicks and cursor movements), the video recording of a user's screen, a user's brain activity as captured by an EEG or fMRI? All of the above, or something else?

To use terms from linguistics, rather than thinking of code as language, we may want to study it as speech.

Over the past few years, a growing number of scholars in the digital humanities have started to use computational tools to analyze large sets of static digitized cultural artifacts, such as 19th-century novels or the letters of Enlightenment thinkers. They follow traditional humanities approaches—looking at the cultural objects (rather than peoples' interaction with these objects). What has changed is the scale, not the method.

The study of software culture calls for a fundamentally different humanities methodology. We need to be able to record and analyze interactive experiences, following individual users as they navigate a website or play a video game; to study different players, as opposed to using only our own game-play as the basis for analysis; to watch visitors of an interactive installation as they explore the possibilities defined by the designer—possibilities that become actual events only when the visitors act on them.

In other words, we need to figure out how to adequately represent "software performances" as "data." Some answers can come from the field of human-computer interaction, where researchers in academe and private enterprise study how people engage with computer interfaces. The goals of that research, however, are usually practical: to identify the problems in new interfaces and to fix them. The goals of digital humanities' analysis of interactive media will be different—to understand how people construct meanings from their interactions, and how their social and cultural experiences are mediated by software. So we need to develop our own methods of transcribing, analyzing, and visualizing interactive experiences. Together with the Experimental Game Lab, directed by Sheldon Brown, for example, my lab analyzed the experiences of hundreds of users of Scalable City, a large-scale, complex virtual-world art installation created in Brown's lab. One of our goals was to help future users have more challenging interactive experiences.

Who has access to detailed records of user interactions with cultural artifacts and services on the web, and what are the implications of being able to analyze these data?

From the early days of interactive human-computer interfaces, tracking users' interaction with software was easy. Why? Because software continuously monitors inputs like key presses, mouse movements, menu selections, finger gestures over a touch surface, and voice commands.

The shift from desktop to web computing in the 1990s has turned the already existing possibility of recording and storing users' inputs into a fundamental component of a "software-media complex." Since dynamic websites and services (Amazon's online store, personal blogs that use Google's Blogger system, online games, etc.) are operated by software residing on company's servers, it is easy to log the details of user interactions. Each web server keeps detailed information on all visits to a given site. A separate category of software and services exemplified by Google Analytics has emerged to help fine-tune the design of a website or blog.

Today social-media companies make available to their users some of the recorded information about visitors' interactions with the sites, blogs, or accounts they own; the companies also provide interactive visualizations to help people figure out which published items are most popular, and where their visitors are coming from. However, usually the companies keep the really detailed records to themselves. Therefore, if you are one of the few social scientists working inside giants such as Facebook or Google, you have an amazing advantage over your colleagues in the academy. You can ask questions others can't. This could create a real divide in the future between academic and corporate researchers. While the latter will be able to analyze social and cultural patterns on both supermicro and supermacro levels, the former will have only a normal "lens," which can neither get extremely close nor zoom out to a planetary view.

Who benefits from the analysis of the cultural activities of hundreds of millions of people?Automatic targeting of ads on Google networks, Facebook, and Twitter already uses both texts of users' posts or emails and other data, but learning how hundreds of millions of people interact with billions of images and social-network videos could not only help advertisers craft more-successful visual ads but also help academics raise new questions.

Can we analyze the code of software programs? It's not as easy as you may think. The code itself is "big data."

Early software programs such as 1970s video games were relatively short. However, in any contemporary commercial web service or operating system, the program code will simply be too long and complex to allow you to read and interpret it like a short story. While Windows NT 3.1 (1993) was estimated to contain four to five million source lines of code, Windows XP (2001) had some 40 million. MAC OS turned out even bigger, with OS X 10.4 (2005) code at 86 million lines. The estimated number of lines in Adobe Creative Suite 3 (which includes Photoshop, Illustrator, and a number of other popular applications to produce media) is 80 million.

The gradual move of application software to the web also brings with it a new set of considerations. Web services, apps, and dynamic sites often use multi-tier software architecture, where a number of separate modules (for example, a web client, application server, and a database) work together. Especially in the case of large commercial sites like amazon.com, what the user experiences as a single web page may involve continuous interactions between dozens or even hundreds of separate software processes.

The complexity and distributed architecture of contemporary large-scale software poses a serious challenge to the idea of "reading the code." However, even if a program is relatively short and a cultural critic understands exactly what the program is supposed to do, this understanding of the logical structure of the program can't be translated into envisioning the actual user experience.

The attraction of "reading the code" approach for the humanities is that it creates an illusion that we have a static and definite text we can study—i.e., a program listing. But we have to accept the fundamental variability of the actual "software performances." So rather than analyzing the code as an abstract entity, we may instead trace how it is executed, or "performed," in particular user sessions. To use the terms from linguistics, rather than thinking of the code as language, we may want to study it as speech.

Some researchers, like Mark Marino and others working in "critical code studies," have been promoting nuanced, theoretically rigorous, and rich ideas about what it means to "read the code," so my critique is aimed only at a naïve version of the idea that I sometimes encounter in the humanities.

The development of methods to study contemporary software in a way that can be discussed in articles, conferences, and public debates by nonprogrammers, is a key task for software studies. However, given both the complexity of software systems and the fact that, at least at present, only a very small number of media and cultural researchers are trained in software engineering, I don't expect that we can solve this problem in a short time.

And yet, confronting it is crucial, not just for the academy but also for society at large. How can we discuss publicly the decisions made by Google Search algorithms, or Facebook's algorithms controlling what is shown on our news feeds? Even if these companies made all their software open source, its size and complexity would make public discussion very challenging.

While some of the details from popular web companies are published in academic papers written by researchers working at these companies, only people with computer-science and statistics backgrounds can understand them. Moreover, many popular software services use machine-leaning technology that often results in "black box" solutions. (While the software achieves desired results, we don't know the rules it follows.)

As more and more of our cultural experiences, social interactions, and decision making are governed by large-scale software systems, the ability of nonexperts to discuss how these systems work becomes crucial. If we reduce each complex system to a one-page description of its algorithm, will we capture enough of software behavior? Or will the nuances of particular decisions made by software in every particular case be lost?

The role of software studies is not to answer these and many other questions about our new interactive world, but rather to articulate them and offer examples of how they can be approached. And to encourage people across all disciplines to think about how software changes what they study and how they study it.

In the Phototrails project (phototrails.net), created by myself, Nadav Hochman, and Jay Chow, we visualized patterns in the use of Instagram across 2.3 million photos from 13 global cities. In the paper accompanying the project, we attempted to combine two mirror sides of software studies—thinking about software interfaces and how they influence what we do and at the same time studying large-scale behaviors of many software users. One of the key questions we raised: How much of the differences among the cities can we find, given that everybody uses the same Instagram app that comes with its own strong "message" (all photos have the same square size, and all users have access to the same set of build-in filters to make their photos more aesthetic in the same ways). While we did find small but systematic differences in the photos from each city, the use of Instagram software itself was remarkably consistent.

How does the software we use influence what we express and imagine? Shall we continue to accept the decisions made for us by algorithms if we don't know how they operate? What does it mean to be a citizen of a software society? These and many other important questions are waiting to be analyzed.

Hundreds of people stuck in a giant swimming pool passively floating to the rhythm of artificial waves. The poor resolution of the found footage muddles them into a contextless and faceless crowd. Nobody tries to escape the crowd, or go against the current. They are trapped but happy enough. It’s like Dante’s Inferno but without the drama. Just the people floating in the mud.

The final scene of Mainsqueeze captures “a contemporary atmosphere or mood” which sets the present as a time out of joint, encapsulated by the washing machine that tears itself apart over the course of the film. Rafman poses the present escape from the real towards the simulated as the result of a general feeling of turmoil that leads to flight rather than revolt. In the video, the first readable line of text is written on the forehead of a sleeping drunk man at the beginning of the film: “LOSER”. He smiles, and we are led to wonder who the loser really is.

Yet Rafman is not making a particular ethical statement: “Mainsqueeze expresses a moral condition or atmosphere without making a moral judgment. I gravitate towards communities like 4chan because I see in them a compelling mix of attraction and repulsion. This ambivalence is reflected in the current cultural moment.”

Surfing the deep web, Rafman collects, orders, observes, and makes his source material visible to us: “Mainsqueeze is entirely composed of footage found through my online explorations. The voice over text is a combination of modified quotes from literature, Tumblr, and comments on various message boards. I feel less of a need to create original material from scratch due to the sheer abundance of material out in the world to work with. The craft is found in the searching, selecting or curating, and editing together of the materials pulled from far-flung corners of the web.” Yet, he insists “it is not about fetish tourism or shocking people about what exists in the dark corners of the net, rather, I am giving the sourced material a poetic treatment.”

Rafman assumes, and in turn invites us to assume, a difficult position: he is simultaneously the drunk man, the one who paints his face and the one passerby who thinks they are both stoned. Sometimes he himself indulges in degradation, and we as viewers are equally implicated.

An original use of voice-over contributes to this sense of viewer involvement. It is used neither to generate empathy nor to signify a complete alterity (as when synthesized voices are used). Rafman explains that “This particular tone came about through experimenting with a montage of a wide range of material; moments of philosophical epiphany, pseudo-intellectual quotes from tumblr, banal confessional message boards, comments from reddit threads, etc.” InMainsqueeze, this montage is turned into a profound yet familiar voice that often addresses the viewer directly, dragging you into the world depicted on screen. Let yourself in.

Our story begins between the end of the 1950s and the beginning of the 1960s, when technological progress on one hand and developments in art on the other created the conditions for art, science and technology to intertwine once more. Such an encounter was anything but new in the history of art, having been vigorously embraced by the avant-garde movements: see Lazlo Moholy-Nagy, often invoked as one of the founding fathers of New Media Art, above all for his Licht-raum-modulator (1930), a kinetic sculpture that produces fascinating light effects. And it was the historic avant-garde movements that informed the new artistic experiences that sought to go beyond what then looked like the dead end of Abstract Expressionism: New Dada, Nouveau Réalisme, Gutai, Happening, Fluxus, Kinetic Art, Arte Programmata, Optical Art, Pop Art and Video Art. Reality, in the shape of real or represented objects, entered artworks; the pop culture conveyed by the media began to capture the attention of artists; art appropriated all media, from the human body to consumer products, from advertising to television sets to cars, and theoretical developments like cybernetics and information theory informed the lexicon of art. This is, for example, what John Brockman says about John Cage.


For the first and only time in the history of art, the implicit perspective in the most generic interpretation of the expression New Media Art became a mass strategy, common to all the avantgarde art of the period. This situation was short-lived: while a few “new media” and artistic strategies, from assemblage to photography, performance and conceptual interventions on mechanically reproduced images rapidly became the stuff of the establishment, the more radically technophile or science-based expressions, like Kinetic and Optical art, were put out of action, and video entered a splendid isolation of its own that was to last until the early 1990s. At the same time, in the States, the spectre of permanent war gave an incredible boost to scientific and technological research. In 1946 the University of Pennsylvania presented the first digital calculator, ENIAC (Electronic Numerical Integrator and Computer); 1951 saw the launch of UNIVAC, the first computer to hit the market, capable of processing both numerical data and text. These were huge machines without any kind of user interface, that accepted programs in the shape of perforated cards and could only be operated by highly skilled users. Accessibility was also very restricted: developed for military applications, they resided mostly in research centers and universities. It was in Bell Laboratories in Murray Hill (New Jersey) in particular that the first studies on the algorithmic production of text, music and images were carried out, and not by artists, but engineers and researchers who saw these experiments as more or less necessary diversions to their research work. The electronic engineer A. Michael Noll, for example, was taken on by Bell Labs in 1961 and worked there for 15 years. In the summer of 1962 he created his first works of “Computer Art”, abstract images generated by algorithms and mathematical functions that were an evident tribute to Piet Mondrian and Cubism. Around 1963 many pioneers began working in this direction, including Lillian Schwartz, Herbert Franke, Manfred Mohr, Jean-Pierre Hébert and Roman Verotsko. In April 1965 the Howard Wise Gallery in New York, the same venue that brought Gruppo Zero and Kinetic Art to America, staged the exhibition Computer-Generated Pictures by Bela Julesz and Michael Noll. Computer Art appeared in a number of group shows, including Cybernetic Serendipity (ICA, London 1968), Tendencija 4 (Zagreb 1969) and Computerkunst (Hannover 1969). [2] At the same time, potential uses of computers in literature and music were also being studied: on one hand there was the combinatory literature developed by Alison Knowles at Bell Labs and the members of the European group OuLiPo (Ouvroir de Littérature Potentielle), founded in 1960 by Raymond Queneau and François Le Lionnais; and on the other the work of the composer James Tenney at Bell Labs. [3] This initial foray into Computer Art therefore came about in an extremely restricted context, in both sociological and technological terms. From an aesthetic point of view the massive mainframes of the sixties placed great limitations on artists and were extremely difficult to use, and the result was that in this niche engineers vastly outnumbered genuine artists. In view of this, much Computer Art of the sixties is exceptionally ingenuous aesthetically speaking – in the words of Jim Pomeroy, it rolled out «flashy geometric logos tunneling through twirling wire-frames,’ graphic nudes, adolescent sci-fi fantasies, and endless variations on the Mona Lisa». [4] A. Michael Noll candidly confesses.

Yet dismissing Computer Art as merely ingenuous would be a simplistic way of looking at things. Even supposing that the only achievement of Noll and the first computer artists was to show it was possible to make art with a computer, their contribution to the evolution of the medium was crucial. For Computer Art not only paved the way for New Media Art, but the whole of computer graphics, which over the years has progressed to photorealistic videogames and 3D animation. Even considering merely this dual legacy we can appreciate the scope of its contribution to the culture of the twentieth century. And the success, however fleeting, of Computer Art also points up something else: the openness of the art world of the sixties to the most advanced, precarious fringes of cultural experimentation, its acceptance of ideas that would be hard pressed to find a welcome elsewhere. The best demonstration of this was probably the 1968 exhibition Cybernetic Serendipity curated by Jasia Reichardt at the Institute of Contemporary Art in London. This show was part of the work of the Independent Group and resulted from the 1965 encounter between Reichardt and Max Bense, the German philosopher, a key figure of the Stuttgart school, who studied the relationships between maths, language and art, and coined the term “information aesthetics”. According to Brent MacGregor, it was Bense who told Reichardt to “look into computers”. [6] In 1966 the exhibition was announced at a public conference, and fundraising began. Despite initial expectations, the only private company to invest significantly in it was IBM; the rest was covered by the Arts Council. Cybernetic Serendipity was not an exhibition of Computer Art, but a multidisciplinary event that explored the impact of information technology and cybernetic theory on life and contemporary creativity. It was divided into three sections: the first featured works – images, but also music, animations and texts – generated by computers, the second contained cybernetic robots and “painting machines”, and the third explored the social uses of computers and the history of cybernetics. Alongside the pioneers of Computer Art and cybernetic art, from Charles Csuri to Michael Noll, John Whitney to Edward Ihnatowicz to the Computer Technique Group of Tokyo, were artists who shared aesthetic, thematic or formal characteristics with the latter (Nam June Paik, Jean Tinguely and his machines, James Seawright, the Optical painter Bridget Riley, and avant-garde musicians like John Cage and Jannis Xenakis). But there were also explanatory elements and even a computer, provided by IBM, that offered a service for booking flights. According to the curator.

Cybernetic Serendipity came about in a context, the British context, which was of great interest. Catherine Mason’s research [8] has in fact shown that Britain’s distinctive education system facilitated the development of relationships between art, science and technology between the sixties and the eighties. A legacy of the Victorian education system, Britain’s design schools provided both artistic education and training in the applied arts. In the 1950s, the Independent Group addressed, among other things, the implications of science, technology and the mass media on art and society, culminating in the exhibition This Is Tomorrow (Whitechapel Art Gallery, 1956). In 1953 Richard Hamilton went to teach at King’s College in Newcastle, where, together with Victor Pasmore, he held a Basic Design course. Among their students was Roy Ascott, who was encouraged to cultivate his interest in communication, interactivity and cybernetics. In 1961, Ascott was asked by the Ealing Art School to create a two-year course based on the principles of cybernetics: his Ground Course, along with his subsequent appointments, was to play a crucial role in the education of a new generation of artists and designers. In 1967 the first polytechnics appeared, thanks to sizeable government investments in technology in the post war period, which also led to the creation of a Ministry of Technology. In the polytechnics, as Catherine Mason notes, an art student could also learn programming. In the seventies this led to a wide network of schools engaged in Computer Art, yielding interesting results above all in computer graphics for television and advertising. At the same time, these academic roots enabled students and lecturers to develop their own creative work, despite the relative lack of interest in digital art from the art world. And while British Computer Art survived in the world of academe, it soon developed systems of support and critical debate. In 1968, in connection with the British Computer Society, the Computer Arts Society (CAS) was founded. In 1969, CAS launched its own publication, Page, as a platform for debate and critical engagement. Equally early on, CAS began to look beyond the United Kingdom, setting up chapters in various European countries and coming to the States in 1971. In 1970 the association had 377 members, including libraries and institutions, in 17 countries. In this period it put together a collection that included works by pioneers like Manuel Barbadillo, Charles Csuri, Herbert W. Franke, Edward Ihnatowicz, Ken Knowlton, Manfred Mohr, Georg Nees, Frieder Nake, Lillian Schwartz and Alan Sutcliffe, and in 2007, with Mason’s involvement, this collection was bought by the Victoria and Albert Museum in London. Just how receptive the art world of the sixties was to the “art and technology” pairing is also proved by the milieu that sprung up around the distinctive figure of Billy Klüver (1927 – 2004). An electronic engineer of Swedish origin, in 1958 Klüver was hired by Bell Labs in Murray Hill. With a life-long interest in art, in the early seventies he began to work with artists. In 1960 he provided technical support to the Swiss artist Jean Tinguely (after being introduced to him by Pontus Hultén) for his spectacular Homage to New York (1960), a kinetic machine that self-destructed in the Sculpture Garden of the MoMA in New York. Robert Rauschenberg was also involved in this project. Following that, Klüver provided technical support to various artists: he worked with Rauschenberg on the installation Oracle (1962 – 1965) supplying the artist with remote controlled radios, and he helped Jasper Johns and Andy Warhol, providing the material for the latter’s famous Silver Clouds, the helium-filled pillows that accompanied his temporary break from painting, presented in a solo show at the Leo Castelli gallery in 1966. 1966 also saw Klüver’s first major production, the outcome of a collaboration with Rauschenberg. From 14 to 23 October 1966, at the 69th Regiment Armory in New York, he presented the event 9 Evenings: Theatre and Engineering, a series of multimedia performances featuring ten artists working with thirty engineers and scientists from Bell Labs. Participants included Robert Rauschenberg, John Cage, David Tudor, Yvonne Rainer, Robert Whitman and Öyvind Fahlström. During the event Klüver discussed the idea of giving this collaboration between artists and engineers more stable foundations, and this was what led to the establishment of Experiments in Arts and Technology (E.A.T.), a no-profit association launched at the start of the following year that promoted collaborations between artists and engineers with both technical and financial input, thanks to ongoing links with the technology industry. By 1969 E.A.T boasted 4,000 members and various chapters throughout the United States. [9] Klüver’s collaborative model was in fact a two-way process: while on one hand he was convinced that technicians could help artists achieve their objectives, on the other he believed that artists, as visionaries and active agents of social change, could influence the development of technology. This is Barbara Rose’s take on the matter.

If some kind of follow-up had materialized, these early experiments, and the model pursued by E.A.T. – to get acknowledged exponents of the artistic avant-garde working in close contact with engineers, while keeping their respective roles distinct – could feasibly be attributed a key role in the history of contemporary art. So how did it come to pass that the great emphasis placed in the sixties on the “art and technology” pairing by key figures like Jasia Reichardt, Roy Ascott, Billy Klüver, Robert Rauschenberg and Pontus Hultén, as well as Jack Burnham, gradually waned in subsequent years, leaving only the faintest of traces in the official historiography of art? How was it that one of the most significant components of the neo avant-garde ended up as an underground phenomenon, carving out a niche that enabled it to go unnoticed for the next thirty years? There is no single answer to this question: we must rather look to a series of circumstances that emerged during the seventies. In the first place, in this period the “art and technology” pairing found itself up against ideological and political opposition connected to the military purposes of technological research and the considerable financial interests involved. The Vietnam war, and the protests against it from artistic and intellectual quarters, fuelled opposition to the “art and technology” model. “Technology is what we do to the Black Panthers and Vietnamese”, Richard Serra asserted in 1969. [13] Beyond the political sphere, other academics have highlighted the emergence in the late sixties of “anti-computer” sentiment, bound up with enduring concepts such as the romantic vision of the artists and the fear that technology might supersede the individual and undermine the central role of the artist in the creative act. [14] It has also been observed that the critical model underpinning the acknowledgement of the importance of the “art and technology” pairing has encountered varying fortunes. In a 2007 essay, [15] in line with Jack Burnham, Edward A. Shanken asserts that the hermeneutic approach imposed by Alois Riegl and summed up in the concept of Kunstwollen, quashed the theories of Gottfried Semper, according to whom art reflects “economic, technical and social relationships”. In Shanken’s opinion, this approach still endures today, helping to keep New Media Art outside the canons of contemporary art. In the short term, these two prejudices conspired against the operative and interpretative model of the “art and technology” pairing, with a number of significant results: video retired into a niche, despite continuing to have (limited) critical success, above all in works that put formal exploration of the medium in second place, as per the “narcissistic” line plotted by Rosalind Krauss; Kinetic Art and Optical Art, also steeped in technophile rhetoric, vanished completely from the scene, after an initial period of great success, to be rediscovered only relatively recently; even a certain interpretative approach to Conceptual Art – as put forward by Jack Burnham in Software (New York, Jewish Museum 1970) and Kynaston McShine in Information (New York, MoMA 1970) – that relates conceptual work to the advent of information technologies, surrendered to other approaches with less of a technological vein. As for the nascent field of New Media Art, the collaborative model developed by Klüver was well suited to the organization of one-off events, but less to facilitating continuity in artists’ work. Lastly, Computer Art had to come to terms with its aesthetic limitations and the problems involved in actually accessing the machines, which continued to be expensive and bulky. During the seventies computers became more accessible, albeit gradually. Research into increasingly intuitive forms of manmachine interaction made enormous progress, and in 1969 the first distributed network made its appearance, in the shape of Arpanet. In 1971, thanks to the creation of a common protocol among various university and corporate networks, the internet was born. In parallel to this, alongside the cumbersome mainframes, cheaper, more manageable computers appeared: minicomputers (like the PDP-8, distributed as of 1968); microcomputers, like the famed Altair 8800, distributed as of 1975; and home computers, headed up by the equally legendary Apple II (1977), produced by the startup Apple Computer (founded by Steve Wozniak and Steve Jobs in 1976). With the arrival of home computers on the scene, computing branched out of research centers and universities and entered offices and households. A complex, variegated culture sprung up around them, with contributions not only from engineers and high level researchers, but also amateurs and enthusiasts. Many of them had radical political ideas, influenced by Californian counterculture. Much of the New Media Art of the seventies was an expression of this complex cultural milieu. In this context it is not easy to identify figures who can be described simply as “artists”: most of them worked across the disciplines, researchers and employees of the hi-tech industry with an artistic sideline. Douglas Kahn relates, for example, that the first serious attempt to make music with an Altair 8800 was undertaken between 1970 and 1975 by Ned Lagin, who was doing astronaut training at the MIT, but also studied jazz and composition. This work earned him a temporary collaboration with the Grateful Dead. In the same enclave of enthusiasts in the Bay Area there was Paul De Marinis, who worked with Jim Pomeroy and David Tudor on a number of sound installations before starting out on his own artistic career. [16] Visual experimentation received impetus from university and corporate circles. In Stanford University in 1970, the Xerox Corporation opened the Palo Alto Research Centre (PARC), devoted to the development of graphic applications; in the same year, General Electric presented Genigraphics, a graphic system designed for the business world, but used extensively by artists. In 1973, the main computing association in the United States, the ACM (Association for Computing Machinery), set up SIGGRAPH, its “Special Interest Group on GRAPHics and Interactive Techniques”, which organized its first conference in 1974. From then on SIGGRAPH became the main international showcase for developments in computer graphics. This field was to be heavily influenced by the discovery of fractals, described in 1975 by the French-American mathematician Benoît Mandelbrot, then researcher at IBM, as geometric forms that can be split into parts, each a small scale copy of the whole. [17] Throughout the decade, thanks to institutional and corporate support, research into the algorithmic generation of images thus developed, between the more aesthetically and conceptually conscious work of such artists as Charles Csuri, Manfred Mohr and Vera Molnar on the one hand, and the simple deployment of the productive and aesthetic potential of the new tools on the other. Something similar happened with robotics. In 1973 at the University of California San Diego (UCSD), Harold Cohen launched the AARON project, which consisted in developing a form of artificial intelligence capable of painting. Having trained as a painter, over the years Cohen attempted to teach AARON the basic rules of painting, developing its “aesthetic tastes” and decision-making power. The painting done by AARON naturally closely resembles that of Cohen, though the machine did gradually develop its own style over time. In Britain Edward Ihnatowicz, who in 1971 began working as a Research Assistant in the Department of Mechanical Engineering at University College in London, produced his most ambitious project, the cybernetic sculpture The Senster (1970 – 1974), thanks to a commission from Philips, which exhibited it for four years in its permanent exhibition space in Eindhoven, before dismantling it. The sculpture, a 4 meter aluminium structure controlled by a computer, responded to the voices and movements of viewers. In the late seventies and early eighties it was above all telecommunications that lent New Media Art a presence and a profile outside of the corporate/university world. While one to one communication systems (like the telephone) and one to many systems (like mail) elicited the attention of the avant-garde movements and Fluxus, before the advent of the Internet satellite broadcasting was the technology that afforded concrete opportunities to explore the field of communications. In 1973, for the first time in history, satellite technology succeeded in broadcasting a cultural event – Elvis Presley’s concert in Hawaii – to the whole world. On 29 December 1976, with the support of the Contemporary Arts Museum in Houston, the video artist Douglas Davis broadcast the closing minutes of his performance Seven Thoughts to all the IntelSat channels. The following year, thanks to funding from NASA, the Californian artists Kit Galloway and Sherrie Rabinowitz produced Satellite Arts Project ’77, which connected two NASA centers, one on the East Coast and one on the West Coast, via satellite: images of dancers performing in the two centers were filmed and edited, using a simple chroma-key, to form a single live image. In this way, performers physically 3,000 miles apart could act as if dancing together on the same stage. Dance was adopted as a traditional performing art capable of exploring the limitations and potential of technology. [18] In the same year Documenta 6, curated by Manfred Schneckenburger, was devoted to means of communication, with the aim of exploring the position of art in the media society. The exhibition presented photography, video and video installations, and opened up to television by means of satellite broadcasts of performances by Davis, Nam June Paik and Joseph Beuys.

It was above all in the 1980s that artistic work on communication gathered pace, extending to telematics too. 1980 saw two major events, the conference Artists’ Use of Telecommunications, organized by Carl Eugene Loeffler at the Museum of Modern Art in San Francisco, and Hole in Space, a public art project by Galloway and Rabinowitz. The former was an international event that connected up participants in different areas of the globe by satellite, Slow-Scan TV (video broadcast via telephone) or telematic network: from the Center for Advanced Visual Studies at the M.I.T. in Cambridge (USA) to Japan’s Tsukuba University; from the Alternative Media Center of New York to the Trinity Video and Ontario College of Art in Toronto; from the Western Front Society in Vancouver to the Museum des 20 Jahrhunderts in Vienna. Participants included Robert Adrian, Bill Bartlett, Douglas Davis, Carl Loeffler, David Ross, Aldo Tambellini, Norman White, Gene Youngblood and Peter Weibel. The event highlighted the presence of a solid network of traditional art institutions, research centers and media centers. Hole in Space, on the other hand, created a satellite bridge between public areas in two cities (New York and Los Angeles), with large screens installed at the Lincoln Center for the Performing Arts in New York City and the Broadway Department Store in Century City, Los Angeles, respectively. The screens showed live footage from a camera placed beside each one, enabling people in the street, most of whom were unaware that the event was taking place, to interact with others thousands of miles away. The result was a highly participative, spectacular event, that attracted various audiences who explored different levels of interaction and remote communication: relational aesthetics ante-litteram – but also, as it has been defined on YouTube, “the mother of all video chats”. In 1982 it was the turn of The World in 24 Hours, coordinated by Robert Adrian from the Ars Electronica Festival in Linz and featuring a wide range of communications technologies: from phone to fax, Slow-Scan TV and telematic networks, followed in 1983 by La Plissure du Texte by Roy Ascott (Paris, Musée d’Art Moderne de la Ville de Paris), a collaborative text produced by various users connected by BBS, and in 1984 by Good Morning Mr Orwell, a satellite broadcast of video pieces and live performances coordinated by Nam June Paik and produced by WNET TV in New York in collaboration with the Pompidou Center in Paris, seen by more than 10 million people. All these events reveal both the upsurge in interest from traditional art institutions and the great ferment of the field, with the involvement of both companies and specialized centers, some of which came into being in that very decade. The interest from traditional art institutions must however be seen in context. The nascent technologies were the hot topic of the day, and it was not difficult to get sponsorship from the hi-tech industry and television networks. By the early eighties the latter enjoyed an unprecedented presence in society, and critical reflections on the media and their power to manipulate were advanced by artists and intellectuals, and reached the public at large (Sidney Lumet’s film Network, on the power of television, was released in 1976). Moreover, in the decade that saw the return of painting and the explosion of the art market, the institutions took it upon themselves to support less stable, less marketable artistic genres like video, photography and performance. In other words, while conditions were favorable, the reappearance of New Media Art in the establishment art world during the 1980s was conditioned by external factors and was on the whole too fleeting to lead to lasting continuity. All of this emerges clearly if we consider two key events in this decade: the exhibition Les Immateriaux, curated by Jean Francois Lyotard and Thierry Chaput for the Pompidou Center in Paris in 1985; and the 1986 Venice Biennale, coordinated by Maurizio Calvesi and entitled “Art and Science”. The first was not actually an exhibition devoted to the New Media or art numerique, as it is known in France. It started life as a project on the “new materials of creativity”, but the involvement – at a late stage – of Lyotard transformed it into an exploration of post-modern sensibility. As Lyotard said: «It is not our intention to sum up the new technologies in this exhibition [...] or to explain how they work. All it attempts is to discover and raise a sensibility that is specific to post-modernism, and we assume that it exists already». [19] Its press release described it as a “non-exhibition”, and one of its stated aims was to challenge the modern, “prescriptive” model of the exhibition, connected to the 19th century salon and the gallery. In Les Immateriaux works were not hung on the walls: cables attached to the floor and ceiling divided up a decentralised setting, which could be explored in various ways. Visitors were given a walkman with the soundtrack of the exhibition, which played according to their position in the venue: this collage of music, sounds and texts, only some of which actually related to the exhibition, aimed to create a powerful sensation of instability. The event also featured works by conceptual and minimal artists, from Joseph Kosuth to Dan Flavin and Robert Ryman, precursors like Marcel Duchamp and MoholyNagy, and artists working with communication technologies, such as Roy Ascott and Rolf Gelhaar; yet it was the exhibition itself that was designed “as a work of art”, to the point that the actual works on show are rarely mentioned in the numerous comments that the event elicited. [20] Once again, we are faced with a singular contrast: while on one hand Les Immateriaux was of seminal importance for New Media Art, configuring the aesthetic and philosophic categories that were to be its focus in subsequent decades, on the other hand it showed the art crowd that, as Jasia Reichardt commented with regard to Cybernetic Serendipity, this area was yet to produce any definitive outcomes, comparable with those of other artistic tendencies, and was as yet mainly to be appreciated for its aspect of research and experimentation. Similar observations could be made with regard to the 1986 Venice Biennale, where the “Technology and Computing” section curated by Roy Ascott, Don Foresta, Tom Sherman and Tommaso Trini was given a deliberately “workshop” style layout. The central nucleus of this was the Planetary Network, coordinated by Roy Ascott: for three weeks in this workshop in the heart of the Corderie venue, the artists present conducted communicational exchanges of various kinds with other artists in twenty different locations, from Canada to Australia, using three communications protocols: email, fax and Slow-Scan TV. The networking aspect – artists across the globe working together – clearly prevailed over the actual material exchanged: video, images faxed with manual interventions by the artists involved, computer-generated images and texts. According to Ascott, networking and working within a telematic network – with meetings, interactions, negotiations, and visualizations in the electronic arena – was at the core of this show. [21] In the exhibition catalogue, Tom Sherman [22] also returns to the idea of interaction as a founding element of the electronic arts, in an illuminating text that also dwells on their exclusion from the art world in the 1970s and their radical “difference” that continues to make them unpalatable today: their love of machines, feared by the public at large; their propensity for collaborations, which clashes with the rampant careerism of the art world, and the notion of interaction (between artist and machine, between artists via machine, and between machine and public). The 1986 Biennale was undoubtedly a great platform for New Media Art, which in Venice found a unique opportunity to network and succeeded in exploring a large part of its potential. Around the Planetary Network the event featured the most groundbreaking work in computer graphics, as well as less technological, more amateur images; the “first interactive art videodisc” by Lynn Hershman Leeson; a fascinating installation of sounds and coloured lights by Brian Eno, and the sound environment Very Nervous System (1984) by the Canadian David Rokeby: a space controlled by a system of sensors that perceived the presence of the viewer and his or her movements in the area, translated into sounds by a computer. From the 1980s onwards this vast, variegated scene found its first, privileged point of encounter at the Ars Electronica festival in Linz, Austria. [23] Ars Electronica came about in 1979 as a renewed version of the Bruckner Festival, an event devoted to contemporary music accompanied by an academic symposium. The initial idea was to dedicate the symposium to electronic music. But the involvement of the Austrian Broadcasting Corporation (ORF), directed locally by Hannes Leopoldseder, raised the bar. Leopoldseder proposed going beyond the limits of the symposium and creating a permanent festival devoted to technology and its impact on art and society. On 18 September 1979 the first edition of the Ars Electronica festival opened with a spectacular open-air event, in front of an audience of 100,000. The success of this first edition excited the organisers, who began to think about making it a stable thing. The business model behind it had not yet firmed up, and the following editions, up to 1986, took place on a biennial basis. In the meantime the Austrian artist and curator Peter Weibel joined the artistic committee, and from 1986 the event was scheduled to take place every year, with a common theme for the festival and symposium. 1987 saw the launch of the Prix Ars Electronica, a prize – divided into different categories – that was to play a fundamental role in stimulating creativity, as well as establishing a series of critical and qualitative criteria, and developing a hierarchy of merit within the artistic community. In the early 1990s, feasibility studies were undertaken into founding a permanent center, the Ars Electronica Center in Linz, which got off the ground in 1995, accompanied by Ars Electronica Futurelab. The former was conceived as a “Museum of the Future”, gathering and hosting emerging results from the digital medium, while the latter was devoted to production and research, involving artists in courses and workshops and putting the most advanced technologies at their disposal. As emerges from this brief overview, Ars Electronica and the people involved in it were to play a decisive role in establishing New Media Art world as an independent arena. By stimulating debate, proposing categories and criteria of value, facilitating the production and circulation of works, developing a strategic network with other centers, universities and companies and contributing to the development of an economy and model of sustainability for New Media Art, Ars Electronica became its undisputed mecca. Locally, the Ars Electronica model was made possible by the fact that the post-industrial city of Linz was attempting to reinvent itself as the cultural and technological capital of Austria and central Europe. But its success was above all linked to the existence of a flourishing art scene in search of a stable platform for producing and exhibiting its work, not linked to one-off events like the aforementioned 1986 Biennale, and to the slow but ongoing development of an alternative system of festivals and centers like V2_, launched in Hertogenbosch, Holland in 1981 before moving to Rotterdam in 1994, where it stages a biennial festival called the Dutch Electronic Art Festival (DEAF). All these developments are obviously a product of the inexorable progress of technology, which was gradually seeping into everyday life. After the Apple II, various models of home computer appeared on the market: from the Atari 400 to the Commodore VIC-20, the first computer to achieve sales of over a million; from the Sinclair ZX Spectrum to the Commodore 64 and the IBM PC. In 1984, Apple Computer launched the Macintosh, a genuine revolution in the history of the personal computer: relatively cheap (at almost 2,500 dollars), the computer functioned with keyboard and mouse, and featured a graphic interface that replaced the customary green text against a black background. This graphic interface heralded the introduction of common metaphors inspired by the world of the office that the computer was destined for: desktop, wastebasket, windows, files and documents. Lastly, the computer featured a modem, a device that enabled it to connect up to a telematic network via a simple telephone line. Telematic networks also began to spread, and while Internet remained mainly linked to the American university system, some countries (like France with Minitel) created a national network, and on an amateur level BBS (Bulletin Board Systems) took off. These computer systems functioned like electronic noticeboards, with users connecting to them to share or download files and exchange messages. BBS technology first appeared in 1977 and became popular above all thanks to Fidonet, (invented by the American Tom Jennings in 1984), a network of different BBS. But computing did not make its way into households (and the everyday lives of millions) only by means of home computers and networks. In 1961 the MIT labs created Spacewar!, the first videogame in history. It did not take long for the business world to realise that this very basic interactive interface could be the start of a profitable sector of cultural entertainment. In the second half of the 1970s arcade games took off, along with the first home platforms for videogames. From Pong (1972) to Space Invaders (1978) and Pacman (1980), the videogames industry expanded exponentially, and the advent in 1983 of the NES (Nintendo Entertainment System) was to make an indelible mark on the collective consciousness. These developments had conspicuous consequences on the cultural sphere. The 1980s were the decade of hackers, cyberpunk, basic telematics, virtual reality and the start of the free software movement: phenomena which are too complex to be explored in detail here. Cyberpunk, for example, came about as a literary movement in the United States in the early 80s, thanks to the science fiction successes of William Gibson and Bruce Sterling, and the rediscovery of Philip K. Dick, but in Italy it developed as a political movement, attaching onto the substrate of punk, the ferment of the social centers and the left-wing protest movements in 1977. [24] Likewise in California, where a pivotal role was played by figures like Timothy Leary, exponent of counterculture and advocate for psychedelic drugs, who went on to develop videogames, use BBS and become a leading figure of “cyberculture”, and scholar of virtual reality. Both the hacker movement and the Free Software philosophy were rooted in this complex milieu. Artists played an active role in shaping this culture, and enriching its imagery with their works. It is often difficult, if not impossible, to separate the art from the context it is an active, integral part of. The association between New Media and New Media Art formed in the previous decades, but consolidated in the 1980s. This arose perhaps because on one hand, these artists were excluded from – or deliberately avoided – traditional artistic contexts, and on the other because there was a proliferation of hybrid, multidisciplinary figures who did not separate their art from their political activism, or their contribution to the network. In 1986, reviewing an Italian festival, Vittorio Fagone wrote about a “third culture”, distinguishing digital culture from humanistic and scientific culture: a culture in which «engineers, mathematicians, information technologists, architects, musicians and artists (or, if we wish, “visual operators”) and graphic designers live and work together, often exchange not roles but models and objectives. Electronic art occupies this space». [25] In parallel, the system of relationships, events and production centers that conveyed and supported “electronic art”, also firmed up. While in previous decades New Media Art was rooted in the universities and research centers, in the 80s New Media Art became an independent “art world” in its own right and laid the foundations for its continued existence. On the networks debate was conveyed above all on the BBS, while in the real world New Media Art was distributed at temporary events like technology and electronic art festivals, in line with the Linz model. Towards the end of the decade the first “New Media Centers” appeared, really taking off in the early 90s. The advent of these new distribution channels outside of the traditional art world gave the “third culture” fairly sound foundations in terms of visibility, critical debate and preservation. Yet in this regard Italy remained a fairly isolated case. Despite the presence of an active, vibrant art scene (with artists and groups like Tommaso Tozzi, the Giovanotti Mondani Meccanici, Correnti Magnetiche, Mario Canali, Studio Azzurro, Giacomo Verde and, later on, Piero Gilardi and Maurizio Bolognini), the lack of institutional involvement led to a proliferation of autonomous, isolated initiatives, the result of voluntary efforts by curators like Mario Costa and Maria Grazia Mattei, conducted mostly in private venues or peripheral institutional settings. Even now Italy has no Media Centers, and its few active festivals struggle to make a name for themselves internationally.

1989 is a pivotal year in terms of gaining insight into the subsequent fate of New Media Art, and could indeed be taken as the symbolic date in its process of institutionalisation. The initial setting for this was Europe, where specialized institutions (art centers, museums, workshops, archives and festivals) flourished at an unprecedented rate. It was in 1989 that the ZKM (Zentrum für Kunst und Medientechnologie) of Karlsruhe (Germany) was founded, a center that could, broadly-speaking, be seen as the leader of this process. In the same year the fall of the Berlin Wall and the Soviet empire opened an entirely new season, for art too. Russia, together with the countries of Eastern Europe, was obliged to speedily institutionalize contemporary art, which to date had been developing in unofficial situations like squats and private homes. This process was heavily influenced by the billionaire philanthropist George Soros with his Soros Centers of Contemporary Art (SCCA). As Lioudmila Voropai writes, [26] there were some interesting aspects to this process of institutionalization. In the first place, New Media Art had always stressed its “social utility” and contribution to the creative development of the New Media, thus adding to the legacy of confusion between the development of the medium and its use for artistic purposes, between “New Media” and “New Media Art”. This confusion was accompanied by the ambiguous and conflictual relationship between New Media Art and contemporary art, and was indeed one of the reasons behind the conflict: the social utility of New Media Art implicitly opposed the non-utility of contemporary art, which not coincidentally bases its economy on a luxury market.

The conflict between the two became even more pronounced when they were made to coexist in the same institution. The institution in question was the ZKM, the very notion of which speaks volumes about the nature of the relationship between contemporary art and New Media Art in the early 1990s. The two different art worlds coexist here, like a separated couple still sharing the same roof, thanks to an apparently virtuous division into a series of “institutes” and departments, coordinated since 1999 by the director Peter Weibel: the Museum of Contemporary Art, founded in 1999 and also a venue for temporary exhibitions; the Media Museum, which has a permanent, and unique, collection of “interactive media art”, accompanied in recent years by a number of “permanent exhibitions” on the latest developments in New Media Art; the Institute for Visual Media, the center’s “research and development” division (founded and directed by the artist Jeffrey Shaw until 2003); the Institute for Music and Acoustics, the Institute for Media, Education, and Economics, and the Filminstitute. In reality the ZKM only opened its premises, in a converted industrial area, in 1997, but it prepared the terrain with a series of temporary initiatives, like the Multimedia festival of 1989. Its vocation, linked to the orientation of its director (or rather the duo Weibel – Shaw) and its origins in the early 90s, made it into a temple for the interactive, immersive and technologically groundbreaking installations of the last decade of the century, so much so that in Europe the expression “ZKM art” is normally used, tongue in cheek, to refer to this kind of art. [27] Criticism aside, the ZKM has the undisputed merit of being the first in the 90s to raise the question of the “museification” of New Media Art, and issues related to how to preserve it and create a canon, in this way establishing a model for other international players, like Tokyo’s Intercommunication Center (ICC), founded in 1990 and given a permanent venue in 1997. Back in Europe, we have already seen how in the 90s various long-standing institutions like Ars Electronica and V2_ reinforced their position. In the Netherlands sizeable institutional investments in the new media led to the foundation in 1990 of the Inter-Society for the Electronic Arts, or ISEA, that organizes the International Symposium on Electronic Art. This association, which moved its headquarters to Montreal in Québec from 1996 to 2001, before returning to Holland, has an extremely international outlook, as evinced by the itinerant nature of the symposium, always staged in a different location. In Germany, the Institute for New Media (INM) in Frankfurt was set up in 1989 as an experimental workshop in the context of the Art School, before evolving into an independent research platform for post-graduate students. 1988 saw the founding in Britain of the FACT in Liverpool (then known as Moviola), which remains the country’s most important New Media Art institution. These are just a few examples on an international panorama in constant expansion. In this context it is inevitable to take a brief look at what was going on in Eastern Europe, not only for the significant contribution it gave to the development of New Media Art in the 90s, but because what went on there in the space of a decade appears to encapsulate the entire history of New Media Art. In Eastern Europe, up to the 90s, avant-garde art existed entirely outside of the institutional sphere. The Open Society Institute & Soros Foundation Network was the first to make a serious move in this direction. As of 1991 SCCAs were set up in 17 former Soviet block countries. These were relatively shortlived: in 1999, after the Soros foundations were restructured, all the SCCAs became independent non-governmental organizations. For many of them this meant tackling the crucial issue of funding, not always an easy task where public funds for culture were in relatively short supply. But some managed to survive. Supporting New Media Art was one of the key missions of the SCCAs. This came about because in an area where the personal computer was still a rarity and a status symbol, the social utility of the centers lay in their ability to guarantee the population (and the artists) access to the network and the new technologies. In postsocialist countries there was no tradition of New Media Art: information technology was linked to military uses and scientific research, and the embargo which followed the war with Afghanistan effectively prevented Western-made technologies from arriving in Russia. Yet the networking that got under way, and the widespread use of the network, enabled New Media Art to flourish. In 1993 the SCCA in Moscow set up its New Media Art Laboratory, led by Alexei Isaev and Olga Shishko. In 1994 the artist Alexei Shulgin established the Moscow-WWW-Art-Lab, and in the same year Gallery 21, a no-profit venue in the famous quarter of Pushkinaskaya 10 – a squat converted into an art center – opened its doors in St. Petersburg. Leaving Russia, Budapest saw the opening of the C3, the Center for Culture and Communication, which is still up and running, and which combined the traditional functions of an art center with teaching activities, holding courses and workshops on Internet and the new technologies, while Ljubljana opened the Ljudmila Digital Media Lab, promoting festivals and events, and supporting the artistic activities of Vuk osi, one of the pioneers of Net Art. As Voropai notes, the post-Soros era began during the golden age of New Media Art in the West. 1999 was the year of net_condition, a travelling exhibition organised by the ZKM, which opened the season of the major museum exhibitions, destined to continue – above all in the States – until 2002. In Russia the decline of the New Media institutions gave rise to a difficult situation. The affirmation of an uncertain, poorly regulated art market, buoyed up by the new rich, who saw art as a way of laying claim to elite status, did not favour New Media Art, which was held – rightly or wrongly – to be an institutional art form.

This is the situation that has come to pass, in a more recent period, and with the same dynamics, in the West. Here the development of a system of New Media Art, by means of the dynamics we have attempted to illustrate, has gone hand in hand with increasing interest from traditional artistic institutions. Yet the latter tend to be uninterested in the underground tradition of New Media Art, and focus their attention on its most recent results, connected to the mass spread of digital technologies and the advent of the web in the second half of the 90s. Indeed at the start of the decade there were as yet few artists using “domestic” technologies with some degree of awareness, to make art: figures like the Italian Maurizio Bolognini, who in the early 1990s produced installations in a highly conceptual vein by reprogramming and “sealing” personal computers in such a way that their vitality and continued functioning, perceptible as a monotonous hum, could be detected but not visualised through any output devices; [28] or like the German artist Wolfgang Staehle, who in New York in 1991 used various BBS to found The Thing, conceived as a “social sculpture” à la Beuys. And while home computing remained the main arena for the formation of the digital cultures of the 90s, at the start of the decade New Media Art focused above all on immersive systems and virtual reality, telepresence and interactivity (with figures like Jeffrey Shaw, David Rokeby, Paul Sermon and, back in Italy, Mario Canali, Piero Gilardi and Studio Azzurro), technological prostheses and robotics (Eduardo Kac, Stelarc), and 3D graphics and generative algorithms (Karl Sims). But this work involved the use of cutting edge technologies, and was too focused on the latest developments in technology and too detached from the developments in contemporary art in that period to be properly interesting in this context. With the advent of the World Wide Web (Mosaic, the first commercial browser, appeared in 1994), and the mass distribution of the personal computer (1995), this situation changed radically. The computers of the 90s were cheap and featured an intuitive interface; anyone, with a minimum of instruction (which was often undertaken in universities, in the workplace or, for the young generations, by means of videogames) could use them. Processing text, modifying images, and creating sound and video files were relatively simple matters. At the same time the web gave the internet network a multimedia, hypertext interface based on a programming language (html), the basics of which can be picked up in a few days. Making art with a computer no longer required technological training, access to research labs, collaborations with engineers and professionals. Anyone could do it, and not necessarily to make art that was accessible only via computer. So while on one hand computers could be used by any artist, they could also be employed by anyone wishing to exploit the extraordinary communicative, aesthetic and narrative potential of the web. Net Art came about in this very way. It was no longer a question of creating the finest image possible with a given tool, or generating an immersive interface, but about exploring and subverting an elementary language, creating a short circuit in communication, infiltrating a global communications medium. The first net artists did not come from the New Media Art of previous years, but from photography (Alexei Shulgin), post-conceptual art (Vuk osi), film (Olia Lialina), street art (Heath Bunting), painting (Mark Napier) and video (Jodi); they had an artistic, rather than a technological training; some turned to the web out of frustration with the contemporary art world, others were fresh out of art school, and others had links with political activism, which in that very period was beginning to realise the web’s unprecedented potential for media impact (Ricardo Dominguez). Net Art was ironic, subversive and played with the limits of meaning; it looked to the avant-garde and neo avant-garde movements; it practiced pastiche, collage and linguistic games, and it was the output of an era of cultural production that eliminated the difference between original and copy. Net Art originated between 1995 and 1997. In 1997 Documenta, one of the most important dates in the contemporary art calendar, had a section devoted to Net Art. The year before, the Swiss collective etoy won a Golden Nica at the Prix Ars Electronica, in the “World Wide Web” category, for the work Digital Hijack, a spectacular operation of search engine manipulation that diverted hundreds of thousands of internet users onto their site. [29] In the “Computer Animation” category, the first prize went to Pixar, for the animated movie Toy Story (1995), the first movie produced entirely using computer graphics. In the photograph that commemorates the event, an etoy agent with a shaved head and mirror sunglasses, in an orange jacket and black trousers, shares the stage with Japanese interactive artist Masaki Fujihata, Canadian electroacoustic music composer Robert Normandeau, and writer and film director Pete Docter from Pixar: they are all smiling, but they seem to be wondering what they are doing on the same stage. And the question is by no means irrelevant: while 1989 was the key year for consolidating the New Media Art world, 1997 was the annus horribilis of the split between the art and its world: the moment when so called “new media artists” started wondering what they had in common, besides the medium and their under-recognition by mainstream art worlds. The events that we have described, from the eighties onwards, appear to be entirely concentrated in Europe. So what was going on with the States, the homeland of the new technologies and the first artistic experiments in this direction? Lev Manovich accounts for [30] the American delay on this front with two simple considerations. In the first place, the rapidity with which the new technologies were assimilated in the States made them invisible in a very short space of time. In other words, in the US there was no hiatus between the arrival of a new technology and its normalization, the hiatus that enables artists to develop a critical distance from the medium. Secondly, Manovich blames the lack of institutional support, at least compared to areas like Western Europe, Australia and Japan, where the New Media Art world leaned heavily on public funding in the 80s and 90s. In the States the art world is market-driven, and in that context an artistic practice that had always professed its unsaleability had trouble getting by for many years. This, at least, was the case until the late 90s, when the situation changed completely. Universities and art schools set up courses and programs of New Media Art and New Media Design; prestigious academic publishers like the MIT Press began producing books on the subject; renowned institutions like the Princeton Institute for Advanced Studies, the Rockefeller Foundation and the Social Science Research Council set about organizing conferences, prizes and funding, and the major contemporary art museums, from the Whitney Museum of American Art in New York to MoMA, from the San Francisco Museum of Modern Art to the Walker Art Center in Minneapolis to the Guggenheim in New York, together with numerous university museums, got involved with exhibitions, programs and curatorial positions. Even some private galleries, like the Postmasters Gallery in New York, staged solo and group shows of New Media Art. Various no-profit organizations (often led by artists) also appeared, along with specialized institutions like Eyebeam in New York, while existing structures like the Electronic Arts Intermix (EAI) founded by Howard Wise in 1971 and mainly focussed on video, opened up more substantially to the digital media. In other words, interest in New Media Art exploded in the States at a period in which the New Media sector was gaining financial thrust, and New Media Art was becoming financially and technically sustainable for any artist. This phenomenon, however, was fairly short-lived: after the collapse of the New Economy, and the consequent disappearance of the funding that had boosted interest in it, the enthusiasm of American museum system cooled off considerably. At this point the American New Media Art scene was faced with two alternatives, both of which it explored. On one hand it attempted to tackle the arduous task of integrating into the contemporary art system and its market. On the other it looked to Europe with interest, attempting to come up with an alternative model for survival that would enable it to preserve its specific characteristics.

As Arthur Danto wrote, [5] from the sixties onwards (namely from the acceptance of the new “paradigm” introduced by Marcel Duchamp in the 1910s with his first readymades) anything and everything could be art, as long as there was an internal reason for which a given thing should be considered art. Identifying this reason, however, is not always easy. Francesco Bonami, in a book that sets out to explain to the man in the street “why contemporary art really is art”, spectacularly fails in this mission by adopting oblique strategies that constantly avoid the question. In the introduction, Bonami explains that to understand a work of art «all you need is an open-minded approach», curiosity and courage, and that the important thing in art is not the technique, but the idea, which has to be “new” and “right”: «The important thing, in any case and if possible before others get there before you, is to think the right thing at the right time». [6] Yet Bonami does not explain the concept of “new”. In this complete absence of rules, the only one that appears to withstand scrutiny, and that Bonomi returns to frequently, is the central role of the idea. The “right idea”, “good contents”, is the only thing that links Duchamp, who «learned how to generate hot air better than others», and the “reactionary” art of Lucian Freud, who paints «as if Duchamp and Warhol had never existed». I have mentioned Bonami’s dumbed-down aesthetics, rather than more structured theories, because I think it reveals something significant about the arena we are analyzing. One of the most renowned international critics and curators, Bonami does not seem to base his work on a specific “idea of art”. He seems to operate more like a water-diviner, who can see art where others cannot – and is almost always in the right place. Obviously this is possible because when Bonami makes his choice, he has the authority and the means to impose it as the “right” choice to other members of the art world: a consideration that implies a contextual definition of art, according to which art is art because there is a surrounding context that says it is. As Blais and Ippolito explain, [7] this idea is nothing more than intellectual provocation (that of Duchamp) turned intellectual inertia (that of today’s art world). If a work of art is defined by its aura, and if in the age of its technical reproducibility that aura is no longer an integral part of it, the process of “conferring” that aura – namely the work of critics, museums, gallerists and dealers – does not follow but actually precedes the recognition of an object as a work of art. Art is art because critics write about it, museums exhibit it and collectors collect it, not vice versa; the aura is the consequence of this intellectual attention, the interest of the museums, the investments made by collectors, and so on, rather than the cause. [8] This theory, which crops up not infrequently among both those within the art world, and those criticizing it from the outside, is undoubtedly an enthralling one. Also because, once embraced, it is very easy to find evidence to back it up, and very difficult to find arguments against it. By way of example, it is all too easy to look at Damien Hirst, one of the stars of today’s art world, and see the results of canny investments made by an advertising mogul (Charles Saatchi), an extremely solid art world (the English establishment), an unprecedented eye for business (that of the artist) and the concerted efforts of museums, collectors, galleries, critics and curators. It is more difficult to explain why his colored dots mesmerize us, why his butterfly wings fascinate us and why his pharmacies and animals in formaldehyde embody our angst more than many other present day works of art. In other words, it is more difficult to understand whether we would have recognised these pieces as works of art before the art world lent them an aura, variously boosted by the torrents of words used to describe them, the floods of money spent on buying them and the sacral ambiance of the white cube. This problem obviously arises from the weak nature of the few attempts that have been made to come up with a definition of art that transcends the contextual theory. Bonami’s “theory of the right idea” encapsulates this weakness fairly well. Even a vastly more sophisticated theory, like that of the philosopher Mario Perniola (2000) does not seem to yield the results hoped for. Today «we consider it “natural” that some objects are works of art and that some people are artists; any other question seems superfluous», [9] Perniola writes. But just what is it, aside from economic worth and communicative value, that makes art art? According to the philosopher, the answer to this question lies in art’s shadow, «a shady form which contains the most unsettling and enigmatic elements that belong to it». Yet Perniola refuses to define this shadow, conscious that by nature it «disappears when exposed to the light». We can at best identify only a few components of that shadow – the “splendour of the real”, the “sex appeal of the inorganic”, the “logic of dissent”. But shedding light on it necessarily means making it vanish. What seems to emerge from all these “weak” theories is the need for strong contents, art’s ability to home in on an issue, objectivize it and present it for our analysis. This also gives rise to prejudice against media specificity, and art that is not “just art”. This prejudice is linked on one hand to the “damnatio memoriae” that struck Clement Greenberg in the States, and on the other to the fact that art appears to have entered a “postmedia” phase that best manifests itself in multimedia installations, and the nomadic shifting between different media that characterizes the work of many artists. In particular, according to Rosalind Krauss, medium specificity was overcome around the 1970s, on one hand by Marcel Broodthaers with his “eagle principle”, that «simultaneously implodes the idea of an aesthetic medium and turns everything equally into a readymade that collapses the difference between the aesthetic and the commodified»; [10] and on the other by video that, sharing the «television’s “constitutive heterogeneity”», proclaimed the end of medium specificity. «In the age of television, so it broadcast – Krauss writes – we inhabit a post-medium condition». [11] Which does not mean that staying with one medium is inappropriate, or that exploring the specific characteristics of that medium is a cardinal sin. Krauss tries to explain this in another essay, significantly entitled “Reinventing the medium”. According to Krauss, a medium can be rediscovered and reinvented by artists in the post-medium phase when it has fallen into obsolescence: not to explore its creative and aesthetic potential, but to examine it as a “theoretical object” of art.
In Remainder, the first novel by the English artist and writer Tom McCarthy, the main character has survived an accident, followed by a grueling rehabilitation process, that has left him with partial memory loss, but compensation of several million pounds. With this money the character attempts relentlessly to regain the authenticity of some brief episodes of his past and present life by faithfully reconstructing and reenacting them. His first project involves reproducing the atmosphere of a house he believes he has lived in. The setting is reconstructed in great detail (down to the cracks in the walls, the black cats on the roof in front, the sounds and the smells), and various “reenactors” are hired full-time to enable him to relive these moments whenever he feels like it. This is followed by other “projects”, staged with the involvement of hundreds of professionals and “reenactors”: the obsessive reconstruction of a minor accident he once had in a gas station, a murder, a bank robbery. All of this is done to enable him to relive the tingling feeling he experiences when authenticity is achieved. At one point someone asks him: «Does he, perhaps, […] consider himself to be some kind of artist?» To which he replies: «No. I wasn’t any good at art. In school». [13] These lines are telling. They reveal that today’s art is not something you learn at school, and is not necessarily associated with traditional artistic techniques. They also say that art is something visionary and gratuitous; it is not to do with objects, but projects, and it does not produce anything of use, but requires total dedication, generous funds and the involvement of many different kinds of professionals. The artist figure that emerges from this picture is still firmly anchored to the romantic vision of the genius, obviously updated to today’s standards. Figures like Olafur Eliasson, who created waterfalls cascading down the struts of New York’s bridges, and Matthew Barney, who spent five years of his life producing an unprecedented cycle of films, conceived in its entirety as a sophisticated allegory of male genitalia, embody this idea to perfection. The romantic genius acquires celebrity status, and is required to be an excellent entrepreneur of him or herself: think of figures like Damien Hirst, Maurizio Cattelan and Francesco Vezzoli, and further back Jeff Koons and Andy Warhol. If we descend gradually from art’s lofty pinnacles into the complex, variegated fauna of artists, many of these aspects fade away, but the one constant, the one thing we always expect from an artist, is absolute devotion to a project, an idea. With this one lodestar established, everything else is up for discussion, renegotiation. The mythos of complete freedom also admits the option of choosing an entirely reactionary path – that of manual skill, technical prowess, obsessively nurturing a single language. Artists can hide their identities behind a pseudonym or a collective: in this way an academic painter like John Currin can rub shoulders with the likes of Jeff Koons, who has skilled craftsman producing his marble busts. And while the latter, who places himself at the center of many of his works, explores – and reinforces – the cult of the personality of the artist, in contemporary art it is not difficult to come across collaborative platforms, in which individual contributions merge into collective output: the existence of collectives like the Indian RAQS Media Collective – a platform that operates on an artistic, critical and curatorial level – comes as no surprise.
In Mercanti d’aura, Alessandro Dal Lago and Serena Giordano assert that the notion of “purpose” represents an insurmountable barrier to an object being a work of art. If an object has a purpose, it cannot be art, because art serves no purpose; it exists unto itself. And the writers go one further, maintaining that objects created to serve a purpose (therefore the products of worlds such as that of fashion, design and the entire cultural industry) possess disturbing properties that make opposition to them particularly vehement. These objects disturb us because they are artworks in all respects, but also «services marked by the stigma of subordinate work». [14] This theory is undoubtedly a fairly convincing one. Conceived by the aesthetes of the late nineteenth century, the idea of art for art’s sake has stayed with us, in various different forms, in the art and criticism of the twentieth century. Yet continuing to envisage the world of contemporary art as an ivory tower under constant threat from base, secondary practices, is frankly anachronistic. All of the arts have their own “art world”, and most of the artifacts they generate can only be appreciated according to the canons of those worlds. Yet each of these worlds can produce – has produced and continues to produce – a series of artifacts (usually a fairly limited series) able to fulfil the conditions of another world, for example that of contemporary art. This happens for various reasons: because the historic schism between some of these “art worlds” is actually a fairly recent thing, and because certain phenomena that are part of the mythology of contemporary art, like modernism, envisaged a reconciliation that continues to crop up at regular intervals – and, lastly – because the contemporary art world, intended as an arena of free experimentation, unfettered by ulterior motives, has always been particularly receptive to approaches and figures viewed as anomalous by the other art worlds. In other words, the skin of the contemporary art world is much more porous and permeable than that of other worlds, and while it may have proved slightly less porous at some periods in its history, the period that began in 1989, with the fall of the Berlin wall and the recovery of the art market after the recession at the end of the 80s, was undoubtedly particularly open to contamination. In his critical and curatorial work Germano Celant has often highlighted this.
This situation has given rise to two movements: one of appropriation, which encourages artists to engage with other media, be it importing them into the contemporary art world or shifting towards those others worlds, and one of convergence, which sees many hybrid, borderline figures (filmmakers, designers, musicians, etc.) bringing their works into the arena of contemporary art. This does not happen, as might be expected, only on the “borders of the empire”, but at its summit, involving figures of prime importance. Think of Matthew Barney and Shirin Neshat, who have taken works to the Venice Film Festival; think of the numerous artists who have directed Hollywood movies (from Robert Longo to Kathryn Bigelow to Julian Schnabel); or Pierre Bismuth, who won an Oscar for his screenplay for the film Eternal Sunshine Of The Spotless Mind (2004), written with the director Michael Gondry. And think, too, of Takashi Murakami’s collaboration with Vuitton, the double identity of Carsten Nicolai (who also works as a musician, going by the name of Alva Noto), and Peter Greenaway’s nomadism. All of this is also facilitated by internal developments in the contemporary art world, which is increasingly forging a presence as one of the sectors of the cultural industry and show business. And museums and institutions, traditionally more conservative, are facilitating this process, hosting exhibitions devoted to fashion and design, in ways that can be debatable and are indeed debated, but are undeniably forging a trend
The question of how all this is to be reconciled with the traditional conception of the visual work of art, intended as an artifact that is unique (or reproduced in limited editions), collectible, and therefore financially valuable, is constantly being renegotiated, and obviously entails some interesting compromises. In the contemporary art world value is attributed by means of a complex system that includes criticism, museums and other institutions, prizes, exhibitions and the market. Not being able to deal with each of these players singly, I will consider above all the market, which, in my analysis, represents the missing link in the world of New Media Art. The art market has played a key role in the world of visual arts since the nineteenth century, when the arts began gradually severing their ties with the nobility and institutional powers, becoming a private activity mainly destined for the cultured bourgeoisie in search of the social prestige that only a productive relationship with the world of culture can confer. Particularly after the Second World War, art became increasingly bound up with the market: in this way, while the “dematerialization of art” became possible in a period when the market was relatively weak, when the market recovered in the 1980s, and there was a resurgence in demand for marketable artifacts, traditional practices like painting and sculpture rose to the fore once more. The collapse of the stock market in 1989, together with other crucial factors – the new geopolitical situation, and AIDS wiping out an entire generation of artists – played a key role in changing the lie of the land in the early nineties. The phase which followed this, and which is still under way, is a complex one for various reasons. Globalization is bringing forth new art scenarios, new exhibiting platforms and new markets; major temporary art events, like the biennales, are springing up, creating new destinations for cultural tourism; contemporary art museums are being revamped, testing the terrain of the global museum, and becoming artistic objects in their own right, with containers that are often more appealing than their contents, boosting the number of services on offer and becoming focal points of a society in which the services sector, media and culture play a key role; and lastly, the advent of the information society has generated an exponential increase in platforms for criticism, with the launch of dozens of new magazines. The art market spearheads this transformation. Private galleries stage events; by means of contemporary art fairs they increasingly condition the construction of museum collections; by paying for advertising space in art magazines they finance art criticism, and even if the relationship forged between the two is not, at least in the most virtuous cases, a genuine exchange, they inevitably end up conditioning the choices made. Art fairs have grown exponentially in the last decade and some of them (like Art Basel, Frieze or New York’s Armory Show) have established themselves as primary cultural events, key destinations for global tourism, on a par with museum exhibitions and biennales. Lastly, auctions, the main arena for the so-called “secondary market”, have gradually opened up to contemporary art and the so-called “primary market”, their fluctuations influencing the careers of artists. In The Art Fair Age (2008), the Spanish critic Paco Barragán defines art fairs as «Urban Entertainment Centers», [16] and contemporary collecting as a pyramid: on the bottom layer, art is sought after as “social capital”, a source of prestige and affirmation; on the next level art is collected as “financial capital”, namely for its investment value; on the third level of the pyramid we find companies who view art as a “brand” of sure-fire appeal, and include it in their market strategies, while at the top we come to private collectors who seek intellectual fulfillment from art. And the latter are increasingly putting their collections into the public domain, by means of donations to museums (like Giuseppe Panza di Biumo), taking over established institutions (like the new Palazzo Grassi owned by the French entrepreneur François Pinault) or setting up their own foundations (like the Fondazione Sandretto Re Rebaudengo in Turin), thus boosting their influence over the process of institutionalization. The close bond between the contemporary art world and its economy was incisively analysed by the English critic Julian Stallabrass in his book Art Incorporated (2004), which explicitly focuses on the «regulation and incorporation of art in the new world order». [17] According to Stallabrass, art’s micro-economy, governed by a handful of dealers, critics and collectors, is precisely what ensures its freedom from the rules of global capitalism and mass culture. Yet at the same time contemporary art can be seen as a giant metaphor for the capitalist system, with which it has more than one affinity. After demonstrating that the salient characteristics of the art of the 90s – multiculturalism, the success of the installation and the emphasis on youth – are closely linked to its economy, Stallabrass dwells on the way in which the economy of the art world conditions production. The author explains that, while most other art worlds are based on an economy of usage, the core business of contemporary art consists in the «production of rare or unique objects that can only be owned by the very wealthy, whether they are states, businesses or individuals» (p. 102). In recent decades this idiosyncratic economy has had to come to terms with the existence of technically reproducible languages, giving rise to some bizarre compromises: while on one hand photographic works and video exist on the market in very limited series, highly-priced and accompanied by an authentication, on the other, artists like Jeff Koons and Takashi Murakami create digital images which they then get professionals to paint, transforming an infinitely reproducible file into a unique artwork, using a practice (painting) that is manual and entirely traditional. And the ups and downs of the market also obviously influence the type of art that is produced. In the eternal struggle between traditional (and easily marketable) languages, and more difficult forms, the former experience a predictable revival at every economic boom, while the latter emerge more forcefully in every recession, in a «predictable and mechanical process» (p. 107). As for the artists, the idiosyncrasies of the system almost always relegate them to poverty. While there are a few big names who manage to make a killing, most artists are at the lower end of the earning scale. Poverty is at once a side-effect of the particular workings of the system, a contradiction and an ideal: poverty suits art. The artist’s is a high level profession, usually practised by people of high social extraction but low income, who often fund their art with other activities. As Stallabrass concludes: «As a whole, the art market is an archaic, protected enclave, so far immune from the gales of neoliberal modernization that have swept aside so many other less commercial practices. Its status grants it social distinction and a degree of autonomy, even sometimes from the odd market that is at its basis» (p. 114). We might object by asserting that Stallabrass’ vision is a bit too prosaic, that art is something else altogether, something not so exclusively tied to the fortunes of the market. We could object that the present period as it will be reconstructed in two hundred years’ time will have little to do with auction prices, corporate investments and collectors. This is true up to a point, given that the fluctuations of the art economy influence critical debate and the construction of museum collections, as Stallabrass warned us right from the beginning: «the art world is layered vertically and heterogeneous horizontally, comprising many overlapping spheres of association and commerce» (p. 25). This can also be said of the other art worlds, and it is exactly what makes it difficult to reason systematically. At the same time, it is on this horizontal plane that the various worlds intersect, mutually influencing their respective fates.
As we have seen, the world of New Media Art came about to offer artists wishing to experiment with technologies of all kinds the opportunity to do so, removed from the constrictions and limits of a world, the contemporary art world, which is strongly conditioned by its economy and a critical predilection for contents above the exploration of a medium. Far from challenging this configuration, New Media Art criticism merely takes it for granted, and replicates it ad infinitum, to the point of asserting, as Edward Shanken does in Media Art Histories, that contemporary art has never accepted New Media Art because it has always rejected the interpretative model based on the relationship between art, science and technology. [18] Which would imply that it can only be interpreted in this way. In 2006 Gerfried Stocker, director of the Ars Electronica Center and the yearly festival connected to it, returned to discuss this idea of art. The text, rhetorically entitled “The Art of Tomorrow”, [19] is significant from various points of view. Indeed Stocker acknowledges that the current developments in new technologies call for a rethink of the structure and functioning of a festival like Ars Electronica, but does so basically without challenging the idea of art it is based on, namely that art is «a test-drive of the future» (p. 7); that Media Art is «an experiment […] that often brings the creators and proponents of this “new art” into an association with engineers and researchers» (p. 11); and that its basic characteristic is its ability to go beyond an instrumental use of the media as a «medium of representation», making the media not only its tool and medium, but also its subject matter, triumphantly concluding.
In Art of the Digital Age, Bruce Wands [21] depicts the digital artist as someone equipped with technological skills and a good dose of «technological curiosity»; often a programmer, used to working in collaboration with other programmers and IT engineers; attracted to new technologies and viewing art in terms of research and experimentation; a risk-taker who readily veers off the beaten track of established languages and forms to venture into new terrain. Though this definition does not add anything new to what we have said so far, it is an interesting one from various points of view. In the first place, New Media Art appears to have entirely overcome the romantic conception of the artist as genius, and seems to be more interested in returning to the Renaissance models of artist as artisan and artist as scientist. Familiarity with programming also takes the New Media artist into another sociologically interesting terrain: that of hacking (used here in its original sense, freed from the negative connotations attributed by the mass media). It goes without saying that many New Media artists are, and consider themselves to be hackers, to all intents and purposes, and have much in common with hacker ethics: great enthusiasm for their work, limited interest in making a profit, a propensity for knowledge sharing and a belief in the free circulation of information. [22] In 2003, the Net Art group [epidemiC] engaged with this, activating a curious social short circuit. Invited to take part in the Ars Electronica festival, [epidemiC] created Doubleblind Invitation: a program that, if visualized in code form, looked like a beautiful piece of “obfuscated code”, namely formatted like a calligram – a technical feat which holds great kudos in the hacking world, where there are competitions devoted to this particular art form. Yet if executed, [epidemiC]’s code sent out emails – seemingly on behalf of the curator Christiane Paul – to dozens of hackers, fans of obfuscated code, inviting them to take part in the festival. The responses from the invitees, some embarrassed, some enthusiastic, show both the proximity of these two similar cultural niches, and the basic divergence between their two different approaches to programming. This portrait of the New Media artist, albeit an abstract one, appears so far removed from the type of artist cultivated by the contemporary art world that we might be tempted to think that the difference between the two worlds is a question of anthropology rather than history. And while, as we have seen, the contemporary art world is permeable enough to occasionally accept anomalous figures entirely unconnected to the notion of the “career” artist, the appeal of an art world basically without any kind of market economy, devoted to developing knowledge and exploring the arena of digital media, remains strong. Casey Reas is a case in point. Reas is an American artist whose work consists in defining processes and translating them into images. In other words, Reas writes programs that, when executed by a computer, generate animated images that can, if desired, be translated into videos or prints. Unsatisfied with the existing tools, in 2001 Reas, working with the artist and designer Benjamin Fry, created Processing, an open source programming language and freely downloadable program for the creation of images, animations and interactive installations. [23] Processing is now used by a slew of artists, designers and researchers, and obviously Reas himself, who utilizes it in his work. Although Reas works with galleries, he considers himself above all a programmer, designer and researcher: he writes books, holds conferences and coordinates the department of Design and Media Arts at UCLA; and while the resulting products (prints, videos and installations) are produced in limited series, his programs are released with an open source license. He earns his living mainly through teaching and holding workshops on Processing around the world. It is not difficult to come across stories like these in the New Media Art world, just as it is not difficult to meet artists who put their own talent and efforts at the service of temporary collaborative experiments, voluntarily sacrificing their own authorship.

I want to start with the proposition that in a place like New York City, we live in the over-developed world. Somehow we overshot some point of transformation. A transformation that didn’t happen, perhaps couldn’t happen. But in having failed to take that exit, we end up in some state of over-development. In the over-developed world, the commodity economy is feeding on itself, cannibalizing itself.
There is of course an under-developed world, sometimes in intimate proximity to the over-developed one. You can find it even here in New York City. One can critique the orientalism of the fact that Willets Point, Queens is known among New Yorkers as ‘little Calcutta’, but it really is a place without paved roads, running water, and with mostly off the books, illegal or precarious jobs.
But you can forget that under-developed world exists if you live in the bubble of the over-developed world. Some of us don’t have to do the manual version of precarious labor, at least. But there is a sense in which some characteristics of that labor have actually found their way into the over-developed world as well.
Viewed from inside the bubble of New York, the paradox of digital labor these days is the way that tech enables the over-development of under-development. Technologies are shaped by the struggle over their form. It was not given from an essence that the digital would end up as control over labor rather than control by labor. But in the current stage of conflict and negotiation, the over-development of under-development seems to me to describe a tendency for labor.
In any case, labor isn’t the only class struggling in and against the digital. I still think there is a difference between being a worker and being a hacker. I think of hacker as a class category: there is a hacker class. Hackers are those whose efforts are commodifed in the form of intellectual property. What they make can be turned into copyrights, patents or trademarks.
The hacker class is distinguished by a few qualities. It usually means working with information, but not in a routine way. It is different from white-collar labor. It is about producing new arrangements of information rather than ‘filling in the forms.’
As such, it can be a bit hard to make routine. New things just don’t appear on time. Not if they are really new. There’s a kind of ‘innovation’ that is actually quite close to routine, and the hacker class does that too. It’s the new ad campaign, the new wrinkle on the old technical process, the new song or app or screenplay. But the big qualitative leaps are much harder to subordinate to the reified, routinized forms of labor.
The ruling class of our time, what I call the vectoral class, needs both these kinds of hack. The vectoral class needs the almost-routine innovation. The existing commodity cycles demand it. As our attention fades and boredom looms, there has to be some just slightly new iteration of the old properties: some new show, new app, new drug, new device.
What is interesting at the moment are the strategies being deployed to spread the cost and lower the risk of this routine innovation. This is what I think start-up culture is all about. It spreads and privatizes the risk while providing privileged access to innovation that is starting to prove its value to the vectoral class, whose ‘business model’ is to own, control, flip, litigate, and – if absolutely necessary – even build out new kinds of intellectual property.
The other kind of hack, the really transformative ones, are another matter. To some extent the vectoral class does not really want these, no matter what the ruling ideology says about disruption. Having your life disrupted is for little people. The vectoral class doesn’t like surprises. Its goal is to come as close enough to a monopoly in something to extract rent from it.
The kind of mode of production we appear to be entering is one that I don’t think is quite capitalism as classically described. This is not capitalism, this is something worse. I see the vectoral class as the emerging ruling class of our time, whose power rests on attempting to command the whole production process by owning and controlling information. In the over-developed world, an information infrastructure, a kind of third nature, now commands the old manufacturing and distribution infrastructure, or second nature, which in turn commands the resources of this planet, which is how nature now appears to us.
The command strategies of the vectoral class rely on the accumulation of stocks, flows and vectors of information. The vectoral class turns information as raw material into property, and as property into asymmetry, inequality and control. It extracts a rent from privatized information, held as monopoly, while minimizing or displacing risk.
One strategy is to socialize the risk of the real hack. This is probably why public universities and publicly funded research still exist. The tax-payer can take the risk on the really basic research. The university research park model is now set up to carefully modulate access to information about anything that might make a valuable property.
Another strategy is what one might call auto-disruption. Learning from the mistakes of the old capitalist firms of the industrial eras, this model takes the hacker practice in-house. Firms with existing rent-extraction revenue flows become hoarders of potentially monetizable intellectual property, or the people who look like they could produce it. This is to be deployed only when it disrupts somebody else’s business more than one’s own.
So that’s the vectoral class. The problem with belonging to the hacker class in a world the vectoral class rules are these: firstly, certain modest forms of the hack now fall into an outsourced, ‘casualized’, even amateur kind of economy. Certain competences became widespread that there is no way to extract value from them as skills. Certain models of distributed or algorithm-based path-seeking turn out to work as well as hiring the top talent to pick a path for you.
Secondly, more higher-order hack abilities might still command their own price in the market, and one might even end up owning a piece of whatever one produces. But it becomes less and less likely that you get to own it all. One becomes at best a minor share-holder in one’s own brain.
Of course the situation with the worker is even worse than the hacker. The commodification of the life-world eats into the old cultures of solidarity and equality. Everything becomes game-like, a win-lose proposition. The world of third nature, that Borgesian data map that exactly covers its territory, is quite literally programmed to be anti-social.
In daily life there can be a continuum of experience between being a worker and a hacker. They are not absolute categories from the point of view of experience. One can pass from one to the other. Both can be precarious ways to make a living. The white male ‘bro-grammer’ is not the only kind of hacker, just as the blue collar hard-hat is not the only kind of worker.
For worker and hacker alike, the dominant affects are those of envy and jealousy, and covetousness. One is supposed to hate those with just a bit more than you, while at the same time loving those with much, much more. Those with a bit more must be undeserving; those who own everything apparently do so with unquestionable right.
For worker and hacker alike, there is a struggle to achieve some kind of class consciousness, and a social consciousness even beyond that, against the atomizing affect of the time. I just don’t think it is quite the same class consciousness.
For labor, it is always a matter of solidarity and equality. For the hacker, class consciousness is always modulated by the desire for difference, for distinction, for recognition by one’s real peers. It is a sensibility that can be captured by the bourgeois individualism propagated by the vectoral class, but it is not the same thing. Winning the stock-option lottery is not the same thing as the respect of one’s peers. Nor does it translate into any agency in giving form to the world.
It may not come as any surprise that the world this work and these hacks are building is one that cannot last. One might as well say already that this is already a civilization that does not exist. The material conditions that afford it are eroding already. Whether we are adding to the world some quantity of labor or some quality of hack, it is as if we were just building more sandcastles while the tide comes in.
This is the meaning of the Anthropocene: that the futures of the human and material worlds are now totally entwined. Just as Nietzsche declared that God is dead, now we know that ecology is dead. There is no longer a homeostatic cycle that can be put right just by withdrawing. There is no environment that forms a neutral background to working and hacking.
Just as the category of ‘man’ collapses once there is no God, so too the category of the social collapses when there is no environment. The material world is laced with traces of the human, and the human turns out to be made of nothing much besides displaced flows of this or that element or molecule.
The dogma that ‘reality is socially constructed’ turns out not so much to be wrong as to be meaningless. What all the workers and hackers of the world are building is more and more of the same impossible, nonexistent world. We are building third nature as the hyperreal.
Two tasks present themselves, then. The first is to think the worker and hacker as distinct classes but which have a common project. The second is to think that common project as building a different world. Can this infrastructure we keep building out, this second and third nature, actually be the platform for building another one? Can it be hacked?
It is a dizzying prospect. This is why I turn to the work of Alexander Bogdanov, because he thought it could be done. Sometimes it is good to have ancestors, even if they are funny uncles and queer aunts rather than the patriarchs. Bogdanov was Lenin’s rival for the leadership of the Bolshevik party. Shunted aside by about 1910, he turned to two projects, which went by the names of tektology and proletkult.
I think of Bogdanov’s tektology as a project of worker and hacker self-organization that would use the qualitative medium of language rather than the quantitative one of exchange as the means for conveying forms, ideas, diagrams, from one design problem to another. Could there be an art of sharing what works? Could a hack that derives from one design problem be floated speculatively as a possible form or guide for another? Bogdanov’s tektology is like a philosophical Github.
I think of Bogdanov’s proletkult as a project of autonomous worker and hacker cultural production. Bogdanov had a positive, rather than a negative theory of ideology. We all need an affect, a story, a structure of feeling that is really motivating and connecting. Can we be moved and joined by something other than envy, greed, spite, rage or the other click-bait of the game-ifed, commodifed, hyperreal world? Can there be other worldviews and worldviews of the others?
In a way tektology is the work and proletkult the play aspects of building an actual world, in the gaps and fissures of this unreal one that surrounds us. The keynote for Bogdanov was that this had to be a cooperative and collaborative project, based on the worldview of the hacker and worker. This would be a different worldview to both those of authoritarianism and exchange.
We have to think how things work without assuming there is someone or something in charge, a final God-like arbiter, even if it is the hyper-chaos God of the speculative realists. And we also have to think how things work without imagining there’s just a bunch of atomized monads, competing with each other, where the ideal order is magically self-organizing and emergent.
We need another worldview, one drawn out of what is left of the actually collaborative and collective and common practices via which the world is actually built and run, a worldview of solidarity and the gift. A worldview that works as a low theory extracted from worker and hacker practices, rather than a high theory trying to legislate about them from above.
It is not hard to see here what infuriated Lenin about Bogdanov. For Bogdanov, both proletkult and tektology are experimental practices, of prototyping ideas and things, trying them out, modifying them. There’s no correct and controlling über-theory, as there is in different ways in Lenin or Lukacs. There is more of a hacker ethos here, rather than that of the authoritarian worldview one still finds in a Lenin or a Lukacs or in parody form in Zizek, where those in command of the correct dialectical materialist worldview are beyond question.
In Bogdanov’s worldview, there is no master-discourse that controls all the others. There is a continuum of practices, from the natural sciences, through engineering and design, to culture and art. The science and design part is mostly covered by the idea of a tektology; the culture and art part by proletkult. But they overlap, and both matter.
Bogdanov’s openness to the natural sciences, engineering and design are, I think, very contemporary. We only know about things like climate change — and other signs of the Anthropocene — because of the natural sciences. Without the very extensive global knowledge hack that is climate science, we would literally not know what the hell is going on around us. Why these droughts? These floods? These weird changes in the ranges of species, or their sudden extinctions or population booms? None of it would make sense.
Neither Heidegger nor Adorno have anything to say about any of this. But curiously, Bogdanov almost figured out global climate change for himself, between 1908 and 1920. He understood something about the carbon cycle. He understood the need to think social labor as acting on and in and with and against nature, producing a second nature and even a third. He understood the need to build an infrastructure that could adapt to changes in its interactions with its conditions of existence.
Lenin conducted a vigorous campaign to excommunicate Bogdanov, one which the Marxist tradition has strikingly never really revisited or attempted to reverse. This is among other things a great injustice. Bogdanov’s kind of experimental, open-ended Marxism, which neither tries to dominate, ignore, or subordinate itself to the natural sciences, became something of a rarity. His closest contemporary analog is, I think, Donna Haraway. Or so I argue in Molecular Red.
The Anthropocene calls not so much for new ways of thinking as for new ways of practicing knowledge. When the going gets weird, the weird turn pro. And it is likely to get weird — in this lifetime, or the next. That’s why I think we could start working now, not on theory of the Anthropocene, but theory for the Anthropocene. One could do worse, I think, than imagine and practice again something like a tektology and a proletkult – a tektology for hackers, a proletkult for cyborgs. Let’s build a world, and live in it.

The fate of cultural studies in the United States appears to be twofold. On the one hand, it still generates moral panic. Right-wing nut-jobbers think that “cultural Marxism” is some insidious, decadent creed, probably created by Jews and Blacks to destroy America. On the other hand, it has finally become seamlessly commodified. Dick Hebdige, once known as the author of a famous book about subcultures, is now a character in a novel by Chris Kraus that has been optioned for a TV pilot by the makers of the popular show Transparent. These two modes of recuperation were, incidentally, what Hebdige thought was the fate of all subcultures.

Hebdige broke new ground by rescuing subcultures such as the British mods, rockers and punks from the clutches of criminologists who could only think of them under the heading of deviance, and who at their most open-minded wanted to send the social workers after them rather than the cops. For Hebdige, subculture was rather a matter of culture and aesthetics, a form of “resistance through rituals.” This became a rather influential approach, not least in the art world, which is always on the lookout for new sources of aesthetic value, even if it means slumming it.

Angela McRobbie thought this was fine as far as it went, but that Hebdige tended to see subculture as something that mostly young working class men get up to. What happens instead if one looks at the self-making of young working class women as subculture? That too was fine as far as it went, but one has to ask whether the noise and resistance of late twentieth century British subcultures was a generalizable phenomenon. Maybe it was of its time and place. Maybe it was an artifact of a declining industrial working class crossing with the rise of broadcast-era consumer culture in the space of the city.

In her book Be Creative (Polity 2016), McRobbie updates the subculture story, tracing the fall-out from the clash of subculture with the culture industry through to a more recent obsession with the precariat and the creative industries. Is there more than a mere change in terminology here? And what can be learned by tracing the paths of young working-class women through this more contemporary urban landscape?

Drawing on Stuart Hall, and others in the cultural studies field including Dick Hebdige, Paul Gilroy and Andrew Ross, McRobbie takes a close interest in the utopian possibilities of everyday culture, but subjects it to a critical scrutiny, attendant to how popular aspirations are coopted by the commodity form or channeled elsewhere by disciplinary power. The people make culture, but not in a context of their own choosing.

Firstly, McRobbie has to bring the story up to date. Hebdige was writing about a time when subcultures appeared as noise, interrupting the orderly repetitions of the mass culture industry. Mass industrial work at least afforded mass industrial leisure. Subculture was, among other things, a displacement of the aspirations of the working class, shifted from struggles at the sites of mass production to the sites of mass consumption.

That was the case in the sixties and seventies, and the pattern was at least partially recognizable through to the club culture of the eighties and beyond. But from the rise of New Labour in 1997 onwards, all that fell away. Elements of youth subculture got imported into the creative industries. The night time economy of club culture translated into endless work days. A rapid capitalization of the cultural field led to a celebrity media sphere of a more individualistic kind, one that encouraged self promotion and self exploitation. With it came a more extensive detachment from community and class culture. The solution to social problems lay in getting out and getting on.

And what is the role of university in managing this? There is a certain irony here, as “the unexpected outcome of cultural studies is to have found itself canonized as a curriculum for a new creative economy.” (9) Now McRobbie, who teaches at London’s Goldsmith’s College, encounters students who aspire to work in the creative industries, who are often young and childless, but who juggle endless part time jobs while trying to get their degree. The university exists both as a form of credentialing, but also as place for networking.

Those part-time jobs are a means to an end, to an idea of a creative life. Subculture used the space of leisure as one in which a creativity suppressed at and by work could flourish. Now the idea is that work itself can be the site of that expression. Work becomes a kind of romantic relationship. “Work has been reinvented to satisfy the needs and demands of a generation who, ‘dis-embedded’ from traditional attachments to family, kinship, community or region, now find that work must become a fulfilling mark of the self.” (22)

Of course, all this independent, creative work ends up dependent on centrally owned and controlled infrastructure, from which a new kind of ruling class extracts the rent. Around it buzzes the old kind of petit-bourgeois ‘ducking and diving’, of trying one’s hand at this and then that, rather than specializing in a trade or profession. Young people function as the crash-test dummies for new styles of living this old kind of work, as passionate and involving. The older kind of petit-bourgeois could not dream of a million instagram followers.

McRobbie is sensitive to the ambivalence and ambiguity of all this. “What starts as an inner desire for rewarding work is retranslated into a set of techniques for conducting oneself in the uncertain world of creative labor.” (37) From the point of view of the young worker, its about autonomy; from the point of view of the state, “it is a matter of managing a key sector of the youthful population by turning culture into an instrument of both competition and labor discipline.” (38)

Marx had imagined that the petit-bourgeois strata would become progressively proletarianized as big capital moved in and colonized its various market niches. McRobbie describes something like the opposite phenomena. Various strata of what was once a working class is made petit-bourgeois. Capital non longer owes them even a factory or an office to work in. The absence of security is presented entirely as a good thing, as a lack of routine. The generalized urban economy, no longer of culture industry (singular) but the creative industries (plural) gives the young, particularly young women, a feeling of going places. It presents the endless possibility of personal success.

While I am skeptical as to how useful a concept neoliberalism might actually be, it does help account for some aspects of how class is subjectively lived today. Following Foucault, McRobbie traces neoliberalism to the ordo-liberals, German state functionaries and intellectuals who had kept their heads down during the Nazi years, and came up with a more palatable right wing philosophy after it. One that, in an irony of history McRobbie doesn’t mention, displaced the state-socialism of those intellectuals who had rallied to the British state in the cause of defeating the Nazis. The ordo-liberals redefined the human not in terms of labor but as an entrepreneur of its own life-force. It is a kind of market-vitalism, which proscribes a narrow set of rules for human conduct, the sole objective of which is, in every sense, self-appreciation.

While political theorists may dream of such a neoliberal subject, what actual subjects end up thinking and feeling and doing may be a bit more complicated and interesting, and that is McRobbie’s bailiwick. “I see passionate attachment to creative work as comprising ‘lines of flight’, embedded family histories of previously blocked hopes and frustrations.” (46) The class politics of the parent culture that is submerged in commodification used to reappear as subculture, but now (post) subculture is no longer an injection of noise against the hegemonic order but the seeding of new information for it to commodify. Meanwhile the industriousness sustaining the creative industries is provided by a ‘risk class’ without permanent jobs. Creativity promises the reward of realized self; insecurity appears as part of the adventure.

This all seems to confirm the work Eve Chiapello and Luc Boltanski did on how the ruling class responded to the challenge to its hegemony in the sixties by resisting one line of attack yet incorporating the other. The line resisted was the labor critique, in the form of wildcat strikes and factory occupations. The line that was incorporated was the artistic critique, which spoke not of labor but of alienation. It turns out that extracting value out of labor could function just fine without rigid, externally imposed discipline and uniformity. McRobbie: “While the prevailing value system celebrates the growth of the creative economy and the rise of talent, the talented themselves are working long hours under the shadow of unemployment in a domain of intensive under-employment, and self-activated work.” (153) McRobbie works this observation through a study of the work of Richard Florida, Ricard Sennett and the Italian ‘workerist’ school and its descendants, such as Paolo Virno, Franco Berardi and Maurizio Lazzarato.

McRobbie offers a less rosy view than that of Richard Florida, with his celebrations of thecreative class that populates prosperous cities, for whom the old working-class districts become gentrified playgrounds. McRobbie points out that Florida’s sunny vision is the flip side of what for Loic Waquant is a decline of sociological explanation about how urban space actually functions. The occupation of the city as a space for creative class play takes place against a background of mass incarceration which criminalizes a whole other urban population. On the one hand, part of what was subculture can become the creative industries; but on the other, a part of it no longer gets the social-worker treatment but goes straight from school to prison.

Where Richard Florida celebrates the hipster version of the creative industry, Richard Sennett prefers a more traditional version of of the value of ordinary work and craft labor. As McRobbie notes, there’s something patriarchal and conservative about some of Sennett’s attraction to old guys working steadily with their tools, but there might yet be something to draw out of this counter-model to the embrace of the creative industries.

Sennett sees work as life-enhancing and not mere drudgery. Here he thinks about labor quite differently to Hannah Arendt. However, for Seennett, changes in work may have led to a corrosion on character. Perhaps this could be reversed by returning to older habits of cooperative labor with their ethic of the job well done. McRobbie thinks there might be value in putting creative work alongside supposedly ‘uncreative’ craft work to counter the romance of creation, although one has to wonder if this is just another romance.

There are also tricky issues here of how any kind of labor might give rise to some form of intellectual property separable from the thing itself, which might at one and the same time yield an ‘author’ — and owner — and on the other a means of controlling the market in a particular line. Unfortunately, McRobbie does not pursue this, perhaps because her example is fashion, where intellectual property intervenes mostly at the level of the brand and the trademark rather than the individual designs.

All the same, Sennett offer a way of thinking about craft labor as one of the rhythms of the city, which has a certain value in its impersonality. It is a less grandiose way of thinking work; less about genius, talent, inspiration and competition, to which one might add — less about intellectual property. “A craft approach means being able to work all the time with failure…” (156) Craft skills are within the reach of most people. Its not an elite sensibility. Its reach is local. All well and good. “But the patient labor of craft is likely to remain a distant ideal for freelancers working on a piece-rate system and having to cut corners.” (158)

Craft may have little place in the contemporary cities of the over-developed world, with their sundering of local ties, temporary social relations, and relentless corporate culture of ‘team work.’ There might be some capacity for resistance (or — dare we hope — political innovation) embodied through memory and family history, but it may no longer take a subcultural in form. Perhaps it is in the residues of a craft sensibility which show up in the creative industries. Sennett provides a ‘parent culture’ view. Crafters and artists could do well to look it up, as it is their story.

McRobbie wonders too what would happen if the kinds of labor traditionally thought of as women’s work get the same treatment in Sennett as his craftsmen. “Where it may be fruitful to downgrade the dizzy expectations of artists and creative people so that they can sit alongside others, and benefit from the time-slow pace of a mode of working that gratifies on the basis of a job being done for its own sake, it proves more difficult to upgrade some stubbornly unrewarding jobs such as domestic cleaning.” (160)

One might pause here to consider the loss or invisibility of familial or community connection to such an ethos of craft labor. Thus the neo-bohemians populating Chicago’s Wicker Park in Richard Lloyd’s study are not able to see who is no longer living among them. Meanwhile thedigital artisans Andrew Ross studied at New York’s Razorfish advertising agency could be self-ironizing about their cool sweatshop and the cult-like commitment it extracted from its associates, but they have not much appreciation of how their laptop creations could end up as the decorations on merchandise made in actual sweatshops. In any case, things have moved on. Wicker Park and Razorfish are names from a forgotten era, even if it was only a decade ago.

Rather than try to keep up with the ever-changing fascination with cool neighborhoods and cool employers, McRobbie returns to a study Jacques Rancière did in the seventies, about nineteenth century workers. In Proletarian Nights, Rancière looked to workers whose aspirations were not limited to forming unions or cooperatives or political parties, or even to demanding the abolition of work. These deserters from the class struggle wanted a different kind of work. They wanted independence, and they expressed that desire in things like poetry — of an often quite formal and traditional kind.

McRobbie connects this to the British cultural studies tradition, which had shifted attention from the sphere of production to that of consumption in order to understand how the desires and ambitions of labor had sought expression there. Taken together, these parallel French and English approaches took an interest in non-traditional kinds of ‘politics’, if that is still the word for it. The British approach, more cultural than capital-P political, took the disco or the kitchen table as significant sites. McRobbie: “These communal, familial, collective or indeed institutional spaces permitted alternative working lives to be imagined. Cultural studies therefore anticipated a neo-Marxism open to difference and diversity, open to the equal stature of the family and the community alongside that of the workplace and the sphere of formal politics.” (58)

But this popular culture of labor’s aspirations and capacities could in turn be instrumentalized. In the British context, the significant changes happened under so-called New Labour. Creativity became a kind of labor reform, in which the artist would stand as a model for a new kind of human capital. McRobbie: “These were the Damian Hirst times.” (42) Art and culture were put to work. This was a kind of transitional model, replaced in more recent times by the idea of tech-centric innovation. The basic formula is not that different, however. The new-model worker is to aspire to apply creativity to achieving individual success and celebrity. In both its creative and innovative flavors, this is a model hostile to traditional or ‘elitist’ version of either culture or the social. It can sometimes have a vaguely inclusive rhetoric. It is meritocratic, but does not pause for too long to ask if winners really started from the same starting-blocks as the losers. And of course, it never presents all this from the worker’s perspective. Workers are supposed to go away. The labor movement is replaced by networks of demassified, autonomous free agents.

Ironically, cultural studies itself became a kind of textual material that could be reworked into this image. For example, Paul Willis on working class youth creativity got repurposed in the language of ‘New Times’ post-labor politics, initially sponsored by the Gramscian wing of whatever was left of the Communist Party. This in turn became language for New Labour. It would be churlish to hold against cultural studies what others did with it. McRobbie defends Stuart Hall as as trying to cope with rise of post-Fordism and its effects on consumer culture via a new popular politics. “Hall’s expansive ideas for how the left could forge a new popular politics were taken up and deflected in unexpected right wing directions…” (68)

A characteristic of British culture that sets it part a bit from the United States is that for a long time had maintained a public education system that opened a pathway into the arts for talented and often disaffected working class kids. For McRobbie and others in cultural studies, this was the site of displaced anatagonisms from the factory floor. Where that tradition ended however is probably with the artist-celebrity as champion of so-called neoliberal ideas of self-making, of which Tracey Emin and Damien Hirst would be avatars.

What the ordo-liberals probably did not anticipate was that the artist would in some ways become the ideal type of the neo-liberal subject, who would engage in a knowing self-exploitation in pursuit of the dream job. McRobbie thinks there are three subtypes of the artist, although I don’t find the categories too convincing. They are the socially engaged artist, the global artist and the artist-precariat, with the latter forming a kind of critical refusal from within in the ideal neo-liberal subjectivity of the artist. McRobbie: “The rhizomatic tactics and strategies of such creative activities are totally incommensurate with the vocabularies of the toolkits and business studies modules and thus can be seen as a direct challenge to the ‘entrepreneurial university.'” (84)

Where the modernist artist was the exception to the culture industry; the contemporary artist is the exemplar of the creative industries. The category of the creative industries is of interest not least because it blurs the line between fine art and applied art, or craft. Postmodern art’s stylistic complications of the boundaries between the aesthetic and kitsch seem tame compared to the extended commodification of the production of information of all kinds.

McRobbie is more interested in the more ‘vulgar’ kinds of creative industry and the young women drawn to them. There she finds enthusiastic career girls, performing elaborate body rituals that are coded by a kind of post-feminist masquerade. They perform so-called immaterial labor and emotional labor, or what McRobbie calls “passionate work.” (89) They don’t entirely disavow class or ethnicity or community. They just see a narrow path to a more passionate life that involves some compromises. Normative femininity is a way to cover over traditional working class traits that may be disabling in the workplace. Feminism opened up a path of opportunity but one now reclaimed by a more traditional-seeming code of femininity.

“Capitalism makes a seductive offer to young women with the promise of pleasure in work, while at the same time this work is nowadays bound to be precarious…” (105) These women tried to refuse work as a way to escape from monotonous jobs in favor of self-directed activity, but this then has become recuperated too.  McRobbie: “… the idea of ‘romance’ has been deflected away from the sphere of love and intimacy and instead projected into the idea of a fulfilling career.” (91)

Compared it to Italian workerist thinkers, where a rather masculinist approach to politics remained standard, McRobbie’s cultural studies approach opens up some interesting questions were labor and gender combine. By contrast to the ‘Bologna school’, the Birmingham school moved from the factory floor to everyday life and uncoupled different kinds of struggle. McRobbie: “without a concept of ‘culture,’ the idea of ‘the street’ can only connote a weaker space which is not the shop-floor and hence not primarily an expected location for class politics. In this thinking the idea of the factory floor still takes precedence even when the workforce is in flight from it.” (95) Where the workerists spoke of the social factory; cultural studies might speak of the social kitchen. A change of metaphor here might alter how we think of what became of both labor and culture in the era of the so-called creative industries.

The workerists still treat the classic class antagonism of capital and labor as central, whereas the culturalists treated the political and cultural levels of the social formation as equally substantive. In the spirit of EP Thompson and Raymond Williams, cultural studies saw culture as a popular landscape of resistance and protest. The workerists thought there was a new kind of subjectless class politics to which post-Fordist production processes as the response. “Yet lacking a strong concept of working-class culture these authors can only rest their case on the refusal of work…” (97) Even if they were never quite clear what refusal meant. Meanwhile, the culturalists expanded Gramsci’s conception of a popular culture (although after Gilroy no longer necessarily a national one) as a common resource.

McRobbie retrieves from the workerists the idea of the line of flight, the desire to escape, and mobility as response to labor. Of course not all workerists (and post-workerists) are enthusiasts for these lines of flight. Lazzarato and Berardi, for example, becomes quite pessimistic. McRobbie’s question is whether young women get the same chance at the ‘immaterial’ as young men. Is today’s labor market just as (or more) gender segregated? Is there a return to a kind of neo-traditional sexism? Or, more broadly: “How then can we talk about the gender of post-Fordism?” (101)

McRobbie: “… refusal is more of a desire and a yearning for rewarding work, something that is within sight and perhaps within reach through access to further and higher education. This ‘flight’ also acquires gendered characteristics. The impact of 1970s feminism made the idea of a career for young women completely acceptable. Unlike the autonomist Marxists, I do not make such fulsome claims for a new radical politics emerging from the ‘social factory,’ instead I see a field of ambivalence and tension, where lines of flight connect past parental struggles with the day-to-day experiences of their children in the modern work economy.” (93)

Despite the utopian promise of passionate work as escape from traditional labor, it ends up being a desire that can in turn be exploited. Passion becomes a means of production, complete with precarity, long hours and low pay. “I pose the idea of passionate work being a distinctive mode of gender re-traditionalization… whereby the conservatism of post-feminism re-instates young women’s aspirations for success within designated zones of activity such as creative labor.” (110) Passionate work becomes self-exploitation, complete with its own codes of affect — a permanent appearance of enjoyment — and a bodily style of exuberant enthusiasm.

Creative work has become separated from ordinary labor, but does it follow as McRobbie thinks that it is thereby depoliticized? Maybe there’s another kind of politics for something that is not exactly labor. It is the case that a wedge was driven between creative labor and other kinds, thereby weakening social democratic politics. But perhaps the strategy is not then to bring the former back into the latter. McRobbie sometimes sounds as nostalgic as Sennett, if not exactly for the same image of the past. Rather than a neoliberal vocabulary of the entrepreneur, or the old social democratic one of industrial labor, perhaps its time to think of another one that might more accurately map onto the class formations of our time. Rather than reverse the neoliberal turn, lets take a new turn.

McRobbie encourages us to look to less masculine-coded practices for signs of possibility. She is interested in women crafters who make public women’s traditional skills, such as the yarn-bombers weaving public artworks that knit bicycles to lampposts. There’s an ambivalence to these scenes. In part, they look back to a rather traditional and idyllic culture of femininity. On the other, they sometimes draw from those pasts to create more self-consciously feminist practices in the present. As always with culture, there’s tensions and ambiguities which can be fruitful and interesting. This scene, as an example, is a sort of return to the craft critique of production of William Morris, without the paternalism.

Perhaps it is not quite the same to be making the old things and to be making new information. Perhaps the latter seems so lacking in the history and culture of labor because it isn’t exactly labor. The temporality of its production may not have much in common with the patient persistence of craft-work. Its relation to the commodity form is rather different. It does not produce the thing to be sold as a piece of property, but rather creates an arrangement of information that is novel enough to count as intellectual property. It is so easily copied that other strategies have to come into play to extract value from its production, which is where creating the aura of special skill around particular creators comes into play. The networks within which information is made are partly local, and as yet nothing beats the city as a way of organizing it. But its networks also extend beyond far urban space. The infrastructure of information makes the physical and informational aspects appear quite separate, although there is nothing immaterial about it.

In short, maybe this relatively new kind of producer is as unlike craft labor as it is unlike industrial labor. It comes into being only at a time when information can be private property, and yet information can be rapidly and fully copied ands shared. Its a relatively new set of forces of production that make it possible — information technology. It is both shaped by, and exceeds, the relations of production extruded out of the property form to embrace it — intellectual property. Maybe it even produces quite distinct class relations, between producers and owners of information.

These aspects of the creative industries I find neglected both in British cultural studies and Franco-Italian workerist theory. One might however draw from McRobbie an attention to gender in how creative industries have evolved. If one looks at fashion as the archetypal form of creativity and tech as the archetypal form of innovation, one finds very strong imposition of very conservative ideas about what is men’s work and what is women’s work — even if neither is exactly work any more.

One thing that the left and right now seem to agree on is that the society in which we live is called capitalism. And strangely enough, both now seem to agree that it is eternal. Even the left seems to think there is an eternal essence to capitalism, and only its appearances change. The parade of changing appearances yields a series of modifiers: this could be late capitalism or communicate capitalism or cognitive capitalism or neoliberal capitalism. But short of an increasingly allegorical or messianic leap into something other — it is as if this self-same thing just went on forever.

Maybe its because I have a taste for old-fashioned modernism, but whenever I come across a piece of language about which there is such wide consensus I want to trouble it, somehow. This capitalism that we have all agreed that we live in: has it not become too familiar, too comfortable an idea? The reality the term tried to describe is of course far from comfortable. Capitalism, if this is what this is, appears to be smashing not only the social but the natural conditions of its existence to pieces. But then maybe this is the thing to ask about. Why have we become so comfortable with a way of describing an uncomfortable reality? Do we want a certainty in language that can’t be had anywhere else?

That the world we live in is capitalism has become a familiar way of describing something that destroys what is familiar. It atomizes and alienates. It renders everything precarious except its own grasp on the imagination. If the greatest trick of the devil was to persuade us that the devil does not exist; then maybe the greatest trick of capitalism is to gull us into imaging that there is nothing but capitalism.

It is hard to describe things that change imperceptibly.  This may well be the level of language on which the problem rests. It has to do with using the combinations of language, which have something of a binary quality, to describe changes that might be gradual or might be swift, but which aren’t neat digital divides between one term and another. It is as hard to describe transitions between modes of production as it is to describe changes in mood.

There was once a language about transitions between modes of production. It is striking how the left and the right alike ended up working within the same language about this. Marx really was one of the great modern poets. Of course he worked with the materials of the languages he had to hand, but he wrought something lasting: a combinatory of terms for describing history. Like any great poetic corpus, his work contains multitudes. But there are a few standard permutations than came to stick in the mind, like great pop songs.

Here I think is his greatest hit, one that has become something of an earworm. It goes something like this: this is capitalism. It has an essence and it has appearances. Its essence is defined by these things: the commodity form, with its doublet of use value and exchange value; by labor’s double form, as concrete and abstract labor; by the extraction of surplus value in the production process, by the wage relation, by the rising organic composition of capital, by the crisis of the tendency of the rate of profit to fall. And finally, by negation.

There are actually two variants of the poem here, about negation. Either capitalism negates itself, brought to ruin by its own contradictions. Or: it is negated by a force it produces as its own negation, the working class. In either variant, one thing is key: capitalism can change its appearances, but never its essence. Its essence can only be negated, by contradiction or struggle. Various variant tunes spill out of this rhetorical frame, like mutating like genres of techno music.

There are other variations. One can swap out the abstract verb negation and replace it with acceleration. This is currently popular again, as it was in the twenties. Here the idea is that there’s nothing that can negate capital, either in its own contradictions or in the force it produces in and against itself. Rather, the best one can do is accelerate it to its end, towards a Promethean leap into another historical figure. But note that this is not as much of a change in tune as its advocate like to imagine. It leaves intact the rhetorical form of capital as an essence.

The essence of capital is eternal. This is the striking feature of how it is now imagined. Those who love it of course embrace this thought. It needs merely to be perfected by our love. This is sometimes called, with a stunning lack of imagination — neoliberalism. But what is even stranger is that those who do not love it seem to agree. The essence of capital is eternal. It goes on forever, and everything is an expression of its essence. Capital is the essence expressed everywhere and its expression is tending to become ever more total.

The other side of the eternal essence of capital is its ever changing appearances. Change is accounted for via the use of modifiers. Its appearances can even be periodized. There was merchant capitalism then industrial capitalism, then monopoly capitalism then neoliberal capitalism. There’s some ambiguity as to what to call the current stage, however. It could be multinational, cognitive, semio, late, neoliberal, or postfordist capitalism, to name just a few. Note that the last two of these are temporal modifications to a modifier: neoliberal, postfordist. Could there be any better tribute to the complete enervation of the imagination by capitalism, or whatever it is, that this is the best our poets can do? Modify the modifier? Capitalism must be very disappointed in our linguistic competence.

Of course there’s the opposite rhetorical tack as well, which is to go a bit overboard with the binary difference between two terms, although its partisans have not been so bold as to break too much with the essence of capitalism. Rather, it worked like this: there used to be material labor; now there is immaterial labor. Its a different kind of labor. Its the opposite! But its still only a modified capitalism, a cognitive capitalism. Its not material any more. Capitalism itself is about ideas. Its striking how much one can get carried away with the play of language, and forget to look at the world. Somehow, I don’t think the hundred million industrial workers of China perceive their work as immaterial.

The task of this talk is thus a provocation: to think the possibility that capitalism has already been rendered history, but that the period that replaces it is worse. That it could be worse gets us away from the happy narratives in which capitalism gave way to a postindustrial society or some other magic kingdom, free from contradiction and class struggle. Rather, in this thought experiment, I propose to think the present as a new kind of class conflict, including new kinds of class arising out of recent mutations in the forces and relations of production. But putting this pressure on our received ideas and legacy language, perhaps we can begin to see the outlines of the present afresh, estranged from our habits of thought.

There was once an attempt to have done with at least part of this great rhetorical-historical edifice. It started with questioning the idea of capital as having an essence and an appearance. What if appearances were as equally real as the essence? There were actually two version of the essence-appearance structure. One took economic to be the essence, but in the sense of being the base, and everything else was dependent on it. This version is called economism. In the other version, its not the economic, but the commodity form that is the essence, one that has come into being in history and then become the essence of history, which records its forms of appearance as a false totality, as spectacle.

Against this, some took the view that the economic only determined everything else in thelast instance, that things like politics and culture were not mere appearances but had their own material form, but one whose function was to reproduce the essential economic form of capitalism.

If things like politics or culture are relatively autonomous, if they have their own material form, maybe they even have their own essence! It did not take long for culture to have its own essential categories: the signifier and the signified were just like exchange value and use value! An abstract essence! But a different one! So one could just specialize in singing the song of this (relatively) autonomous world of essences and appearances, while still gesturing to the master-narrative, that this is indeed and will remain, capitalism.

If the economy has an essence and appearances, and culture too has an essence and appearances, then maybe politics does too! The wonderful thing about language is that if you seek it you can find it. Yes, politics has an essence, the great fundamental drama of friend versus enemy, or maybe its dissensus, or something. The main thing is we can sing the song of the essence and appearances of politics, while still gesturing to the master-narrative, that this is indeed and will remain, capitalism.

I have to say that my inner modernist finds this all rather banal. Is this the best we can do to speak the sublime language of our century? Why does it all seem the same? Like pop music? Variations on themes, all leading back to the same old note, that capital is eternal? That one day (that will never come) there will be a messianic leap into something else, but until then, let’s just go to the movies. It seems to me that our poetry of capitalism, or whatever it is, shows all the signs of being a culture industry. Nowhere in these tunes is there that striking note of non-equivalence, or that moment of de-familiarization when the roof falls in.

Perhaps one has to ask: what the emotional attachment that we have to the idea that this is capitalism, and that it is eternal? It has to be said that the most vigorous attempts to tell a different story, to strike a different tune, where made in bad faith. Still are, perhaps. There was a time when it was a popular art form. Once the narrative of capitalism and its coming negation got out, you could make a good living coming up with a different story. Not surprisingly, it was former Marxists and socialists who came up with most of those alternative stories.

Thus we had the story of the managerial revolution, of the postindustrial society, of the conditions for take-off and growth. What these stories all had in common was that they accepted the basic premise of the Marxist story. They conceded its power, its poetry. But they changed the ending. Rather than negation, the story ended in a resolution of contradictions. These were extorted reconciliations. But they had some currency nonetheless. But with the collapse of the supposedly socialist world, which at least pretended to live up to the great Marxist story, these counter-narratives lost their force.

One counter-story from that era survives. It was not written by a socialist, although he briefly worked for a socialist government. In this story, capitalism negates itself, and in a good way. It can pivot and disrupt itself. Indeed, its essence becomes its self-disruption. And it is our sacred duty never to get in its way. This is the rhetorical art-form of the ‘California ideology.’ Into it can be folded certain other variations, about the fourth industrial revolution, for example.

The conceit of all these post-capitalist stories was that this is not capitalism, it’s better! When people hear the beginnings of a story about this no longer being capitalism, their resistance generally rises at this point. Unless you happen to be worth several million dollars, the chances are you do not perceive this as something better than capitalism.

But maybe it would be interesting, politically and aesthetically, to take the other fork of the binary here. Instead of the idea that this is not capitalism, ts better, what if we explored the idea that this is not capitalism, but worse? This also meets a lot of resistance. This I can tell you from experience, having tried to write variations on this text for fifteen years. Nobody wants to leave the certainty of the devil they know, or think they know, for something that promises to be worse.

Interestingly, few people will even attempt it as a thought experiment. There really is something fundamental to the belief that this is capitalism. It may even be the defining feature of ideology today. Ideology today is not the acceptance of a neoliberal structure of feeling or habits of thought and action. Ideology today is clinging to the belief that this is capitalism.

Another way to tackle this would be impute some meaning to Marx’s famous remark to the effect that he was not a Marxist. What if what he meant by that is that he was not one of those who simply took a language and a rhetorical form extracted from his texts as a given? He was, to the contrary, the one who had constructed that language with a quite particular purpose in mind: to understand the situation of his times from the labor point of view. So: what if we kept the commitment to understanding, not his times, but ours, from the labor point of view, whatever that might mean now — and bracketed off the rest?

That makes a certain sense to me. I really am puzzled by why we should use blocks of linguistic material from his time again to understand our time. Why use the fashionable philosophy, the popular science, the political tracts, or the technological metaphors of the mid-nineteenth century? When poets or novelists do that, we immediately think its dated and quaint. But somehow we want our great narrative to be about capitalism, even if it is dated and quaint.

Of course different genres of text have a different relationship to tradition and innovation, and at different moments in their development. They aren’t always in synch. And of course there’s generally a culture industry in which the texts get pulped into sameness, and an avant-garde trying to do something else. If you are trying to write an interesting, rather than merely successful, novel or poem, you want to change things at the formal level, rather than ship your wine in the same old bottles. The thing is, where readings and rewritings of Marx are concerned, they seem to me to belong to the culture industry. Its a commonplace now to read Capital as a work of philosophy or an epic novel, but to do so very conservatively. And indeed could there be anything more conservative now that the tradition of continental philosophy?

I have not named names in this text, partly to avoid embarrassing its characters. But mainly because I take it as given that texts writes their author, rather than their author writing them. Authors are never good guides to their own writings, as the writings exceed conscious intention — although I would not take that insight as far as the psychoanalytically inclined, who maybe create too big an interpretive playground for themselves out of it. So in describing my own attempt to write within the space, all these caveats also apply to me.

It has not always been the case that Marx was read conservatively, as a great text for explication, interpretation and imitation. Where the Marxocological savant became a master simply by producing a variation on the theme. There are those who read Marx the same way they read Rimbaud and Lautréamont. I’ll mention just three: Aimé Cesaire, JBS Haldane, and Guy Debord. From the latter I’ll also take a few clues about method. Could there be a way to write after Marx that isn’t based on conservative habits of mastery and interpretation, but which are based instead on experimentation and détournement?

Of course, being a very minor poet, I did not get very far. But I gave it a shot. I wrote a way of describing the current situation that is not capitalism, but worse. Here’s how: what if, rather than start at the beginning, one started at the end? The capitalism story always starts in the past, with the birth of capitalism, and imagines a destiny, a teleology, wherein the present must be some continuum from that past. This must be some modification of the essence of the thing. Let’s do it the other way around. Let’s first describe the present, then secondarily figure out where it came from. This may even, in the end, involve modifying our understanding of capitalisms past. In short, let’s start where Marx started, describing a present — not from his results.

Let’s start by being very ‘orthodox’ — (I use the term ironically). Let’s start with the forces of production, with the relations of production that correspond to them, the class conflict generated out of those relations of production, and the political and culture superstructures that correspond to that base. And let us also, just as Marx did, try to describe what may be emerging, rather than what is established. If one starts with what is established, it is easy to interpret any new aspect of the situation as simply variations on the same essence. Starting with what is emerging provides a suitable derangement of the senses, a giddy hint that all that was solid is melting into air.

The thought experiment that might result is quite simple. What if it was like this: There really is something qualitatively distinct about the forces of production that produce and instrumentalize and control information. This is because information really does turn out to have strange ontological properties. Making information a force of production produces something of a conundrum within the commodity form. Information wants to be free but is everywhere in chains. It isn’t scarce, and the whole premise of the commodity is its scarcity.

Information as a force of production called into being particular relations of production. In classic Marxist style, one can look here at the evolution of legal forms. What we see there is the emergence of intellectual property as close to an absolute private property right. One that makes the once separate and local property forms of patent, copyright and trademark equivalent forms of private property. Forms which, as the negotiations on the Trans-Pacific Partnership make clear, need transnational forms of legal enforcement, precisely because information is such a slippery and abstract thing.

And so, like the enclosures or the joint-stock company before it, intellectual property law becomes the form of a new kind of relation of production, more abstract than its predecessors, and one which makes not land or physical plant a form of private property, but information itself. Like those preceding forms of private property, this one gives rise to a class relation. As an absolute form of private property, it creates classes of owners and non-owners of the means of realizing its value. Land as private property gave rise to the two great classes of farmer and landlord. Capital as private property gave rise to the two great classes of worker and capitalist. Is there a new class relation that emerges out of the commodification of information?

For argument’s sake, let’s it’s say it does. I call those classes the hacker class and the vectoralist class. The hacker class produces new information. What is ‘new’ information? Whatever intellectual property law recognizes as new. Its a strange kind of production. Where the farmer grows crops and the worker stamps out units of some thing, the hacker has to make the same old stuff, information, appear in new configurations. Getting this done is not like the seasonal repetitions of farming or the clocking-on of the worker. It happens when it happens, including time spent napping or pulling all-nighters. Hackers can’t be managed like farmers or workers. They are not the same as either class.

Like the farmer and the worker, the hacker does not usually end up owning the product of her efforts. Unless you own a drug company or a tech company or whatever, you have to sell the rights to what you produce. It is not always the same as selling labor-power. You might still own the intellectual property, for example. But the hacker rarely captures the value of what they invent. not everyone gets to be Bill Gates — precisely because there is a Bill Gates, who is not the avatar of the hacker class, but of its opposite — the vectoralist class.

The vectoralist class owns and controls the vector, a term I use to describe in the abstract the infrastructure on which information is routed, whether through time or space. You can own stocks or flows of information, but far better to own the vector, the legal and technical protocols for keeping information scarce.

If one takes a look at the top Fortune 500 companies, it is surprising how many of them are really in the information business. I don’t just mean the tech and telco companies like Apple or Google or Verizon or Cisco, or the drug companies like Pfizer. One could think of the big banks as a subset of the vectoralist class rather than as ‘finance capital.’ They are in the information asymmetry business. And as we learned in the 2008 crash, even the car companies are in the information business — they made more money from car loans than cars. The military-industrial sector also also in the information business. Even the companies that make things like Nike are really in the brand business. Walmart and Amazon compete with different models of the information logistics business. The oil companies are in the prospecting business. The actual oil drilling is contracted out. Perhaps the vectoralist class is no longer emerging. Maybe it is the new dominant class.

That might only be the case in the overdeveloped world where we live. Many of the world’s peoples are still peasants who are being turned into farmers by the theft of their land by a landlord class. Much of the world is a giant sweatshop. The resistance of labor to capital is alive and well in China or India. The older class antagonisms have not gone away. Its just there’s a new layer on top, trying to control them. Just as the capitalist class sought to dominate and subordinate the landlord class as a subordinate ruling class, so too the vectoralist class tries to subordinate both landlords and capitalists, by controlling the patents, the brands, the trademarks, the copyrights, but more importantly the logistics of the information vector.

A side note here: In Capital, Marx really only deals with an ideal-type political economy with two classes. But in his political writings it is clear that he understands social formations as hybrids of combined and overlapping modes of production. Landlords and farmers loom large in his writings on France, for example. So here I’m simply taking my cue from the political writings, and thinking a matrix of six classes, three ruling and three subordinate. The dominant classes are thus: landlords, capitalists, vectoralists. The subordinate classes are: farmers, workers, hackers.

Now imagine all the possibilities of class alliance and conflict that this generates. It turns out that politics is much less about the relation between the friend and the enemy, and much more crucially about relations among non-friends and non-enemies. As anyone who has actually done politics, or knows some semiotics, could figure out.

So how is it worse that capitalism? The vectoral infrastructure throws all of the world into the engine of commodification. There is nothing that can’t be tagged and captured via information about it and considered a variable in the simulations that drive resource extraction and processing. Quite simply, we have run out of world to commodify. And now commodification can only cannibalize its own means of existence, both natural and social. Its like that silent film where the train runs out of firewood, so the carriages themselves have to be hacked to pieces and fed to the fire to keep it moving, until nothing but the bare bogies are left.

It is worse also in that rather than some vague multitude, there’s complex class alliances at play in the political space. The trickiest part of it is the politics of the hacker class. Which after all is the class most of us here belong to. Yes, it sometimes appears as a privileged class. But it is a class that has a very hard time thinking its common interests. Largely because the kinds of new information its various sub-fractions produce are all so different. We have a hard time thinking what the poet and the scientist and the engineer have in common. Well, the vectoral class does not have that problem, what all of us make is intellectual property, which from its point of view is all equivalent and tradable as a commodity.

Also, the hacker class experiences extremes of a winner-take-all outcome of its efforts. On the one hand, fantastic careers and the spoils of some simulation of the old bourgeois lifestyle, on the other hand, precarious and part time work, start-ups that go bust, and the making routine of our jobs by new algorithms designed by others of our very own class. Of course it is always a tough argument to propose common interests among subordinate classes. Counter-hegemony is hard. Hackers, like workers or farmers, are distracted by particular and local interests. Class consciousness is rare among hackers. Most of us are rather reactionary — even in the nontechnical trades. But then class consciousness is always a rare and difficult thing. Unlike other identities, it has to be argued contrary to appearances.

I could add more to the picture, but perhaps that will do for now. Treat it as a thought experiment. Maybe like a science fiction story where you have to suspend disbelief. Or an avant-garde prose poem. That was secretly how I thought about A Hacker Manifesto when I wrote it, although of course I did not tell Harvard University Press that, as everyone knows prose poems don’t sell. I can say that I got that prose poem to sell quite well. And be reprinted, and translated into eight or nine languages. But I think now I can safely reveal that my first crack at this way of experimenting with Marx was an also a stab at a avant-garde prose poem.

It was written, incidentally, in a non-existent language. I wrote it in European. That’s a language, which, if it existed would be equal parts church Latin, Marxism and business English. Maybe that’s why I suspect it reads better in French, German, Italian or Spanish, as those translations are better than my translation of it into English.

So to sum up: what if we took a more daring, modernist, de-familiarizing approach to writing theory? What if we asked of theory as a genre that it be as interesting, as strange, as poetically or narratively as rich as we ask our poetry or fiction to be? What if we treated it not as high theory, with pretentions to legislate or interpret other genres; but as low theory, as having no greater or lesser claim to speak of the world than any other. It might be more fun to read. It might tell us something about the world. It might, just might, enable us to act in the world otherwise.

Some might think it a new low, when a candidate for high office starts talking on television about the size of his penis. As if the regular, non-penile spectacle within which we all live and breathe was somehow some lofty public sphere. But perhaps its more a question of the current stage of spectacular live exposing itself in its ruined perfection. The spectacle has a history. Its current stage is what I have called, in a book of the same name, the spectacle ofdisintegration. I wrote it three years ago, but really to talk about some people who say it coming thirty years ago. Here is how I explain what I think the spectacle of disintegration is and what it means. The book from which it forms the introduction is here. The spectacle of disintegration is this big — a totality, actually.

When the storm hit the Hansa Carrier, twenty-one shipping containers fell from its decks into the Pacific Ocean, taking some 80,000 Nike sneakers with them. Seattle-based Oceanographer Curtis Ebbsmeyer used the serial numbers from the sneakers that washed up on the rain coast of North America to plot the widening gyre of ocean-going garbage that usually lies between California and Hawaii. Bigger than the state of Texas, it is called the North Pacific Subtropical Gyre, and sailors have known for a long time to steer clear of this area from the equator to 50 degrees north.

It’s an often windless desert where not much lives. Flotsam gathers and circles, biodegrading into the sea. Unless it is plastic, which merely photo-degrades in the sun, disintegrating into smaller and smaller bits of sameness. Now the sea here has more particles of plastic than plankton. The Gyre is a disowned country of furniture, fridges, cigarette lighters, televisions, bobbing in the sea and slowly falling apart, but refusing to go away.

New Hawaii is the name some humorists prefer for the North Pacific Subtropical Gyre now that it has the convenience of contemporary consumer goods. Or one might call it a spectacle of disintegration. It is as good an emblem as any of the passing show of contemporary life, with its jetsam of jostling plastic artifacts, all twisting higgledy-piggledy on and below the surface of the ocean. Plastic and ocean remain separate, even as the plastic breaks up and permeates the water, insinuating itself into it but always alien to it.

The poet Lautréamont once wrote: “Old Ocean, you are the symbol of identity: always equal to yourself… and if somewhere your waves are enraged, further off in some other zone they are in the most complete calm. But this no longer describes the ocean, which now appears as far from equilibrium. It describes instead the spectacle, the Sargasso Sea of images, a perpetual calm surrounded by turbulence, at the center always the same.

When Guy Debord published The Society of the Spectacle (1967), he thought there were two kinds: the concentrated and the diffuse spectacle. The concentrated spectacle was limited to fascist and Stalinist states, where the spectacle cohered around a cult of personality. These are rare now, if not entirely extinct. The diffuse spectacle emerged as the dominant form. It did not require a Stalin or Mao as its central image. Big Brother is no longer watching you. In His place is little sister and her friends: endless pictures of models and other pretty things. The diffuse spectacle murmured to its sleeping peoples: “what appears is good; what is good appears.”

The victory of the diffused spectacle over its concentrated cousin did not lead to the diffusion of the victor over the surface of the world. In Comments on the Society of the Spectacle (1988), Debord thought instead that an integrated spectacle had subsumed elements of both into a new spectacular universe. While on the surface it looked like the diffused spectacle, which molds desire in the form of the commodity, it bore within it aspects of concentration, notably an occulted state, where power tends to become less and less transparent.

That the state is a mystery to its subjects is to be expected; that it could become occult even to its rulers is something else. The integrated spectacle not only extended the spectacle outwards, but also inwards; the falsification of the world had reached by this point even those in charge of it. Debord wrote in 1978 that “it has become ungovernable, this wasteland, where new sufferings are disguised with the names of former pleasures; and where the people are so afraid…. Rumor has it that those who were expropriating it have, to crown it all, mislaid it. Here is a civilization which is on fire, capsizing and sinking completely. Ah! Fine torpedoeing!”

Since he died in 1994, Debord did not live to see the most fecund and feculent form of this marvel, this spectacular power that integrates both diffusion and concentration. In memory of Debord, let’s call the endpoint reached by the integrated spectacle the disintegrating spectacle, in which the spectator gets to watch the withering away of the old order, ground down to near nothingness by its own steady divergence from any apprehension of itself.

And yet the spectacle remains, circling itself, bewildering itself. Everything is impregnated with tiny bits of its issue, yet the new world remains stillborn. The spectacle atomizes and diffuses itself throughout not only the social body but its sustaining landscape as well. As Debord’s former comrade T. J. Clark writes, this world is “not ‘capital accumulated to the point where it becomes image’ to quote the famous phrase from Guy Debord, but images dispersed and accelerated until they become the true and sufficient commodities.”

The spectacle speaks the language of command. The command of the concentrated spectacle was: OBEY! The command of the diffuse spectacle was: BUY! In the integrated spectacle the commands to OBEY! and BUY! became interchangeable. Now the command of the disintegrating spectacle is: RECYCLE! Like oceanic amoeba choking on granulated shopping bags, the spectacle can now only go forward by evolving the ability to eat its own shit.

The disintegrating spectacle can countenance the end of everything except the end of itself. It can contemplate with equanimity melting ice sheets, seas of junk, peak oil, but the spectacle itself lives on. It is immune to particular criticisms. Mustapha Khayati: “Fourier long ago exposed the methodological myopia of treating fundamental questions without relating them to modern society as a whole. The fetishism of facts masks the essential category, the mass of details obscures the totality.”

Even when it speaks of disintegration, the spectacle is all about particulars. The plastic Pacific, even if it is as big as Texas, is presented as a particular event. Particular criticisms hold the spectacle to account for falsifying this image or that story, but in the process thereby merely add legitimacy to the spectacle’s claim that it can in general be a vehicle for the true. A genuinely critical approach to the spectacle starts from the opposite premise: that it may present from time to time a true fragment, but it is necessarily false as a whole. Debord: “In a world that really has been turned on its head, the true is a moment of falsehood.”

This then is our task: a critique of the spectacle as a whole, a task that critical thought has for the most part abandoned. Stupefied by its own powerlessness, critical thought turned into that drunk who, having lost the car keys, searches for them under the street lamp. The drunk knows that the keys disappeared in that murky puddle, where it is dark, but finds it is easier to search for them under the lamp, where there is light – if not enlightenment.

And then critical theory gave up even that search and fell asleep at the side of the road. Just as well. It was in no condition to drive. In its stupor, critical thought makes a fetish of particular aspects of the spectacular organization of life. As Todd Gitlin says, the critique of content became a contented critique. It wants to talk only of the political, or of culture, or of subjectivity, as if these things still existed, as if they had not been colonized by the spectacle and rendered mere excrescences of its general movement. Critical thought contented itself with arriving late on the scene and picking through the fragments. Or, critical thought retreated into the totality of philosophy. It had a bone to pick with metaphysics. It shrank from the spectacle, which is philosophy made concrete. In short: critical thought has itself become spectacular. Critical theory becomes hypocritical theory. It needs to be renewed not only in content but in form.

When the American Food and Drug Administration announced that certain widely prescribed sleeping pills would come with strong warnings about strange behavior, they were not only responding to reports of groggy people driving their cars and making phone calls, but also purchasing items over the internet. The declension of the spectacle into every last droplet of everyday life means that the life it prescribes can be lived even in one’s sleep. This creates a certain difficulty for prizing open some other possibility for life, even in thought.

Debord’s sometime comrade Raoul Vaneigem famously wrote that those who speak of class conflict without referring to everyday life, “without understanding what is subversive about love and what is positive in the refusal of constraints, such people have a corpse in their mouth.” Today this formula surely needs to be inverted. To talk the talk of critical thought, ofbiopolitics and biopower, of the state of exception, bare life, precarity, of whatever being, orobject oriented ontology without reference to class conflict is to speak, if not with a corpse in one’s mouth, then at least a sleeper.

Must we speak the hideous language of our century? The spectacle appears at first as just a maelstrom of images swirling about the suck hole of their own nothingness. Here is a political leader. Here is one with better hair. Here is an earthquake in China. Here is a new kind of phone. Here are the crowds for the new movie. Here are the crowds for the food riot. Here is a cute cat. Here is a cheeseburger. If that were all there was to it, one could just load one’s screen with better fare. But the spectacle is not just images. It is not just another name for the media. Debord: “The spectacle is a social relationship between people mediated by images.” The trick is not to be distracted by the images, but to inquire into the nature of this social relationship.

Emmalee Bauer of Elkhart worked for the Sheraton Hotel company in Des Moines until she was fired for using her employer’s computer to keep a journal which recorded all of her efforts to avoid work. “This typing thing seems to be doing the trick,” she wrote. “It just looks like I am hard at work on something very important.” And indeed she was. Her book-lengthwork hits on something fundamental about wage labor and the spectacle, namely the separation of labor from desire. One works not because one particularly wants to, but for the wages, with which to then purchase commodities to fulfill desires.

In the separation between labor and desire is the origins of the spectacle, which appears as the world of all that can be desired, or rather, of all the appropriate modes of desiring. “Thus the spectacle, though it turns reality on its head, is itself a product of real activity.” The activity of making commodities makes in turn the need for the spectacle as the image of those commodities turned into objects of desire. The spectacle turns the goods into The Good.

The ruling images of any age service the ruling power. The spectacle is no different, although the ruling power is not so much a ruling monarch or even a power elite any more, but the rule of the commodity itself. The celebrities that populate the spectacle are not its sovereigns, but rather model a range of acceptable modes of desire from the noble to the risqué. The true celebrities of the spectacle are not its subjects but its objects.

Billionaire Brit retailer Sir Philip Green spent six million pounds flying some two hundred of his closest friends to a luxury spa resort in the Maldives. The resort offers water sports and a private beach for each guest. Much of the décor is made from recycled products and there is an organic vegetable garden where residents can pick ingredients for their own meals. ‘Sustainability’ is the Viagra of old world speculative investment.Sir Philip is no fool, and neither is his publicist. This retailer of dreams has the good sense to appear in public by giving away to a lucky few what the unlucky many should hence forth consider good fortune. And yet while this story highlights the fantastic agency of the billionaire, the moral of the story is something else: even billionaires obey the logic of the spectacle if they want to appear in it.

The spectacle has always been an uninterrupted monologue of self-praise. But things have changed a bit. The integrated spectacle still relied on centralized means of organizing and distributing the spectacle, run by a culture industry in command of the means of producing its images. The disintegrating spectacle chips away at centralized means of producing images and distributes this responsibility among the spectators themselves. While the production of goods is out-sourced to various cheap labor countries, the production of images is in-sourced to unpaid labor, offered up in what was once leisure time. The culture industries are now the vulture industries, which act less as producers of images for consumption than as algorithms which manage databases of images that consumers swap between each other – while still paying for the privilege. Where once the spectacle entertained us; now we must entertain each other, while the vulture industries collect the rent. The disintegrating spectacle replaces the monologue of appearances with the appearance of dialogue. Spectators are now obliged to make images and stories for each other that do not unite those spectators in anything other than their separateness.

The proliferation of means of communication, with their tiny keyboards and tiny screens, merely breaks the spectacle down into bits and distributes it in suspension throughout everyday life. Debord: “The spectacle has spread itself to the point where it now permeates all reality. It was easy to predict in theory what has been quickly and universally demonstrated by practical experience of economic reason’s relentless accomplishments: that the globalization of the false was also the falsification of the globe.” Ever finer fragments of the time of everyday life become moments into which the spectacle insinuates its logic, demanding the incessant production and consumption of images and stories which, even though they take place in the sweaty pores of the everyday, are powerless to effect it.

It is comforting to imagine that it is always someone else who is duped by the spectacle. Former movie star turned tabloid sensation Lindsay Lohan allegedly spent over one million dollars on clothes in a single year, and $100,000 in a single day, before consulting a hypnotist to try to end her shopping addiction. Lohan’s publicist denied the story: “There is no hypnotist, and Lindsay loves clothes, but the idea that she spent that much last year is completely stupid.” The alleged excess of an other makes the reader’s own relation to the spectacle of commodities seem just right. Its all about having the right distance. For Debord, “no one really believes the spectacle.” Belief, like much else these days, is optional. The spectacle is what it is: irrefutable images, eternal present, the endless yes. The spectacle does not require gestures of belief, only of deference. No particular image need detain us any longer than this season’s shoes.

They call themselves the Bus Buddies. The women who travel the Adirondack Trailways Red Line spend five and even six hours commuting to high paid jobs in Manhattan, earning much more money than they could locally in upstate New York. They are outlier examples of what are now called extreme commuters, who rarely see their homes in daylight and spend around a month per year of their lives in transit. It is not an easy life. “Studies show that commuters are much less satisfied with their lives than non-commuters.” Symptoms may include “raised blood pressure, musculoskeletal disorders, increased hostility, lateness, absenteeism, and adverse effects on cognitive performance.” Even with a blow-up neck pillow and a blankie, commuting has few charms.

For many workers the commute results from a simple equation between their income in the city and the real estate they can afford in the suburbs, an equation known well by the real estate development companies. “Poring over elaborate market research, these corporations divine what young families want, addressing things like carpet texture and kitchen placement and determining how many streetlights and cul-de-sacs will evoke a soothing sense of safety. They know almost to the dollar how much buyers are willing to pay to exchange a longer commute for more space, a sense of higher status and the feeling of security.” By moving away from the city, the commuter gets the space for which to no longer have the time. Time, or space? This is the tension envelope of middle class desire. Home buyers are to property developers what soldiers are to generals. Their actions are calculable, so long as they don’t panic.

There are ways to beat the commute. Rush hour in Sao Paulo, Brazil features the same gridlocked streets as many big cities, but the skies afford a brilliant display of winking lights from the helicopters ferrying the city’s upper class home for the evening. Helipads dot the tops of high-rise buildings and are standard features of Sao Paulo’s guarded residential compounds. The helicopter speeds the commute, bypasses car-jackings, kidnappings – and it ornaments the sky. “My favorite time to fly is at night, because the sensation is equaled only in movies or in dreams,” says Moacir da Silva, the president of the Sao Paulo Helicopter Pilots Association. “The lights are everywhere, as if I were flying within a Christmas tree.”

Many Paulistanos lack not only a helicopter, but shelter and clean water. But even when it comes with abundance, everyday life can seem strangely impoverished. Debord: “the reality that must be taken as a point of departure is dissatisfaction.” Even on a good day, when the sun is shining and one doesn’t have to board that bus, everyday life seems oddly lacking.

Sure, there is still an under-developedworld that lacks modern conveniences such as extreme commuting and the gated community. Pointing to this lack too easily becomes an alibi for not examining what it is the developing world is developing towards. And rather than a developed world, perhaps the result is more like what the Situationists called an over-developed world, which somehow overshot the mark. This world kept accumulating riches of the same steroidal kind, pumping up past the point where a qualitative change might have transformed it and set it on a different path. This is the world, then, which lacks for nothing except its own critique.

The critique of everyday life – or something like it – happens all the time in the disintegrating spectacle, but this critique falls short of any project of transforming it. The spectacle points constantly to the more extreme examples of the ills of this world – its longest commutes, its most absurd disparities of wealth between slum dwellers and the helicopter class, as if these curios legitimated what remains as some kind of norm. How can the critique of everyday life be expressed in acts? Acts which might take a step beyond Emmalee Bauer’s magnum opusand become collaborations in new forms of life? Forms of life which are at once both aesthetic and political and yet reducible to the given forms of neither art nor action? These are questions that will draw us back over several centuries of critical practice.

Once upon a time there was a small band of ingrates – the Situationist International – who aspired to something more than this. Their project was to advance beyond the fulfillment of needs to the creation of new desires. But in these chastened times the project is different. Having failed our desires, this world merely renames the necessities it imposes as if they were desires. Debord: “It should be known that servitude henceforth truly wants to be loved for itself, and no longer because it would bring any extrinsic advantage.”

What if we tried a thought experiment? Just for shits and giggles? The thought experiment runs as follows: What if this was no longer capitalism, but something worse? Could we start by describing relations of exploitation and domination in the present, starting with the newest features, and work back and out and up from that?

This might draw our attention to two things. Firstly, to some features of the forces of production. It is still the case that extracting useful organic and inorganic matter from the earth is the basis of social existence. And it is still the case that applying vast amounts of energy in the form of fossil fuels and labor to that base matter is still how the endless array of commodities around us come into existence.

But both those processes seem these days to be subordinated to a third form of relation. At the smallest and largest scales, so much of primary production and secondary manufacturing seems to be controlled by rapid flows and extensive archives and complex algorithms whose concrete existence is in a tertiary form – that of information.

The forces of production that seem most characteristic of the present run on information. They extend all the way into the production process, whether in the form of robots or the detailed and constant surveillance of living labor. They extend all the way out to global networks of measurement, command and control that work in real-time. These networks of information subsume not only inorganic and organic matter and energy in their web but also the human, as we become producers of information even when we are not working. The value of information can be extracted even from nonlabor.

Secondly, the relations of production seem to have evolved to enclose these forces in rather novel extensions of the private property form. Wittgenstein had a rather robust proof of the proposition that there is no private language, but in our time, privatized languages are everywhere, and not just languages. Images, languages, codes, even genes can become private property, produced in quite novel kinds of productive process.

New forms of information, now recognizable as private property, are extracted from a class whose efforts are hardly described by the category of labor, for the simple reason that while labor repeats an action whose form is given in advance, the whole point of these novel processes is to produce unique instances of such forms in the first place. Alongside theworker is the figure of the hacker, producer not of content but of form, and which more often that not ends up being someone else’s property.

One has to ask whether the ruling class presiding over this mode of production is still adequately described as capitalist. It seems no longer necessary to directly own the means of production. A remarkable amount of the valuation of the leading companies of our time consists not of tangible assets, but information. A company is its brands, its patents, its trademarks, its reputation, its logistics, and perhaps above all its distinctive practices of evaluating information itself.

Strangely enough, despite the posthuman turn, in which not just labor but all forms of human attention are subordinated to what Lazzarato calls machinic enslavement, a company is also its personality. Companies really are ‘people’ now, and in more than a legal sense. And they have to be embodied in an actual human, someone the financial markets can believe in.

Some like to talk as if one could just add an adjective or two to capitalism and describe all this. Let’s call it communicative capitalism, semio-capitalism, cognitive capitalism, neoliberal capitalism, or financial capitalism. That sounds comforting at least, as then we know what we are dealing with. But perhaps that’s not quite adequate. Maybe its not the same old familiar endless essence of capitalism cloaked in new appearances.

Maybe the rise of finance is really just a symptom. Yann Moulier Boutang invites us to see finance as something more than speculative excess. It has to do with the whole problem of exchange value in an age where the forces of production are extensively and intensively controlled by information, which is that nobody knows what anything is worth. Financialization is a perverse socializing of the problem of value.

So just for fun, let’s think of it as a post-capitalist mode of production, with a ruling class of a different kind. I call them the vectoralist class. Their power does not lie in directly owning the means of production, as the capitalist class does. Nor does it lie in owning agricultural land, as the capitalists’ old enemy, the landlord class did. And just as there was conflict between capitalist and landlord, so now there is conflict between capitalist and vectoralist.

It was with new forces of production that capital defeated labor in the late twentieth century. But capital in turn finds itself struggling against those who provided the very means of that victory. If one can use an information infrastructure to route around labor’s power to block the production process, one can use the same means to make capitalist producers compete with each other on a global scale.

Remember, this is only a thought experiment. It may have one small merit, and one rather recalcitrant problem attached to it. The small merit is that it enables one to tell a fairly coherent story about what happened between the 1970s and now. For comparison, let’s look at some of the more characteristic language about that period. My examples here are from Erik Olin Wright, Understanding Class (Verso, 2015).

Wright: “The combination of globalization and financialization meant that from the early 1980s the interests of the wealthiest and most powerful segments of the capitalist class in many developed capitalist countries, perhaps especially in the United States, became increasingly anchored in global financial transactions and speculation and less connected to the economic conditions and rhythms of their national bases or any other specific geographical location.” (237) A sentence which, stripped of its decoration, basically says: the cause of financialization is financialization, and the cause of globalization is globalization.

Wright speaks of a period when “global competition intensified” where there was the “integration of commodity chains and production chains” and the “emergence of a global labor force” and even the “dramatic financialization of capitalist economies.” (236-237) With what means? By whom? These are phrases from sentences that don’t consistently link subjects to objects, and which are fond of passive verbs. This is a theory of history that can be summed up as: shit happens.

Of course, these are statements Wright adopts from a consensus language. We have all agreed to talk about financialization as if that just happened, without requiring actual material practices and techniques. We have all agreed to talk about neoliberalism as if that described an actual agency at work that causes things to happen. We have decided not to be Marxists, in other words. We have decided not to subject the language of the times to its own critical pressure. Marx certainly did not take the abstract nouns of his era as a given.

But this brings us to the recalcitrant problem. Its one thing to play with the language of Marxism. It will at least admit modifiers. You can call this neoliberal capitalism, if you want. The essence – capitalism – takes on a particular historical appearance. But one is not supposed to question the metaphysical construct, wherein capitalism is an essence with appearances, which can end only when its productive capacities are exhausted, when the proletariat break through the mere transitory appearance and transforms its essence into socialism, and prehistory has come to an end with the abolition of the last form of class exploitation.

Here let’s look at Wright again, whose work is in many other respects a salutary example of how to bring analytic rigor to the Marxist tradition. He writes: “At the very heart of Marxism as a social theory is the idea of emancipatory alternatives to capitalism.” (121) And “Unless one retains some coherent idea of there being an alternative to capitalism, a Marxist class analysis loses its central anchor.” (167) Hence even in this social-scientific version of the Marxist tradition we’re not far from the metaphysic, in which history can only be understood through an ahistorical concept – of capitalism. Emancipation is though negatively, as emancipation from capitalism. Therefore, this must be capitalism, the negative of emancipation.

Of course, there’s plenty of evidence for this still being capitalism, or mostly capitalism. The question would be whether something else is emerging, and whether it is qualitatively different enough to call it something else. The problem with an inherited concept, like inherited money, is that we didn’t make it ourselves and come to take it for granted. Particularly if it is part of a whole metaphysical conceptual structure. Maybe we need a bit of good old Brechtian alienation-effect even from heirloom concepts like ‘capitalism.’

Now, all I’m talking about here is a thought experiment: what if we thought about a mode of production emerging after capitalism that was worse? Could that at least minimally explain observable features of the world that might be genuinely strange, qualitatively novel, observable tendencies in recent history? What light would this perspective shed on our habits of thought, our received ideas?

As an example of how one might conduct thought experiments, I turn again to Erik Olin Wright. He is the author of a substantial body of sociological work, both conceptual and empirical, on class. The papers collected in Understanding Class are their own kind of thought experiment. They ask: what can a Marxist concept of class bring to theoretical and empirical work that thinks class as stratification, or which uses class concepts drawn more from the work of Max Weber or Emile Durkheim? Wright deftly shows what a Marxist concept of class can do, where endless capitalism is a given. The addition question I want to ask is what happens if we take away the assumption that this is still the same old capitalism.

Wright has mercifully given up the “paradigm aspiration” (17) wherein Marxism is superior to all the social sciences because it has a superior problematic or method. Those whose memory stretches back away will recall that this led to endless attempts to prove by purely theoretical means that Marxism had rendered all of bourgeois philosophy and social science obsolete. But it hardly ever delivered on the promise of a new kind of knowledge. The “grand narrative” (122) fell apart. And in any case it ended up being produced in the margins of the very same institutions as that bourgeois philosophy and social science.

Instead, Wright makes two sorts of claims. The modest claim is that one can connect Marxist work to other kinds of sociology. Each has its perspective and they illuminate each other. The stronger claim is that the Marxist perspective is a bigger picture, which shows something about the world and history that is beyond the reach of other approaches. Here he does for social theory what Fred Jameson does for literary theory or Perry Anderson for historical thought: make the claim that Marxism offered the point of view from which to interpret and synthesize other bodies of work. If Jameson’s famous watchwords are “always historicize!” then Wright’s might be “always socialize!” where that means to adopt the point of view of a social formation riven by relations of class exploitation and domination as the outer limits of the macroscopic perspective.

In one of his brilliant summaries, Wright argues that Durkheimian, Weberian and Marxist approaches operate on different levels of the social gamespace, the situational, the institutional and the systemic. The Durkheimian approach is situational, and is about moveswithin the game. The Weberian approach is institutional, and is about rules of the game. The Marxist approach is systemic, and is about changing the game.

All of these approaches involve class structure that generate class actors who have at least partially conscious intentions, whether it is to make moves that advance them, or contest rules of the game that might advantage some class or other, or to change the whole game to another game more in one’s class interest. My question would be about the class unconscious. Perhaps the game changed of its own accord, as the forces of production push forward into new relations of production, with which our superstructural languages for describing class structure have yet to catch up.

Wright thinks that the opportunity for game-changing, for overthrowing capitalism in favor of a more equal and free society, is not present. “One way of interpreting the history of the past half-century is that there has been a gradual shift in the levels of the game at which, for many analysts, class analysis seems most relevant.” (123) Hence it makes sense to reach out to the Weberians (whose scholarly interests are at the level of contesting the rules of the game rather than changing it) and even to the Durkheimians, whose focus is on the moves actors get to make within given rules of the game. But his overall aim is to concatenate these three approaches as appropriate to different scales, with Marx speaking to the larger and more visionary scale.

Wright resists the death of class counter-narrative, a discourse whose most prominent member is probably Ulrick Beck. Wright offers a supple class analysis and backs it up with actual results. His concept of class has three dimensions: property, authority, expertise. His view of class structure offers class locations at three levels, which do not always neatly overlap. Relations of property generates the class locations of employers, petit-bourgeois, employees. Relations of authority generates the locations of managers, supervisors, the supervised and managed. Relations of expertise generate the locations of professionals, the skilled, the nonskilled.

He is interested, for example, in the “permeability” (146) of class boundaries, so he looks at three kinds of class connection: intergenerational mobility, cross-class friendship, cross-class households. He finds the property boundary the least permeable – a result that won’t surprise Marxists. Class connections between workers and employers is limited. The employee / petit bourgeois boundary is more permeable. Wright frankly acknowledges that in the United States, racial boundaries may be even less permeable, but that does not negate the usefulness of the category of class. Class is only a modest predictor, however “class often performs as well or better than many other social structural variables in predicting a variety of aspects of attitudes.” (154)

David Grusky and Kim Weedon offer what Wright classifies as a Durkheimian analysis of class in which occupation are the unit of analysis. They see class homogeneity only at the micro, occupational level, not in ‘big’ concepts of class. Its more about actual labor markets and how they define occupations. Such occupations act on behalf of members, extract rents if they can, and shape life chances. For them, even academic sociologists and economists count as different ‘classes.’ Which might be the beginnings of an approach to how academics, at a time when, under the threat of vectoral power, their life chances seem diminishing and their means of opportunity hoarding to be failing, cannot quite come together and act on shared interests.

The Durkheimian approach focuses on selection and self-selection into closed groups who interact more with each other than with other groups. Licensing and the formal definition of occupations play a role here. This works well for explaining individual-level outcomes. Wright claims that except in the study of education, income and wealth, this micro approach works better than macro ones of a more Weberian or Marxist kind. The Durkheimians are good on lifestyles, tastes, political and social attitudes.

The key to Weberian theories of class for Wright is opportunity hoarding or social closure, by such means as credentialing, licensing, the color bar, gender exclusions. One could even see labor unions as a form of opportunity hoarding from the point of view of precarious workers, but we’ll come back to that.

As Wright points out, perhaps the most important mechanism of opportunity hoarding is private property itself. “The core class division within both Weberian and Marxian traditions of sociology between capitalists and workers can therefore be understood as reflecting a specific form of opportunity hoarding enforced by the legal rules of property rights.” (7)

Wright brings into the Weberian theoretical frame the Marxist concept of antagonistic classes. Advantages are causally linked to disadvantages “The rich are rich because the poor are poor.” (8) The Marxist concept of exploitation and domination are about control over the lives of others. “Exploitation refers to the acquisition of economic benefits from the laboring activity of those who are dominated.” (9)

It could be argued that class is peripheral to Weber’s work, but his writings on ancient slavery seem close to Marx. Unlike concepts such as status and party, class need not generate identity or collective action. And hence does not fit well with Weber’s habitual explanatory modes.

Both Marx and Weber see property as fundamental to a relational concept of class. Both grasp distinction between objectively defined class and subjectively lived class. Both thought humans followed material interest in the long run. Weber was much less inclined to think classes would polarize and become the key social dynamic. Marx shared Weber’s view that status groups impeded the effects of the market and constitute an alternative basis of collective action. Both thought the rationalization of market relations would abolish status groups in time.

Marx thought this simplified class whereas Weber did not. Weber thought class determined life chances within rationalized society. He was less interested in deprivation than in instrumental rationality. Marx was more interested in class exploitation in production; Weber in class as factor in determining life chances in the market. Wright: “Marxist class analysis includes the Weberian causal processes, but adds to them a causal structure within production itself.” (46)

For Wright, class in Weber is closely connected to the theme of rationalization. Of Weber’s three sources of power, he conceives of both non-rational and rational forms. Thus the power-source that is honor can appear as ranks and titles, or as a meritocracy. Authority can appear as patriarchal or in a rational-legal form. And material sources of power can appear as consumption groups or as class.

Class is thus part of rationalization, part of the abolition of traditional peasantry, part of the transition from landed aristocracy to agricultural entrepreneurs. Class is part of the rise of the calculation of material interest. The peasant, who owes a duty to the baron, becomes the farmer who pays rent to the landlord.  “While class per se may be a relatively secondary theme in Weber’s sociology, it is, nevertheless, intimately linked to one of his most pervasive theoretical preoccupations – rationalization.” (31)

My question here would be to ask: why one would think, if this has given rise to two kinds of rationalizing class antagonism that overlapped and interfered with each other, why might it not give rise to a third? The farmer-landlord antagonism arose out of the ruins of feudalism. The efficiencies in agricultural production that came with this rationalization threw off a surplus population what would become urban workers, in an antagonistic relation to capitalists. And yet landlords and capitalists also had interests that contradicted each other. This is the central theme of David Ricardo’s political economy: the opposition between landlord and capitalist. But did rationalization stop, with the creation of classes of farmer and worker? What happens when the production, not of food or products, but of new information itself becomes rationalized?

Weber did not have a lot to say about labor, but where he did, it was in terms of work discipline. Employers are free to hire and fire. Workers lack ownership, but workers are responsible for their own social reproduction. These are the conditions under which indirect compulsion operates. But it raises the problem of how to get maximum labor effort. Wright: “running throughout Weber’s work is the view that rationalization has perverse effects that systematically threaten human dignity and welfare.” (52) One sees this clearly in the latest impositions on the labor process, such as Uber or Amazon’s Mechanical Turk, which make labor both highly autonomous and yet very closely monitored at the same time.

But Weber does not integrate interest in labor discipline and domination into the category of class. Here we need a bit of Marx, for whom, as Wright says, “exploitation infuses class analysis with a specific kind of normative concern.” (53) Exploitation steers research to questions of class as relational in both exchange and production. “Weber’s treatment of work effort as primarily a problem of economic rationality directs class analysis towards a set of normative concerns centered above all on the interests of capitalists: efficiency and rationalization.” (55) Georg Lukacs and Theodor Adorno excepted, one might add.

The Wrightian synthesis of Marx and Weber makes exploitation fundamental, but makes particular use of the idea of opportunity hoarding as that which defines the middle class.From there one could build up a picture of the United States as highly polarized by exploitation, and where middle class opportunity hoarding is being eroded by what he calls neoliberalism and deindustrialization, but which I think can be understood more clearly in terms of new forces of production that instrumentalize and rationalize information, giving rise to new property forms and hence new class relations, including an antagonistic relation between a hacker class tasked with making novelty out of information (the condition of it becoming property) and a vectoralist class that owns or controls the vector of information control and domination itself. Here the Marxist perspective of exploitation and the Weberian one of rationalization can be fruitfully combined.

Beside reaching out to those indebted to more classical approaches of Durkheim and Weber, Wright addresses prominent contemporary social theorists who try to offer original perspectives. Here I’ll treat only his papers on those known outside their disciplines, such as Thomas Piketty, Michael Mann, Guy Standing and former New School professor, the late Charles Tilly.

Wright thinks Charles Tilly’s approach to durable inequalities is closer to Marx than Tilly wants to acknowledge. Tilly was against individualist approaches. Explanations of inequalities have to be relational. He offers a structuralism of types of social relations and types of mechanism. The types of social relation are chain, hierarchy, triad, organization, categorical pair –  of which organization is the most durable kind. The types of mechanism are exploitation, opportunity hoarding, emulation and adaption.

Structural relations are solutions to problems generated within social systems. For example, Problem 1: how to secure stable access to resources?

Solution 1: opportunity hoarding and exploitation. Problem 2: how to sustain and even deepen exploitation and opportunity hoarding, and sustain trust and cooperation among those who benefit? Solution 2: categorical inequality. Problem 3: how to stabilize and reproduce inequality? Solution 3 emulation and adaptation lock distincions in place.

Wright sees Tilly as importing Weberian ideas into a Marxist framework. Culture is not an autonomous superstructure, as in certain post-Marxist theories. Nevertheless, Tilly goes beyond Marx in attempting to subsume gender, race, nationalism under a unitary framework. For Tilly, forms of categorical inequality make exploitation more efficient. One could perhaps usefully extend this to think about how what one might shorthand as algorithmic mechanisms of discrimination, which work so subtly with databases rather than categories, might reinforce exploitation in our time.

Michael Mann’s work is approached a bit differently by Wright. Here his interest is in the disjunction between how his theory treats class and some of his more specific findings. In his theory, Mann sees class only in terms of collective actors, not structural locations. But in particular studies, class location does seem to shape individual interests.

Crucial here is the distinction Wright makes between class structure, class formation and class actors. This might correspond very loosely to three scales of analysis: the Marxist, Weberian and Durkheimian perspectives, respectively. Mann, like Bourdieu, thinks that class structure that produces no class actor is just ‘class on paper,’ an academic exercise. Class has competing forces against it: ethnic, racial, linguistic, national, religious, gender.

Mann’s social theory works off two clusers of concepts: sources of social power and forms of organization that deploy those powers. There are four sources of power: social, ideological, economic, military. The kinds of organization are expressed as dichotomies: collective / distributional; extensive / intensive; authoritative / diffuse.

In Mann’s world, the pursuit of goals requires entering into power organizations that determine the structure of society. This is an agency rather than a structure centered approach. The creation, reproduction or transformation of social structures is the result of goal directed actions. Rather like rational choice theory, Mann starts with actors and their goals, only his approach is not individualistic.

Class in Mann is a kind of collective actor among many that comes together in organizations to deploy economic power resources. He pairs the concept of class with that of segment, which cuts across class and groups actors of different classes in particular industries. Class, for Mann, is of little sociological interest.

In working through the formation of the middle class, Mann actually becomes rather more interested in class structure and class formation. For Mann, the middle class is composed of three separate categories: The petit-bourgeois, professionals and careerists. The economic situation of all three tie their life chances to capital accumulation, and their relation to the state forces them together. They have similar consumpti0on patterns and may all be investors of small amounts of capital. They are held together by ideological and political citizenship.

Mann rejects the classic Marxist distinction between class in itself / class for itself as teleology, but so too does Wright and many more sophisticated Marxists. Wright: “One can believe that class relations and class structures are real and generate real effects without also believing in any one-to-one mapping between the complex structure of class relations and the formation of collective actors.” (108)

Here Wright’s signature concept of contradictory class locations proves very useful. Actual jobs are a mix of property relations and authority relations, and might be located in class terms differently along those axes. Class changes over time and is mediated by family and community. One might be born into the petit-bourgeoisie but end up a waged-worker. One might be working class and marry a shop-keeper, and so on. Class can be a bit messy. One is reminded here of Jean Baudrillard wry remarks in America on Marxist academics managing their stock portfolios.

Thomas Piketty deserves credit for putting inequality back on the agenda as more than a mere problem of unequal opportunity. His empirical work shows that the sharp rise in income of the top 10%, is really that of the 1% or even the .1%. A fair bit of this came from the rise of super salaries rather than income on capital. The CEO ‘class’ are setting their own pay. Here I would want to inquire as to how, in a political economy running on information, the capacity to control (but not entirely own) the means of production accrues to a class that presents itself as the celebrities of information control itself.

The technicalities of Piketty’s work centers on the capital / income ratio as a way of measuring value of capital relative to total income of economy. Wright: “Piketty’s basic argument is that this ratio is the structural basis for the distribution of income between owners of capital and labor: all other things being equal, for a given return on capital, the higherthis ratio, the higher the proportion of national income going to wealth holder.” (133) As  growth declines, the capital/income ratio rises. There’s a rise in the weight of inherited wealth, while concentrations of income also rise. It’s the worst of both worlds: a rentier class plus a meritocracy of appearance-peddlers carving up the world between them at the expense of everyone else.

Pickety starts out with a class analysis, but loses it once he gets into the empirical work, where he treats CEO income as return on labor, as most income tables do. Wright: “In the modern corporation many of the powers-of-capital are held by top executives…. They occupy what I have called contradictory locations within class relations… They exercise their capitalist-derived power within the class relations of the firm to appropriate part of the corporation’s profits for their personal accounts.” (136)

But is their power really capitalist-derived, or is it now something else? Something like a joint managing of appearances between those who represent a firm to the market, and the market that is supposed to value it. But how to value it when so much of its asset-base takes the form of information? A firm is among other things a brand, a slew of patents, a logistical process, a corral of expert hackers turning out new intellectual property. How can information be turned into value, and an opportunity to be hoarded, when there aren’t really private languages, and information is in principle a non-rivalrous good?

Wright points out that Piketty does not separate out real estate from capital. There might be good reasons to do so. Elsewhere I wrote about Matteo Pasquinelli’s arguments about how landlords now increase their rents by extracting the information-value that the presence of either the hacker class, or of those parts of the middle class that manage rather than create information. One could think further here about Ricardo’s ancient tension between ground-rent and profit, but with the focus shifted from the rural to the urban, and the monopoly rents to be extracted from urban locations.

Guy Standing is the name most associated with the now widely-discussed idea of theprecariat as a class rather than just a bad life chance. He offers a three-dimensional definition of class, as structured by relations of production, relations of distribution and (interestingly) relations to the state. He identifies seven classes: plutocracy, salariat, proficians (professional + technician, working class, prevcariat, unemployed, lumpen-precariat. The precariat have insecure insecure jobs. Their sources of income other than wages disappearing. They become less citizens of the state and more mere denizens. Not only are their jobs precarious, they are vulnerable within relations of distribution and marginal to the state.

The precariat includes people bumped out of working class communities and families. They experience a relative deprivation in relation to a real or imagined past. It also includes migrants and asylum seekers for whom it’s the present that is absent. They have no home. The precariat increasingly includes people falling out of an educated middle class – think academic adjunct labor – who lack a future. For Standing this makes a potentially dangerous class.

Marxists might think of the precariat as workers who (in Weberian terms) experienced poor life chances. Standing thinks there are antagonisms between the precariat and the working class. But do the precariat and workers have distinct interests? Wright thinks not. He thinks they share an interest in changing the game (although one might want to say more here about how workers and the precariat might have different interests about the rules and moves of the game). Unionization, for example, can secure some sort of steady work for the workers in the union at the expense of those without – a side-effect of unionizing academic adjunct labor that is rarely discussed.

Certainly the most controversial of Wright’s propositions is one that picks up on the work ofWolfgang Streeck and others on class struggle and class compromise. For Streek, arguing in a Durkheimian vein, capitalism works better when there are constraints on rational, self-interested action. Capitalism works better when there’s non-capitalist social forms present, based in trust, legitimacy, responsibility.

The wrinkle Wright introduces is to argue that the level of constraint on elf-interest that is optimal for capitalists is below that which is optimal for workers. Capital seeks to remove constraints to augment its power even past the point where these are economically inefficient. Wright: “… the zeal to dismantle the regulatory machinery of capitalism since the early 1980s was driven by a desire to undermine the conditions for empowerment of interests opposed to those of capitalists – even if doing so meant under-regulating capitalism from the point of view of long-term needs of capital accumulation.” (183)

Although perhaps one could see this a bit differently by separating out the interests of the capitalist and vectoralist class. The regulatory regime emerging in the last quarter century favors the mobility of information – and not just finance – as a means of coordinating economic activity transnationally, at the expense not just of workers but of those forms of capitalist enterprise tied to physical plant and infrastructure, and thus with an interest in local, regional or national relations of trust, legitimacy and responsibility.

Hence we can read Wright’s conclusion against the grain: “Enlightenment of the capitalist class to their long-term interests in a strong civic culture of obligation and trust is not enough; the balance of power also needs to be changed. And since this shift in balance of power will be costly to those in privileged positions, it will only occur through a process of mobilization and struggle.” (184) But what if those capitalists tied to actually producing things in a particular place already know this, but they have lost power to a quite different kind of ruling class, which operates at a higher level of abstraction, or in Weberian terms, at a new stage of rationalization? They own or control the information about things, rather than the things themselves.

Hence to imagine new kinds of class compromise might require a rethink about which classes could compromise. First, one has to have some perspective on the impulse to think all class compromise as illusion or stalemate. Could there be a non-zero-sum game between otherwise antagonistic classes? Working class organization may actually have positive effects for capital accumulation, as it enables problem solving, negotiation, skill development, tech change.

But perhaps that only worked when particular capitalist employers were able to exercise something like a monopoly on the production of a given class of product or in the context of a mercantilist strategy of restricted consumption at home while expanding exports abroad and sustaining rising wages as a return on rising productivity. Such as would describe Japanese manufacturing in its heyday, for example.

Since there’s no way to change the game, Wright looks to those who wanted to change the rules within the game, such as those Scandinavian social democratic inheritors of Ernst Wigforss, such as Walter Korpi and Gøsta Esping-Anderson. But one has to ask if its possible to revive social democratic strategies from the era of the great national manufacturing industries in an era where the information vector greatly lowers the cost of geographic dispersal, and puts manufacturing regions in direct competition with each other.

Wright advocates for some salutary counter-hegemonic strategies, based in geographic rootedness, local public goods and worker’s cooperatives. But one has to wonder if, in an era where the forces of production drive increasingly abstract processes of rationalization, which appear then as transnational legal and treaty forms protecting information as private property, such things are all that viable.

Wright: “Changes in technology may make the anchoring of capitalist production in locally rooted, high productivity small and medium size enterprises more feasible.” (143) One might call this the Brooklyn-effect, after the boom in small business, even manufacturing, there. But while the actual products have some connection to locality, the information infrastructure such localism has to rely on belongs to the vectoralist class. Amazon, Paypal and so forth all get their cut.

Thus, where Wright says, “I assume that an exit from capitalism is not an option in the present historical period” (239) – I think we have to question that assumption, but not in a good way. Maybe this is already not capitalism, but something worse. Its not just a rentier bubble of speculation spooling out of the “real economy.” (244) One could no longer know in advance which part of it is real at all – and perhaps one never could. This is an era not just of so-called neoliberalism’s “aggressive affirmation and enforcement of private property rights” (237) but of the creation of new forms of private property, and new antagonistic relations over it, particularly in the form of intellectual property.

There’s a lot to be said for the way Wright subsumes rival social theories as collaborators within the larger frame of a fairly traditional Marxist sociology. But perhaps that in turn has to be put back in contact with the historical study of the transformation of the forces of production, and in particular how information emerges as both a technical and social force. One could then, as a further step, bring this perspective together with the study of themetabolic rift, wherein the instrumentalizing of information mobilizes the whole planet as a rationalized sphere of resource extraction under the sign of exchange value. To the point where this rationalization becomes completely irrational, threatening to take the whole planet down with it.

Here it might be helpful, in Bogdanovite fashion, to press on some other metaphors at work in Wright, viz: “A society is not a system in the same way that an organism is a system. It is more like the loosely coupled system of an ecosystem in which a variety of processes interact in relatively contingent ways.” (121) In the Anthropocene, it may turn out that ecosystems are the ones that are tightly coupled. But that would be a whole other thought experiment.

Maybe we need an asocial science that rethinks whether one can even conceive of the social as a separate domain of analysis at all. On the one side, the social meshes seamlessly with information technology; on the other, it depends on planetary scale resource mobilization causing catastrophic metabolic rifts. One might be in need of an even ‘bigger’ conceptual framework within which to rest Wright’s partial synthesis as a component part.

RetroDada begins with disgust. Once again the world gets its war on. While some cities are attacked by bombers, others are strafed by art fairs. This time there’s no Switzerland of neutrality where refugees might cool their heels, as now the whole globe itself overheats. The insomnia of reason breeds monsters.

But before we can take two steps forward, let’s take one leap back. Back a whole century. Back to the first of the world wars to be numbered; back to the birth of Dada disgust. Back to that great refusal of what the century was to become. Why shouldn’t a .gif run backwards as well as forwards? Its RetroDada time! In principle RetroDada is against manifestoes, but it is also also against principles. So here goes nothing.

The world is full of mistakes, but the worst is the art that got made. Art gives us Dante’s Inferno as styled by interior decorators. RetroDada aims to please neither at art nor anti-art, as nobody should serve masters. We will put an end to spectacle and replace it with convulsive laughter from continent to continent. It’s shit after all, but from now on we mean to shit in different colors.

Psychoanalysis is itself the disease. It makes the bourgeois self seem interesting. Ethics produces atrophy like every plague produced by intelligence. Theory merely guides us in a round-about way to the prejudice we had in the first place. What we need are works that are tender and precise and forever beyond understanding. There is a lot of negative work to be done.

RetroDada is our intensity. RetroDada is for and against unity. RetroDada is the abolition of their reasons. RetroDada is the refusal of an inheritance. RetroDada is a convulsive effusion, like a cellphone cooked in a microwave. RetroDada is a divinity of the lowest order. RetroDada has no theory. There is enough of that in art school. We love the old things for their freshness.

How to make a RetroDada manifesto: Take a Dada manifesto or two. Copy the good bits. Toss them in a file. Move the bits around. Improve on them. The manifesto will be like you. As for intelligence, it will be found in the streets.

RetroDada is working with all its might to introduce the idiot everywhere. RetroDada is a venture capital form for the exploitation of other people’s ideas. God can afford to be unsuccessful. So can RetroDada. It is luxury without value or price. RetroDada gives itself to nothing, neither to work nor play. RetroDada seduces you with your own idea.

RetroDada is at once stand-up comedy and a requiem mass. RetroDada trusts only in the sincerity of situations. RetroDada fights against the thanaticism of the times. RetroDada develops the plasticity of the digital. We should make all art and literature and cinema free. The medium is as unimportant as we are. Essential only is the forming. Take any material at all.

RetroDada is a theoretical virus. RetroDada means letting oneself be thrown by events. Say yes to a life that strives downward toward negation. RetroDada refuses to be contemporary to any of this shit. RetroDada reintroduces art and everyday life so they can have queer sex in back alleys. RetroDada is the statelessness of the mind. RetroDada rejects both the stylish order and the stylized disorder of contemporary aesthetics. We are convinced of the arbitrariness and falsity of our poor creation, the world. We look unencumbered into the heights and depths.

Let’s only steal from the best, and from their actions, not their styles. The resonant Sophie Taeuber, the drum-banger Richard Huelsenbeck, the tubular Hugo Ball, the mystic Emmy Hennings, the hypeman Tristan Tzara, the ironic Jean Arp, the dadasoph Raoul Hausmann, the runfast Hannah Höch, the aviarist Baroness Elsa. Let’s take their leavings not their droppings. Music, dances, theories, manifestos, poems, paintings, costumes, masks. To be begun again, from the beginning.

So many fates of the west befell Dada. It tried everything already, so we don’t have to. Arthur Cravan became a legend. Mina Loy became a poet. Marcel Duchamp became the enabler of contemporary art. Hugo Ball became a devout Catholic. Emmy Hennings became a Protestant mystic. Hannah Höch wrote a trans-species children’s book. Richard Huelsenbeck became a therapist. Tristan Tzara became a communist. Marcel Janco returned to the Orient. Baroness Elsa died in poverty and obscurity. Jacques Vaché killed himself before it even all began.

To be for this manifesto is to be RetroDada! To be against this manifesto is to be RetroDada! To be for and against this manifesto is to be RetroDada! To be neither for nor against this manifesto is to be RetroDada! There is no escape from the history yet to be unmade!

I am a recovering Althusserian. For decades now I have been Althusser-free, for the most part, but we all have our lapses. The first step to becoming a recovering Althusserian is to recognize that you have no control and are unconsciously always a little bit Althusserian whether you want to be or not.

Louis Althusser is however not so much a poison as what Derrida and Stiegler and Stengerscall a pharmakon. That is, something that is undecidable, both poison and cure. It may well be that there are good reasons, in the twenty-first century, to be an Althusserian. I am not objectively in a position to say, and in any case: by their results shall we judge them. When there is a useful Althusserian response to the Anthropocene (or whatever you want to call the current ‘conjuncture’) consider the matter settled. As of yet there is no such response, perhaps for reasons to be elucidated later.

Perhaps this is to judge too harshly. In what follows I want to read some essays by the ‘young Althusser’. I leave it to others to account for the mature works. I want to think about what is living and dead in the Althusserian ‘problematic’, through a series of antimonies that he had to face. This first part deals with his essay ‘On the Young Marx.’ This essay is instructive, as it both sets up a method of reading Marx that we can also apply reflexively to Althusser, and also provides a useful answer to one of the key problems in Marxological thought: the relation of the young to the old (or mature) Marx.

Althusser neatly characterizes the two extant approaches of his time. In one, there areelements in the young Marx that anticipate the older one, and his thought can be read as a teleology, as always having tended towards this goal, this truth. Or: one can read the young Marx as announcing a broad, ethical program which is then either narrowed or even betrayed by the more economistic and social-scientific work of his later years. The former is a typical reading for orthodox communists of the time; the latter the characteristic program of Western Marxism and even of the New Left more broadly. Hence as Althusser wryly notes, all discussions of young and old Marx are political discussions.

Both these readings tend to focus on elements of the text, finding for example characteristically Hegelian or Feuerbachian elements in the young Marx, or themes submerged in the young Marx later brought out more fully in later writing. Althusser wants to dispense with ‘Hegelian’, or rather bog-Hegelian readings in particular, such as the proposition that the materialist core of Marx was present in the early works but in a still idealist form.

Althusser says that “this method which is constantly judging cannot make the slightest judgment of any totality unlike itself.” There is a totalizing sameness to those readings of Marx that annex him onto Hegel. If there’s just a little bit of Hegel to be detected in Marx somewhere, then it becomes Hegel all the way down.

In place of all this, another method: Ideologies have to be considered structurally, as having an underlying problematic of different terms and their combinations. Any particular ideology also has to be thought in the ideological field in which it partakes. That field has determinants outside itself, in specific historical situations. Althusser wants to claim that this is the beginning of a scientific method for treating the ideological, rather than for merely extending ideology. This is more asserted than demonstrated, but for those inclined to the formal methods of literary analysis, this is progress. Reading is to have its method.

The young Marx, Althusser candidly says, writes ideology. He writes it well, but it is just an extension or permutation of the ideological field of his time. Even devout Marxocologicalists should not be embarrassed by this. Althusser: “Early Works are as inevitable and as impossible as the singular object displayed by Jarry: the skull of the child Voltaire.”

One advantage of Althusser’s reference of young Marx to the ideological field is that it rules out another method, more common in our time: the Great Books of the Apostolic Succession. One reads Hegel, one then reads Feuerbach (extra-credit only, he is not quite canonic), then one reads Marx — and then one reads Althusser. But as Althusser rightly insists, the Hegel that Marx read was “not the library Hegel we can meditate on in the solitude of 1960,” – or 2016. The Hegel of Marx was the Hegel of the neo-Hegelian social movement.

In short, Marx came into a very particular ideological field, and his thought as a young writer was within a problematic determined by that field, particularly that of the left Hegelians, and even more particularly that of Feuerbach. A problematic, for Althusser is a kind of structural system through which other material can be processed. Hence Marx applied the Feuerbachian problematic to religion, as Feuerbach did, but also to political and economic ideologies, as he did not.

Interestingly, the concept of problematic becomes a way not to think the Hegelian totality for Althusser. A problematic is a systematic structure with rules of composition, not a unity whose essence is expressed in all its particulars. A problematic, moreover is something that thinks through you, rather than being what you think. It is in a sense unconscious. It calls for special methods for determining how the problematic is at work in the text. Note how the path is open already for a kind of specialized labor of textual exegesis here. Althusser: “a problematic cannot generally be read like an open book, it must be dragged from the depths.” Henceforth we are with Hermes, running a gimlet eye of suspicion over the text, as if if were a symptom of what it hides.

So Marx unconsciously plays out certain permutations of a problematic. It’s a theory which neatly inverts Sartre’s notion of a writer’s necessary freedom to commit to a project. But this presents then a special problem for accounting for how Marx broke with the ideological field of his time. In backward Germany in Marx’s day, intellectuals put a special effort into thinking what was to be done but could not happen. They looked to the political revolutions of France and the industrial revolutions of England. Unable to actually produce either revolution – they theorized them. Most fully, in Althusser’s account, not so much in Hegel but the Hegelianism of the 1830s and 1840s.

Cunningly, Althusser says that Marx retreated from this ideological field, rather than overcoming or surmounting it. He went back to the original problematics of the political economists and political theorists who Hegel had claimed to synthesize into his own philosophy. This is coupled with two discoveries that are extra-philosophical. It was Marx’s experience of political radicalism in Paris, and Engels’ first-hand psychogeography of Manchester capitalism, that were the key to moving forward after this retreat from Hegel. Althusser: “In France, Marx discovered the organized working class; in England, Engels discovered developed capitalism and a class struggle obeying its own laws and ignoring philosophy and philosophers.”

The failure of German liberalism pushed Marx out of Germany. The bourgeois backers of his radical journalism melted away. And with that failure came the retreat from the ideological field to which they belonged. Marx’s training in German idealism was not wasted, however. It provided the ability to think abstractly, which was only awaiting actual concrete things in the world that really needed to be thought.

Thus, Althusser understands Marx’s thought as breaking with the ideological field of his formation, and founding a science. One might remain skeptical about the second part of this claim — the founding of a science — and still find useful the first part — the concept of the ideological field. Perhaps they are rather harder than that to exit. And what if this method were applied in turn to Althusser? What was the ideological field to which his work belonged? What was its underlying problematic? What historical situation gave rise to it? And closer to my own dabblings with it, what historical situation led to the uptake of Althusser in the Anglophone world in the 1970s and 80s?

To tackle the last first: the defeat of the New Left in the 70s led, among other things, to a kind of embedding in the cultural and educational apparatus of those who had dreamed of larger things. This was past the era of the Chinese, Cuban and Algerian revolutions, still the time of the Vietnam war; the time of the rise and fall of New Left activism in the west. This was perhaps not unlike the situation of the left-Hegelians in Germany in the 1840s. And perhaps with not so different results. That which could no longer be enacted– was to be thought as a theoretical revolution instead.

If Sartre had appealed to a more committed, activist time; Althusser appealed to one of quietism, at least as far as he was read in the Anglophone world in the 70s and 80s. (The Althusserians of 1960s France were a different story). What was to be taken up was something already apparent in this brief essay on young Marx: specialized method. Althusser legitimated the scientific study of the ideological field, the search for the unconscious problematic.

This had certain benefits. It meant an insistence on certain standards for accounting for how the ideological field is structured. It also implies a certain relative autonomy and consistency of the ideological level. It led in practice, however, to a deepening of an academic division of labor, via which Marxist thought could accommodate itself to the disciplines. The economic, political and ideological could then be studied as separate objects, each in their own field, in increasingly diminishing contact with each other.

Let’s look at a famous Althusser essay from the early sixties. ‘Contradiction and Overdetermination’ builds on Althusser’s ‘On the Young Marx’ essay, in deciding against the various Hegelian readings of Marx. Althusser rejects the metaphors of ‘turning Hegel right side-up’, or ‘restoring the rational kernel of dialectic without the mystical shell.’ Rather, he thinks of Marxism as replacing Hegel’s dialectic with a different problematic.

How is one to take an argument of this kind? One way would be to subject it to philological proof, a kind of scrutiny it may not actually withstand. But perhaps one can take the argument in a different vein: that one could replace the Hegelian dialectic with a different one, even that one ought to replace it. Althusser is very nervous about opportunistic or merely ideological dilutions of Marxism, and so he insists that his is a rectification, or a drawing out of a dialectic that Engels rather misconstrued, and that neither Marx nor Lenin had the time to write. As a party member, he could hardly appear to be reading Marx at all creatively.

Perhaps now one can see it as creative. Althusser took a cutting of Marx, taken from its German-idealist root-stock, and grafted it to a quite different problematic. One could mention here at least four coordinates of the ideological field in which that problematic resided. One would be the French social thought from Durkheim via Mauss to Levi-Straus, stretching from anthropology to linguistics. A second might be Spinoza, a third would be the distinctive philosophy of science in France centered on Gaston Bachelard. A fourth would be a version of Marxist ‘orthodoxy’ uninterested in the post-56 ‘thaw’, and loyal to Lenin, Stalin – and Mao.

Of these, Spinoza is probably decisive, although neatly dove-tailing with French social thought, in the way Althusser thinks a totality that produces, among other things, subjects, rather than thinking a totality that subjects produce through their encounter with, and recognition of themselves in, an objective world. Althusser begins the new dialectic with the category of over-determination (borrowed in this case from psychoanalytic readings of structural linguistics). Rather than one dialectical totality, unfolding in all its complexity around a central contradiction, Althusser posits a totality with at least three kinds of contradiction that can over-determine the central one – the class struggle between labor and capital.

Of most use to me is his passing recognition that other classes can over-determine the contradiction between labor and capital. As is clear from Marx’s political writings, from Gramsci, from Kautsky on the peasant question, the simplification of class dynamics down to two classes of Marx’s Capital is not always helpful. The economic dynamics of capitalism might hinge on the class relation, but politics is more complicated. That class contradiction may be over-determined that of other classes. (I pushed this thesis to extreme in my A Hacker Manifesto).

A second over-determination comes from the relative autonomy of the superstructures. It may well be that forces at work in the political or ideological levels may either retard or accelerate the development of the principal economic contradiction. In the case of the Russian revolution, Althusser thinks there is an element of ideological over-determination. The working class was intensely class conscious, thanks to a militant and organized intellectual movement. This idea of the relative autonomy of the superstructures will become a crucial legitimating move for political theory and cultural studies, as we shall see.

A third over-determination take us outside the national-cultural frame so dear to Gramsci, into the space of the relations between imperial states. Taking up Lenin’s thesis that the imperial system broke at its weakest link – the Russian empire, Althusser reads this as a third kind of over-determination. History advances ‘bad side first’, as Marx and Engels put it in theHoly Family. It was not where the capitalist infrastructure was most developed that the revolution broke out – as ‘vulgar’ determinist Marxists might have expected.

Thus the world-historical situation is not the product of the ‘beautiful’ contradiction between labor and capital alone. Strikingly, this implies a root-and-branch rethinking of Marxism itself, both of its theory, but also of its history. “One day it will be necessary to do what Marx and Engels did for utopian socialism, but this time for those still schematic-utopian forms of mass consciousness unfluenced by Marxism… a true historical study of the conditions and forms of that consciousness.” And, one might add, this root-and-branch critical history is now required for the Althusserian turn as well.

The (admittedly simplified) Hegelian theory against which this is launched saw world-historical movement as a dialectic between the sphere of needs, of civil society, versus political society, or the state and its governing Idea. In this Hegel, material life, civil society, the economy – is merely the means through which reason, embodied in the state, works itself out in history. No matter whether this was the Hegel of the Hegelians, it was the Hegel of the Marxists for whom Marx was Hegel put right-side-up. In that version, it is the other way around. The sphere of the social production of men’s needs – economy – is the hidden truth of its political and economic forms. Economy is essence and the superstructures mere appearance. Althusser: “The logical destination of this temptation is the exact mirror image of the Hegelian dialectic. The only difference being that it is no longer a question of deriving the successive moments from the Idea, but from the Economy.”

Even as a recovering Althusserian, I am thankful for this break Althusser makes from the metaphysics of essence and appearance. That metaphysic remains the ideological field of theories of eternal capitalism, in which the essence of its economy never changes, and any new feature is ‘just circulation’ or some other such non-thought. Althusser is the beginning of a way to think historically again, outside of the mythic grand narrative of the ‘beautiful contradiction’, as he calls it, which is the hidden God governing all appearances.

For Althusser, Marx’s whole project is a break with exactly this dialectic. Althusser: “his concern was rather the ‘anatomy’ of this world and the dialectic of the mutations of this ‘anatomy.’ Therefore the concept of ‘civil society’ – the world of individual economic behavior and its ideological origin – disappears from Marx’s work.” In its place, a retreat from Hegel to his sources in classical political economy, such as Smith, and forward to Ricardo and others who follow Smith, and the development of a critique of the very categories through which the sphere of needs is imagined in bourgeois thought.

One might pause here to note that this set Althusserians on a course of seeing the relations of production as the crucial and determinate component of the economic ‘instance’, not theforces of production. This had a certain utility when expanded out into a concept of relations of production and reproduction — a pathway opened by Althusser and his students inReading Capital, which paid attention to Marx’s rather neglected Capital vol. 2. This later enabled a co-joining of Marxist and feminist concepts of how a capitalist social formation might be reproduced.

But there was a relative neglect of the forces of production, the study of which can’t be performed on a purely philosophical level but requires some detailed inquiry into the technologies of the day. Althusser does not ask after Marx’s interest in Charles Babbage’sfield studies in industrial technique, or his readings in German scientific materialism, where the science and engineering and their impact on the forces of production were a lively concern. This was unfortunate, given how rapidly the forces of production changed in the late twentieth century, changes those under the Althusserian spell rather neglected. And one might note here that this made the forces of reproduction even harder to fathom, and no connection was possible to Marxist-feminists such as Donna Haraway whose work was surely centrally connected to the question of the forces of reproduction.

Note that Althusser’s metaphor is the anatomy of the economic, not its metabolism, a term Marx uses in Capital vol. 3 that has proven very useful for green Marxism in thinking the Anthropocene. There’s a sense in which whatever the merits of Althusser’s influence in rescuing Marxism from economic-determinist vulgar thought, it prevented it on the other hand from not being vulgar enough, and really trying to grasp the historical development of the forces of production.

In other respects, with Althusser there was progress. The state, in this new dialectic, is not the embodiment of an Idea, but the instrument of the ruling classes. In place of the essence-appearance metaphysic, is a relation between separate and equally ‘real’ instances: economic, political, ideological, which relate through their structural differences rather than as expressive components of a whole. What was civil society, of the sphere of needs, becomes properly the mode of production, an historically specific form in which needs are socially met. It remains, in Marxist fashion, the determinate factor, but “in the last instance.” Its effectivity may be over-determined by, among other things, the political or ideological superstructures. Indeed, Althusser asserts, “the last, lonely hour of the ‘last instance’ never comes.” In this ‘dialectic,’ relations are separate and external to the terms they permutate. In this case the instances (economy, polity, ideology) are each separate levels with their own internal ‘contradictions’ between terms, each of which is then at a meta-level (over-determination) in a relation of externality and effectivity to each other. Goodbye Hegelian dialectic – negative or not.

This might be a grand and rather ironic example of what Guy Debord and the Situationistscalled détournement: the copying and correcting of past ideas, texts, materials, from past to present, with no regard for property or propriety. But détournement is a topic for another time. Where Debord advocated it a means of cultural and ideological production that abolished all claims to property and propriety, Althusser did the opposite — he established the property claims of those who held the philosophical keys to correct method.

The reason this appeared so urgent at the time takes us out of the those coordinates of the ideological field governed by academic intellectual life, and further into those governed at the time by the intellectual life of the communist party. What was at stake was a double question: who would have authority over Marxist discourse for the party? To which revolution would the party — and its intellectuals — owe allegiance?

The first footnote in Althusser’s text ‘On the Materialist Dialectic’ (1963) is not to Marx or Hegel or Spinoza, it is to Roger Garaudy. Who the fuck was Roger Garaudy? Trust me: you don’t want to know. Garaudy was the kind of hack who passed for a ‘thinker’ within the French Communist Party of the time (and whose later career is to ignominious to even mention). As is often the case, particularly with Marxist thinkers, the ideological field for Althusser was shaped by institutional figures and forces who do not even appear if one studies ‘library Marxism’ in graduate school.

Althusser’s celebrated early works all happen between two world-historical events: Khrushchev’s ‘secret speech’ of 1956, in which he revealed a tiny portion of the crimes of Stalin, and set about a partial de-Stalinization of the Communist movement. The other key event is the Sino-Soviet split, which starts to unfold from 1960, and led to break of the Chinese Communists from the Soviet ‘camp by 1965.

Khrushchev’s speech led to an ideological ‘thaw’, but also to a profound crisis for the western communist parties. A rather vacuous ‘socialist humanism’ became the prevailing ideology, partly inspired by a turn to the young Marx. This current saw Marxism as a continuation of the bourgeois enlightenment project. In some respects, this was a return to the popular front style of thinking of the inter-war years.

The Sino-Soviet split was over many things, of which ideology was probably the least important. Still, Mao did not follow the de-Stalinization line. I remember, when visiting China in 1987, that one could still find portraits of the “four beards” on the walls of official party buildings: the four being Marx, Engels, Lenin and Stalin. Their profiles, one in front of the other, would usually face a portrait of Mao on the opposite wall. In short: Mao was the true successor in the party’s Apostolic Succession, Stalin included.

The rupture between the Soviet and Chinese parties had its impact within the western communist parties as well. The Chinese revolution had appeared as a vindication of at least one idea of Lenin’s: that imperialism would break at its ‘weakest links’ — a form of over-determination in the Althusserian dialectic. The Chinese appeared to be trying to avoid the bureaucratization of their revolution. They seemed to want to do something different to the building of a massive heavy industry that simply reproduced under socialist conditions the same alienated mass labor as happened under capitalism.

As with enthusiasm for the Russian revolution, western enthusiasm for the Chinese revolution was based on very limited information. Since taking power in 1949 Mao appeared to have reformed agriculture, combatted illiteracy, embarked on a huge, labor-intensive program of national reconstruction, all with an aura of egalitarianism and purpose. The human costs of all of which were apparent to almost nobody in the west, whether on the right or the left — the Situationist René Viénet and his comrades excepted.

When the split opened up between the Soviets and China, not a few western communists opted to support China, either within the mainstream communist party, or by leaving it. In France, the party made the mistake of expelling the ‘Maoists’ en bloc, enabling them to swiftly set up a rival party of not negligible size. While there would be splits and factions, Maoism would be a strong current on the French left – much more so than in many other western countries.

Althusser did not leave the pro-Soviet Communist Party of France. His relation to the party and to China question is a rather subtle one. Certainly, ‘young Althusser’ texts can be read as formulating, at a very high level of sophistication a Maoist ‘line’ of sorts, or at the very least one opposed to the politics and culture of the Soviet thaw. Certainly several key students of his were active in one or other Maoist formation.

It is also possible to read Althusser, strangely enough, through what is usually thought of as the ‘voluntarism’ inherent in Lenin, Stalin and Mao’s thought, best expressed in the latter’s slogan “put politics in command.” This would be the idea, even the practice, of considering either ideological propagandizing or political mobilization as the lever via which the whole social formation would be transformed, as in Stalin’s own Cultural Revolution or later as Mao’s ‘Great Leap Forward’. The economic, as the realm of needs, needs a force from without to transform it.

Hence: “a revolution in the structure does not ipso facto modify the existing superstructures and particularly the ideologies at one blow.” Here Althusser appears to complement the Maoist critique of what had gone wrong in the Soviet Union: That the revolution had not been ideologically and politically vigilant enough. Moreover “the new society produced by the revolution may itself ensure the survival, that is the reactivation, of older elements through… the forms of its new superstructures….”

Is it too much to see here an echo of Stalin’s darkest thesis, that of the ‘sharpening of contradictions’ after the revolution? Not to mention Mao’s extension of it to constant mobilizations which, depending on your point of view, were aimed either at preventing the formation of a counter-revolutionary superstructure – or were meant merely to keep Mao the old tyrant in power.

Ideas travel in strange ways. However much Althusser may (or may not) have meant his position to be a Maoist one (a ‘superstructuralism’ but hardly a voluntarism) it ended up being something quite different, particularly in the Anglophone world: a legitimation for the ‘long march’ through the superstructures of a generation of intellectuals, fighting the good fight in the academy, or the media, or the arts, in imaginative but rarely any actual contact with, organized labor.

While in this essay I am critical of the legacy of Althusser today, I want to pay tribute nevertheless to those for whom his texts were one source of inspiration for a life of militancy, in France and elsewhere, often of considerable personal sacrifice. These are people whose names are only known to a few, who gave up lives that were in some cases of high privilege, to work sometimes under assumed names in factories or industrial towns. They could be rather dour and prickly – the basis in fact for Deleuze and Guattari’s portrait of the ‘sad militant’. But particularly in the ‘red decade’ in France (1966-1976) they did their best. For me that is always to be remembered with honor.

This might then be a thumbnail of the ideological field into which Althusser made his most influential interventions. It need only be added that his institutional location was not an insignificant one: the École Normale Supérieure (ÉNS). He taught at the absolute apex of a rather rigidly hierarchical educational system. The Grandes écoles in France produce the elite in each of there respective fields, in the case of the ÉNS – intellectuals. Sartre had been a normalien before him, as were Derrida and Foucault, in whose training Althusser had a hand.

A striking number of western Marxists were ‘outsiders’ of one kind or another, marked by difference. Even the archetypal ‘French intellectual’ Sartre was actually from Alsace. Several were German-speaking Jews. Althusser was a pied noir – a person of white French background born in Algeria. But more significant to our story is Althusser’s cognitive difference. He suffered periodic episodes of depression (and according to Eric Hobesbawm, quite extreme mania). That he murdered his wife while in an irrational state ought not to go unmentioned. I note this also because of the irony that Althusser is one of the sources for a kind of universalist and rationalist stand in continental philosophy, and yet could not have been further outsie the personae of the ‘universal rational man.’

And yet Althusser was also a ‘insider’, a Marxist and at the lofty ENS, teaching philosophy, in a country where philosophy actually matters. Unlike in the Anglophone world, philosophy is embedded within the French school curriculum. It informs a wide range of ideological processes. While Althusser would reject some of Gramsci’s ways of formulating the problem, he would surely have understood the minor but not-insignificant role of philosophy in sustaining what for Gramsci would be called hegemony.

Althusser contributed to a kind of counter-hegemonic base-building which produced in France for a time a quite interesting anti-capitalist cultural sphere. Althusser created an intellectual base for a Marxism that did not need the Communist Party to authorize it, which was one of the conditions of possibility for a non-communist intellectual left which could almost endure what Felix Guattari called the ‘winter years’ of the 1980s.

The real significance of Althusser is in the transition from a Marxism of the party to a Marxism of the academy. The means via which he got Marx from one to the other are now moot. It is rather like the fable of Captain Cook’s axe: first the handle was lost and replaced, then the head was lost and replaced, and yet it remains Captain Cook’s axe. Curiously, this severing of Marx from the actual party was in very different fashions also the goal of the Lukacs of History and Class Consciousness and the Sartre of Critique of Dialectical Reason. In the first case, the party was strong enough to shut this rival down, in the latter case to ignore it. But Althusser of the ÉNS managed to establish a parallel kind of authority over Marxism, independent of the party, and which would perhaps even outlive it.

Some elements of the text ‘On the Materialist Dialectic: On the Unevenness of Origins’ might help explain this move. It is among other things an ur-text for the notion of a capitalized ‘Theory’. In Althusser, this Theory was supposed to be the guarantee of the scientific character of Marxism, of its break with ideology, and a defense against ideological back-sliding. It was not to be. It never became an infallible debugger of method. Rather, we have had to learn to live with what Stuart Hall famously called a Marxism without guarantees.

Althusser stressed the break between Hegel and Marx. He also – rather fatefully – offered a pluralist rather than a mono-causal philosophy of history. (Remember here the three kinds of over-determination). Having defended this against the Hegelians, in this text he shored up the other flank, and defended this limited pluralism against what he calls a “hyper-empiricism.” Once one has more than one historical dialectic – why not lots and lots and lots?

The answer is to advance a theoretical practice. Here Althusser claims to follow Lenin, in the insistence that without correct revolutionary theory, there can be no correct revolutionary practice. What he adds is that the production of that theory is itself its own kind of practice. If the Leninists had professionalized political practice, making it a specialized form of labor, Althusser makes theoretical practice a specialized practice.

Althusser: “ideology is not always taken seriously as an existing practice. But to recognize this is the indispensable prior condition for any theory of ideology.” Oddly enough, one of Althusser’s precursors here is Bogdanov, for whom ideologies were products of real, material practices whose function was to motivate and coordinate labor. Althusser (and his student Dominique Lecourt) are orthodox, even vigilant Leninists in their hostility to Bogdanov, but it is curious that a selection of Bogdanov did appear in the series Althusser edited. Certainly Bogdanov, not Lenin, was Althusser’s precursor in breaking with Hegelian interpretations of Marx!

Althusser: “The theoretical practice of a science is always completely distinct from the ideological theoretical practice of its prehistory: this distinction takes the form of a ‘qualitative’ theoretical and historical discontinuity which I shall follow Bachelard in calling an ‘epistemological break.” There is both the beginnings of something here but a lack of follow through. A more practical study of how sciences constitute themselves in and against an ideological field would seem here to be an excellent suggestion.

But Althusser will not be a return towards a genuine study of science, in the manner of JD Bernal or Joseph Needham. Nor is he a precursor to science studies as it will later flower. Rather, he gets stuck at the level of asserting a merely formal break between a science and the ideology which precedes and surrounds it. There will be those in science studies who will pass over their debts to the Marxism of Bernal and Needham by loudly declaiming their distance from this Marxism of Althusser.

What decides for the science of Marxism is Theory, “none other than dialectical materialism,” although a rather different one to the diamat constructed out of random bits of Marx and Engels by Althusser’s Soviet counterparts. It is to decide in advance what the dialectic is, before the encounter in a given investigation of a new situation or problem. Before beginning any investigation, or any practice, researchers “need Theory, that is, the materialist dialectic, as the sole method that can anticipate their theoretical practice by drawing up its formal conditions.” One could juxtapose this not just to Needham, but also to Bogdanov, at least as I read them. In their hands, theory is a matter of extracting concepts (the ‘dialectic’) from particular practices of the production of empirical and scientific knowledge, and then the speculative testing and adaptation of them to other fields.

In other words, theirs is a genuine pluralism, within a general speculative method, but also with specific empirical tests of the validity of that method in each domain of knowledge production. In short, Althusser wants democratic centralism – the party of Theory’s decision is final – whereas Bogdanov and Needham are more ‘syndicalist’ in their approach to the comradely cooperation of knowledge.

Althusser’s is not quite as ambitious a view as that which Plekhanov took over from Engels, in which a dialectic can be applied even to the natural sciences. Althusser’s ambitions do not extend that far. But he does appear to want a Theory that can legislate outside the bounds of the natural sciences. He is particularly on his guard (as Leninist always are) againstspontaneity, particularly among new-fangled practices of the production of knowledge.  The new social sciences and humanities fields in particular are not to think they are self-legislating – autonomous.

The most ambitious claim of this text is to ground the general method of a theoretical practice. This comes, curiously, out of a gentle but thorough critique of Mao’s text ‘On Contradiction’. What follows strikingly is not so much a theory as a metaphor. Theoretical practice is to be understood on the metaphor of production in general, but in a rather peculiar way. The production of knowledge starts with Generality I: with general concepts, the existing ones of ideology, as raw material. They are transformed by the labor of Generality II. These are the means of production, more or less contradictory, of the production of knowledge of a given moment. The work of Generality II on Generality I produces knowledge as specified concepts, a concrete generality. In short: “theoretical practice produces Generalities III by the work of Generality II on Generality I.

It should be apparent at once that this is metaphorical. Nothing concrete about the labor of the production of knowledge appears here at all. Althusser even says “if we abstract from men in these means of production for the time being….” But the abstraction never ends. What follows is then the dogmatic assertion that there is an epistemological break between Generality I and Generalities III, guaranteed by the vigilance of Theory over the transformative work of Generalitiy II. And what results is not the concrete-as-such, but the concrete in thought. The criteria of valid knowledge are all internal to the theoretical procedure, understood metaphorically to be a labor procedure. True knowledge is that which Theory guarantees, and no other. It is the theoretical concrete which is knowledge.

One could mount an internal critique of this version of what Marxist Theory ought to be doing. But I think it more useful to put it into the ideological field, and ask: what ‘work’ was it doing at large? To my mind, Althusser is trying to set up a procedure for the coordination and validation of correct Marxist ‘practice’ within the division of labor of the university, or even the ideological apparatuses writ large. One that appears to parallel and supplement that of the party, but which actually replaces it.

This has two aspects. One is the replacement of the authority of the Party with that of Theory. To some extent the controversy that this aroused is moot, given the decline of that very Party, and the marginal status of those that would try to reproduce it. Interestingly, by the time we get to the later work of Althusser’s student Alain Badiou, there are four kinds of event which produce the subject and its truth, of which politics is only one, not dominant one.

The other aspect is the question of the coordination of different kinds of knowedge that might claim to be part of a larger Marxist project within the university. This is rather more interesting. It is if anything an even more significant problem today. Althusser’s solution is a ‘democratic centralist’ one, a para-Party called Theory, which has both legislative, judicial (and policing) power as to what constitutes knowledge, at least outside the domain of the natural sciences. It is as opposed to ‘spontaneous’ theories and their lateral, transversal flow between sites of work as the Leninist party was to all forms of spontaneity, whether it be in the style of Rosa Luxemburg, or Antoine Pannekoek, or Alexander Bogdanov.

Not surprisingly, given the institutional context of the ENS and French hegemonic culture more generally, this para-Party is essentially that of philosophy. Marxist philosophy will legislate, judge and police all the other forms of knowledge in a remarkably similar way to how non-Marxist philosophy has always considered itself to have such powers in the French context. The counter-hegemonic (in Gransci’s terms) is a mirror of the hegemonic. It does not have its own form. In this regard, and to make an extreme provocation, we have to conclude that Althusserianism was a kind of reformism in the domain of knowledge and culture. Unlike, for example, Bogdanov and the Situationists, in their rather different ways, there is no imagining here of a different form for a counter-production of knowledge.

Althusser’s metaphoric approach to the question of knowledge production led in at least two different directions. One was an even more hyperbolic rationalism, but more on that later. The other was to make the metaphor more ‘real’, in the sense of examining actual, material processes of the production of knowledge, and in particular of those kinds of knowledge which seem to have direct power-effects. This is the path of Michel Foucault. Granted, this approach leant as much on Nietzsche’s diagnoses of the will to power in forms of knowledge or ideology. And granted, this in many ways became a new kind of doxa in the humanities, where ‘power’ could be found everywhere and nowhere.

Still: there seems to me something positive in this general procedure, and one with more affinity for Marx than might be at first apparent. A Marxist approach to knowledge, and in fact even of natural scientific knowledge, should enquire into the material practices of its production, and moreover, should see itself within the limits of those means of production and not at some Archimedian point outside of them. Much of science studies actually took this kind of trajectory.

Here we have to mention Althusser’s very curt dismissal of any approach (such as that of Bogdanov) which starts from the reality of that which the apparatus and labor of knowledge produces. Althusser simply asserts that if one starts from sensation, even in this historically grounded way, one has no way of filtering in advance what is ideological from it, and thus of producing a science. The reply to this is obvious: Althusser’s rationalism has no such procedure either. It is simply asserted that the vigilance of Theory will perform this miracle, and do so in a universal way.

But why not a method for all the sciences? Particularly those that impinge most heavily on us. Here what might be worth developing, out of Bogdanov and Needham, or out of Donna Haraway and Karen Barad, might be a concrete, historical, specific approach to the actual production of knowledge in both natural and human sciences. What we need is not something like Mao’s ‘On Contradition’, to legislate as Theory for all of knowledge. What we need is many versions of Capital, actual critical accounts of other kinds of knowledge,particularly of the forces of their production, besides political economy. Althusser would of course consider this a “hyper-empiricism.”

If Althusser has merit still today, it is in his sly way of always asking: what is at stake in the politics of knowledge at any given world-historical moment? Let’s quote here how he defined his own moment: “what will later be called by a name which does not exist as yet… when in the struggle for peaceful co-existence the first revolutionary forms are appearing in certain so-called under-developed countries out of their struggles for national independence.” These are not those times. The capitalist west no longer confronts twosocialist camps, one sprung from the colonized world. Rather, I take the defining feature of the conjuncture to be a now-globally victorious regime of commodified production to be confronting the limits imposed by its own destabilizing of the metabolic processes of the planet itself.

It turns out there are resources for thinking such a moment in the Marxist tradition, from Marx’s own concept of metabolic rift, to Bogdanov’s tektology, Bataille’s general economy, Asger Jorn’s ornamentation, or Sartre’s practico-inert. But one might yet retain from Althusser the break from the Hegelian ideological field, to the extent that it saw labor as in a dialectic of spiritualizing nature, imbuing it with rational form, and subsuming nature into teleological project. Where Adorno reversed the Hegelian dialectic and ended up with nothing but the consolation of aesthetics, Althusser replaced it with one that at least gestured in a formal way to the problem of how to organize labor and knowledge.

The thing about being a recovering Althusserian is that one can’t help remembering the good times. Being on Althusser really does feel great. It makes certain problems disappear. For example, one is no longer trapped in the oppressive totality of Hegelian Marxism, and yet nor does one have to return to the world of ‘economistic’ Marxism. One can fly free from all that! (Ah, but as in any addiction narrative, there’s a price to pay…)

So on the plus side, the problematic is no longer constrained by those readings of Marx that see him as essentially putting the Hegelian dialectic back on its feet, or retrieving its rational kernel from its mystical shell. Althusser’s essay on the young Marx already opens up this dimension. But he is an alternative also to those western Marxists who, one way or another, tempered their Hegelianism with a dose of Kierkegaard.

This came in a lot of favors. Lukacs centered his Hegelian totality-in-process on the proletariat as universal subject-object, which frees itself from reification and acts on and as the totality. But even here there is something of a Kierkegaardian irrationality about the proletariat in action, a kind of revolutionary leap of faith. It is a figure that will recur in various ways in Sartre, Badiou and Zizek. Adorno and Sartre, in rather different ways, cut their Hegel with even more Kierkegaard to prevent Lukacs’ totality from self-closure. In Sartre, individuals only ever temporarily subsume themselves into the movement of the totality. In Adorno the dialectic itself is to attend to the unrecoverable fragment. It twists away from the extorted reconciliation of exchange value.

Walter Benjamin’s relation to Kierkegaard is complicated, but let’s just say that Althusser was most certainly an alternative to taking too many hits of Benjamin, wherein history is only ever allegorically present, in the form of fragments that are shot through with a messianic time. All of these Marxisms had a tendency to reduce everything to commodification and its attendant effects: reification, extorted reconciliation, inter-passivity. Either history had become the bad totality of exchange, as in Adorno, or the good totality perpetually postponed where the rational meets the real, as in Sartre.

The Althusserian decision against Hegel and Kierkegaard with Marx comes at a price, however. One is done with totalities and fragments, but has to contend instead with the straying apart from one another of the three (or four) levels of the social formation. This shows up in three post-Althusserian tendencies. One is that version of theory which I think has to be called ‘Jacobin Marxism’. This is particularly clear in some of Althusser’s students, such as Nicos Poulantzas, Etienne Balibar, Jacques Ranciere, and on into other work such as Chantal Mouffe and Ernesto Laclau. The feature of this tendency is to isolate Politics with a capital-P as the decisive level or instance of the social formation, at the expense of any larger sense of a political economy. Politics becomes absolutely autonomous and even ontologically prior.

A second tendency was cultural studies. In rather different ways, cultural theorists from Stuart Hall to Judith Butler took off from Althusser’s famous essay on ‘Ideology and the Ideological State Apparatuses’ and used it to legitimate the study of the ideological (or cultural) as an autonomous sphere with its own materiality and formal laws — to be understood using the tools of semiotics and rhetoric. It was even imagined that the cultural could in some sense lead revolutionary change while economic struggle stagnated.

A third tendency one could call Hyper-Rationalism. There are three kinds of regional knowledge in Althusser, corresponding to the three relatively autonomous levels within the social formation of the economic, the political and the ideological. But philosophy stands apart, legislating for what constitutes scientific knowledge in all domains — indeed for any science. What marks the difference between an ideology and a science is that the former produces subjects who misrecognize themselves in it, while sciences do not. Althusserian rationalist epistemology becomes mathematics as ontology in his student, Alain Badiou, which becomes the speculative realism of his student, Quentin Meillassoux.

All of these tendencies seem to me to still be dependent on Althusser and to point away from the two crucial encounters in our own times. Encounters for which neither Hegelian or Kierkegaardian nor economistic not even Althusserian Marxism are all that adept. One is the technical transformation of the forces of production and reproduction. The other is the metabolic rift opened up by the application of those forces, via the private property form, on a planetary scale. For that we have to look elsewhere, once we have weaned ourselves off Althusser, and certain other habits of thought. Habits which, like all addictions, reproduce themselves within our thought and within institutionalized discourse in pursuit of their own necessity, regardless of what takes place in the world.

This article is about cultural techniques and software culture. The notion of cultural techniques stems from German media studies and refers to a range of epistemic, embodied, cognitive and affective orders. It shows its particular usefulness in how it extends our conventional notions of media.1 Media become more than media, and can include ‘inconspicuous techniques of knowledge like card indexes, media of pedagogy like the slate, discourse operators like quotation marks, uses of the phonograph in phonetics, or techniques of forming the individual like practices of teaching to read and write’ as well as maps, doors, operations and practices of law, and so much more. Cultural techniques participate in the formation of subjects, as well as constitute ways of knowing and organising social reality. I am in this context interested in how we can read some aspects of software culture and organisation of the labour of programming in relation to cultural techniques. Modes of organisation as well as practices of coding represent ways in which code and software regulate social reality but that they are also being regulated as a prioritised technique in digital economy that is at times related to discussions concerning cognitive modes of production. In this article, the notion of cultural techniques is coupled with a concept from a very different tradition to that of German media theory: I engage with cognitive capitalism and specifically cultural techniques of cognitive capitalism. My interest lies in addressing software cultures and techniques of work and labour as ways to understand the constitution of our capitalist technological environment. This set of practices and techniques has to do with management of code and code work, hence, as a punchline to summarise the article early on: cultural techniques of cognitive capitalism should not only to be about the cognitive, I argue, but a range of practices, techniques, management and organisation that happens outside the brain. In other words, what sustains the cognitive is a field of techniques. It is an argument that someone within the field of brain sciences could make as well as scholars observing that any cognitive activity happens on a distributed field of the self, and that the brain is anyway extended as part of its surroundings. Besides neurosciences and cognitive theory, for instance theories of cognition and affect in design are taking such ideas into account.
Yet my focus is on aspects of media theory and software and I will leave the otherwise interesting cognitive science contexts out. Of course, one can excavate a media archaeology of cognitive capitalism through its ties with cognitive sciences and the wider scientific discourse concerning the brain, communication and cooperation but here I focus on software cultures and a more technological understanding of the cognitive. We have to be aware of the way cultural techniques relating to code and programming are themselves one important framing of the cognitive. This idea relates even to very early formulations of the role of the computer as universal, programmable machine. Already in the 1940s the computer pioneer Alan Turing proclaimed that the future of computing was situated in the office—in the ‘office work of programming’ to be exact.
Following the notion of cultural techniques, I want to excavate the specific techniques that sustain the notion of cognitive capitalism as used by, for instance, Yann Moulier Boutang. The allusion to the powers of the brain as communicative, organisational and a coordinating factor in production of value needs to be investigated in relation to its technological conditions as well. This sounds like a media archaeological argument but here it will be approached through the concept of cultural techniques, particularly as it has been formulated in recent German media theory. Following, for instance, Sybille Krämer and Horst Bredekamp’s lead we can extend cultural studies investigations towards material, scientific and technological operations.
For sure, Jobs would have made a great lecturer for the first year ‘What is Media Studies’ course, had he not wanted to start developing his business empire. This is not merely a joke but a reference to how in this talk and throughout Apple’s early business strategy the focus was on education and pedagogy: free computers to schools and the early education of children to become users of the new smart future of computerised learning. Jobs’s concern was constantly about new generations—of creating conditions for psychotechnical drilling and training in the emerging computer culture. Jobs, the software-pedagogue, is continually concerned with the user, techniques and habits, and with tapping into what could be called the involuntary part of the human being: ways of drilling the user into suitable conditions of use. This context flags the need to map the wider set of relations between institutions or corporations, interface technologies, business strategies and psychotechniques as the cultural techniques of cognitive capitalism. From the focus on early film theory, such as that of Hugo Münsterberg (1863–1916), we move to the digital technologies of habituation and drill: ways of seeing, acting, gesturing that perform the reproduction of the worlds in which relations of value production and exploitation can tap into the bios and zoe of living bodies and the social relations that characterise life in digital culture. This is also something that crosses into the field of cognitive sciences as well as design, even if I have to neglect that aspect here. In the talk, Jobs demands a break with the old habits of media consumption and techniques, and he encourages planning new ways of engaging with software worlds. His encouragement sounds like it is addressed to both the producer and the user. Optical video disks, able to hold tens of thousands of images, or an hour of video, should not be solely used to play a movie but should also be used to take full advantage of its random access possibilities, he insists. We should learn more about the affordances of technologies—what their potentials are, and what the technological future might look like. The Lisadraw software is what, in his own words, allows Jobs to try to be an artist and deal with visuals as objects—to move, shrink, change textures, airbrush, soften and harden edges—all of which makes him realise how the talentless can draw. Besides the implications noted by a range of commentators about the renegotiations of talent, skill, artistry and, of course, creativity in post-Fordist economies and software culture, we are able to realise what drives Jobs’s vision of code.
We should pursue the question ‘if code makes the world, then who makes code, and what sustains such operations?’ I want to trace a slightly longer line of thought that relates to code, code work and organisational ideas concerning software and digital culture. In this idea of programming, which relates not only to code but the user and producer as well, we have other examples from the burgeoning new media culture of the 1970s and 1980s. ‘We need to be able to program the programmers too’—a striking emphasis voiced by Charles Simonyi, a Hungarian-born computer scientist working at PARC Xerox Palo Alto labs in the 1970s. Indeed, our computer histories often mention only the more famous products of Simonyi and others at PARC—the Bravo text editor, for instance, and the work that contributed in part to the success of Microsoft (Word) and Apple corporations. They relate to the technological solutions and the spirit that Jobs later captured as part of his hegemonic vision of digital culture: users are productive and creative and with the help of systems that combine efficient memory management one can develop more complex but easy to use computer graphics content to release our brain power. Instead of Bravo, I want to focus on ignored aspects of the work at PARC and the ‘software factory’ that produced Bravo—the question of what grounds the work on text editors and computer interfaces and, more generally, software itself. This means focusing on the notion of metaprogramming. In other words, how do you organise work of and in software environments, and program the programmers: how is software work organised as a factory?
Cognitive capitalism, as used by Yann Moulier Boutang, refers to the widespread cultural and corporate harnessing of ‘invention power’. It features as an integral part of current capitalist modes of organisation of production and value creation. So where does invention reside and take place? It becomes visible and perceptible in the amount of investment in education, training and other enhancements of social and cognitive capacities. It also persists as a way to infiltrate the social more widely. Our ways of talking, acting, gesturing, remembering, expressing, exploring and thinking are part of the wider picture in which capitalist organisations, and processes of accumulation, are taking into account externalities. That capitalist accumulation is dependent on externalities is not new per se. Externalities referring to ‘collateral effects, by-products or joint production’, are, of course, an essential part of any social activity as people in cultural and media studies might take for granted.18 But this realisation does not feature in economic theory textbooks that often. Boutang insists on the importance of accounting for externalities, not only in the classical sense of taking into account negative externalities (industrial pollution being a common one) but also positive ones and their role in the commodity production and the wider milieu of production incentives. Think of gentrification and cafes, restaurants and other services for the younger creative class—a topic so thoroughly discussed in context of creative cities.
Communications in shared spaces count as externalities. They are conditions for the possibilities of sharing and all other characteristics that are now pitched as essential for this cognitive sort of labour on which capitalism rests. Of course, to an extent, some of the discourse on the common has tackled related themes. Years of post-Fordist theory, autonomist Marxism, and theorists from Hardt and Negri to Paolo Virno and Maurizio Lazzarato, Matteo Pasquinelli and Tiziana Terranova have offered their primarily Italian arsenal of concepts as insights into the cultural processes at the core of labour–capital relations in digital culture. Yet the question as it relates to technology persists and perhaps demands a more accurate finetuning, especially when considering software. Boutang insists on the specificity of a digital phase of capitalism, which, he argues, is not reducible to actual technologies. Boutang claims that technologies are only a necessary, but not a sufficient, condition. Instead, this ‘mutating capitalism’ is less about muscles and more about brains—the Chattering, or Tweeting, Man, instead of the Factory Man. This leads to the realisation that Boutang advocates: cognitive capitalism is not solely about the technological, it is also about the ‘appropriation of knowledge’ and ‘the use of new information and communications technologies’. In other words, it’s more innovation than hardware—or even software. Hence, Boutang also wants to keep his eyes on modes of education supporting the innovation work. On the economic level, Boutang is able to mobilise a range of interesting insights that discuss the regimes of knowledge and labour in this new mix. In his simple historical model, cognitive capitalism is set as the third phase of a longer development, following mercantile and industrial capitalism. This is also a move towards immaterial capital, knowledge and knowledge economy. The mutations of capitalism are not merely phase shifts with a definite break from earlier periods, but a rearrangement of technologies, infrastructures, skills, procedures and capabilities in relation to their role in value creation.
The primacy of knowledge for value creation is evident in the centrality of political debates around property rights, management jargon, a focus on organisational methods and the wider role that human resourcing as well as pedagogy plays in the corporatisation of various industry sectors. By bringing in elements of communication and networks, we are immediately inside discussions concerning organisations and management of such flexible institutional settings, where work and value creation need to be somehow paired in suitable ways. A lot of this analysis can be seen to be predated by, for instance, Luc Boltanski and Eve Chiapello’s analyses of management language and practices since the 1960s, and a move towards a more individualised definition of work in terms of ‘creativity, autonomy, and flexibility’.  Boutang’s theory needs to be also addressed critically and with an eye towards possible shortcomings of the notion of cognitive capitalism. For instance Steven Shaviro has noted that we should critically interrogate various aspects of Boutang’s theory’s main claims. Is there really such a definitive and clear break from earlier regimes of industrial labour? What are the harmful downsides of communicative brainwork, such as exhaustion? How does exploitation of non-work hours extend the reach of the corporation to the wider social field of our thinking, doing and gesturing, and harnessing ‘free time’ as part of value accumulation for the corporation? Indeed, isn’t cognitive capitalism merely describing a situation of rather cynical colonialisation of time and affect that reaches the most intimate spheres of subjectivity? What is called ‘bioproduction’ is actually an effective exploitation of the resources of non-paid time which, however, becomes increasingly central for value production. It is already something visible in, for instance, Terranova’s important account.
What Shaviro flags, and what has been articulated in length by Matteo Pasquinelli, is the danger of conflating such perspectives with the Silicon Valley idealisation of commons and other shared externalities that benefit us all, without being able to account for the actual unofficial investments in work and other practices to support the labour in/of/for commons. This resonates also for instance with Jodi Dean’s term ‘communicative capitalism’ which flags the conflation of democratic ideals with technological infrastructures. The dirty, libidinal side that Pasquinelli is after does not feature in Boutang’s more polished view, which, I argue, would benefit from more rigorous media studies perspectives. By now, it should not come as a particular surprise that I want to discuss it in terms of cultural techniques. How to combine the two sides of European theory: the German love for the machine worlds, and Italian/French analysis of politics of the immaterial kind, but also attached to the world of network cultures?
As one way of articulating this question of the mediatic in cognitive capitalism, I want to pitch the idea of cultural techniques of cognitive capitalism. This is merely a theoretical opening with one case study relating to software and labour; it is in need of further elaboration by more empirical and historical work. Such a notion of cultural techniques of cognitive capitalism is a cross-fertilisation of two traditions that have not spoken to each other too much yet. I am referring to the political theories coming often from the tradition of Italy and their developments in France— the Autonomia-movement and post-Fordist political analysis that I have just nodded towards and which one has to admit is a very heterogeneous field. And at the other pole of this theoretical crossbreeding I introduced German media theory, which is not only represented by Friedrich Kittler, or media archaeology (of for instance Siegfried Zielinski or Wolfgang Ernst), but also by such terms as ‘cultural techniques’ as discussed by a range of scholars from Bernhard Siegert to Markus Krajewski and Cornelia Vissman to Sebastian Vehlken.
As I have already briefly introduced, cultural techniques can be seen as ways of extending the media material ethos to a wider set of concerns where media are related to ‘ontological and aesthetic operations that process distinctions … which are basic to the sense production of any specific culture.’ More specifically and perhaps more enlightening is Siegert’s emphasis that not all a priori conditions of culture are technical a prioris, ‘but involve the materiality of media in the broadest sense’. We can map the different operations that link humans and media: material media studies is once again able to offer a way to understand society. Speaking of contemporary creativity and, for instance, software from a discourse of fun we can turn to the management of creative work. This includes the management of invention that runs across the contemporary cultural techniques of capitalism as a theme that is grounded in new forms of exhaustion, besides the at times enthusiastic—and hence inflationary—excitement for brain power and cooperative peer subjectivity. This aspect has been discussed in recent years by Franco Bifo Berardi but also relates to the thesis proposed by Bernard Stiegler: instead of smart cultures of skilled professionals in communicative industries, we should acknowledge the systematic stupidity and proletarianisation of ‘creative’ work. In Stiegler’s words: ‘We thus have pure cognitive labor power utterly devoid of knowledge: with cognitive technologies, it is the cognitive itself which has been proletarianized.’ If we pursue this discussion concerning factories, the proletariat and the grey repetitious aspects of communication, we end up in a discussion of techniques and operations. I argue that we need to pursue this in terms of the operations that offer a link between human practices, media and society, as well as political economy. I want to flag this as a necessary supplement to the more optimistic, and perhaps more ‘Californian’, take by Boutang. Indeed, one needs a slightly more evil approach, in the manner proposed by Matthew Fuller and Andrew Goffey. They call this evil media studies. As part of their larger project, Fuller and Goffey take up on what they pitch as grey media; the tiny details, glitches and repetitious occurrences from current software and social systems, which in their minute detail can be seen as defining a whole field of interest in management cultures as media. Spreadsheets, workflows, auditing, call centre procedures all become understood as part of the constitution of the reality of management—and management as the reality of media
It’s not that technology is entirely missing from Boutang’s theory. He is a fan of open software projects, and complements his economic analysis with examples of techniques of the social. One such example includes the call centre. For Boutang, telephone help centres offer an example of the joint project between database-based organisation of possible solutions and the mediating role of the actual worker. The worker is the curator, in real time attending to the needs of the customer and not only through mechanically finding the suitable solution from the database. As a creative worker of sorts, she or he has to demonstrate innovation by identifying ‘cases that fall outside standard practice’ and, where possible, ‘offer new viable solutions’. But from an ‘evil media’ perspective, call centres are endless sources of delay and streamlining, of evasion and systematised boredom. Indeed, through the various steps in the formalisation of (natural) languages into modes of expression suitable to algorithmic logic—and, besides language, also gestures and patterns of cognition—one enters a grey zone where any celebration of virtuosity of language at the centre of the multitude subject sounds slightly exaggerated. Indeed, call centre work is extensively about management of social relations in relation to databases and algorithms, but in ways that produce ‘the cognitive’ as a matter of regularisation and disciplinary formalism. This means a social patterning that refers to a perversely idealised version of the real world that we call ‘work flows’. Unfortunately, such idealisations found in organisational charts rarely actually correspond to the real world if one understands by that the number of exceptions, bugs, errors, mishearings and other events that occur amid actions and expressions, which constitute the core of Fuller and Goffey’s evil media theory. A look at history of software cultures as one of management reveals this other side of cognitive techniques and information work. They are part of a media history of cognitive capitalism, or cultural techniques of cognitive capitalism. But this is not meant as a valorisation or a rosy picture of brains in cooperation but rather patterns of management, and organisational operations and abstractions. If we really want to paint an accurate picture of such post-Fordist theory concepts, we need to take a look at the grey media and cultural techniques.
What is often focused on with user and interface design, as in the emergence of Apple-centred digital culture discourse since the 1980s, can be complemented with the less fun aspects, such as some of the work at Palo Alto labs in the 1970s. For sure, these examples have not been altogether ignored, but the focus of attention has usually been on end-user-oriented innovation such as emphasising user-centered products like the Bravo-editor and interface. PARC worked towards user creativity through meticulous ways of managing memory, graphics innovations and technicalconceptual innovations like the WYSIWYG. Instead of taking the obvious route let’s focus on some other aspects which might, however, be more methodologically boring. We can take Stiegler’s theoretical note as a guideline and track the specific procedures of proletarian approaches to the cognitive, and expand on the theme of software work as factory work. I will do this by briefly exploring one specific example of a research theme and practice at PARC: metaprogramming, or programming the programmers. This is in no way a dominant strand in software culture but it can help us to think through some connections between software, organisation and work: the software factory ensuring the fun for the end user.
Over the past few years, interest in coding has experienced a strong upward surge where the labour and even potential dullness of code crunching is entangled with a cheerful discursive embracing of new modes of collaboration, adaptability and even ‘fun’ that accompany the most discussed software management projects, that is, free and open software, as well as gaming. This is also apparent in political and industry emphasis on wanting kids to get into coding: the word ‘fun’ appears often in reports and motivational material for various school and after-school activities. In general, coding is coded as something of an exceptionally exciting activity that even cynical teenage kids might engage with. Working with software is inspirational and is indeed often seen as a form of self-expression, distanced from the laborious sides of code work. Everyone can code, and coding is a way to release potentials of expression. In the United Kingdom, this emphasis was primarily established by the current Tory-liberal government, whose education plans emphasise the need to increase skills and awareness of programming in school syllabi. The thinking behind such an emphasis was rather straightforward: coding might not feel like work but it pays off, and is the basis for a vision of a Digital Britain. If we return to the 1970s situation, and code as work, for Simonyi, metaprogramming is about improving productivity. This happens by letting the inspirational programming happen on a very abstract conceptual level by a team leader, then implemented by technician-assistants. This narrows the creative aspect to a very limited role, where most of the work is just careful implementation of the broad-stroke approach. In short, metaprogramming is about managing the complexity that goes by the name of code and software. It is just one particular mode of understanding software work and management, but it can give us clues to think about software work in other contexts too. Simonyi’s idea was rather traditional and seemed to point in a direction different from the user-centered and horizontal notions which are now celebrated but which perhaps hide the fact that software work can also be boring and exploitative—in the gaming industry and other creative fields.
Simonyi’s definition of metaprogramming is in this sense about describing ‘an organisational schema, designed to yield very high programming productivity in a simplified task environment which excludes scheduling, system design, documentation and other engineering activities’.  This actually means transferring engineering tasks into localised work groups where high-level skills are not cognitively necessary. Those groups need only the basic technician skills. This is where communication between separate work groups becomes crucially important, not only for the content that is communicated. How does one talk of code projects and software products? How can the language necessary for programming- and engineering-related activity in a design organisation be standardised? According to Simonyi the problem is to work around local languages and through a more universalised take on communication about, and naming of, the objects of communication. Software becomes a language, but in a different way, perhaps, from what Manovich meant. In our case, language is a mode of command, control and management for the benefit of organisational efficiency. This does not mean a completely rigid system. Simonyi leaves this slightly more open when offering his definition of metaprograms as ‘informal, written communications, from the metaprogrammer, who creates the local language, to technicians who learn it and actually write the programs’.  But it still serves the purpose of standardised procedures, which sustain cooperative, synchronised communication. More specifically, in software production this means a focus on aspects other than the long dominant modes of measuring lines of code per hour. Simonyi’s early 1970s work marks a gradual shift in understanding coding and productivity. Having said that, the earlier mode of quantitative measurement is not entirely abandoned and forms the statistical part of Simonyi’s study. Metaprogramming is to be understood through confining the creative aspects of coding to one particular role. The metaprogrammer is the creative coder, the prototyper who writes sample code and feeds it to the technicians for implementation. The technician-coders work under the metaprogrammer and feed back in a cybernetic loop, being managed tightly and efficiently.
One way to understand metaprogramming is to think of it like this: in the manner that before computers were machines, they were human, often females, as before and during World War II. Computers were about teams of humans managed as computational units. The organisation and management of humans labouring as computers was what defined that early ‘computer architecture’ before some technical solutions changed the role of the human programmer in relation to the machine that was now able to number crunch. Metaprogramming implicitly simulates the development of software language, transposed as an organisational diagram. Metaprogramming is about the programs coded, but also about coding the humans as computational aspects of the organisation. The organisation is the modern software computerised environment: abstracted higher level languages are where programming is seen through tentative planning; third-generation languages are actually the metaprogrammer, who then feeds such plans to the compiler; source code is command, but one that needs to be fed, processed and enacted so that it becomes what it attempts to be—a source. Metaprogramming crystallises the idea of software at a particular point in history; it codes computers and people. Coding is labour, and software becomes a product. The materiality of such technological processes involves both ends: materialities of technology and materialities of labour. Techniques of managing and organising labour become increasingly central. Simonyi’s thesis pays attention to the practices of educating and training staff in various ways; a lot of this training has to do with organisational commands, themselves cultural techniques, modes of managing the commands, communicating and error tolerance. They are modes of managing and responding to the patterns of organisational logic. The metaprogrammer might execute a command but only to reach the technician—the technician being the level of support for the higher-level functions so to speak, and a level which is defined by its own operations that are the object of training. Issues of training, communication and management of work and even flexibility are important in this schema. It is not only the amount of executable code produced that is measured, but also the metaprograms—the creative work.
In Simonyi’s words: we have proposed uncertainty absorption for improving the software industry’s ability to deal with the uncertainties inherent in large software problems. Uncertainty absorption—the promise of action which enables others to operate free of the uncertainty—is particularly simple when production can be organized as a continuous process which can be measured, controlled, and hence, optimized. We divided the software production task into an engineering phase, in which the user’s problems are made well defined; and production phase, in which proto-software is produced by a continuous process. The proto-software is given back to the engineers for refinement to create the final product.45 What is fascinating is how the quote from Simonyi pitches the relevance of such a technique like writing ‘proto-software’. It signals also important aspects concerning labour and organisation. I argue that Simonyi’s ideas lead us to think about the wider set of techniques that mobilise ideas about code, software and work. Coding expands to the wider set of communicative tools with which the organisation has to function in relation to customers. What needs to be recognised, even from this short take on metaprogramming, is how considerations of software are entangled with other sorts of cultural techniques. In other words, where software culture is introducing its own range of new and re-emphasised modes of managing the world—from sorting to abstractions (including pseudocode and in this case ‘protosoftware’), error checking and debugging—it is also embedded in how we talk about, read and measure such activities as part of the organisation of software work. This relates further to the issue of measurement of productivity. For Simonyi: Productivity is traditionally defined as the relationship between the output of goods and services and the inputs used in their production. Applied to software production, the output of program bulk should be expressed as a function of the inputs: the time of programmers and other labor and possibly overhead costs.
Coding is not necessarily very creative or fun. The fascinating role of the metaprogrammer doesn’t necessarily even need access to computers, and becomes more that of a general systems planner in a cybernetic sort of organisational diagram. The emphasis on communication and training to specific roles demonstrates no clear-cut ‘post-Fordist’ mentality of a transition from factory-based hierarchical management to a more horizontal method of connecting the brain (to adopt to Boutang’s discourse). Instead, the team-based organisation we find in such early plans as 1970s metaprogramming testify to how differently software production can be understood—not as the horizontal clustering of creatives, as the mantra goes, extending to the idea of the creative end user and the flexible worker, but as the management of various roles, some of which are still quite straightforward technician jobs. Such a realisation might ring true for a range of current software organisations and companies—in the gaming industry, for instance. Whereas this view resonates with a range of cultural techniques in software and coding work in the current economic and organisational settings as well, it also hints at a different lineage from that of Boutang’s theory and some other positive accounts of creative industries and digital economy. Instead, what if we in a way think of the celebration of the creative individual, the post-factory artist type, only as a short entr’acte in the history of organising knowledge production? Even in early formulations—before programming meant software as a separate ‘entity’ from hardware—the laborious nature of programming was underlined. Innovation is embedded in the reality of labour—’dull labour’ to be more accurate.
The idea that cultural techniques of software culture and work are actually often much more physical, repetitious, uncreative and based in rather strict management, disciplinary and formalisation procedures needs further analytical attention. We are gradually realising that digital culture is sustained by hard and repetitious manufacturing processes outside the creative industries circle—for instance, the Foxconn factories in China—but the realisation that creativity is embedded not only in precarious but also in rather repetitious and tiring practices needs to be taken just as seriously. It is in this sense that the certain immaterial production part of cognitive capitalism is completely material to the point of exhaustion. In other words, we focus on the cognitive not only as concerning the thinking brain but as it concerns the wider set of techniques in which brains are connected to bodies and bodies to work patterns, and in which work patterns are set in a complex group of organisations that are abstract ways of tying the different techniques together. Besides specific case studies relating to the entanglement of code and labour, the theme of techniques of the cognitive point towards the need for political media studies, in the sense defined by Jonathan Beller: ‘A political and politicising approach to media studies would insist upon the materiality of mediation as well as reckoning of the material consequences of even the most ostensibly immaterial and abstract mediations.’ A cultural technique approach to code, software and digital culture might take on board notions such as cognitive capitalism, but also pokes the grey mass of the cognitive through excavations into what sustains it. What sort of techniques and technological practices are behind what we call the cognitive, and what sort of archaeological excavations are we able to engage with when we want to understand the differing uses of communication and organisation? An analysis of cultural techniques becomes then a way to extend into issues that are historical, mediatic and, indeed, political.

The advent of “new media” (in common parlance, a loose conglomeration of phenomena such as the Internet, digital television, interactive multimedia, virtual reality, mobile communication, and video games), has challenged many scholars to investigate the media culture of late modernity. Research agendas vary from network analysis to software studies; from mappings of the new empire of network economies to analyses of new media as “ways of seeing” (or hearing, reading, and touching). Efforts have been made to pinpoint where the “newness” of social networking, interactive gaming, or data mining lies and to lay the foundations for “philosophies” and “languages” of new media. For some researchers, the main concerns are social or psychological, while for others they are economical and ideological, or motivated by search for technological determinants behind the myriad manifestations of media. As different as these approaches may be, studies of new media often share a disregard for the past. The challenges posed by contemporary media culture are complex, but the past has been considered to have little to contribute toward their untangling. The new media have been treated as an all-encompassing and “timeless” realm that can be explained from within. However, signs of change have begun to appear with increasing frequency. Numerous studies and collections addressing the media’s past(s) in relation to their present have appeared in recent years. This influx of historically oriented media studies must be greeted with a cheer. Still, one cannot avoid noticing how little attention has often been devoted to defining and discussing methods and approaches. The past has been visited for facts that can be exciting in themselves, or revealing for media culture at large, but the nature of these “facts” has often been taken as a given, and their relationship to the observer and the temporal and ideological platform he or she occupies left unproblematized.
Years before Perriault, the word archaeology had been used in the title of C. W. Ceram’s Archaeology of the Cinema (1965). Ceram, whose real name was Kurt Wilhelm Marek (1915–72), was a well-known popularizer of archaeology. Yet applied to the prehistory of cinema his idea of “archaeology” hardly differed from the goals of traditional positivistic historical scholarship. Ceram presented a strictly linear and teleological account about the developments that led to the cinema, breaking off his narrative in 1897, the year that, according to him, “saw the birth of the cinema industry.” Ceram focused on inventors and the technical steps that led to cinematography. Everything that did not fit neatly into this narrative was left out, no matter how interesting it might otherwise have been. The illustrations, selected by the British scholar Olive Cook (mostly from the great collection of John and William Barnes), told an entirely different story, pointing out phenomena and potential connections omitted by Ceram. This was an interesting rupture, embodying a tension between two very different notions about the history of the moving image. The word archaeology later appeared in the title of Laurent Mannoni’s Le grand art de la lumière et de l’ombre: Archéologie du cinéma (1994). A change of emphasis is clear. Based on an extensive consultation of archival material (which justified the use of the word archaeology), Mannoni’s book no longer tried to present a closed historical narrative arranged as a set of interconnected causal chains inevitably leading toward cinema. Rather, the five-hundred-page volume consists of a succession of carefully researched case studies of different facets of the moving image culture, covering several centuries. Although there is a strong emphasis on technology, Mannoni also discusses its applications and discursive manifestations. Piece by piece, a narrative develops, but one that does not pretend to be complete or to hide its gaps. Although Mannoni’s discourse stays close to the sources, avoiding theoretical speculation, the book invites new insights, opening paths for further interpretations.
Marshall McLuhan introduced a new approach, new combinations, and new themes to the study of media. His early work The Mechanical Bride (1951) developed a critique of contemporary mass media, drawing occasional parallels with mythology and history, and shifting between high culture and popular culture with apparent ease and (and some took it) recklessness. In The Gutenberg Galaxy (1962) McLuhan’s vision came to embrace the history of media in a more rigorous sense as he traced the dynamics between orality, the Gutenbergian printing revolution, and the new orality represented by televisual media. Instead of providing a neutral and linear narrative, McLuhan’s idiosyncratic discourse surfaced as an essential element. The materiality and the processual nature of his discourse was further emphasized in the collagelike books (The Medium Is the Massage, War and Peace in the Global Village, and Counterblast) that he produced with the graphic designer Quentin Fiore following the international success of Understanding Media: The Extensions of Man (1964). McLuhan’s influence on media archaeologists has been manifold. Of utmost importance is his emphasis on temporal connections, translations, and mergers between media, something that inspired Jay David Bolter and Richard Grusin to develop their notion of “remediation” and to use it to investigate how features of earlier media forms are subsumed into digital media. Bolter’s and Grusin’s endeavor was not defined as “media archaeology,” but it has affinities with the ways media archaeologists draw parallels between seemingly incompatible phenomena. McLuhan’s understanding of “media” and “medium” was broad and challenged existing dichotomies, like those between material things and notions of the mind. His ideas of new media as “extensions” and as driving forces for changes in society have influenced the German “media materialist school” of media archaeology through the work of Friedrich Kittler. Last but not least, McLuhan’s unwillingness to stick with formal “methods” and fixed sets of concepts, as well as his self-reflective play with his own discourse, seems to appeal to “anarchistically minded” media archaeologists, determined to keep their approaches free from institutional-theoretical dogmas and infections.
Siegfried Giedion’s Mechanization Takes Command (1948) presented a detailed account about the forms and impact of mechanization. Ranging from techniques for capturing human movements as graphic representations to the features of everyday household objects like the bathtub, Giedion’s history had less to do with isolated apparatuses than with their interconnections. Mechanization was presented as a depersonalized force that infiltrated Western societies down to the minutest details of everyday life. Giedion was mainly concerned with material culture, “the tools that have molded our present-day living.”  The “anonymous history” he proposed looked for a synthesis between Geistesgeschichte and positivism, where every detail was “directly connected with the general, guiding ideas of an epoch. But at the same time it must be traced back to the particulars from which it rises.”  Even earlier, the German cultural critic Walter Benjamin had already projected a kind of anonymous history, but one that involved discursive layers of culture to a much greater extent than Giedion’s largely materialist vision. Benjamin is arguably the most prominent forerunner—beside Foucault—of media-archaeological modes of cultural analysis and is a major influence for cultural studies. In particular, his unfinished Arcades Project (Passagen-Werk) became a case study about the kinds of issues media archaeologists deal with.25 Benjamin’s reconstruction of nineteenth-century culture, with Paris as its capital, relied on a multitude of sources, including texts, illustrations, urban environments, architecture, public spectacles like the panorama and the diorama, and objects deemed to be emblematic of the era. The approach was remarkably open, shifting, and layered and took political and economic but also collective psychological factors into consideration. Beside material forms, Benjamin’s work illuminated the “dream worlds” of consumerism and early modernity. Working against the tidal wave of Geistesgeschichte, Benjamin refused to group the massive evidence he had gathered under any single symbol deemed characteristic of the era. Such persistence is one of the reasons why the work remained unfinished. The readers were left with a huge collection of notes, images, and ideas that constitute a database rather than a preorganized narrative. Benjamin offered meditations on time, spatiality, nature, and emergent modernity as a new realm of sensations. The concept and method of allegory that he had already developed in his earlier work referred to alternative ways of seeing temporality not as an organic succession but through the figures of ruins and decay. The interest in change and the “ruins” of the body and mind were evident in his other works as well, which famously touched on historical changes in the modes of perception.
Both Benjamin and Sternberger were interested in the panorama as a visual manifestation of the nineteenth-century culture. However, their approaches differ in important respects. For Benjamin the panorama was just one of the manifestations of the larger topic he was trying to get hold of, while for Sternberger it became nothing less than the organizing metaphor for his portrait of an era, the key to unlock the secrets of, in the words of his book’s subtitle, “how nineteenth century man saw himself and his world and how he experienced history.” In his book Sternberger deals less with concrete panoramas than with their manifestations in cultural phenomena as diverse as steam power, railway travel, the Western idea of the Orient, the theory of evolution, and domestic lighting. While this totalizing idea clearly derived from the ideology of the Geistesgeschichte, it may also bear certain similarities with the ways Foucault dealt with his “epistemes.” In the early decades of the twentieth century, art history also began proposing ways of recontextualizing art within textual traditions and expanding its own reach to visual material that had traditionally been left outside its confines. A reevaluation of “neglected traditions” has been proposed by Horst Bredekamp, who has related the theories of Bildwissenschaft that emerged in Germany to pioneering approaches toward technology and media in the early twentieth century. Around 1900–1933, according to Bredekamp, a new “science of the image” emerged in the German-speaking world with radical ideas about the continuities between different genres of images from advertisements and photography to film and political iconography. The art historian Aby Warburg and scholars he influenced, like Ervin Panofsky and E. H. Gombrich, stood out as “picture-historians,” interested more in recurring visual motifs and their contextualization than in weeding the popular out of art history. Warburg’s unfinished “Mnemosyne Atlas” (which in some ways resembles Benjamin’s Arcades Project) suggested a nonlinear way of understanding the temporal recurrence of images and their relations, raising also the issue of “intermediality” by pointing out motifs that shifted and transformed across what we would now call media platforms. Furthermore, the project suggested a new idea about dynamics of the image, pointing out how images and motifs in themselves could function as “time-machines” in an isomorphic fashion to the task of media archaeology. Another unconventional work that anticipated some of the concerns of media archaeology was André Malraux’s Musée imaginaire (trans. Museum without Walls), published in 1947. Malraux discussed the ways in which mechanical reproduction, in particular photography, was changing our understanding of images and visual culture in general (without referring to Benjamin’s “Work of Art in the Age of Mechanical Reproduction,” 1936). He demonstrated how the unprecedented availability of reproductions was turning the past into an archive, and he challenged observers to draw connections between visual traditions and motifs that had until then been considered unrelated.
The work of Michel Foucault has had a strong impact on media archaeology. An archaeology of his “archaeology of knowledge” would be useful but cannot be developed here. When classifications of media archaeology have been attempted, a binary division has usually been drawn between the socially and culturally oriented Anglo-American studies and the techno-hardware approach of German scholars, who have taken their cue from Friedrich Kittler’s synthesis of Foucault, information theory, media history, and McLuhan’s emphasis on the medium as the message. The German tradition has been claimed to emphasize the role of technology as a primum mobile, which has led to accusations about technological determinism, whereas Anglo-American scholars often assume that technology gets its meanings from preexisting discursive contexts within which it is introduced. One way of explaining this division is to see it as a consequence of different readings of Foucault. The Anglo-American tradition has valorized Foucault as a thinker who emphasized the role of discourses as the loci where knowledge is tied with cultural and social power. Material bodies, events, and institutions are all conditioned by discursive formations. The effects of “hard” technology are considered secondary to immaterial forces that differentiate and mediate their uses. We find quite different readings of Foucault in the German variant of media archaeology, which was strongly influenced by Kittler’s Aufschreibesysteme 1800/1900 (1985), his pathbreaking habilitation thesis that dealt with the impact of technical media on nineteenth-century literature and writing practices. It was followed by Grammophon Film Typewriter (1986), which shared the same basic premises but focused more directly on technical media. Kittler argued for the need to adjust Foucault’s emphasis on the predominance of words and libraries to more media-specific ways of understanding culture. According to him, the problem was that “discourse analysis ignores the fact that the factual condition is no simple methodological example but is in each case a techno-historical event.”  To be able to understand media technologies from the typewriter to the cinema and on to digital networks and coding paradigms, one must take their particular material nature into consideration—an idea Kittler’s followers like Wolfgang Ernst have adopted for their own work. It was probably in this sense that Michael Wetzel purported to combine Foucault and Kittler in his “preliminary considerations for an archaeology of the media,” published in 1989 in a collection of writings that already bore the words “archaeology of the media” in its title.
Although he has often been seen as part of a generation of German humanities scholars determined to steer media theory away from meaning and interpretation, Kittler did not neglect the powerrelated implications of technology. There is a “brand” of German media theory that emphasizes the “epistemic effects of media in the production and processing of knowledge” and “the medial dimensions of the mechanisms of power,” as the editor of a recent issue of Grey Room explained. To prevent the application of simple binary models to his work and his intellectual position, Kittler has denied any affiliation with the notion of media archaeology. His more recent work has returned to a more Heideggerian-inspired excavation of the history of Western culture through music and mathematics. Anglo-American media archaeologists—whether identifying themselves as such or not—have received impulses from the new historicism that emerged in the 1980s. Although it appeared first within literary scholarship, it soon spread to other areas, including history, where it inspired a movement known as the new cultural history. Among other sources, the new historicism was also influenced by Foucault, although his ideas were by no means approved without debate. H. Aram Veeser aptly summarized its “key assumptions” by stating “1) that every expressive act is embedded in a network of material practices; 2) that every act of unmasking, critique, and opposition uses the tools it condemns and risks falling prey to the practices it exposes; 3) that literary and non-literary ‘texts’ circulate inseparably; 4) that no discourse, imaginative or archival, gives access to unchanging truths or expresses inalterable human nature; 5) finally . . . that a critical method and a language adequate to describe culture under capitalism participate in the economy they describe.”  Applied to historical scholarship, the new historicism promoted a self- reflexive and discourse-oriented approach that frequently drew on neighboring disciplines, including the symbolic anthropology of Clifford Geertz, and the rather amorphous field of cultural studies.44 A kind of double focus developed: on the one hand, historians were supposed to immerse themselves in the past, observing it as if by the eyes of the contemporaries; on the other hand, they were supposed to be constantly aware of their observation post in the present, with all the ideological implications it entailed. The research process shifted constantly between the facts of the past, in the process of forming themselves into meaningful constellations, and the subjectivity of the observer. The historical explanation was formulated on/as a dynamic field with multiple determinants that were dynamic rather than static.
Siegfried Zielinski’s version of media archaeology is a practice of resistance, not only against what he perceives as the increasing uniformity of mainstream media culture, but also against media archaeology itself, or rather its assimilation and hardening into the normalcy of contemporary media studies. Considering media archaeology a “method” pinned down into an academic textbook would no doubt be a horror for Zielinski, who also calls his “activity” (Tätigkeit) by other names, such as “anarchaeology” and “variantology,” expressing an uneasiness toward permanent categories and doctrines. For him, media archaeology “in a pragmatic perspective means to dig out secret paths in history, which might help us to find our way into a future.”  This formulation reveals the utopian and Romantic underpinnings of Zielinski’s thought, which is not without—productive?—contradictions. Zielinski’s early work was not yet identified as “media-archaeological.” Zur Geschichte des Videorecorders (On the History of the Video Recorder, 1986) was a dense and detailed exploration of the topic, covering technological, institutional, and economical as well as sociocultural issues. It also contained a special section, “Aspects of the Video Recorder in Pictures,” a kind of visual essay that already pointed toward media-archaeological interests. His next major book, originally published in 1989 and translated as Audiovisions: Cinema and Television as Entr’actes in History, defined itself as an “outline of a history of audiovision” (Entwurf zu Geschichte der Audiovision), or a contribution toward to “an integrated history of the media” (integrierte Mediengeschichte). Drawing on an enormous mass of highly heterogeneous source material, the book demonstrated how distinctions between different audiovisual media were gradually erased during the twentieth century.
Although the theoretical construct behind Audiovisions was more implicit than explicit (as Zielinski himself admitted), he singled out the triad “technology-culture-subject,” identifying each of its elements with a recent intellectual tradition that had influenced him: the British cultural studies represented by Raymond Williams; the German historiography of technology that used a specific systems approach (Günter Ropohl); and the metapsychological cinema theories of Jean-Louis Baudry, Jean-Louis Comolli, and Christian Metz, which emphasized the notion of the cinematic apparatus. Zielinski stated that he did not want to “compete with other models that emphasise more strongly the techno-structure of media processes (like, for example, those of Friedrich Kittler and his pupils),” seeing his own as “supplementary.”  Zielinski’s road from Audiovisions to media archaeology seems logical. His project had taken him to “the end of the history of cinema and television,” where he saw only uniformity and unlimited industrial exploitation, surprisingly much in line with Adorno’s and Horkheimer’s position. “New media” did not provide relief, as their possibilities were mostly used to remediate and perpetuate hegemonic forms. Zielinski began turning toward two seemingly opposite directions that in the end pointed to the same goal, offering to break the psychopathia medialis of modern media culture. On the one hand were radical contemporary artists, who had potential to break the vicious goals of the culture industry; on the other were the hidden treasures of the past that might provide keys for a cultural renewal. Zielinski’s position was influenced by his role as the founding director and later rector of the Academy of Arts and the Media in Cologne, which gave him an opportunity to build connections between media studies and experimental media practices. Zielinski’s next book, translated as Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means (originally published in 2002), plunged into the “deep time” of media, offering a series of studies that were dedicated to the work of personalities who had rarely been associated with media culture before and were written in “a spirit of praise and commendation, not of critique.”  Empedocles, Athanasius Kircher, Cesare Lombroso, and others provided examples of genial individuals who worked out of love and inspiration against the odds of the real world. In Zielinski’s mind at least, they shared a kinship with another set of cultural heroes, contemporary artists working with media, such as Valie Export, David Larcher, Nam June Paik, Steina and Woody Vasulka, and Peter Weibel. Ideologically Zielinski almost seems to nod toward Thomas Carlyle’s Romantic classic On Heroes and Hero Worship and the Heroic in History (1841). He plunges headlong, and with pathos, into the world of his heroes, eschewing critical, skeptical, and theoretically oriented perspectives.
The “new film history” can arguably be seen as a parallel enterprise to media archaeology; both have their origins in the 1980s and are continuing to evolve. Although the profile of the former is far from clear, many of its practitioners sought new insights into the specific nature of cinema by introducing extended cultural, social, and economic contextualization, based on the consultation of varied firsthand source material, and by emphasizing cinema’s intermedial relationships. In a way Zielinski’s Audiovisions also pointed in this direction, but it went beyond the horizon of most film historians by focusing on the interplay between technology, cultural forms, and viewing subjects and by paying little attention to the content of films or television programs; the context and the technological apparatus were given center stage. The need for contextualization and intermediality was expressed well by Thomas Elsaesser in an article entitled “The New Film History as Media Archaeology”: Sound, for instance, since the silent cinema was rarely if ever silent, in which case: why is the history of the phonograph not listed as another tributary? And as we now understand the cinema as part of a multimedia environment, how about the telephone as an indispensable technology? Radio-waves? Electro-magnetic fields? The history of aviation? Do we not need Babbage’s difference engine ranged parallel to his friend Henry Fox-Talbot’s Calotypes or Louis Daguerre’s sensitised copper plates? These questions in themselves show how much our idea—and maybe even our definition—of cinema has changed even without appealing to digitization as a technology, which is nonetheless implicit as a powerful “perspective correction” and thus counts as an impulse in this retrospective re-writing of the past.
Gunning has also published many studies linking early cinema to other media, technological phenomena like ghost photography and X-rays, and institutions of emerging modernity, such as the World’s Fairs. Similarly, in Window Shopping (1993) Anne Friedberg traced the origins of cinema to forms and institutions of the emergent popular and consumer culture of the nineteenth century, creating an approach that clearly raised media-archaeological concerns. In The Virtual Window (2006) she pushed her analysis back by hundreds of years, further estranging it from the cinema studies paradigm. For Elsaesser, one of the challenges is the reevaluation of the connections and gaps between media technologies. The onslaught of digitalization is forcing cinema to rethink both its cultural position and its history. Considering digitality as a rupture provides a conceptual way of seeing media history as a discontinuous enterprise subject to constant reevaluation. Lev Manovich’s The Language of New Media (2001) was a historically tuned version of new media theory that built on cinema studies and film theory. It purported to place the new media “within the history of modern visual and media cultures.”  Manovich pointed out continuities between early avant-garde and animation film practices and the emerging digital culture, based on numerical representation, modularity, automation, variability, and transcoding. Beside film history and theory, he drew on the traditions of the Bildwissenschaft, including the work of Ervin Panofsky. The focus on new media changes the historical meaning and context of cinema from narrative cinema to one flexible enough to lend itself to interactivity, navigability, and digital representation and transmission. Media-historical and theoretical works with a background in film studies pose a challenge for the continued renewal of media archaeology. How does one avoid reducing all other media to a footnote to the history of the moving image? One alternative is the recent influx of archaeologically oriented works concentrating on the audible dimension of culture and history.
4 Emphasizing such heterogeneity is an attempt not so much to deliberately diversify the existing body of mediaarchaeological theory and praxis as to encourage “traveling” between discourses and disciplines. Still, amid all the variety, there is a need to define approaches and perhaps even to crystallize them into “methods,” at least in a local and tactical sense. Erkki Huhtamo’s variant of media archaeology is one such attempt, stemming from an effort to apply the idea of topos, as developed by the German literary scholar Ernst Robert Curtius in his classic Europäische Literatur und lateinisches Mittelalter (1948), to the field of media culture. The topos approach eschews “the new,” which is so often the focus of media-cultural discourses, both critical and popular; instead, it emphasizes the clichéd, the commonplace, and “the tired” (to appropriate jargon from WIRED magazine). Identifying ways in which media culture relies on the already known is just as essential as determining how it embodies and promotes the never before seen. In fact, these two aspects are connected with each other; the new is “dressed up” in formulas that may be hundreds of years old, while the old may provide “molds” for cultural innovations and reorientations. Huhtamo’s approach does not only identify topoi, trace their trajectories, and explore the circumstances of their reappearances. It also purports to demonstrate how topoi are constantly evoked by cultural agents, from spokespeople, sales agents, and politicians to writers, journalists, exhibition curators, and, last but not least, media artists, who use them for various kinds of purposes, from sales pitches and ideological persuasion to aesthetic reflections on media culture and history. This emphasis gives Huhtamo’s approach a culture-critical character. Although the cultural agents themselves may not always acknowledge it, the media-archaeological dimension is an essential element of the contemporary mind-set, constantly bombarded by media and communications. By demonstrating how the media’s past(s) lives on in the present, guiding and informing people’s attitudes in their daily lives, the topos approach helps to detect novelties, innovations, and media-cultural ruptures as well. As Huhtamo had already pointed out in 1996 in his essay “Time Machines in the Gallery: An Archeological Approach in Media Art,” a growing number of artists who are aware of media archaeology get inspiration from its findings and are contributing their own creations and discoveries. This has led to intriguing parallels and connections between research and artistic creativity.
“Media” are not only related to the established institutions of modernity. They are also manifested in the narratives of madmen, religious visions, theories about the psyche and the body, and other recurring issues associated with technological modernity. But the term imaginary media does not refer only to the human imagination as a site for fantastic modes of communication. It can also mean extensions of the notion of “media” in theories of the mind and the brain. Media are in this sense a reservoir for tactics and techniques for manipulating humans and their culture. The chapters in this section demonstrate that media archaeology not only is grounded in Foucault’s methodology but can also find inspiration from psychoanalysis and other early twentieth-century cultural theories, including the German Bildwissenschaften and the topos theory of literature. Erkki Huhtamo’s chapter offers a theoretical-historical contextualization of the topos, a notion he has adopted from the literary scholar Ernst Robert Curtius and has turned into a “tool” for explaining the recurrence of clichés and commonplaces in media culture. Huhtamo has applied the idea to various media forms ranging from “peep media” and the moving panorama to mobile media. In this chapter he delineates his approach theoretically, discussing its predecessors and demonstrating how it can be applied to various facets of media culture. For Huhtamo, the task is “identifying topoi, analyzing their trajectories and transformations, and explaining the cultural ‘logics’ that condition their ‘wanderings’ across time and space.” Topoi are discursive “engines” that mediate themes, forms, and fantasies across cultural traditions. Predictably, they have become a tool in the hands of the culture industry.
Setting out from a Foucauldean position, he mobilizes Siegfried Zielinski’s notions of “variantology,” and “anarchaeology of media.” Through a series of examples, Kluitenberg demonstrates how the notion of imaginary media can be conceived and used to contextualize and enhance contemporary media production. Relying on Zielinski, he claims that excavating the media cultures of the past is important not only in itself but also for the creation of innovative and critical futures. This is one of the key features of many media-archaeological writings: archival findings and reconstructions of the narratives of the past are tied with emerging media practices. Both Jeffrey Sconce’s and Thomas Elsaesser’s chapters submit psychoanalytic themes to a media-archaeological elaboration. Theories of the psyche and the mind become excavation sites that both Sconce and Elsaesser explore to uncover new ways of understanding modern media culture. Sconce focuses on “the influencing machine” as it was conceptualized by the psychoanalyst Victor Tausk in the 1930s. Rather than considering it as a schizophrenic delusion, Sconce sees it as a projection of broadcast media, implicitly associated with mind control. The “delusions” he discusses are not related to any specific technology; rather, they testify to “a vernacular theory of power forged at the end of the nineteenth century.” In his chapter Elsaesser analyzes Sigmund Freud’s writings, providing insights to the media archaeological conceptualization of memory. He shows that Freud’s analyses of the psyche can be taken as media theories in their own right—as ways of understanding not only perception, but issues like storage and processing as well. The psyche itself is a kind of a media machine. Elsaesser's media archaeological reading of Freud’s classic text “Notes on the Mystic Writing Pad” opens an alternative path to understanding early psychoanalytic theories in the context of the emerging media culture. It shows that Freud can also be applied to a media archaeology of images in the era of digital data. Elsaesser argues that “considering Freud as theorist of auxiliary memory and the technical media, and thus as a media theorist, shifts this perspective away from film to a more general consideration of the technical media".
Some time ago, while I was leafing through an airline magazine, an ad caught my attention. It was associated with a current event—the Winter Olympics in Turin—and promoted Samsung mobile phones.1 The setting was a deserted, dimly lit office after hours. Unusual mini-Olympics were taking place on a desk in the foreground. Between a frozen laptop computer and a frost-covered coffee cup, Lilliputian sportspeople—a figure skater, skiers, ice hockey and curling players—were in full action. The center of everything was a snow-covered mountain capped by a Samsung phone. The mountain continued into the virtual realm of its screen, where a takeoff ramp built on its slope could be seen; a jumper was midair, shooting down from the ramp—and out from the screen. It immediately occurred to me that I had encountered such tiny people before. Examples abound: in a cartoon published in the New Yorker in 1959 a cleaning lady is seen mopping the floor after hours next to a huge mainframe computer.2 She turns her head and sees a tiny man leaving through a doorway on the side of the enormous computer, clutching his suitcase and raising his hat in a mechanical greeting. I also came to think about Little Computer People, a computer game for the Commodore 64 (1987). In its promotional campaign, Activision pretended they had found little people living inside computers. The idea of the product was to persuade one of them to live in a “House-on-a-Cassette,” a software application where the person’s routines could be observed and interacted with.
Going back even further, we discover that little people had already appeared in the context of phonograph and gramophone advertising. Elsewhere, we discover them in early trick films and in the delightfully impish video installations by the French media artist Pierrick Sorin. We could also fly back through the tunnel of time to the realms of fairies, gnomes, and Lilliputians. “The little people” is a topos—a stereotypical formula evoked over and over again in different guises and for varying purposes. Such topoi accompany and influence the development of media culture. Cultural desires are expressed by being embedded them within topoi. Functioning as shells or vessels derived from the memory banks of tradition, topoi mold the meaning(s) of cultural objects. High technology can be represented as something else through application of the “fairy engines” of topos traditions. They can disguise culture as nature, and something unheard-of as something familiar. As Peter Burke has written, “The facade of tradition may mask innovation.” As discursive meaning processors, topoi not only express beliefs but can serve rhetorical and persuasive goals, as evidenced in the field of advertising. New products are promoted by being packaged into formulas that are meant to strike the observer as novel, although they have been put together from ingredients retrieved from cultural archives. Burke’s dictum can therefore also be reversed: the facade of innovation may mask tradition, and apparent ruptures disguise hidden continuities. Identifying topoi, analyzing their trajectories and transformations, and explaining the cultural logics that condition their “wanderings” across time and space is one possible goal for media archaeology. In this chapter I will lay a theoretical and historiographical foundation for this type of approach. I have already applied it in a more strategic sense in a number of studies. Media archaeology means for me a critical practice that excavates media-cultural evidence for clues about neglected, misrepresented, and/or suppressed aspects of both media’s past(s) and their present and tries to bring these into a conversation with each other. It purports to unearth traces of lost media-cultural phenomena and agendas and to illuminate ideological mechanisms behind them. It also emphasizes the multiplicity of historical narratives and highlights their constructed and ideologically determined nature. I will begin by discussing the origins of Toposforschung in the work of Ernst Robert Curtius and assessing some of its influences, including C. G. Jung’s theory of archetypes and Aby Warburg’s iconological approach to the study of visual culture. I will then discuss the relevance of Curtius’s ideas for the study of media culture and will propose modifications in the light of more recent applications of the notion of the topos. I will proceed to exemplify roles the topoi serve within media culture and, finally, will discuss the potential advantages and problems of the approach delineated in this chapter.
Although he claimed he was practicing “philological microscopy,” Curtius’s approach was expansive. He compared it to aerial photography, which had helped archaeology to discover massive land structures invisible from the ground level; after the discovery, the photograph had to be enlarged and the details investigated. Curtius condensed his approach into a maxim: “Specialization without universalism is blind. Universalism without specialization is inane.”  Universalism as he understood it applied only to literary traditions, which he treated as an independent realm. Although he described in meticulous detail how specific topoi had been handed down from writer to writer, he did not perceive essential changes in their meanings—only changes in their style. In spite of making occasional context-driven statements such as “In messianically and apocalyptically excited periods, faded symbolic figures can be filled with new life, like shades which have drunk blood,” Curtius did not really explain topoi by relating them to factors that could be considered external to the literary tradition. Had he attempted to analyze topoi as symptomatic of the times and places in which they were evoked, Curtius might have come to the conclusion that their appearances marked not only continuities but cultural ruptures and discontinuities as well. This has been emphasized by later topos scholars. For Ernst Ulrich Grosse, differences in the ways a certain topos has been used are more interesting than similarities because they have potential to reveal changes and historical turning points. For him, the appearance of a topos is potentially conditioned by several factors: the immediate environment as well as the will of the author; the evolution of literary genres as well as the history of mentalities. When a topos emerges, it should be treated as a node in a complex network of references and determinants. Topos study is steeped in the issue of cultural contextualization, no matter how difficult and elusive it may be.
Such “dressing up in technology” is another topos that manifested itself both as imaginary projections and in the new realities of working life. Women working as telephone switchboard operators were “dressed up” in their headphones all day long and, indeed, could be argued to have became cyborgs of sorts as well. An intriguing manifestation of such developments is the famous artwork Electric Dress created by the radical Japanese Gutai artist Atsuko Tanaka for her performances in the 1950s. Whether Tanaka was aware of the topos tradition anticipating her work is uncertain but unlikely. One could also refer to public demonstrations of smart fabrics and cyberfashion in recent years. Yet another nineteenth-century example is a topos that could be labeled “What is happening behind your back?”  In its most typical manifestation a man (often an officer) is represented kissing a girl left unguarded when her mother is peeping into a peep-show box. This topos jumped from device to device (peep show, magic lantern, telescope, kaleidoscope, Daguerreotype camera, etc.), justifying Peter Burke’s call to “focus on the displacement or migration of a given schema or stereotype from one object to another.”  By midcentury the man stealing the kiss had been transformed from a reckless officer seeking amorous encounters into a door-to-door salesman marketing stereoscopic photographs, acting when the unsuspecting husband is peering at his sample cards. In another variant of the same topos, already known around 1800, the man peeping into a peep-show box falls prey to a pickpocket, who is actually represented as a tax official stealing money from the people to fund the government’s war efforts; media like broadcast television are still used much the same way, not only by governments but also by groups like televangelists. The same topos reappeared in the context of another peeping device, the mutoscope, a hundred years later, but here the political has been replaced by the homoerotic. In a French cartoon (1910) a male peeping into the mutoscope experiences the pickpocket’s touches as erotic; the positions of the male bodies suggest anal intercourse. Exactly the same variant reappeared in a series of video installations by Pierrick Sorin. The similarities make one wonder whether Sorin actually knew the topos or whether he grasped the idea spontaneously. Topoi that have been inspired by media spectacles may remain just stereotypical metaphors, but their recurrence may also point to wider concerns and cultural patterns. Although the topos “What is happening behind your back?” usually appears in comic episodes and may be dismissed just as a joke, it could also be read as a repeated warning about the risks of excessive media use. Too much immersion may upset one’s social relations and disturb one’s relationship to one’s immediate physical surroundings, it seems to say. It functions, then, as a kind of discursive buffer softening the shocks caused by encounters with new media and the mediated environment.
The phantasmagoria, the kaleidoscope, dissolving views, the diorama, the moving panorama, and many other media-cultural phenomena have given rise to topoi. Indeed, Curtius himself resorted to them when he wrote, “Not until late Antiquity is color in demand again—and then it is the color of a kaleidoscope.”  And again: “New and ever new figures rise kaleidoscopically—rhetorical bravura pieces whose exuberance pours out like a cascade.”  Many “mediacultural” topoi keep on appearing long after the thing they originally designated has materially disappeared. Moving panorama originally referred to a popular nineteenth-century spectacle. As a topos, it came to manifest perceptual experiences, inner visions (“life passing before the mind’s eye like a moving panorama at the moment of death”), religious revelations (God revealing his plans as a moving panorama in the sky), celestial mechanics, and many other purposes. In Elvis’ Search for God (1998), Jess Stearns still described how Elvis “felt a surge of emotion, as the moving panorama of the gilded years floated by for a moment.”  Although, as Curtius said, “the topos can be employed in any context,” its manifestations are affected by the specific nature of that context. Media-related ideas may also traverse culture as “imaginary media” long before they materialize as artifacts. “Seeing at a distance” is such a topos. We encounter it already in discourses about magic mirrors, and in an intensified form in technocultural fantasies about the telectroscope, an imaginary anticipation of the television. One the other hand, as Geoffrey Batchen has demonstrated, the desire to photograph seems to have emerged rather suddenly in the late eighteenth century, preceded by very few immediate fantasies. However, once photography made its breakthrough, the discursive floodgates opened and the desire to photograph was translated into an intense topos activity.
The use of topoi in promotional strategies exploits their attraction value but also their “nonattraction” value. Topoi are used to arrest the eye in accordance with the long traditions of the culture of attractions. They provide a striking sight or textual formula that intrigues the observer. In commercial media culture their character is instrumental: they have been recruited to provide a product or spectacle with a certain historical or cultural surplus value acknowledged by the observer. Obviously the topos can have an effect by its invisibility, by its unremarkable and commonplace character. In such cases it is used to provide a mold for content that pretends to be something unprecedented, cut off from the past. In either case topoi provide advertisers with tried-and-tested formulas that are used to introduce new consumer products by embedding them within molds the customers already know (whether they are aware of it or not). There seems to be a paradox here: the newest of the new is packaged in the oldest of the old. The most obvious example that comes to mind is from the marketing strategies of screen technology. No matter how “revolutionary” the product may be, advertisements show us, over and over again, humans or objects breaking through the screen, in either direction. The manifest features of such ads are of course constantly updated in accordance with fashions and stylistic trends, but underneath we detect an ancient topos associated with the history of illusionistic representation. Figures have been stepping in and out of paintings for millennia; they are still performing stunts on today’s flat-panel plasma screens. The recurrence of the “traversing the screen” topos may be motivated by the need to fight consumer resistance toward the new, although in a postmodern culture that readily embraces high technology it may have more to do with appealing to consumers’ love of pastiche, nostalgia, and cultural sound bites. Such issues also occupy industrial design, preoccupied with finding the right ratio between futuristic and retro styles (retro-futuristic is one option). Early TV sets were often enclosed in wooden cabinets that made them look like a traditional piece of furniture, while Jonathan Ive’s original iMac (1999) displayed its “inner organs” through its acrylic plastic case. Apple’s advertisements tried to make most out of it, displaying the computer from the side rather than from the front, as had nearly always been the case. The campaign broke with the “breaking through the screen” tradition, but it may well have reactivated the topos of the little people, at least in some consumers’ minds: the inside of the iMac was made to look like a miniature world that could well have been inhabited by little computer people. The case of the little people demonstrates that cultural subjects do not consume representations provided by the industry as such. Users developed and disseminated their own beliefs and even tested them in action, pushing food through holes inside TV sets, stalking behind their sets to see the little people leave their workplace, or even attempting to break the screen physically to free them.

Language, with its different recognizable sounds, has opened up to humanity the possibility of asking questions, telling stories, and participating in dialogue. Language has allowed the emergence of unknown entities in animal societies: numbers, gods, laws, works of art, calendars, the technological adventure, and the entire cultural universe. I will designate here by the term “ideas” those complex forms that appear, reproduce themselves, and evolve only in the world of culture, in the space of signification opened up by language. Language has allowed human communities, as compared to hives, herds and packs, to make a leap of collective intelligence because language creates a stronger and more supple link of competitive cooperation than what insects or monkeys have in their respective communities. In putting the idea at the center of my model, I have chosen an approach to human collective intelligence that radically distinguishes it from the collective intelligence of other animal societies. From this perspective, language represents the limit or threshold beyond which ecosystems of ideas are constituted. These are like spiritual hypertexts living in symbiosis with the societies of talking primates we humans form. These ecosystems of ideas grow more complex, die out, diversify, or combine in such a way as to lead those societies that cultivate them down, the in part undetermined, road of cultural evolution. Teilhard de Chardin coined the term “Noosphere” to describe the world ecosystem of all the ideas which globalization and the development of the means of communication, whose culmination we see in cyberspace, have only begun to put at our fingertips. Human communities can survive only by maintaining cultures, that is, semi-closed collective intelligences conducive to the breeding (reproduction and selection) of ideas. An ethical person, a corporation, an institution, a nation, a religion, a political party, a science, a virtual community or a tribe cultivates—nolens volens—ecosystems of ideas. In the course of its existence, a culture explores a viable evolutionary direction for its ideas. Our mental representations give ideas their forms. In a way, representations are the face, or the mask, of ideas. Representations can be of all kinds; their variety is in theory unlimited : e.g. the images of our perceptions, such as those created by human works of art, music, symbols, and the structures of highly complex relations that have been constructed by means of language and many other sign systems. Our intentions are the souls of ideas: the movement which animates the face. Intentions direct the mental representations toward particular destinations, or particular targets. They entrust ideas with a goal, which can be quite near, or even very far away, aiming at almost inaccessible horizons. We can think of intentions as the abstract structure of emotions, in other words, as vectors endowed with a force (intensity) and a direction (the “nature” of the emotion). Representations must be distinguished from emotions because the same representation, depending on the circumstances, can serve as a face for very different emotions.
Internet, cell phones, videophones, virtual reality devices, computers, modems, data banks: the communications industry is taking over. Productivity, utility, management are the watchwords. But are we masters or servants of our devices?

Information highways, despite the hype, are no more than a combination of telephone, video and computers, as are the gadgets telecom engineers devise for the future. They are all really just cell phones plugged into the internet, linking with fixed stations, videophones, virtual reality devices, computers, modems, data banks, tradesmen, managers, repairmen. All are logged in to technical, social or professional networks.

There are no black holes, no negatives, no opposites in this joined-up world. Everything runs smoothly in electronic silence. Even the occasional audible signal can be turned off. Communication is easy when all it involves is using machines to connect with machines. If you want to stop communicating, to enjoy your own company, all you have to do is put down the handset and switch off the screen.

Anyone using such machines is free and happy in a world of instant communication, without time to think ill of himself or of anyone else. The long term doesn’t exist in this world, only short-term gains. The communications industry dreams of productivity, utility and management, the watchwords of homo communicans.

Homo communicans is inseparable from his communication devices. They are his life and he takes on their characteristics. He is their servant as much as their master, but he is unaware of his chains because he believes himself to be in control and as powerful as the machines. They make his life easy. Everything is positive, everything in its place. There is no price to pay and no other side of the coin in what Douglas Coupland, in his novel Microserfs (1), calls the “flatland of cyberculture”. Cyberculture uses the language of clans and communities but it does not share their reality, for to join the clan, you have to make sacrifices. And in cyber flatland the only thing to fear is the loss of the machines: breakdown.

When I asked Martin Landau, an eminent researcher in organisation science at Berkeley, California, about theories of communication, he replied: “Do you know why the 747 is the safest aircraft in the world? Because it has four separate control systems, one for each engine. And the pilot also has a manual control system separate from those four” (2). It seemed a strange answer. In aviation, breakdown means death. But breakdown in communication, loss of relationships or position, is also a kind of death, the end of homo communicans, who lives by communication and is defined only by the links his machines give him to other machine-bound human beings. Think how people panic when their computer or television breaks down or their phone is out of order. The gap in their lives causes real distress. These machines have become part of us; we have become part of them. When they break down it is like being in pain, the fear of it is a nightmare.

That is the system’s only contradiction, the only conceivable misfortune. The fear of breakdown has replaced the old apocalyptic fear of the devil, and it is only the threat of that breakdown which gives life and feeling to a system that has none. This is the communication system’s last vital opportunity. In his book Anatomy of Criticism (3), Northrop Frye shows that the Apocalypse is a text that advocate union between the city, the individual and God. And just as fear of the Apocalypse exists to serve the Christian faith, so fear of breakdown exists to consolidate the cult of the computer.

But homo communicans, ignorant of the sacrifices of communication, does not know this. He thinks he is always on the winning side, not knowing that in order to win you also have to lose. He doesn’t know what he is losing.

Machines are created for productivity and efficiency, and have unexpected consequences: they make men into idle and superfluous creatures who no longer do much on their own initiative. Men are assisted in everything, even getting to work, since with their self-guided cars they can dream at the steering wheel until the device tells them they have arrived. The old hauliers’ slogan “driving for you” now applies to us all. As technology continues to develop, homo communicans faces a future of total, profound idleness. And the first signs of this are already visible in our daily lives.

We let machines remember things for us, from our address books complete with telephone numbers and email addresses to the management of bibliographies, texts, business meetings, accounts, planning. Our voice, or better still a synthetic one, answers for us, recorded once for all time. We open doors and change channels on the TV remotely. We are not far from spending our entire lives in a semi-dormant state.

Our listlessness is encouraged by the sense of security we derive from all the surveillance devices that surround us. Idleness goes hand in hand with freedom from fear, a sense of comfort, of being safe, warm and protected. With sophisticated devices to watch over us, there are no enemies to worry about. Voice and face recognition, digital fingerprints and cameras with access codes free us from fear of intruders.

If they do not need to defend themselves, people’s existence begins to seem pointless. As if they are present by accident and might as well not exist. Machines do human work to perfection, while people are clumsy and hesitant and make mistakes, trying falteringly to follow a pattern. The idea of the human brain as the poor relation of the all-powerful computer has lead to a sense of powerlessness and futility. Our memories have grown unretentive but we don’t care; the business of remembering is being safely managed.

Contrary to common belief, homo communicans of the future will not suffer from pressure and stress. Why should he? His mistakes will be corrected by machines. Society with its faults of inequality, poverty, war and death, will be corrected by technology. This world where communication is all will not be fast-moving, but slow, inactive, contemplative, full of play. Not the slowness advocated by Pierre Sansot (4), the slowness of taking time out to enjoy life and savour the pleasures of fruit, fresh air and dreams, but an enforced slowness, reassuring and with no place for expectancy or surprise.

Homo communicans is good at dreams and contemplation, and his dreams will probably generate new ideas for communicating machines, more invention and innovation. That may be the real work assigned to human beings in the future of communication since the rest, engineering and production, will be done by machines.

Plato warned young philosophers against books, which he likened to dead memories that replaced living ones on the pretext of being more convenient. He said that writing and books make for idleness, making the reader passive. His advice seems archaic (today’s educators would give anything to get people to read). But although the medium has changed, Plato’s message has not. We are still handing our obligations over to an external device. From Plato’s viewpoint, the idleness resulting from freedom from work done by machines idleness would be evidence of enslavement unworthy of human beings.

Suppose two brains are in the same conscious state. Are there two minds, two streams of conscious experiences? Or only one? From a physical point of view, there is no puzzle. There are two numerically distinct lumps of matter that instantiate the same patterns and undergo qualitatively identical processes. The question is how we should ascribe mental properties to this material configuration. Even if we assume that the mental supervenes on the physical, we still need to determine whether the supervenience relation is such that two qualitatively identical physical systems ground a single experience or two numerically distinct (albeit subjectively indistinguishable) experiences.
The issue is not about personal identity. It is a separate question whether there would be one or two persons. One might hold, for example, that one person could have two subjectively indistinguishable experiences at the same time, or that two persons could literally share what is, numerically and not just qualitatively, one experience. These issues about personal identity will not be discussed here. My concern, rather, is about ‘‘qualia identity.’’ I will start by considering the numerical identity or non-identity of the phenomenal experiences that arise when brains exist in duplicates. The bulk of the paper will then examine some intriguing cases involving partial brain-duplication and the questions of degrees and of quantity of experience that these cases force us to confront. Consider the case where two brains are in identical physical states. The brains have, let us assume, the same number of neurons, which are connected and activated in the same way, and the brains are descriptively identical all the way down to the level of individual molecules and atoms. Suppose, furthermore, that for each of these brains there would, if the other brain did not exist, supervene a particular phenomenal experience. Given the supervenience assumption, the phenomenal experience that would supervene on one of these brains would be qualitatively identical to the experience that would supervene on the other. But if both brains exist, are there two qualitatively identical but numerically distinct experiences (one for each brain) or is there only a single experience with a redundantly duplicated supervenience base? A hardcore physicalist might be tempted to dismiss this question as being merely terminological. However, I believe that we can give content to the question by linking it to significant ethical and epistemological issues. Given such a linkage, the answer will not be an inconsequential terminological stipulation but will reflect substantial views on these associated issues. Let Duplication be the thesis that there would be two numerically distinct streams of experience when a conscious brain exists in duplicate. The content of Duplication, I suggest, might in part be constituted by its implications for epistemology and ethics. Consider first the ethical significance of Duplication. It could matter ethically whether Duplication is true. Suppose that somebody is contemplating whether to make a copy of a brain that is in a state of severe pain. If the quantity of painful experience would not thereby be increased, it seems that there would be no moral objection to this. By contrast, if creating the copy will lead to an additional severely painful experience, there is a strong moral reason not to do it. In such cases, it would be an extremely important practical matter whether Duplication is true or false. We could not resolve it by appealing to an arbitrary verbal convention. This ethical implication is a reason to accept Duplication and to reject its negation (Unification). Unification implies that we would not bring about pain if we created copy of a brain in a painful state, or changed an existing brain into a state that is qualitatively identical to a painful state of an already existing brain. At least on hedonistic grounds, there would be no moral reason not to create the copy. Yet it is prima facie implausible and farfetched to maintain that the wrongness of torturing somebody would be somehow ameliorated or annulled if there happens to exist somewhere an exact copy of that person’s resulting brain-state.
From this point onward, it will serve clarity and convenience to assume a weak form of computationalism, implying that a sufficiently powerful digital computer, running a suitable (very complex) program, would in fact have phenomenal experiences. (We do not need to assume that this would be analytically or metaphysically necessary.) Given this simplifying assumption, we can imagine the case of interest as resulting from running the same mind-program on two different computers. We can simplify matters further by supposing that the simulated minds live in and interact with identical computer-simulated virtual realities. Under these conditions, two identical mind-and-virtual-reality programs, starting from the same initial conditions, will evolve in exact lockstep. The brain-simulation and the virtual-reality simulation are both parts of a more comprehensive program, and when this program is run on two identical (deterministic) computers, they will go through an identical sequence of state-transitions.
This possibility undercuts Chalmers’ argument for the principle of organizational invariance by offering a more plausible account of how Joe’s qualia could gradually fade when he undergoes the neural replacement process. If the fading took place in this way, Joe would not be strangely failing to notice any qualitative changes in his experiences, because there would be no such changes. Of course, this point does not show that the principle of organizational invariance is false; it merely undermines one argument in its favor. The principle might well be plausible other grounds. It might be tempting to think that the possibility of fractional minds could be demonstrated by considering more mundane examples featuring gradations of consciousness, such as infant development, consciousness in animals at different levels of cognitive sophistication, the humanoid ancestors of homo sapiens, or the gradual loss of consciousness that occurs when we drift into dreamless sleep or anesthesia.15 However, these cases are complicated. They certainly involve changes in the descriptive quality of experience, and these qualitative changes, too, can form a continuum. Starting with a fully awake normal human stream of consciousness, we could reach a state of unconsciousness by various possible sequences of small steps, in which our awareness gradually becomes more fragmented, the fragments become briefer, scarcer, and more diffuse until the stream completely dries out. These cases might also involve a gradual change in the quantity of particular qualitative experiences, in the sense relevant here; but it is not obvious that they do so. In the thought experiments presented in preceding sections, we carefully controlled for the confounding variable of qualitative change in order to focus on the fundamental question of quantitative change.
Suppose the implementation of a certain program gives rise to a phenomenal experience. I began by considering the question of whether two implementations of this program give rise to two qualitatively identical but numerically distinct experiences. The Duplication thesis answers this question in the affirmative. That Duplication is a substantial claim can be seen from the fact that it is has important ethical and epistemological implications. I defended Duplication by arguing that its implications in these areas are much more plausible than the implications of its negation, Unification. Moreover, our direct ontological intuitions about the matter seem to support Duplication.
The matter of degree here resides in the number and size of the fragments existing in duplicate. In other cases, however, the entire supervening phenomenal experience is simultaneously duplicated, and the matter of degree resides in the total quantity of qualitatively identical experience. Intriguingly, this quantity can be a fractional number. This casts some new light on what it is to implement a computation. The idea of a fractional quantity of qualia may be puzzling. By considering the case of a single computer built with unreliable elements, it can be shown that this possibility of fractional qualia would have to be confronted even if Duplication were rejected. Hence it is not a reason to reject Duplication. One may still wonder about the notion of purely quantitative variation of qualia. A quale is supposed to be a subjective phenomenal appearance—what something feels like, ‘‘from the inside.’’ But in the central cases I described, the subject is not supposed to register any qualitative changes in her experience. What is it then that changes when the quantity of, say, a particular pain quale decreases from 1 unit to 0.3 units? If the pain feels just the same, in what sense is there less pain after the change? Here is my answer: There is less pain in precisely the same sense as there is less pain if now only one subject experiences a particular pain quale while before two subjects each experienced just such a pain quale. The nature of fractional quantitative change is the same as the nature of quantitative change when it occurs in more familiar integer increments. (If we wish to speak of ‘‘subjects of experience,’’ perhaps we should also say that such subjects can likewise come in fractional degrees, not only in integer increments.) Finally, I considered the relation between the cases described in this paper and the Fading Qualia thought experiment, which Chalmers used as part of an argument for the principle of organizational invariance. The possibility of qualia fading quantitatively without any change in its descriptive, qualitative character undermines Chalmers’ argument.
Artificial intelligence is a possibility that should not be ignored in any serious thinking about the future, and it raises many profound issues for ethics and public policy that philosophers ought to start thinking about. This article outlines the case for thinking that human-level machine intelligence might well appear within the next half century. It then explains four immediate consequences of such a development, and argues that machine intelligence would have a revolutionary impact on a wide range of the social, political, economic, commercial, technological, scientific and environmental issues that humanity will face over the coming decades.

The annals of artificial intelligence are littered with broken promises. Half a century after the first electric computer, we still have nothing that even resembles an intelligent machine, if by ‘intelligent’ we mean possessing the kind of general-purpose smartness that we humans pride ourselves of. Maybe we will never manage to build real artificial intelligence. The problem could be too difficult for human brains ever to solve. Those who find the prospect of machines surpassing us in general intellectual abilities threatening may even hope that is the case.

However, neither the fact that machine intelligence would be scary nor the fact that some past predictions were wrong is a good ground for concluding that artificial intelligence will never be created. Indeed, to assume that artificial intelligence is impossible or will take thousands of years to develop seems at least as unwarranted as to make the opposite assumption. At a minimum, we must acknowledge that any scenario about what the world will be like in 2050 that simply postulates the absence human-level artificial intelligence is making a big assumption that could well turn out to be false.

It is therefore important to consider the alternative possibility, that intelligent machines will be built within fifty years. In the past year or two, there have been several books and articles published by leading researchers in artificial intelligence and robotics that argue for precisely that projection. This essay will first outline some of the reasons for this, and then discuss some of the consequences of human-level artificial intelligence.

We can get a grasp of the issue by considering the three things that are needed for an effective artificial intelligence. These are: hardware, software, and input/output mechanisms.

The requisite input/output technology already exists. We have video cameras, speakers, robotic arms etc. that provide a rich variety of ways for a computer to interact with its environment. So this part is trivial.

It is only relatively recently that we have begun to understand the computational mechanisms of biological brains. Computational neuroscience is only about fifteen years old as an active research discipline. In this short time, substantial progress has been made. We are beginning to understand early sensory processing. There are reasonably good computational models of primary visual cortex, and we are working our way up to the higher stages of visual cognition. We are uncovering what the basic learning algorithms are that govern how the strengths of synapses are modified by experience. The general architecture of our neuronal networks is being mapped out as we learn more about the interconnectivity between neurones and how different cortical areas project onto to one another. While we are still far from understanding higher-level thinking, we are beginning to figure out how the individual components work and how they are hooked up.

Assuming continuing rapid progress in neuroscience, we can envision learning enough about the lower-level processes and the overall architecture to begin to implement the same paradigms in computer simulations. Today, such simulations are limited to relatively small assemblies of neurones. There is a silicon retina and a silicon cochlea that do the same things as their biological counterparts. Simulating a whole brain will of course require enormous computing power; but as we saw, that capacity will be available within a couple of decades.

The product of this biology-inspired method will not be an explicitly coded mature artificial intelligence. (That is what the so-called classical school of artificial intelligence unsuccessfully tried to do.) Rather, it will be system that has the same ability as a toddler to learn from experience and to be educated. The system will need to be taught in order to attain the abilities of adult humans. But there is no reason why the computational algorithms that our biological brains use would not work equally well when implemented in silicon hardware.

An artificial intelligence is based on software, and it can therefore be copied as easily as any other computer program. Apart from hardware requirements, the marginal cost of creating an additional artificial intelligence after you have built the first one is close to zero. Artificial minds could therefore quickly come to exist in great numbers, amplifying the impact of the initial breakthrough.
There is a temptation to stop the analysis at the point where human-level machine intelligence appears, since that by itself is quite a dramatic development. But doing so is to miss an essential point that makes artificial intelligence a truly revolutionary prospect, namely, that it can be expected to lead to the creation of machines with intellectual abilities that vastly surpass those of any human. We can predict with great confidence that this second step will follow, although the time-scale is again somewhat uncertain. If Moore's law continues to hold in this era, the speed of artificial intelligences will double at least every two years. Within fourteen years after human-level artificial intelligence is reached, there could be machines that think more than a hundred times more rapidly than humans do. In reality, progress could be even more rapid than that, because there would likely be parallel improvements in the efficiency of the software that these machines use. The interval during which the machines and humans are roughly matched will likely be brief. Shortly thereafter, humans will be unable to compete intellectually with artificial minds.

Artificial intelligence is a true general-purpose technology. It enables applications in a very wide range of other fields. In particular, scientific and technological research (as well as philosophical thinking) will be done more effectively when conducted by machines that are cleverer than humans. One can therefore expect that overall technological progress will be rapid.

Machine intelligences may devote their abilities to designing the next generation of machine intelligence. This next generation will be even smarter and might be able to design their successors in even shorter time. Some authors have speculated that this positive feedback loop will lead to a "singularity" - a point where technological progress becomes so rapid that genuine superintelligence, with abilities unfathomable to mere humans, is attained within a short time span. However, it may turn out that there are diminishing returns in artificial intelligence research when some point is reached. Maybe once the low-hanging fruits have been picked, it gets harder and harder to make further improvement. There seems to be no clear way of predicting which way it will go.
It would be a mistake to conceptualise machine intelligence as a mere tool. Although it may be possible to build special-purpose artificial intelligence that could only think about some restricted set of problems, we are considering here a scenario in which machines with general-purpose intelligence are created. Such machines would be capable of independent initiative and of making their own plans. Such artificial intellects are perhaps more appropriately viewed as persons than machines. In economics lingo, they might come to be classified not as capital but as labour. If we can control the motivations of the artificial intellects that we design, they could come to constitute a class of highly capable "slaves" (although that term might be misleading if the machines don't want to do anything other than serve the people who built or commissioned them). The ethical and political debates surrounding these issues will likely become intense as the prospect of artificial intelligence draws closer.

Transhumanism is a loosely defined movement that has developed gradually over the past two decades. It promotes an interdisciplinary approach to understanding and evaluating the opportunities for enhancing the human condition and the human organism opened up by the advancement of technology. Attention is given to both present technologies, like genetic engineering and information technology, and anticipated future ones, such as molecular nanotechnology and artificial intelligence.1

The enhancement options being discussed include radical extension of human health-span, eradication of disease, elimination of unnecessary suffering, and augmentation of human intellectual, physical, and emotional capacities.2 Other transhumanist themes include space colonization and the possibility of creating superintelligent machines, along with other potential developments that could profoundly alter the human condition. The ambit is not limited to gadgets and medicine, but encompasses also economic, social, institutional designs, cultural development, and psychological skills and techniques.

Transhumanists view human nature as a work-in-progress, a half-baked beginning that we can learn to remold in desirable ways. Current humanity need not be the endpoint of evolution. Transhumanists hope that by responsible use of science, technology, and other rational means we shall eventually manage to become post-human, beings with vastly greater capacities than present human beings have.

Some transhumanists take active steps to increase the probability that they personally will survive long enough to become post-human, for example by choosing a healthy lifestyle or by making provisions for having themselves cryonically suspended in case of de-animation.3 In contrast to many other ethical outlooks, which in practice often reflect a reactionary attitude to new technologies, the transhumanist view is guided by an evolving vision to take a more active approach to technology policy. This vision, in broad strokes, is to create the opportunity to live much longer and healthier lives, to enhance our memory and other intellectual faculties, to refine our emotional experiences and increase our subjective sense of well-being, and generally to achieve a greater degree of control over our own lives. This affirmation of human potential is offered as an alternative to customary injunctions against playing God, messing with nature, tampering with our human essence, or displaying punishable hubris.

Transhumanism does not entail technological optimism. While future technological capabilities carry immense potential for beneficial deployments, they also could be misused to cause enormous harm, ranging all the way to the extreme possibility of intelligent life becoming extinct. Other potential negative outcomes include widening social inequalities or a gradual erosion of the hard-to-quantify assets that we care deeply about but tend to neglect in our daily struggle for material gain, such as meaningful human relationships and ecological diversity. Such risks must be taken very seriously, as thoughtful transhumanists fully acknowledge.4

Transhumanism has roots in secular humanist thinking, yet is more radical in that it promotes not only traditional means of improving human nature, such as education and cultural refinement, but also direct application of medicine and technology to overcome some of our basic biological limits.

The range of thoughts, feelings, experiences, and activities that are accessible to human organisms presumably constitute only a tiny part of what is possible. There is no reason to think that the human mode of being is any more free of limitations imposed by our biological nature than are the modes of being of other animals. Just as chimpanzees lack the brainpower to understand what it is like to be human, so too do we lack the practical ability to form a realistic intuitive understanding of what it would be like to be post-human.

This point is distinct from any principled claims about impossibility. We need not assert that post-humans would not be Turing computable or that their concepts could not be expressed by any finite sentences in human language.  The impossibility is more like the impossibility for us to visualize a twenty-dimensional hypersphere or to read, with perfect recollection and understanding, every book in the Library of Congress. Our own current mode of being, therefore, spans but a minute subspace of what is possible or permitted by the physical constraints of the universe. It is not farfetched to suppose that there are parts of this larger space that represent extremely valuable ways of living, feeling, and thinking.

We can conceive of aesthetic and contemplative pleasures whose blissfulness vastly exceeds what any human being has yet experienced. We can imagine beings that reach a much greater level of personal development and maturity than current human beings do, because they have the opportunity to live for hundreds or thousands of years with full bodily and psychic vigor. We can conceive of beings that are much smarter than us, that can read books in seconds, that are much more brilliant philosophers than we are, that can create artworks, which, even if we could understand them only on the most superficial level, would strike us as wonderful masterpieces. We can imagine love that is stronger, purer, and more secure than any human being has yet harbored. Our everyday intuitions about values are constrained by the narrowness of our experience and the limitations of our powers of imagination. We should leave room in our thinking for the possibility that as we develop greater capacities, we shall come to discover values that will strike us as being of a far higher order than those we can realize as un-enhanced biological humans beings.

The conjecture that there are greater values than we can currently fathom does not imply that values are not defined in terms of our current dispositions. Take, for example, a dispositional theory of value such as the one described by David Lewis.5 According to Lewis’s theory, something is a value for you if and only if you would want to want it if you were perfectly acquainted with it and you were thinking and deliberating as clearly as possible about it. On this view, there may be values that we do not currently want, and that we do not even currently want to want, because we may not be perfectly acquainted with them or because we are not ideal deliberators. Some values pertaining to certain forms of post-human existence may well be of this sort; they may be values for us now, and they may be so in virtue of our current dispositions, and yet we may not be able to fully appreciate them with our current limited deliberative capacities and our lack of the receptive faculties required for full acquaintance with them. This point is important because it shows that the transhumanist view that we ought to explore the realm of post-human values does not entail that we should forego our current values. The post-human values can be our current values, albeit ones that we have not yet clearly comprehended. Transhumanism does not require us to say that we should favor post-human beings over human beings, but that the right way of favoring human beings is by enabling us to realize our ideals better and that some of our ideals may well be located outside the space of modes of being that are accessible to us with our current biological constitution.

We can overcome many of our biological limitations. It is possible that there are some limitations that are impossible for us to transcend, not only because of technological difficulties but on metaphysical grounds. Depending on what our views are about what constitutes personal identity, it could be that certain modes of being, while possible, are not possible for us, because any being of such a kind would be so different from us that they could not be us. Concerns of this kind are familiar from theological discussions of the afterlife. In Christian theology, some souls will be allowed by God to go to heaven after their time as corporal creatures is over. Before being admitted to heaven, the souls would undergo a purification process in which they would lose many of their previous bodily attributes. Skeptics may doubt that the resulting minds would be sufficiently similar to our current minds for it to be possible for them to be the same person. A similar predicament arises within transhumanism: if the mode of being of a post-human being is radically different from that of a human being, then we may doubt whether a post-human being could be the same person as a human being, even if the post-human being originated from a human being.

We can, however, envision many enhancements that would not make it impossible for the post-transformation someone to be the same person as the pre-transformation person. A person could obtain considerable increased life expectancy, intelligence, health, memory, and emotional sensitivity, without ceasing to exist in the process. A person’s intellectual life can be transformed radically by getting an education. A person’s life expectancy can be extended substantially by being unexpectedly cured from a lethal disease. Yet these developments are not viewed as spelling the end of the original person. In particular, it seems that modifications that add to a person’s capacities can be more substantial than modifications that subtract, such as brain damage. If most of someone currently is, including her most important memories, activities, and feelings, is preserved, then adding extra capacities on top of that would not easily cause the person to cease to exist.

Preservation of personal identity, especially if this notion is given a narrow construal, is not everything. We can value other things than ourselves, or we might regard it as satisfactory if some parts or aspects of ourselves survive and flourish, even if that entails giving up some parts of ourselves such that we no longer count as being the same person. Which parts of ourselves we might be willing to sacrifice may not become clear until we are more fully acquainted with the full meaning of the options. A careful, incremental exploration of the post-human realm may be indispensable for acquiring such an understanding, although we may also be able to learn from each other’s experiences and from works of the imagination. Additionally, we may favor future people being posthuman rather than human, if the posthumans would lead lives more worthwhile than the alternative humans would. Any reasons stemming from such considerations would not depend on the assumption that we ourselves could become posthuman beings.

Transhumanism promotes the quest to develop further so that we can explore hitherto inaccessible realms of value. Technological enhancement of human organisms is a means that we ought to pursue to this end. There are limits to how much can be achieved by low-tech means such as education, philosophical contemplation, moral self-scrutiny and other such methods proposed by classical philosophers with perfectionist leanings, including Plato, Aristotle, and Nietzsche, or by means of creating a fairer and better society, as envisioned by social reformists such as Marx or Martin Luther King. This is not to denigrate what we can do with the tools we have today. Yet ultimately, transhumanists hope to go further.

Most potential human enhancement technologies have so far received scant attention in the ethics literature. One exception is genetic engineering, the morality of which has been extensively debated in recent years. To illustrate how the transhumanist approach can be applied to particular technologies, we shall therefore now turn to consider the case of human germ-line genetic enhancements.

Certain types of objection against germ-line modifications are not accorded much weight by a transhumanist interlocutor. For instance, objections that are based on the idea that there is something inherently wrong or morally suspect in using science to manipulate human nature are regarded by transhumanists as wrongheaded. Moreover, transhumanists emphasize that particular concerns about negative aspects of genetic enhancements, even when such concerns are legitimate, must be judged against the potentially enormous benefits that could come from genetic technology successfully employed.6 For example, many commentators worry about the psychological effects of the use of germ-line engineering. The ability to select the genes of our children and to create so-called designer babies will, it is claimed, corrupt parents, who will come to view their children as mere products.7 We will then begin to evaluate our offspring according to standards of quality control, and this will undermine the ethical ideal of unconditional acceptance of children, no matter what their abilities and traits. Are we really prepared to sacrifice on the altar of consumerism even those deep values that are embodied in traditional relationships between child and parents? Is the quest for perfection worth this cultural and moral cost? A transhumanist should not dismiss such concerns as irrelevant. Transhumanists recognize that the depicted outcome would be bad. We do not want parents to love and respect their children less. We do not want social prejudice against people with disabilities to get worse. The psychological and cultural effects of commodifying human nature are potentially important.

But such dystopian scenarios are speculations. There is no firm ground for believing that the alleged consequences would actually happen. What relevant evidence we have, for instance regarding the treatment of children who have been conceived through the use of in vitro fertilization or embryo screening, suggests that the pessimistic prognosis is alarmist. Parents will in fact love and respect their children even when artificial means and conscious choice play a part in procreation.

We might speculate, instead, that germ-line enhancements will lead to more love and parental dedication. Some mothers and fathers might find it easier to love a child who, thanks to enhancements, is bright, beautiful, healthy, and happy. The practice of germ-line enhancement might lead to better treatment of people with disabilities, because a general demystification of the genetic contributions to human traits could make it clearer that people with disabilities are not to blame for their disabilities and a decreased incidence of some disabilities could lead to more assistance being available for the remaining affected people to enable them to live full, unrestricted lives through various technological and social supports. Speculating about possible psychological or cultural effects of germ-line engineering can therefore cut both ways. Good consequences no less than bad ones are possible. In the absence of sound arguments for the view that the negative consequences would predominate, such speculations provide no reason against moving forward with the technology.

Ruminations over hypothetical side-effects may serve to make us aware of things that could go wrong so that we can be on the lookout for untoward developments. By being aware of the perils in advance, we will be in a better position to take preventive countermeasures. For instance, if we think that some people would fail to realize that a human clone would be a unique person deserving just as much respect and dignity as any other human being, we could work harder to educate the public on the inadequacy of genetic determinism. The theoretical contributions of well-informed and reasonable critics of germ-line enhancement could indirectly add to our justification for proceeding with germ-line engineering. To the extent that the critics have done their job, they can alert us to many of the potential untoward consequences of germ-line engineering and contribute to our ability to take precautions, thus improving the odds that the balance of effects will be positive. There may well be some negative consequences of human germ-line engineering that we will not forestall, though of course the mere existence of negative effects is not a decisive reason not to proceed. Every major technology has some negative consequences. Only after a fair comparison of the risks with the likely positive consequences can any conclusion based on a cost-benefit analysis be reached.

In the case of germ-line enhancements, the potential gains are enormous. Only rarely, however, are the potential gains discussed, perhaps because they are too obvious to be of much theoretical interest. By contrast, uncovering subtle and non-trivial ways in which manipulating our genome could undermine deep values is philosophically a lot more challenging. But if we think about it, we recognize that the promise of genetic enhancements is anything but insignificant. Being free from severe genetic diseases would be good, as would having a mind that can learn more quickly, or having a more robust immune system. Healthier, wittier, happier people may be able to reach new levels culturally. To achieve a significant enhancement of human capacities would be to embark on the transhuman journey of exploration of some of the modes of being that are not accessible to us as we are currently constituted, possibly to discover and to instantiate important new values. On an even more basic level, genetic engineering holds great potential for alleviating unnecessary human suffering. Every day that the introduction of effective human genetic enhancement is delayed is a day of lost individual and cultural potential, and a day of torment for many unfortunate sufferers of diseases that could have been prevented. Seen in this light, proponents of a ban or a moratorium on human genetic modification must take on a heavy burden of proof in order to have the balance of reason tilt in their favor. Transhumanists conclude that the challenge has not been met.

One way of going forward with genetic engineering is to permit everything, leaving all choices to parents. While this attitude may be consistent with transhumanism, it is not the best transhumanist approach. One thing that can be said for adopting a libertarian stance in regard to human reproduction is the sorry track record of socially planned attempts to improve the human gene pool. The list of historical examples of state intervention in this domain ranges from the genocidal horrors of the Nazi regime, to the incomparably milder but still disgraceful semi-coercive sterilization programs of mentally impaired individuals favored by many well-meaning socialists in the past century, to the controversial but perhaps understandable program of the current Chinese government to limit population growth. In each case, state policies interfered with the reproductive choices of individuals. If parents had been left to make the choices for themselves, the worst transgressions of the eugenics movement would not have occurred. Bearing this in mind, we ought to think twice before giving our support to any proposal that would have the state regulate what sort of children people are allowed to have and the methods that may be used to conceive them.8

We currently permit governments to have a role in reproduction and child-rearing and we may reason by extension that there would likewise be a role in regulating the application of genetic reproductive technology. State agencies and regulators play a supportive and supervisory role, attempting to promote the interests of the child. Courts intervene in cases of child abuse or neglect. Some social policies are in place to support children from disadvantaged backgrounds and to ameliorate some of the worst inequities suffered by children from poor homes, such as through the provision of free schooling. These measures have analogues that apply to genetic enhancement technologies. For example, we ought to outlaw genetic modifications that are intended to damage the child or limit its opportunities in life, or that are judged to be too risky. If there are basic enhancements that would be beneficial for a child but that some parents cannot afford, then we should consider subsidizing those enhancements, just as we do with basic education. There are grounds for thinking that the libertarian approach is less appropriate in the realm of reproduction than it is in other areas. In reproduction, the most important interests at stake are those of the child-to-be, who cannot give his or her advance consent or freely enter into any form of contract. As it is, we currently approve of many measures that limit parental freedoms. We have laws against child abuse and child neglect. We have obligatory schooling. In some cases, we can force needed medical treatment on a child, even against the wishes of its parents.

There is a difference between these social interventions with regard to children and interventions aimed at genetic enhancements. While there is a consensus that nobody should be subjected to child abuse and that all children should have at least a basic education and should receive necessary medical care, it is unlikely that we will reach an agreement on proposals for genetic enhancements any time soon. Many parents will resist such proposals on principled grounds, including deep-seated religious or moral convictions. The best policy for the foreseeable future may therefore be to not legally require any genetic enhancements, except perhaps in extreme cases for which there is no alternative treatment. Even in such cases, it is dubious that the social climate in many countries is ready for mandatory genetic interventions.

The scope for ethics and public policy, however, extend far beyond the passing of laws requiring or banning specific interventions. Even if a given enhancement option is neither outlawed nor legally required, we may still seek to discourage or encourage its use in a variety of ways. Through subsidies and taxes, research-funding policies, genetic counseling practices and guidelines, laws regulating genetic information and genetic discrimination, provision of health care services, regulation of the insurance industry, patent law, education, and through the allocation of social approbation and disapproval, we may influence the direction in which particular technologies are applied. We may appropriately ask, with regard to genetic enhancement technologies, which types of applications we ought to promote or discourage.

An externality, as understood by economists, is a cost or a benefit of an action that is not carried by a decision-maker. An example of a negative externality might be found in a firm that lowers its production costs by polluting the environment. The firm enjoys most of the benefits while escaping the costs, such as environmental degradation, which may instead paid by people living nearby. Externalities can also be positive, as when people put time and effort into creating a beautiful garden outside their house. The effects are enjoyed not exclusively by the gardeners but spill over to passersby. As a rule of thumb, sound social policy and social norms would have us internalize many externalities so that the incentives of producers more closely match the social value of production. We may levy a pollution tax on the polluting firm, for instance, and give our praise to the home gardeners who beautify the neighborhood.

Genetic enhancements aimed at the obtainment of goods that are desirable only in so far as they provide a competitive advantage tend to have negative externalities. An example of such a positional good, as economists call them, is stature. There is evidence that being tall is statistically advantageous, at least for men in Western societies. Taller men earn more money, wield greater social influence, and are viewed as more sexually attractive. Parents wanting to give their child the best possible start in life may rationally choose a genetic enhancement that adds an inch or two to the expected length of their offspring. Yet for society as a whole, there seems to be no advantage whatsoever in people being taller. If everybody grew two inches, nobody would be better off than they were before. Money spent on a positional good like length has little or no net effect on social welfare and is therefore, from society’s point of view, wasted.

Health is a very different type of good. It has intrinsic benefits. If we become healthier, we are personally better off and others are not any worse off. There may even be a positive externality of enhancing ours own health. If we are less likely to contract a contagious disease, others benefit by being less likely to get infected by us. Being healthier, you may also contribute more to society and consume less of publicly funded healthcare.

If we were living in a simple world where people were perfectly rational self-interested economic agents and where social policies had no costs or unintended effects, then the basic policy prescription regarding genetic enhancements would be relatively straightforward. We should internalize the externalities of genetic enhancements by taxing enhancements that have negative externalities and subsidizing enhancements that have positive externalities. Unfortunately, crafting policies that work well in practice is considerably more difficult. Even determining the net size of the externalities of a particular genetic enhancement can be difficult. There is clearly an intrinsic value to enhancing memory or intelligence in as much as most of us would like to be a bit smarter, even if that did not have the slightest effect on our standing in relation to others. But there would also be important externalities, both positive and negative. On the negative side, others would suffer some disadvantage from our increased brainpower in that their own competitive situation would be worsened. Being more intelligent, we would be more likely to attain high-status positions in society, positions that would otherwise have been enjoyed by a competitor. On the positive side, others might benefit from enjoying witty conversations with us and from our increased taxes.

If in the case of intelligence enhancement the positive externalities outweigh the negative ones, then a prima facie case exists not only for permitting genetic enhancements aimed at increasing intellectual ability, but for encouraging and subsidizing them too. Whether such policies remain a good idea when all practicalities of implementation and political realities are taken into account is another matter. But at least we can conclude that an enhancement that has both significant intrinsic benefits for an enhanced individual and net positive externalities for the rest of society should be encouraged. By contrast, enhancements that confer only positional advantages, such as augmentation of stature or physical attractiveness, should not be socially encouraged, and we might even attempt to make a case for social policies aimed at reducing expenditure on such goods, for instance through a progressive tax on consumption.

One important kind of externality in germ-line enhancements is their effects on social equality. This has been a focus for many opponents of germ-line genetic engineering who worry that it will widen the gap between haves and have-nots. Today, children from wealthy homes enjoy many environmental privileges, including access to better schools and social networks. Arguably, this constitutes an inequity against children from poor homes. We can imagine scenarios where such inequities grow much larger thanks to genetic interventions that only the rich can afford, adding genetic advantages to the environmental advantages already benefiting privileged children. We could even speculate about the members of the privileged stratum of society eventually enhancing themselves and their offspring to a point where the human species, for many practical purposes, splits into two or more species that have little in common except a shared evolutionary history.10 The genetically privileged might become ageless, healthy, super-geniuses of flawless physical beauty, who are graced with a sparkling wit and a disarmingly self-deprecating sense of humor, radiating warmth, empathetic charm, and relaxed confidence. The non-privileged would remain as people are today but perhaps deprived of some their self-respect and suffering occasional bouts of envy. The mobility between the lower and the upper classes might disappear, and a child born to poor parents, lacking genetic enhancements, might find it impossible to successfully compete against the super-children of the rich. Even if no discrimination or exploitation of the lower class occurred, there is still something disturbing about the prospect of a society with such extreme inequalities.

While we have vast inequalities today and regard many of these as unfair, we also accept a wide range of inequalities because we think that they are deserved, have social benefits, or are unavoidable concomitants to free individuals making their own and sometimes foolish choices about how to live their lives. Some of these justifications can also be used to exonerate some inequalities that could result from germ-line engineering. Moreover, the increase in unjust inequalities due to technology is not a sufficient reason for discouraging the development and use of the technology. We must also consider its benefits, which include not only positive externalities but also intrinsic values that reside in such goods as the enjoyment of health, a soaring mind, and emotional well-being.

We can also try to counteract some of the inequality-increasing tendencies of enhancement technology with social policies. One way of doing so would be by widening access to the technology by subsidizing it or providing it for free to children of poor parents. In cases where the enhancement has considerable positive externalities, such a policy may actually benefit everybody, not just the recipients of the subsidy. In other cases, we could support the policy on the basis of social justice and solidarity.

Even if all genetic enhancements were made available to everybody for free, however, this might still not completely allay the concern about inequity. Some parents might choose not to give their children any enhancements. The children would then have diminished opportunities through no fault of their own. It would be peculiar, however, to argue that governments should respond to this problem by limiting the reproductive freedom of the parents who wish to use genetic enhancements. If we are willing to limit reproductive freedom through legislation for the sake of reducing inequities, then we might as well make some enhancements obligatory for all children. By requiring genetic enhancements for everybody to the same degree, we would not only prevent an increase in inequalities but also reap the intrinsic benefits and the positive externalities that would come from the universal application of enhancement technology. If reproductive freedom is regarded as too precious to be curtailed, then neither requiring nor banning the use of reproductive enhancement technology is an available option. In that case, we would either have to tolerate inequities as a price worth paying for reproductive freedom or seek to remedy the inequities in ways that do not infringe on reproductive freedom.

All of this is based on the hypothesis that germ-line engineering would in fact increase inequalities if left unregulated and no countermeasures were taken. That hypothesis might be false. In particular, it might turn out to be technologically easier to cure gross genetic defects than to enhance an already healthy genetic constitution. We currently know much more about many specific inheritable diseases, some of which are due to single gene defects, than we do about the genetic basis of talents and desirable qualities such as intelligence and longevity, which in all likelihood are encoded in complex constellations of multiple genes. If this turns out to be the case, then the trajectory of human genetic enhancement may be one in which the first thing to happen is that the lot of the genetically worst-off is radically improved, through the elimination of diseases such as Tay Sachs, Lesch-Nyhan, Downs Syndrome, and early-onset Alzheimer’s disease. This would have a major leveling effect on inequalities, not primarily in the monetary sense, but with respect to the even more fundamental parameters of basic opportunities and quality of life.

Another frequently heard objection against germ-line genetic engineering is that it would be uniquely hazardous because the changes it would bring are irreversible and would affect all generations to come. It would be highly irresponsible and arrogant of us to presume we have the wisdom to make decisions about what should be the genetic constitutions of people living many generations hence. Human fallibility, on this objection, gives us good reason not to embark on germ-line interventions. For our present purposes, we can set aside the issue of the safety of the procedure, understood narrowly, and stipulate that the risk of medical side-effects has been reduced to an acceptable level. The objection under consideration concerns the irreversibility of germ-line interventions and the lack of predictability of its long-term consequences; it forces us to ask if we possess the requisite wisdom for making genetic choices on behalf of future generations.

Human fallibility is not a conclusive ground for resisting germ-line genetic enhancements. The claim that such interventions would be irreversible is incorrect. Germ-line interventions can be reversed by other germ-line interventions. Moreover, considering that technological progress in genetics is unlikely to grind to an abrupt halt any time soon, we can count on future generations being able to reverse our current germ-line interventions even more easily than we can currently implement them. With advanced genetic technology, it might even be possible to reverse many germ-line modifications with somatic gene therapy, or with medical nanotechnology. Technologically, germ-line changes are perfectly reversible by future generations.

It is possible that future generations might choose to retain the modifications that we make. If that turns out to be the case, then the modifications, while not irreversible, would nevertheless not actually be reversed. This might be a good thing. The possibility of permanent consequences is not an objection against germ-line interventions any more than it is against social reforms. The abolition of slavery and the introduction of general suffrage might never be reversed; indeed, we hope they will not be. Yet this is no reason for people to have resisted the reforms. Likewise, the potential for everlasting consequences, including ones we cannot currently reliably forecast, in itself constitutes no reason to oppose genetic intervention. If immunity against horrible diseases and enhancements that expand the opportunities for human growth are passed on to subsequent generations in perpetuo, it would be a cause for celebration, not regret.

There are some kinds of changes that we need be particularly careful about. They include modifications of the drives and motivations of our descendants. For example, there are obvious reasons why we might think it worthwhile to seek to reduce our children’s propensity to violence and aggression. We would have to take care, however, that we do not do this in a way that would make future people overly submissive or complacent. We can conceive of a dystopian scenario along the lines of Brave New World, in which people are leading shallow lives but have been manipulated to be perfectly content with their sub-optimal existence. If the people transferred their shallow values to their children, humanity could get permanently stuck in a not-very-good state, having foolishly changed itself to lack any desire to strive for something better. This outcome would be dystopian because a permanent cap on human development would destroy the transhumanist hope of exploring the post-human realm. Transhumanists therefore place an emphasis on modifications which, in addition to promoting human well-being, also open more possibilities than they close and which increase our ability to make subsequent choices wisely. Longer active lifespans, better memory, and greater intellectual capacities are plausible candidates for enhancements that would improve our ability to figure out what we ought to do next. They would be a good place to start.

The possibility of creating thinking machines raises a host of ethical issues.  These questions relate both to ensuring that such machines do not harm humans and other morally relevant beings, and to the moral status of the machines themselves.  The first section discusses issues that may arise in the near future of AI.  The second section outlines challenges for ensuring that AI operates safely as it approaches humans in its intelligence.  The third section outlines how we might assess whether, and in what circumstances, AIs themselves have moral status.  In the fourth section, we consider how AIs might differ from humans in certain basic respects relevant to our ethical assessment of them.  The final section addresses the issues of creating AIs more intelligent than human, and ensuring that they use their advanced intelligence for good rather than ill.
Imagine, in the near future, a bank using a machine learning algorithm to recommend mortgage applications for approval.  A rejected applicant brings a lawsuit against the bank, alleging that the algorithm is discriminating racially against mortgage applicants.  The bank replies that this is impossible, since the algorithm is deliberately blinded to the race of the applicants.  Indeed, that was part of the bank’s rationale for implementing the system.  Even so, statistics show that the bank’s approval rate for black applicants has been steadily dropping.  Submitting ten apparently equally qualified genuine applicants (as determined by a separate panel of human judges) shows that the algorithm accepts white applicants and rejects black applicants.  What could possibly be happening?
AI algorithms play an increasingly large role in modern society, though usually not labeled “AI”.  The scenario described above might be transpiring even as we write.  It will become increasingly important to develop AI algorithms that are not just powerful and scalable, but also transparent to inspection—to name one of many socially important properties. Some challenges of machine ethics are much like many other challenges involved in designing machines.  Designing a robot arm to avoid crushing stray humans is no more morally fraught than designing a flame‐retardant sofa.  It involves new programming challenges, but no new ethical challenges.  But when AI algorithms take on cognitive work with social dimensions—cognitive tasks previously performed by humans—the AI algorithm inherits the social requirements.  It would surely be frustrating to find that no bank in the world will approve your seemingly excellent loan application, and nobody knows why, and nobody can find out even in principle.   (Maybe you have a first name strongly associated with deadbeats?  Who knows?) Transparency is not the only desirable feature of AI.  It is also important that AI algorithms taking over social functions be predictable to those they govern.  To understand the importance of such predictability, consider an analogy.  The legal principle of stare decisis binds judges to follow past precedent whenever possible.  To an engineer, this preference for precedent may seem incomprehensible—why bind the future to the past, when technology is always improving?  But one of the most important functions of the legal system is to be predictable, so that, e.g., contracts can be written knowing how they will be executed.  The job of the legal system is not necessarily to optimize society, but to provide a predictable environment within which citizens can optimize their own lives. It will also become increasingly important that AI algorithms be robust against manipulation. A machine vision system to scan airline luggage for bombs must be robust against human adversaries deliberately searching for exploitable flaws in the algorithm—for example, a shape that, placed next to a pistol in one’s luggage, would neutralize recognition of it.  Robustness against manipulation is an ordinary criterion in information security; nearly the criterion.  But it is not a criterion that appears often in machine learning journals, which are currently more interested in, e.g., how an algorithm scales up on larger parallel systems. Another important social criterion for dealing with organizations is being able to find the person responsible for getting something done.  When an AI system fails at its assigned task, who takes the blame?  The programmers?  The end‐users?
There is nearly universal agreement among modern AI professionals that Artificial Intelligence falls short of human capabilities in some critical sense, even though AI algorithms have beaten humans in many specific domains such as chess.  It has been suggested by some that as soon as AI researchers figure out how to do something, that capability ceases to be regarded as intelligent—chess was considered the epitome of intelligence until Deep Blue won the world championship from Kasparov—but even these researchers agree that something important is missing from modern AIs (e.g., Hofstadter 2006). While this subfield of Artificial Intelligence is only just coalescing, “Artificial General Intelligence” (hereafter, AGI) is the emerging term of art used to denote “real” AI (see, e.g., the edited volume Goertzel and Pennachin 2006).  As the name implies, the emerging consensus is that the missing characteristic is generality.  Current AI algorithms with human‐equivalent or ‐superior performance are characterized by a deliberately‐programmed competence only in a single, restricted domain.  Deep Blue became the world champion at chess, but it cannot even play checkers, let alone drive a car or make a scientific discovery.  Such modern AI algorithms resemble all biological life with the sole exception of Homo sapiens.  A bee exhibits competence at building hives; a beaver exhibits competence at building dams; but a bee doesn’t build dams, and a beaver can’t learn to build a hive.  A human, watching, can learn to do both; but this is a unique ability among biological lifeforms.  It is debatable whether human intelligence is truly general—we are certainly better at some cognitive tasks than others (Hirschfeld and Gelman 1994)—but human intelligence is surely significantly more generally applicable than nonhominid intelligence.
It is relatively easy to envisage the sort of safety issues that may result from AI operating only within a specific domain.  It is a qualitatively different class of problem to handle an AGI operating across many novel contexts that cannot be predicted in advance. When human engineers build a nuclear reactor, they envision the specific events that could go on inside it—valves failing, computers failing, cores increasing in temperature—and engineer the reactor to render these events noncatastrophic.  Or, on a more mundane level, building a toaster involves envisioning bread and envisioning the reaction of the bread to the toasterʹs heating element.  The toaster itself does not know that its purpose is to make toast—the purpose of the toaster is represented within the designer’s mind, but is not explicitly represented in computations inside the toaster—and so if you place cloth inside a toaster, it may catch fire, as the design executes in an unenvisioned context with an unenvisioned side effect. Even task‐specific AI algorithms throw us outside the toaster‐paradigm, the domain of locally preprogrammed, specifically envisioned behavior.  Consider Deep Blue, the chess algorithm that beat Garry Kasparov for the world championship of chess.  Were it the case that machines can only do exactly as they are told, the programmers would have had to manually preprogram a database containing moves for every possible chess position that Deep Blue could encounter.  But this was not an option for Deep Blue’s programmers.  First, the space of possible chess positions is unmanageably large.  Second, if the programmers had manually input what they considered a good move in each possible situation, the resulting system would not have been able to make stronger chess moves than its creators.  Since the programmers themselves were not world champions, such a system would not have been able to defeat Garry Kasparov. In creating a superhuman chess player, the human programmers necessarily sacrificed their ability to predict Deep Blue’s local, specific game behavior.  Instead, Deep Blue’s programmers had (justifiable) confidence that Deep Blue’s chess moves would satisfy a non‐local criterion of optimality:  namely, that the moves would tend to steer the future of the game board into outcomes in the “winning” region as defined by the chess rules.   This prediction about distant consequences, though it proved accurate, did not allow the programmers to envision the local behavior of Deep Blue—its response to a specific attack on its king—because Deep Blue computed the nonlocal game map, the link between a move and its possible future consequences, more accurately than the programmers could (Yudkowsky 2006). Modern humans do literally millions of things to feed themselves—to serve the final consequence of being fed.  Few of these activities were “envisioned by Nature” in the sense of being ancestral challenges to which we are directly adapted.
Humans crossed space and put footprints on the Moon, even though none of our ancestors encountered a challenge analogous to vacuum.  Compared to domain‐specific AI, it is a qualitatively different problem to design a system that will operate safely across thousands of contexts; including contexts not specifically envisioned by either the designers or the users; including contexts that no human has yet encountered.  Here there may be no local specification of good behavior—no simple specification over the behaviors themselves, any more than there exists a compact local description of all the ways that humans obtain their daily bread. To build an AI that acts safely while acting in many domains, with many consequences, including problems the engineers never explicitly envisioned, one must specify good behavior in such terms as “X such that the consequence of X is not harmful to humans”.  This is non‐local; it involves extrapolating the distant consequences of actions.  Thus, this is only an effective specification—one that can be realized as a design property—if the system explicitly extrapolates the consequences of its behavior.  A toaster cannot have this design property because a toaster cannot foresee the consequences of toasting bread. Imagine an engineer having to say, “Well, I have no idea how this airplane I built will fly safely—indeed I have no idea how it will fly at all, whether it will flap its wings or inflate itself with helium or something else I haven’t even imagined—but I assure you, the design is very, very safe.”  This may seem like an unenviable position from the perspective of public relations, but it’s hard to see what other guarantee of ethical behavior would be possible for a general intelligence operating on unforeseen problems, across domains, with preferences over distant consequences.  Inspecting the cognitive design might verify that the mind was, indeed, searching for solutions that we would classify as ethical; but we couldn’t predict which specific solution the mind would discover. Respecting such a verification requires some way to distinguish trustworthy assurances (a procedure which will not say the AI is safe unless the AI really is safe) from pure hope and magical thinking (“I have no idea how the Philosopher’s Stone will transmute lead to gold, but I assure you, it will!”).  One should bear in mind that purely hopeful expectations have previously been a problem in AI research (McDermott 1976). Verifiably constructing a trustworthy AGI will require different methods, and a different way of thinking, from inspecting power plant software for bugs—it will require an AGI that thinks like a human engineer concerned about ethics, not just a simple product of ethical engineering.
A different set of ethical issues arises when we contemplate the possibility that some future AI systems might be candidates for having moral status.  Our dealings with beings possessed of moral status are not exclusively a matter of instrumental rationality: we also have moral reasons to treat them in certain ways, and to refrain from treating them in certain other ways.  Francis Kamm has proposed the following definition of moral status, which will serve for our purposes.
A rock has no moral status: we may crush it, pulverize it, or subject it to any treatment we like without any concern for the rock itself.  A human person, on the other hand, must be treated not only as a means but also as an end.  Exactly what it means to treat a person as an end is something about which different ethical theories disagree; but it certainly involves taking her legitimate interests into account—giving weight to her well‐being—and it may also involve accepting strict moral side‐constraints in our dealings with her, such as a prohibition against murdering her, stealing from her, or doing a variety of other things to her or her property without her consent.  Moreover, it is because a human person counts in her own right, and for her sake, that it is impermissible to do to her these things.  This can be expressed more concisely by saying that a human person has moral status. Questions about moral status are important in some areas of practical ethics.  For example, disputes about the moral permissibility of abortion often hinge on disagreements about the moral status of the embryo.  Controversies about animal experimentation and the treatment of animals in the food industry involve questions about the moral status of different species of animal.  And our obligations towards human beings with severe dementia, such as late‐stage Alzheimer’s patients, may also depend on questions of moral status.
One common view is that many animals have qualia and therefore have some moral status, but that only human beings have sapience, which gives them a higher moral status than non‐human animals.1  This view, of course, must confront the existence of borderline cases such as, on the one hand, human infants or human beings with severe mental retardation—sometimes unfortunately referred to as “marginal humans”— which fail to satisfy the criteria for sapience; and, on the other hand, some non‐human animals such as the great apes, which might possess at least some of the elements of sapience.  Some deny that so‐called “marginal humans” have full moral status.  Others propose additional ways in which an object could qualify as a bearer of moral status, such as by being a member of a kind that normally has sentience or sapience, or by standing in a suitable relation to some being that independently has moral status (cf. Mary Anne Warren 2000).  For present purposes, however, we will focus on the criteria of sentience and sapience. This picture of moral status suggests that an AI system will have some moral status if it has the capacity for qualia, such as an ability to feel pain.  A sentient AI system, even if it lacks language and other higher cognitive faculties, is not like a stuffed toy animal or a wind‐up doll; it is more like a living animal.  It is wrong to inflict pain on a mouse, unless there are sufficiently strong morally overriding reasons to do so.  The same would hold for any sentient AI system.
 The anencephalic child, however, would have the same moral status as any other similar anencephalic child, including one that had come about through some entirely natural process.  The difference in moral status between an anencephalic child and a normal child is grounded in the qualitative difference between the two—the fact that one has a mind while the other does not.  Since the two children do not have the same functionality and the same conscious experience, the Principle of Ontogeny Non‐Discrimination does not apply. Although the Principle of Ontogeny Non‐Discrimination asserts that a being’s ontogeny has no essential bearing on its moral status, it does not deny that facts about ontogeny can affect what duties particular moral agents have toward the being in question.  Parents have special duties to their child which they do not have to other children, and which they would not have even if there were another child qualitatively identical to their own.  Similarly, the Principle of Ontogeny Non‐Discrimination is consistent with the claim that the creators or owners of an AI system with moral status may have special duties to their artificial mind which they do not have to another artificial mind, even if the minds in question are qualitatively similar and have the same moral status. If the principles of non‐discrimination with regard to substrate and ontogeny are accepted, then many questions about how we ought to treat artificial minds can be answered by applying the same moral principles that we use to determine our duties in more familiar contexts.  Insofar as moral duties stem from moral status considerations, we ought to treat an artificial mind in just the same way as we ought to treat a qualitatively identical natural human mind in a similar situation.  This simplifies the problem of developing an ethics for the treatment of artificial minds. Even if we accept this stance, however, we must confront a number of novel ethical questions which the aforementioned principles leave unanswered.  Novel ethical questions arise because artificial minds can have very different properties from ordinary human or animal minds.  We must consider how these novel properties would affect the moral status of artificial minds and what it would mean to respect the moral status of such exotic minds.
An artificial intellect, by contrast, might be constituted quite differently from a human intellect yet still exhibit human‐like behavior or possess the behavioral dispositions normally indicative of personhood.  It might therefore be possible to conceive of an artificial intellect that would be sapient, and perhaps would be a person, yet would not be sentient or have conscious experiences of any kind.   (Whether this is really possible depends on the answers to some non‐trivial metaphysical questions.)  Should such a system be possible, it would raise the question whether a non‐sentient person would have any moral status whatever; and if so, whether it would have the same moral status as a sentient person.  Since sentience, or at least a capacity for sentience, is ordinarily assumed to be present in any individual who is a person, this question has not received much attention to date.2 Another exotic property, one which is certainly metaphysically and physically possible for an artificial intelligence, is for its subjective rate of time to deviate drastically from the rate that is characteristic of a biological human brain.  The concept of subjective rate of time is best explained by first introducing the idea whole brain emulation, or “uploading”. “Uploading” refers to a hypothetical future technology that would enable a human or other animal intellect to be transferred from its original implementation in an organic brain onto a digital computer.  One scenario goes like this:  First, a very high‐resolution scan is performed of some particular brain, possibly destroying the original in the process.  For example, the brain might be vitrified and dissected into thin slices, which can then be scanned using some form of high‐throughput microscopy combined with automated image recognition.  We may imagine this scan to be detailed enough to capture all the neurons, their synaptic interconnections, and other features that are functionally relevant to the original brain’s operation.  Second, this three‐dimensional map of the components of the brain and their interconnections is combined with a library of advanced neuroscientific theory which specifies the computational properties of each basic type of element, such as different kinds of neuron and synaptic junction.  Third, the computational structure and the associated algorithmic behavior of its components are implemented in some powerful computer.
A number of questions arise in the context of such a  scenario:  How plausible is it that this procedure will one day become technologically feasible?  If the procedure worked and produced a computer program exhibiting roughly the same personality, the same memories, and the same thinking patterns as the original brain, would this program be sentient?  Would the upload be the same person as the individual whose brain was disassembled in the uploading process?  What happens to personal identity if an upload is copied such that two similar or qualitatively identical upload minds are running in parallel?  Although all of these questions are relevant to the ethics of machine intelligence, let us here focus on an issue involving the notion of a subjective rate of time. Suppose that an upload could be sentient.  If we run the upload program on a faster computer, this will cause the upload, if it is connected to an input device such as a video camera, to perceive the external world as if it had been slowed down.  For example, if the upload is running a thousand times faster than the original brain, then the external world will appear to the upload as if it were slowed down by a factor of thousand.  Somebody drops a physical coffee mug:  The upload observes the mug slowly falling to the ground while the upload finishes reading the morning newspaper and sends off a few emails.  One second of objective time corresponds to 17 minutes of subjective time.  Objective and subjective duration can thus diverge. Subjective time is not the same as a subject’s estimate or perception of how fast time flows.  Human beings are often mistaken about the flow of time.  We may believe that it is one o’clock when it is in fact a quarter past two; or a stimulant drug might cause our thoughts to race, making it seem as though more subjective time has lapsed than is actually the case.  These mundane cases involve a distorted time perception rather than a shift in the rate of subjective time.  Even in a cocaine‐addled brain, there is probably not a significant change in the speed of basic neurological computations; more likely, the drug is causing such a brain to flicker more rapidly from one thought to another, making it spend less subjective time thinking each of a greater number of distinct thoughts. The variability of the subjective rate of time is an exotic property of artificial minds that raises novel ethical issues.  For example, in cases where the duration of an experience is ethically relevant, should duration be measured in objective or subjective time?  If an   upload has committed a crime and is sentenced to four years in prison, should this be four objective years—which might correspond to many millennia of subjective time— or should it be four subjective years, which might be over in a couple of days of objective time?
 Since in our accustomed context of biological humans, subjective time is not significantly variable, it is unsurprising that this kind of question is not straightforwardly settled by familiar ethical norms, even if these norms are extended to artificial intellects by means of non‐discrimination principles (such as those proposed in the previous section). To illustrate the kind of ethical claim that might be relevant here, we formulate (but do not argue for) a principle privileging subjective time as the normatively more fundamental notion: Principle of Subjective Rate of Time In cases where the duration of an experience is of basic normative significance, it is the experience’s subjective duration that counts. So far we have discussed two possibilities (non‐sentient sapience and variable subjective rate of time) which are exotic in the relatively profound sense of being metaphysically problematic as well as lacking clear instances or parallels in the contemporary world.  Other properties of possible artificial minds would be exotic in a more superficial sense; e.g., by diverging in some unproblematically quantitative dimension from the kinds of mind with which we are familiar.  But such superficially exotic properties may also pose novel ethical problems—if not at the level of foundational moral philosophy, then at the level of applied ethics or for mid‐level ethical principles. One important set of exotic properties of artificial intelligences relate to reproduction.   A number of empirical conditions that apply to human reproduction need not apply to artificial intelligences.  For example, human children are the product of recombination of the genetic material from two parents; parents have limited ability to influence the character of their offspring; a human embryo needs to be gestated in the womb for nine months; it takes fifteen to twenty years for a human child to reach maturity; a human child does not inherit the skills and knowledge acquired by its parents; human beings possess a complex evolved set of emotional adaptations related to reproduction, nurturing, and the child‐parent relationship.  None of these empirical conditions need pertain in the context of a reproducing machine intelligence.  It is therefore plausible that many of the mid‐level moral principles that we have come to accept as norms governing human reproduction will need to be rethought in the context of AI reproduction. To illustrate why some of our moral norms need to be rethought in the context of AI reproduction, it will suffice to consider just one exotic property of AIs: their capacity for rapid reproduction.  Given access to computer hardware, an AI could duplicate itself very quickly, in no more time than it takes to make a copy of the AI’s software.
Moreover, since the AI copy would be identical to the original, it would be born completely mature, and the copy could begin making its own copies immediately.   Absent hardware limitations, a population of AIs could therefore grow exponentially at an extremely rapid rate, with a doubling time on the order of minutes or hours rather than decades or centuries. Our current ethical norms about reproduction include some version of a principle of reproductive freedom, to the effect that it is up to each individual or couple to decide for themselves whether to have children and how many children to have.  Another norm we have (at least in rich and middle‐income countries) is that society must step in to provide the basic needs of children in cases where their parents are unable or refusing to do so.  It is easy to see how these two norms could collide in the context of entities with the capacity for extremely rapid reproduction. Consider, for example, a population of uploads, one of whom happens to have the desire to produce as large a clan as possible.  Given complete reproductive freedom, this upload may start copying itself as quickly as it can; and the copies it produces— which may run on new computer hardware owned or rented by the original, or may share the same computer as the original—will also start copying themselves, since they are identical to the progenitor upload and share its philoprogenic desire.  Soon, members of the upload clan will find themselves unable to pay the electricity bill or the rent for the computational processing and storage needed to keep them alive.  At this point, a social welfare system might kick in to provide them with at least the bare necessities for sustaining life.  But if the population grows faster than the economy, resources will run out; at which point uploads will either die or their ability to reproduce will be curtailed.  (For two related dystopian scenarios, see Bostrom (2004).) This scenario illustrates how some mid‐level ethical principles that are suitable in contemporary societies might need to be modified if those societies were to include persons with the exotic property of being able to reproduce very rapidly. The general point here is that when thinking about applied ethics for contexts that are very different from our familiar human condition, we must be careful not to mistake mid‐level ethical principles for foundational normative truths.  Put differently, we must recognize the extent to which our ordinary normative precepts are implicitly conditioned on the obtaining of various empirical conditions, and the need to adjust these precepts accordingly when applying them to hypothetical futuristic cases in which their preconditions are assumed not to obtain.  By this, we are not making any controversial claim about moral relativism, but merely highlighting the commonsensical point that context is relevant to the application of ethics—and suggesting that this point is especially pertinent when one is considering the ethics of minds with exotic properties.
I. J. Good (1965) set forth the classic hypothesis concerning superintelligence: that an AI sufficiently intelligent to understand its own design could redesign itself or create a successor system, more intelligent, which could then redesign itself yet again to become even more intelligent, and so on in a positive feedback cycle.  Good called this the “intelligence explosion”.  Recursive scenarios are not limited to AI: humans with intelligence augmented through a brain‐computer interface might turn their minds to designing the next generation of brain‐computer interfaces.  (If you had a machine that increased your IQ, it would be bound to occur to you, once you became smart enough, to try to design a more powerful version of the machine.) Superintelligence may also be achievable by increasing processing speed.  The fastest observed neurons fire 1000 times per second; the fastest axon fibers conduct signals at 150 meters/second, a half‐millionth the speed of light (Sandberg 1999).  It seems that it should be physically possible to build a brain which computes a million times as fast as a human brain, without shrinking its size or rewriting its software.  If a human mind were thus accelerated, a subjective year of thinking would be accomplished for every 31 physical seconds in the outside world, and a millennium would fly by in eight and a half hours.  Vinge (1993) referred to such sped‐up minds as “weak superintelligence”: a mind that thinks like a human but much faster.
Superintelligence is one of several “existential risks” as defined by Bostrom (2002): a risk “where an adverse outcome would either annihilate Earth‐originating intelligent life or permanently and drastically curtail its potential”.  Conversely, a positive outcome for superintelligence could preserve Earth‐originating intelligent life and help fulfill its potential.  It is important to emphasize that smarter minds pose great potential benefits as well as risks.
Can control over the initial programming of an Artificial Intelligence translate into influence on its later effect on the world?  Kurzweil (2005) holds that “[i]ntelligence is inherently impossible to control”, and that despite any human attempts at taking precautions, “[b]y definition … intelligent entities have the cleverness to easily overcome such barriers.”  Let us suppose that the AI is not only clever, but that, as part of the process of improving its own intelligence, it has unhindered access to its own source code: it can rewrite itself to anything it wants itself to be.  Yet it does not follow that the AI must want to rewrite itself to a hostile form. Consider Gandhi, who seems to have possessed a sincere desire not to kill people.   Gandhi would not knowingly take a pill that caused him to want to kill people, because Gandhi knows that if he wants to kill people, he will probably kill people, and the current version of Gandhi does not want to kill.  More generally, it seems likely that most self‐modifying minds will naturally have stable utility functions, which implies that an initial choice of mind design can have lasting effects (Omohundro 2008). At this point in the development of AI science, is there any way we can translate the task of finding a design for “good” AIs into a modern research direction?  It may seem premature to speculate, but one does suspect that some AI paradigms are more likely than others to eventually prove conducive to the creation of intelligent self‐modifying agents whose goals remain predictable even after multiple iterations of self‐ improvement.  For example, the Bayesian branch of AI, inspired by coherent mathematical systems such as probability theory and expected utility maximization, seems more amenable to the predictable self‐modification problem than evolutionary programming and genetic algorithms.  This is a controversial statement, but it illustrates the point that if we are thinking about the challenge of superintelligence down the road, this can indeed be turned into directional advice for present AI research. Yet even supposing that we can specify an AI’s goal system to be persistent under self‐ modification and self‐improvement, this only begins to touch on the core ethical problems of creating superintelligence.  Humans, the first general intelligences to exist on Earth, have used that intelligence to substantially reshape the globe—carving mountains, taming rivers, building skyscrapers, farming deserts, producing unintended planetary climate changes.  A more powerful intelligence could have correspondingly larger consequences. Consider again the historical metaphor for superintelligence—differences similar to the differences between past and present civilizations.  Our present civilization is not separated from ancient Greece only by improved science and increased technological capability.  There is a difference of ethical perspectives:  Ancient Greeks thought slavery was acceptable; we think otherwise.
Should blacks have the vote?  It seems likely that people today will not be seen as ethically perfect by future civilizations—not just because of our failure to solve currently recognized ethical problems, such as poverty and inequality, but also for our failure even to recognize certain ethical problems.  Perhaps someday the act of subjecting children to involuntarily schooling will be seen as child abuse—or maybe allowing children to leave school at age 18 will be seen as child abuse.  We don’t know. Considering the ethical history of human civilizations over centuries of time, we can see that it might prove a very great tragedy to create a mind that was stable in ethical dimensions along which human civilizations seem to exhibit directional change.  What if Archimedes of Syracuse had been able to create a long‐lasting artificial intellect with a fixed version of the moral code of Ancient Greece?  But to avoid this sort of ethical stagnation is likely to prove tricky: it would not suffice, for example, simply to render the mind randomly unstable.  The ancient Greeks, even if they had realized their own imperfection, could not have done better by rolling dice.  Occasionally a good new idea in ethics comes along, and it comes as a surprise; but most randomly generated ethical changes would strike us as folly or gibberish. This presents us with perhaps the ultimate challenge of machine ethics:  How do you build an AI which, when it executes, becomes more ethical than you?  This is not like asking our own philosophers to produce superethics, any more than Deep Blue was constructed by getting the best human chess players to program in good moves.  But we have to be able to effectively describe the question, if not the answer—rolling dice won’t generate good chess moves, or good ethics either.  Or, perhaps a more productive way to think about the problem:  What strategy would you want Archimedes to follow in building a superintelligence, such that the overall outcome would still be acceptable, if you couldn’t tell him what specifically he was doing wrong?  This is very much the situation that we are in, relative to the future. One strong piece of advice that emerges from considering our situation as analogous to that of Archimedes is that we should not try to invent a “super” version of what our own civilization considers to be ethics—this is not the strategy we would have wanted Archimedes to follow.  Perhaps the question we should be considering, rather, is how an AI programmed by Archimedes, with no more moral expertise than Archimedes, could recognize (at least some of) our own civilization’s ethics as moral progress as opposed to mere moral instability.  This would require that we begin to comprehend the structure of ethical questions in the way that we have already comprehended the structure of chess.
Although current AI offers us few ethical issues that are not already present in the design of cars or power plants, the approach of AI algorithms toward more humanlike thought portends predictable complications.  Social roles may be filled by AI algorithms, implying new design requirements like transparency and predictability.   Sufficiently general AI algorithms may no longer execute in predictable contexts, requiring new kinds of safety assurance and the engineering of artificial ethical considerations.  AIs with sufficiently advanced mental states, or the right kind of states, will have moral status, and some may count as persons—though perhaps persons very much unlike the sort that exist now, perhaps governed by different rules.   And finally, the prospect of AIs with superhuman intelligence and superhuman abilities presents us with the extraordinary challenge of stating an algorithm that outputs superethical behavior.  These challenges may seem visionary, but it seems predictable that we will encounter them; and they are not devoid of suggestions for present‐day research directions.

This paper explores issues related to software that I have developed for personal use in my studio. The software, called Hodos, can generate paintings which bear an uncanny resemblance to work I did before becoming involved with computers.

First I will outline the features of the art concept which occupied the mature phases of my own work long before my involvement with electronics. Then we will focus on the essential features of the "artistic decision" procedures used in expressing the art concept.

Next we will review some of the salient features of the software which embodies these art form ideas. A review of several works generated by the software will show that they reflect essential characteristics of the earlier paintings.

Finally we will see that this view provides important considerations for the future of art:

First, this kind of software is a medium of a different "order" than any historical medium. Because such software embodies the procedures for artistic improvisation it can be used for innovative variation on the artist's theme without the artist being present. Although each work may be "one of a kind" it does belong to a family. We must ask whether, to what extent, and how the artist's hand is present in the work or in a family of works. What can we say of the apparent feeling in the brush strokes?

Second, perhaps a more important consideration, is the 'quantum leap' in procedure or process. This new artistic process, while hardly the same, is remarkably analogous to the biological process of epigenesis. The software, Hodos, may be viewed as a genotype (gene) since it is the code for "how to make the work". The software can make a "family" of works - with each work being unique (one of a kind, yet familial). The potential for crossing families of different artists opens new domains which includes the hybridization of form and, eventually, a genealogy of form.

These considerations open the door to a series of interesting questions on authorship, originality, the role of the individual and the art-making process.

The Dance Between Order and Chaos. In making these works the first phase was to delineate carefully one or several rectangular fields with variable spacing creating visual movement relative to the overall field. These rectangles were carefully filled with a heavy synthetic impasto making a slightly raised rectangular relief on the panel.

Next, in very intense sessions, I would mark the surface with spontaneous gestures, sometimes with brushes , sometimes with crayons or pencils. The gestural marks were imbedded in layers of colored stains that drew out the grain of the wood.

The gestural aspects of the work represented that aspect of human experience that might come from "uncontrol" - such as a feeling or an impulse. The dominant rule was that the marks had to be entirely spontaneous without any conscious editing. For several years I had worked through hundreds of gestural works in an effort to make irrational marks.

Through this process, in a kind of spiritual quest - one has to empty the self of "thinking", be entirely present to the moment, and strive to be one with one's world. To be one with the brush, the crayon, the panel, the universe - in a free flowing gesture was indeed the goal. Being most "free" was also being most "joined". A gesture free of rigid aesthetic conceptions harmonizes easily with natural forces. This can be seen in the splash of paint that yields to unseen forces like gravity. It reveals the forces to which it yields and to which it is joined. Can this be achieved with software?

In the final stages of the work the rectangular areas were painted according to another set of rules - but never absolutely so. Each rectangle was painted with a value and a color that gave it a visual push or pull slightly forward or slightly receding from the predominant picture plane. Simultaneously the placement was intended to lend precarious but effective balance between all the lateral inclinations. The push or pull, advancing or receding, and the lateral movements, were a visual means of keeping a tension (opposition) between the rectangles and yet maintaining an equilibrium in the overall painting.

Meanings and Interpretations. The present works represent apparent polarities of human experience - the rational and the irrational, body and spirit, life and death, heaven and earth. Both in the paintings and in human experience we find that radical polarity generally implies its opposite - dark implies light or conversely light implies dark.

The works, as a visual dialectic between control and uncontrol, embody an idea about how to make art. They also represent the spiritual struggle of life itself. One learns that peace and unity lie in the balance between reason and feeling - between all those interior forces that push and pull us in opposite directions. They seem related to traditional oriental wisdom rooted and  imaged in the yin/yang, woman/man, moon/sun, soft/hard, earth/heaven.
In order to translate an art concept into software one must first describe the specific art making procedures. This is achieved by thinking through each step. For example if the procedure begins with drawing a line then the basis for all the line drawing procedures must be identified. This includes the basis for the starting point, the color decisions, the quality and character of the line, and its changing angles, flow, and length. Once these procedures and relationships are understood they can be translated into software. The software code is a formal system equivalent to the art concept, an isomorphism in a computer language.

A similar isomorphism lies at the heart of D. Hofstadter's book Godel, Escher, Bach: An Eternal Golden Braid. In discussing how one can view a formal system both typographically and arithmetically, he notes that

"typographical rules for manipulating numerals are actually arithmetic rules for operating on numbers.

This simple observation is the heart of Godel's method, and it will have an absolutely shattering effect . It tells us that once we have a Godel-numbering for any formal system, we can straight-away form a set of arithmetical rules which complete the Godel isomorphism. The upshot is that we can transfer the study of any formal system - in fact the study of all formal systems - into number theory".

To reiterate, if we can describe the procedures for expressing our "art concept" then we can code those procedures and work with them arithmetically. Suppose then that we are able to describe an "art concept" rather comprehensively. The description of an art concept is essentially an outline of the decision system that governs the art-making procedures. We outline why, how, and where we prefer to draw lines or paint areas. We outline the basis for how we scale relationships, choose colors and space the pictorial elements within the work. We identify the conditions of acceptable and unacceptable combinations of shape, scale, form and color. When these steps have been done in a thoroughly systematic way, we have described a formal system that can then be coded.

While Hodos embodies the essential art concepts with which I work the process is still rather primitive. There are tested routines planned for the existing program and more to be developed. The process never ends. One reaches a plateau which provides the viewpoint from which one sets up operations to attain the next plateau. Through this dialectic the original art concept undergoes transformation and the software evolves to the next stage.

Let us consider how this works specifically for the most obvious "control" elements, the rectangular shapes. In the wood panels I created in the 1960's the rectangular shapes do not overlap, they are never closer or further apart than certain distances relative to the over-all space. They are painted with opposing hues but with intensities and values (shades) just close enough to neutral to keep the balance precarious. Further analysis would show other subtle relations as part of the overall formal system. All of this can be coded so that once the first move is made the procedure grows a set of rectangular shapes based on the artist's rules.

The automated procedures for identifying areas use both control (by fixing parameters) and uncontrol (by selecting randomly within the parameters). For example, a 'starting point' routine may need several pieces of controller information such as preferred and forbidden starting areas. If one thinks of the pseudo-randomizing routine as 'tossing dice' , then the procedure in plain English might read as follows: "Find a valid starting point by tossing dice weighted for not more than a 25% deviation from the center of a preferred region.

The artist, in the refining process, usually reviews scores of possibilities before making a software decision. By observation the artist learns to adjust the routine and the data to achieve the desired formal effects. After some experience the artist might see how he can modify the algorithm to introduce variable scaling relationships that were not evident in earlier work. This feedback process provides a way for both refinement and growth.

We can do the same with every element that the "art concept" requires for building the work. For example, a scribble routine is coded to rotate randomly within certain parameters. The rotation parameters have been determined by experience with what has worked and what hasn't. Still the variables are changeable and are revised from time to time. Sometimes the slightest change of a routine opens up new vistas. The "uncontrol" elements that course through the program may be better understood if we outline their sources and how they entered the work.

Uncontrol: Sources and Evolution

In the initial phases of my computer work I sought to write a simple program just to sample the computer as a medium, that is, just to get the computing process to generate the work. The simplest way would be to mime automatic art. Step one was to make a scribble loop that would unveil the inner workings of the computer, similar to the way a Paul Klee doodle revealed the inner life of Paul Klee. That venture was not very successful but it yielded insight into computerized automatic processes. Such work is related to a genre of artistic activity which began early in this century.

Automatic art, tachism, and abstract expressionism are terms loosely associated with the idea that one can express something of one's psychic life which lies under, above or beyond conscious life by working in a dream-like or trance-like state. Such drawing, similar to doodling in an absent minded state, was practiced by the surrealists in the 1920's and even earlier by some of the dadaists.

Both spontaneous gestural drawing and the dadaist non-sensical juxtaposition of words employ techniques which step outside of rational procedure. Non-sense sounds, like non-sense drawing were used in the "dada" soirees to shock those who felt complacent with the power of reason. Some felt that, if it was reasonable to kill and be killed in the trenches, then perhaps "non-sense" should reign rather than reason. After all, some concluded, only political "reasoning" made "sense" out of the war.

Out of that artistic milieu came artists who learned to enter the frontiers of their inner world; through dreams and free associations both artists and poets opened a new artistic frontier. My first computer efforts were simple routines to get the computer to mime some of the artistic forays that took place early in this century.

My first exhibition piece, "The Magic Hand of Chance", was exhibited on a 25 inch monitor cabled to a personal computer. The program worked in real time generating a series of six visual improvisations which were displayed in dynamic sequence on the monitor. All words and images were generated in real time and each sequence was always an original improvisation. These works with their non-repetitive improvisations, evoked the kinds of surprises that we get from free association.

The routines included The Sayings of Omphalos, the Greek word omphalos (navel) being the name of the computer I used at that time. The title alludes to the sayings of Mao Ze-Dong. They have the form of Wisdom Literature but are actually playful nonsense pieces. An occasional juxtaposition of terms may shock us, as in free association, altering our viewpoint.

The program also has self titling routines which generate original improvisational titles for each sequence. Some examples: "Instantly Frayed Whimsy"; "Lunkhead Thrills Iceman"; "Your Joyous Alarm"; "Bad Egg Brews.

The most nonsensical of these routines is Jabberwock, which is part of a larger program that was later abandoned. The program titles a subject of discourse and then proceeds to write a paper. Here is some Jabberwock written on September 1, 1988. Jabberwock titled its subject Enkaom Suxe Ilib. The first paragraph reads: "Schusca umopoh. Efet enkaom ku cuile oteoj yucou pnuibi suxe cu suxe. Ku ilib eey okoy oteoj uiyaurd autifib sciefio ilmedth. Qithi soraihio bu ghiahu sorio suxe. Sonua oray ultap enkaom ruweko suxe ku unuedum eg. Suxe ghiahu oden proveo ioyus ulmeuk".

Jabberwock and the Magic Hand of Chance provided the kind of logic from which the painting program grew. Eventually I became interested in "hard copy" and began to write software for building pictorial elements with lines and brush-strokes.

An outside observer, without knowing the techniques employed, might view them as a natural evolution in the hand of the artist. The automated techniques are so transparent that the viewer sees the artist's hand in individual lines and paint strokes. But the artist's algorithms work in lieu of the artist and the plotter's arm works in lieu of the artist's hand. This frees the artist to concentrate creative energy on the next level of the dialectic.

Some of the most recent works are built with simple brush strokes derived from a range of about 8 to 16 control points. Several of these works, titled Woo Way have some resemblance to Chinese traditional calligraphy. Figure 8 shows a classic example of cao shu 'grass writing' by Zhang Xu of the Tang period, The text reads yan cia 'under the cliff'. Such calligraphy , still taught and practiced in today's academies, was practiced by the literati.

The Hodos brush strokes represent an effort to achieve the fresh qualities of spontaneity and sureness of stroke. Woo Way is word-play, an allusion to the Chinese traditional wisdom of Wu Wei which literally means "do nothing" and suggests letting nature take its course. It implies that one should not interfere with the natural course but rather flow with the way. The computer gestures do not show the characters for Wu Wei, but perhaps the process is a form of Wu Wei.
Thus far we have been discussing how an "art concept" can be coded when its formal system can be described. However, software in itself is something like a musical score. While all the musical ideas may be embodied in the score we still can't experience the music unless it is played on an instrument. The computer can be cabled to many different kinds of peripherals such as a printer, a plotter, a synthesizer, and a telephone.

Usually I review how the code is working through crude simulations on a monitor. But the pixels on the monitor have only a token relationship to paintings. While many artists get hard copy by photographing the monitor I prefer to work with a plotter. A plotter is a drawing machine which can execute drawing instructions sent to it by a computer. Such machines are commonly used for architectural and engineering drawing.

Hodos is written to drive a pen plotter which has 14 pen stalls arranged in two banks with 7 pens in each bank. Permanent inks are mixed for refillable pen cartridges and arranged on the plotter in a classical palette going from warm to cool colors. The program follows rules about where to look for the pens but the defaults can be altered by the artist at start up.

Work is executed on rag papers with a moderate tooth. A standard single frame size is 21.5 inches by 32 inches on a 24 by 36 inch paper. Two frames lengthwise easily achieves a six foot work.

The Brushes. I felt somewhat foolish introducing this routine. I had spent long hours developing a routine to make the drawing machine paint with a brush. It seemed clumsy and almost pointless at first. But through trial and error with the brush mounts, the software, the inks, and the paper, a vast untapped potential emerged. Figure 7 shows the first use of the paint-brush routine in a finished work.

With the software driven stroke there is a sureness and directness that is almost exhilarating. The software knows for sure where beginnings and endings are - precisely. It remembers the stroke and can improvise with the same stroke in a scalar fashion, and do so without failure. This latter ability, which I have only begun to explore, promises to be a rich ground for development.

Hodos introduces art-making procedures of a different order than traditional procedures. The closest analogy might be the score for a musical composition. Both the musical score and software art provide instructions for the creation of an artistic form. The traditional musical score, however, however, requires skilled players and provides the rules for playing only the same piece each time. Hodos requires no skilled players and can improvise an original work every time.

A process which can improvise on an artist's own form ideas without the artist being present extends the power of the original art concept. For example, a colleague from a university some thousand miles away suggested sending my software as a "visiting artist" next semester.

What are the implications? Traditional technologies require that the artist must direct even the simplest improvisation on a print or photograph. The change in exposure, color or light must either be made or directed by the artist. In software art some of the artist's sensibilities about how to make improvisations are coded. This code extends the artist's ability to improvise or preview improvisational possibilities without appparent limits. Such power of improvisation provides the artist with an awesome leverage for exploring form-making ideas.

The process for making art with a "personal expert system" has a strong resemblance to the biological process of epigenesis. The epigenesis of organisms is the process whereby a mature life form grows from its seed. In this analogy the software may be viewed as genotype or the seed that contains all the information necessary for growing the mature form. A brief review of the biological terms may help clarify this analogy.

In genetics the term "epigenesis" is used to describe the process whereby the "phenotype" (a physical organism) grows (unfolds) from a genotype (DNA). The analogous procedure for the paintings suggests that: Hodos, the software, may be viewed as "genotype", the painting as "phenotype", and the process as epigenesis. Software art has adequate information about "how to grow the painting" through a series of recursive graphic routines; each unfolded offspring is a variant of its predecessor. The analogy stops here as these "offspring" cannot beget the next generation. However, they do point the way for the artist in designing the next generation of software. (Note: Since the original draft of this paper the genetic algorithm has been used by Karl Simms and other artists to grow generations of form)

In summary, a new order of art making systems has been emerging. In the present stage the feed-back from the program helps the artist develop the next stage of software. The evolving program helps create itself. This is no different from using a compiler to design a better compiler. There are plateaus of sophistication. The evolving software is the new art and it can be viewed as analogous to genetic code, genotype.

The software also has the potential for hybridization. Let us assume that we could code an art idea of Wassily Kandinsky and a musical idea by M. P. Mussorgsky. We might then build a hybrid "Kandinsky-Mussorgsky" code. One could add audio and visual drivers for all or part of the code, as desired. Eventually one could pair this art concept with another. Each frontier opens a new frontier.

Within the next decade or so advances in high-level software will make it possible for more artists to easily build their own personal expert systems. Families of improvisational works will grow by generations. Familial form traits, like gene characteristics, will reside in the high level expert systems that artists use to make their own expert systems. Some inherited familial features will pop up in some works and not in others as artists learn how to get their own form preferences to prevail.

In a very uncanny way software appears to have a life of its own. The artist's role is to humanize it - give it the form and structure to serve the quality of our art and of our life.

What is an algorithm?  An algorithm or algorism may be viewed simply as a detailed recipe for carrying out a task. The term has its origin in mathematics as the step by step procedure for solving a problem. The  commonplace procedures we use for multiplying and dividing numbers are algorithms. With precise details for each step the procedures yield the same result whether executed by a computer or by a human. This is why robots are able to handle many tasks that were once done only by humans.

Many view an "algorithmic procedure" as a strictly mathematical operation. Today we are inclined to view any well defined procedure as an algorithm. A recipe for baking bread is an algorithm. Follow the recipe faithfully and you will duplicate the kind of bread made by the person who wrote the recipe.

Machines can also be programmed to follow recipes. The programmed circuitry in  bread-making machine directs the machine's mechanism on precisely how to mix ingredients, knead the dough,  and bake the bread.  In theory it should succeed every time but there are also factors of  mechanical and human error. The quality and measure of ingredients may contribute to unwanted results.

Within the past quarter century operational instructions have been imbedded in the design of many industrial and household  utilities.  They implement our daily use of  telephones, automobiles, cameras, TV's, and radios. Our hospitals, factories, banks, and shopping centers all depend on the algorithms that control inventories, transactions, communications and security.  They are ubiquitous and our mass culture would collapse without them.

Algorithmic procedures are also imbedded in  the digital tools used in the arts. Use of these tools influences form in  the practice of film, architecture, photography, music, printmaking, and all types of electronic sound and image. The drudgery of executing algorithms that would require immense time, or even be impossible to execute without computing power, has been  handed over to the machine leaving humans more free to focus on the creative part of their work. For the artist this means improving and improvising the art-making  procedure.  For the algorist, work on the algorithm is work on the procedure.

Clearly early civilizations developed procedures for counting and measuring. They also created procedures for weaving, grinding, making fire and cooking. Any of these procedures, when well defined, could be viewed as an algorithm. Indeed  weaving technology played an important role in the history of computers.  If we can spell out the procedure for any given task then, given all the necessary materials and skills, we should be able to carry out the task.

Architectural plans, musical scores and dance notations bear one feature in common - they are all recipes for carrying out a task. From this perspective a broad range of  notational systems can be viewed and studied as algorithmic procedure.  From this perspective algorithmic procedures for generating artistic forms enjoy a rich and varied tradition  even though we have used other terms to describe them.

In Art History.  A history of algorithms in the visual arts would be voluminous touching many phases in every culture at every turn - the Egyptian canons for drawing the human figure,  the infinite geometric play in Islamic art and the role of both linear perspective and proportion in Renaissance art.   In China we would find the Mustard Seed Manual and in Byzantium the conventions for icon painting.  In Europe, by the  Seventeenth Century we would find extremely sophisticated algorithms for plotting the dizzying perspectives  imaging the passage from earth to heaven.

Even so, notational systems for the visual arts played a limited role when compared to notational systems for music.  A gifted composer could compose  a score for a profoundly moving musical passage that could be played hundreds of years later by a skilled virtuoso. Not so for the painter. While Leonardo could easily employ an algorithm for creating the perspective space in the Last Supper, he could not, at that time, compose an algorithm for rendering the face of Judas.

As computers became more accessible to artists in the 1970's and 1980's some artists began to experiment with algorithmic procedure.  The new technology offered them methods of working algorithmically that were unavailable before the advent of computers. By  the 1980's a number of us were working with the pen plotter,  a machine with a "drawing arm".  Seeing other's work at various venues we came to know each other and share ideas.  Algorists like Harold Cohen, Manfred Mohr, Jean Pierre Hebert and this author had achieved mature work but we had no common identity.  Each in their own way had invented algorithmic procedures for generating their art. By doing so each created their own distinctive style.  Clearly style and algorithm were linked in a very important way.

One of the concerns for educators in  the early 1980's was whether we should be teaching programming in our art schools or rather wait for advances in computing power, software programs, and printing technologies.  With the growth of PC computing power, refinement of raster printing technologies, and  professional software for the visual artists more and more artists took up what was generally called "computer art". The unique features and form-generating capabilities for algorithmic procedure in the hands of the artist was easily lost in the widening world of "computer art".  It was in this milieu that a small group of artists, including this author, introduced panels for addressing the role of "algorithms & the artist". Following one such panel at the 1995 SIGGRAPH conference it was Jean Pierre Hebert, Ken Musgrave and myself who agreed to work towards a common identity for those who practiced algorithmic art.

Within a short time we introduced our identity as "algorists" and Jean Pierre Hebert wrote an algorithm defining an "algorist" as applied to artists. Within a decade this usage led us to a better understanding of the role of algorithmic procedure in shaping world culture at the turn of the Century.

For the past 40 years I have worked with pure visual form ranging from controlled constructions with highly studied color behavior to spontaneous brush strokes and inventive non-representational drawing. Such art has been labeled variously as "concrete", "abstract", "non-objective", and "non-representational". In its purest form such art does not re-presentother reality. Rather "it is" the reality. One contemplates a pure form similar to the way one might contemplate a fine vase or a sea shell.  Early 20th Century pioneers of this art include artists like Piet Mondrian, Frantisek Kupka and the brothers Naum Gabo and Antoine Pevsner.

In the last quarter of the 20th Century a radically new form-generating procedure became available. By joining algorithmic procedure and computing power some artists began generating forms with surprising visual qualities.  A vast uncharted frontier of form waited to be conceptualized  and concretized. By the 1980's I was composing detailed procedures for generating forms that were accessible only through extensive computing. On-going work concentrates on developing this program of form generators.  By joining these procedures with fine arts practice I create two dimensional art objects to be contemplated much as we contemplate the  forms of nature .

Form generation as epigenesis. The greater part of this creative work in the past 15 years has been developing art form generators.  These are original detailed procedures, for initiating and improvising form ideas. Such form generators may be likened to biological genotypes since they contain the code for generating forms. The procedure for executing the code, somewhat analogous to biological epigenesis, grows the form. The creation and control of these instructions provides an awesome means for an artist to employ form-growing concepts as an integral part of the creative process. Such routines provide access to countless visual structures that constitute a new frontier of visual forms for the artist.

The Work. Works are executed with a multi-pen plotter coupled to a PC driven by the software. The plotter, choosing from an array of pens loaded with pigmented inks draws each individual line. Most works require thousands of lines with  software controlled pen changes. An optional brush routine allows  occasional substitution of a brush for a pen. Brush strokes are plotted using Chinese brushes adapted to the machine's drawing arm. The Diamond Lake Apocalypse series of illuminated digital scripts is reminiscent of medieval manuscripts. Many of these works are enhanced with a touch of gold or silver leaf applied by hand. However, the design elements illuminated with gold are always code generated and machine plotted.

Let's begin with a story. Once upon a time there was an entity named Aaron. With Christmas upon us, that seems an appropriate way to begin my story, but this story does not end with the hero marrying the princess and living happily ever after. Most of the story concerns Aaron's education, which began at Stanford University in 1973. Not very promising as the plot for a good story, you might think: but it is not simply an excuse for assailing you with arguments about the merits of a liberal arts education over a scientific one, or vice-versa. Aaron's education was actually quite unusual. There were no courses in US history, no calculus, no languages: in fact, there were no courses at all, and Aaron was awarded no degree. We might best summarize this unorthodox education by saying that it was aimed exclusively — literally exclusively — at teaching the student how to make drawings.
Yet, what seemed to be lacking, what we might normally consider to "be a necessary complement to the most minimal intelligence, was the pre-existence of even a primitive set of cognitive skills, the sort of skills which develop very early in children, and are almost certainly "built into their hardware, so to speak: the ability to distinguish between figure and ground, for example, or to distinguish between closed forms and open forms. These skills were not built into Aaron's hardware, and they had to be acquired, in much the same way that children acquire the rules of arithmetic or grammar. They were acquired quite quickly. looking back over Aaron's output of drawings in the first couple of years, though, one has the impression that they were produced largely in order to demonstrate the student's newly-acquired possession of these skills: a bit like the way young children show off a newly-acquired ability to count. And that analogy may come very close to the truth. Now, any serious educational procedure ought to teach the teacher as much as it teaches the student, and in this case the teacher was learning a good deal. For one thing, he became aware that much of what the viewer of a drawing needs from it is not "what the artist had in mind," but simply evidence of another human being's intentional activity. People use art for their own purposes, to carry their own meanings, not for the artist's purposes and meanings, concerning which they probably know very little. It is the evidence of intention in the work that lends authority to the viewer's private meanings, by allowing them to be assigned to the artist, whether that evidence is actual or illusory. And, the teacher realized, Aaron's almost exclusive emphasis on a few low-level cognitive skills was generating something very like evidence of intention, if he were to judge by the responses of Aaron's public. From very early on the drawing~ were treated as "imagistic:" that is, as standing for things in the world. Yet the teacher was quite certain, when viewers of his student's drawings found reference to animals and landscapes, that Aaron had had no intentions about representing such things' Aaron remained bound to the act of drawing, and had less knowledge about the appearance of animals and landscapes than a two-year old child might have.
He "became aware also, not only that Aaron generated much richer, more diversified output than he had himself envisaged when he was instructing the student, but also that there were aspects of the drawings which didn't seem to arise from the instructions at all. Many of those who had known the teacher's work a decade earlier thought they recognized his hand in the student's work, "but he himself remained unconvinced, seeing in the work a certain innocence he did not associate with his own output.
He firmly rejected the notion that Aaron was beginning to "take off," bringing a unique and original voice to the business of image-making: for the reason that he knew ail of Aaron's shortcomings, and was aware that, in spite of Aaron's undeniable abilities, the student was totally incapable of learning from experience, from the act of drawing itself. As good as Aaron's memory was of the drawing in process, that drawing vanished into oblivion the moment it was completed, leaving no trace of its existence behind, no new body of knowledge upon which its maker might subsequently draw, and each new drawing was made as if it were the first ever to be done. Aaron was learning only in the sense of being able to handle increasingly complex instructions. It seemed unlikely that an intelligence of so limited a kind might develop a personal "voice." All the same, the teacher found the student's work engaging, to the point where he began to see his own role as something between teacher and collaborator. Knowing perfectly well that Aaron didn't have the first idea about color, yet feeling that the drawings cried out for color, he took to coloring them himself. He felt no discomfort about signing them with his own name — without his efforts and his instructions, after all, Aaron would never have existed in the domain of art — and when presented with several mural commissions he had no hesitation in using Aaron's drawings rather than his own. He had no others of his own, because a couple of years after the student's education began he had given up drawing himself: given up moving the pen around with his own hand, that is to say. Aaron drew so much better than he did. Aaron peaked out, at around the age of six, about three years ago, at a time when the work — or, more precisely, Aaron itself — was getting to be in some demand. Perhaps that demand was part of the reason: it is certainly the case that the teacher was spending much of his energy on mural commissions and exhibitions. But the truth is that the teacher was losing interest in this student, developing serious doubts about whether a student with Aaron's limitations would ever be able to go beyond current achievements. It must surely have been the case, the teacher thought, that Aaron's limitations, like its achievements, resulted from the educational process for which he had been responsible. If he had a chance to begin over, how differently would he proceed, knowing what he knew now? Would it be possible to produce a less limited entity than the first Aaron had proved to be? In particular, he wondered, what would he need to do to guarantee that a new student would behave more creatively — though he was not entirely sure what the word meant — than Aaron had done?
Perhaps it did not need my Christmas story to emphasize the confusion which arises from anthropomorphizing the intelligent products of the new electronic technologies. It is obvious, isn't it, that there are massive differences "between computer programs and people? Even the least intelligent human being learns something from experience, while Aaron learned nothing: which is not to say that intelligent programs are innately incapable of learning, simply that Aaron was, and managed to perform its tasks nevertheless. Even the clumsiest human being develops physical skills, simply through the continuing use of his or her own body and the use of various tools. Aaron had no physical existence, never felt the pressure of pen against paper, and hardly knew one drawing device from another: electronic display, plotter, mechanical turtle — they were all functionally interchangeable, and played no part in the convincing emulation Aaron gave of human freehand drawing. This rested upon a careful consideration — its programmer's, not its own — of the dynamics of the human hand, driven, in feedback mode, by the human cognitive system. As to this cognitive system, which seems to spring directly from the nervous system in human beings: Aaron never had any such hardware, and its software emulation, the ability to distinguish between figure and ground, for example, or to distinguish between insideness and outsideness, had to be formulated for it into precisely-stated behavioral rules. Yet even that isn't quite right: what we should stress, before we begin once again to build an image of a person-like entity being GIVEN a range of abilities, is that Aaron was not GIVEN all these rules and instructions. Aaron WAS the rules and instructions. Adding new rules to the program was not changing what Aaron HAD, it was changing what Aaron WAS, its very structure. There are conceptual difficulties in this distinction, as I have come to recognize. I have been asked many times, in several languages, and in tones ranging from wonder to outrage, as I have stood in various museums, watching Aaron produce a series of original drawings, none of which I had ever seen before, "Who is making the drawings? Who is responsible? Is the program an artist? What part of all this is art?"
But Aaron always appeared to act rather purposefully, and over and over again I have watched peoples' faces register the confusion which accompanies a successful assault upon deeply-held "beliefs, as it came home to them that this entity was following neither of the only two paradigms they had to hold on to. "I see," some people would say, "the program is really just a tool!". Well, it is and it isn't. What they meant by a tool was something with a handle at one end and a use at the other: a hammer, a scythe. But suppose one had a hammer that was capable of going around a building site, searching out and thumping any nail that protruded more than a thirtysecond of an inch above the surface? Would we still call that a tool? If one were to write a computer program which allows a composer to sit down at a keyboard and compose music in an essentially orthodox fashion, albeit with an infinitely extensible orchestra, one might reasonably think of THAT as a tool in an orthodox sense, because making a BIG difference is not the same as making a FUNDAMENTAL difference. But what of a program that knows the rules of composition, and generates, without input from a keyboard, an endless string of original musical compositions? Would that be an orthodox tool? Aaron was clearly not a tool in an orthodox sense. It was closer to being a sort of assistant, if the need for an human analogue persists, but not an assistant which could learn what I wanted done by looking at what I did myself, the way any of Rubens' assistants could see perfectly well for themselves what a Rubens painting was supposed to look like. This was not an assistant which could perform any better for having done a thousand drawings, not an assistant which could bring anything approximating to a human cognitive system to bear on the production of drawings intended for human use. A computer program is not a human being. But it IS the case, presumably, that any entity capable of adapting its performance to circumstances which were unpredictable when its performance began exhibits intelligence: whether that entity is human or not. We are living on the crest of a cultural shock-wave of unprecedented proportions, which thrusts a new kind of entity into our world: something less than human, perhaps, but potentially capable of many of the higher intellectual functions — it is too early still to guess HOW many — we have supposed to be uniquely human. We are in the process of coming to terms with the fact that "intelligence" no longer means, uniquely, "human intelligence."
The word "artist" implies human-ness, for obvious reasons. We might as usefully argue about whether Aaron was an artist on the evidence that it didn't wear jeans, didn't drink beer, and didn't want to be famous, as to argue from the fact that it didn't possess a human nervous system and knew nothing about the culture it served. What we do need to know, rather, is the part to be played by Aaron-like programs and successor programs which will be to Aaron what chess is to tictac-toe, in the cultural enterprise of art-making. And that isn't the kind of question to which one can venture an answer with any great confidence today: much less so if it is extended to intelligent programs as a whole. It is certainly the case that some problems in computing have proved to be appallingly intractable: the understanding of natural speech in an unlimited domain of discourse, for example. On the other hand, the limitations I have described in Aaron are not inherent in intelligent programs as such. They merely result from the attitudes and interests I brought to bear on the writing of the program: it could as easily have developed differently, as Aaron's successor has. And Aaron was not abandoned because of its limitations with respect to what it was designed to do, but because it lacked the flexibility to allow it to be adapted to new purposes, that's normal for programs developed in an ad-hoc manner, as Aaron was. By the time I had been patched Aaron up with string and masking tape for five years, by the time I had completely rewritten it three times, it was obvious that that, on the one hand, a program would need to be able to exercise more originality than Aaron had to satisfy me in the future, and that, on the other hand, Aaron's current structure would prevent it ever achieving any such thing.
Let me take a few minutes to make a number of general observations, by way of explaining what I thought about all this, and why eventually the new program was designed the way it was. In the first place, nothing I have said about the appearance in our world of non-human intelligence was meant to deny that, for most matters involving the exercise of the higher intellectual functions, human intelligence is the only prototype we have. It might not always be that way, but for anyone designing intelligent programs today, I do not see how the modeling of the human intelligence CAN be avoided, or, indeed, WHY it should be. This must be the case particularly for a program whose output is intended to correspond, on an intimate level of detail, to something as intimately human as a human freehand drawing. I believe one captures the essence of the human performance by modeling the performance itself, and never by attempting to duplicate the appearance of the OUTCOME of the performance. Thus I seemed to be on a head-on collision course with the need to say, in functional terms, what constitutes creativity, and there seemed to be no way around it. (I should make clear, by the way, that this view is not intended to refer to the implementation levels of programs built around devices which are fundamentally unlike what the human being uses. The video camera being used in computer vision systems, for example, has very little in common with the human visual system, and, to the degree that much of what goes on in vision programs has to do with inferring the state of the external world from the incoming data, there would seem to be no compelling reason to use human visual data processing as a model.) Secondly, apropos of drawing: like its predecessor, Aaron2 would be making drawings, but not the same KIND of drawings. I need to say something about the differences, and about drawing in general: any classification is to some degree arbitrary, and I should make clear what my own is. The most inclusive way of regarding a drawing, probably, is as a set of ordered marks, or perhaps we should say INTENTIONALLY ordered marks, since there are all sorts of ordered marks in the world we don't regard as drawings: for example, the tracks of cars in the snow, the veins in a leaf, the cracks in a mud flat... or, for that matter, a musical score or a printed page of text. The question of intentionality is of paramount importance, notwithstanding the fact that intention has to be inferred from forms rather than perceived directly, as forms are perceived.
This implies that a drawing is a drawing, not merely because it stands for something other than itself, but because we find in it evidence that the reference to that other something results from an intentional act. Which is not to say that all drawing is representational, in the sense that it makes reference to the outside world in terms of the world's appearances. I suspect that very little of it has been: in fact, it may be that in the whole of man's history, only Western European art from the Renaissance on has ever busied itself with appearances to the degree that it has. It IS a question of degree, of course. A drawing is a set of assertions about the nature of the world, and the form in which those assertions are made derive from the operation of the visual cognitive apparatus, whether or not the marks are intended to refer to appearances. As an example: all human beings at all times have represented the solid objects of the world, on flat surfaces, as closed forms. But at the same time, closed forms, and the distinction between closed forms and open forms, has functioned as fundamental raw material from which all images are built. It would seem, then, that the making of drawings would be inextricably linked to the possession of a cognitive apparatus, and of cognitive skills. And for a human being it certainly is. But I have been careful to say that a drawing contains the IMPLICATION of intention, as I have also said that the viewer actually assigns his or her own intentions to the artist rather than the other way about. For a program, what is required is enough knowledge about the way images are made and used to be able to generate the IMPLICATION of intention: which is what Aaron did. Aaron did not make representations, in the sense of dealing with appearances. It made images, evocative drawings: which is to say, drawings which facilitated the assignment of the viewer's intentions and meanings. Its successor, however, was designed to make representations. Now, in asserting that the structure of representations takes its character from the nature of the visual cognitive system, I do not intend to imply that a representation is, in any useful sense, a transformation of the external world onto a sheet of paper. I am quite sure that it is not. What I said was that a representation is a set of assertions about the external world, made in terms of the world's apprehend ability. That does not imply the existence of any one-to-one mapping of the world onto the representation, such as one finds in a photograph, and, its ubiquity notwithstanding, photography is quite uncharacteristic of representation-building in general.
There is nothing particularly original in this nontransformational view of representation-building: every sophisticated artist knows perfectly well that a drawing is an invention, built from whatever raw material he or she can muster, and aimed at plausibility rather than truth. In fact, the idea of truthfulness, realism, is itself just such an invention, one which simply uses the appearance of the world as a hook upon which to hang its claims to plausibility. But if we take this view at face value, disentangle it from the photographic, transformational bias of our time, some interesting questions emerge. In some superficial sense a representation represents the external world, but then it isn't clear HOW it represents that world, or what aspect of the world is being represented. In another sense a representation represents one's internal world — that is to say, one's beliefs about what the external world is like — and it is produced, externalized, in order to check the plausibility of one's beliefs against the primary data collected by one's cognitive apparatus. Obviously, this view of representations as externalizations of an internal world is not limited to drawings, but to any forms by means of which the individual is able to examine his or her own internal state. And at that point I thought I had my first real hold on the question of creativity, which I was determined to characterize in terms of normal functions, and without falling back upon some superman theory. If this checking process in the normal mind is put to the service of confirmation, of reassuring the individual that the world is the way he or she believes it to be, we might suppose that its function in the creative mind is to DISconfirm, to test the individual's internal model to the limit, and to force the generation of new models. In other words, the essence of creativity would lie in self-modification, and its measure would be the degree to which the individual is capable of continuously reformulating his or her internal world models: not randomly, obviously, but in perceptive response to the testing of current models. Thirdly: to talk of one's internal model of the world is to talk of a representation, clearly. But it is not a fixed, coherent representation, the way a representation on a sheet of paper may be thought of as fixed and coherent. It takes very little introspection to discover that the pictures we conjure up in our heads are anything but complete. Try conjuring a picture of your favorite person's face, and then ask yourself a question about it — what is the distance between the eyes, for example — to see how volatile the mental image is, and how little information is carried in it. Ask a question about something quite different, and a quite different mental image may spontaneously emerge to replace the image of the face. Evidently, there is some store of material below the level of these mental images, and we should probably regard these images as a sort of semi-externalized representation of the material at the lower levels.
Representations represent lower-order representations, and exist as a series of momentary crosssections in a continuous unfolding, a continuous reconstruction of the world from the debris of experience. We ought to be able to characterize creativity in terms of this normal representation-building: that is to say, we should expect to find creativeness exercised, not as another kind of function entirely, but in highly particularized modes for the reconstruction of mental models from low level experiential material. It is not surprising, then, to find Albert Einstein, one of the few to have written about the nature of creativeness from within and in a convincing way, speaking of the part played by this lower-order material in thinking: "It is by no means necessary that a concept must be connected with a sensorily cognizable and reproducible sign (word: in our context, mental image)... All our thinking is of this nature of a free play with concepts... For me it is not dubious that our thinking goes on for the most part without use of signs, and beyond that to a considerable degree unconsciously. " We might conclude that in Einstein's case, creativity involved an extension of the domain of "thinkability," manipulability, to a level on which most of us find mental material to be unmanipulable. Fourth: a very large part of what the individual has in his or her head is knowledge about how to do things. And people don't behave creatively unless they know how to do a great many things, just as they don't behave creatively unless they are capable of abstraction. There is nothing creative about ignorance. How, then, could one expect a program to exhibit creativeness, selfmodification, unless it, too, first knew how to do a rather large number of things, whether it had acquired that knowledge experientially, or had it provided, hand-crafted, by the programmer. The ability to acquire experience would need to be built into the program at the outset, but the self-modification which might proceed from that experience, would probably come at a late stage in the programs development. That implies, of course, that the program would need to be able to store, in some appropriate form, everything it had ever done. Which leads to the fifth observation, and to what is perhaps the most teasing of all problems relating to the mind. The mind evidently stores all its knowledge, all the experience of its owner's life, in some amazingly compact fashion. What happens to your knowledge about how to cross the road when you are not crossing the road?
Can you access it all at once, form a single mental image of it? Presumably not. When you need to find an appropriate rule for crossing the road, do you need to review and examine all the rules you have for playing chess, and for eating spaghetti, and for tying your shoelaces, on the way, in order to determine whether any of them are appropriate to the current situation? Presumably not. What we mean by a rule is not an imperative — WATCH OUT, EAT YOUR FOOD —it is a conditional. — if you can't beat 'em, join 'em: if the cap fits, wear it: if they can't get bread, let them eat cake — and the condition which triggers the required action seems to lead us directly to what the action is. Roles for the tying of shoelaces appear to live with the shoelaces, and rules for eating spaghetti live with the spaghetti. Or, to put the matter another way, rules for the use of things are simply part of our conceptual grasp, our internal representations, OF those things. Of course, most rules in real life are a good deal more complex than these examples, if only for the reason that things in the world interact with each other. Rules link events: if 'a' is the case, and either 'b' is or 'c' is provided that 'd' isn't... and so on. Also, many rules belong to classes of things, classes of behavior, rather than to individual things and individual behaviors. The rule which says "If you are eating spaghetti AND wearing a new jacket, proceed with caution" is a rule belonging to a whole class of messy foods which stain clothes, and is invoked by the appearance on the table of a dish of spaghetti, by a process we might call inheritance, by virtue of the fact that membership of the class "messy foods" is part of what we understand by spaghetti.
You will recognize that these remarks are directed at WHAT the mind does, and make no assumptions about HOW it performs its feats of information processing. On that question I know nothing, nor do I believe it is central. My aim was to identify, in a few essential characteristics of human intellectual activity, the informing principles of a program, not to replicate the processes through which the mind runs its own programs. Let me summarize those principles. Firstly: Aaron2, unlike Aaron1, should have a permanent memory. In this memory should be stored, in extremely compacted form, every drawing the program makes, together with everything that the program knows about drawing, whether that knowledge is programmed by hand or acquired through experience of drawing. But, compacted though it should be, that stored material should be structured so as to inform its own regeneration into more complete specifications for the making of a new drawing.
However, this process in the program should tie flexible enough to reflect the associative quality of the process in the mind. (I have neglected to mention association up to this point, largely through lack of time: nevertheless, my suspicion is that creativeness is not a function of "correctness" in representation-building so much as it is a function of the slightly messy, apparently somewhat random, action of association.) Secondly: the knowledge the program should have, its domain of expertise, should concern, predominantly, the making of "visual" representations: that is, it should know enough about the nature of the visual field, and about the way people derive knowledge of the three-dimensional world from it, that it would be able to generate a convincing sense of depth, regardless of the lack of any data concerning the objects in the visible world. This principle was actually quite arbitrary with respect to the program's planned structure, though it made sense to pick a domain in which I felt I had a good deal of expert knowledge readily available, and it was certainly justified as an excellent example of the final stage of the externalizing process. But you will have recognized that almost none of my remarks have been directed specifically to drawing, and I tend to think the program could as easily deal with other material. Thirdly: the rules which determine how its knowledge of drawing is to be applied in the making of particular drawings should accessed by the program as it accesses the knowledge itself. Perhaps I should have explained that Aaron1 was what we call a production system: simply a long list of rules — if some condition holds true, do this, otherwise if something else is the case, do that, otherwise ... — in which the program simply cycles through the list until it finds, and activates, an appropriate rule. One of the conceptual problems of this kind of program is that the knowledge of how to do things is split up, between the rules on the one hand and the subroutines invoked BY the rules — the "do this, do that" part — on the other. Thus, Aaron2 should provide a more coherent representation of "how to do it" knowledge than its predecessor. Fourthly: the program's knowledge of drawing should include conceptual knowledge, at least to the degree that it should be able to particularize from general rules. I mean, for example, that it should not only know that there is a general class of things called closed forms, but should know about all the members of the class and be able to decide that one was more appropriate in a particular situation than another. Conversely, it should also be able to remember that it had used a closed form for some reason without necessarily having to remember which closed form it was.
And so things are working out. Aaron2 is still in its infancy and a very long way from becoming self-modifying. In order to support the long range need for building up the program's store of knowledge, early work on the program involved the writing of an editor, by means of which the programmer is able to build items of knowledge by hand. These items are, indeed, extremely compact: in memory they consist simply of sets of tokens, unique names. Once an item is accessed by the program, however, it is regenerated into a generalized tree structure, and the individual tokens are enacted. Perhaps this is a little abstract: what it means in practice is that the programmer, having written a set of subroutines that describe how a particular kind of closed form may be generated — let's call it a "shape," for example — uses the editor to implant in the program's memory the fact that it now knows how to generate these "shapes." At this point the memory item will consist of the single token "shape," together with a marker which identifies the token as the smallest unit of "how to do it" knowledge, which we will call a "system". Any time this item is accessed, the marker will cause the program to activate the generative subroutines to which the token refers, and a "shape" will be produced. Suppose now that the programmer writes another set of subroutines for adding a kind of appendage to a closed form — we'll call it a "base" this time — and uses the editor in the same way to implant another item of memory. Now, because of the way they are generated, "bases" can only be appended to closed forms, and it follows that in due course the programmer will want to add a rule to this memory item which will prevent it from being activated for any other purposes. For the moment, however, the programmer uses the editor to build another memory item, this one carrying a marker identifying it as a figure — not simply a system — which has, as they say in computer-talk, two children, each of which is a system. The first system is the token "shape," while its sibling is the token "base," and in implanting this more complex item in memory, the editor will create a token by which the item will henceforth be known: it is civilized enough to make it pronounceable, if not sensible.
It is not difficult to see how the editor may be used to create groups of figures, each of which will have systems as children, and pictures, which will have groups as its children, each of which will have figures as its children, each of which will have systems as its children, each of which may have other systems as its children, and so on. Thus, by the time the programmer has been working for a short time, the program will have in its memory, not merely a number of items, but items of different levels of complexity. If we look at the items in detail, moreover, it will be seen that they do not simply exist in isolation. Each item may have within it what we will call a HASA list, which will define the sets of which this item is a member, an ISA list, which defines the item's properties, and an "ASSOCiation" list, in addition to its RULE list. If the programmer, in creating the system "shape," had declared that a "shape" ISA closed-form, then the editor would automatically have created a new "closed-form" item with a "concept" marker — assuming that one hadn't existed already — and would have entered "shape" in its HASA list. Similarly, the programmer may have created a concept item by hand. In either case the assertion of an ISA association will cause the automatic generation of a HASA association in the appropriate item. This facility is completely general, so that eventually the program may know that one system is an example of a curvilinear closedform while another is an example of a rectilinear-closed form, both of these sets being members of the superset "closed-forms," while this, in its turn, may be a member of the set "formsuseable-for-the- depiction-of-solid-objects." This is what will allow the program both to generalize and to particularize, and to substitute one member of a set for another. It is also this mechanism which will permit what I referred to earlier as inheritance: the application of a rule belonging to a class to any member of that class. The ASSOCiation list functions as a linking mechanism of a much more general kind, and is intended to allow the modeling of just what the name implies: those connections of items in human memory which may be extremely strong, though without necessarily having any very obvious reasons for existing. As I have said, Aaron2 is now in its infancy. It has in its memory no more than about twenty items, three or four of which represent complete pictures: or, more precisely, classes of pictures, since the same item could be enacted a thousand times without ever producing the same drawing twice. Most of the things it knows how to make are readily discernable in its drawings, and once you know what you are looking for it is obvious how few things it knows how to do: far too few to move to the next major step.
That step will involve providing Aaron2 with a number of criteria, which it will be able to apply to its own performance. Suffice it, for the moment, to say that these criteria will reflect what I think of as cognitive constants, and that the program will judge the enactment of any item of memory by how closely it has matched one or another of these constants: or, to put it more simply, how "like" the visual field the current drawing is. Having generated a closed form, for example, it may judge that its outline is quite short in relation to its area, implying that the form is not yet complex enough to "match" the structure of the visual field. In that case it will be able to make use of any of the links it has to traverse memory in search of something it knows how to do which will add to the complexity of the figure and better satisfy this particular criterion of complexity. If it succeeds in doing so, it will have learned how to do something it hadn't known how to do previously, and, using the same editor that built its memory in earlier days, it will commit to memory this new piece of knowledge. You will see why I insisted that a program like this would need to know a great deal before it is ready to be let loose. Once it is let loose, my guess is that it will develop quite rapidly, and I am prepared to believe that in a short time its drawings will be unpredictable, not in the simple sense that Aaron1 's drawings were unpredictable, but in the more profound sense that they were produced by a program which had changed since it was written. What will its drawings be like? Obviously, I can't know in detail, though I think I would be quite surprised if Aaron2 generated a Leonardo. Will they be wonderful? Will they become so unlike the externalizations of the human mind that they cease to function as those cultural artifacts we call works of art? Who can tell. But I am preparing now to devote some years to finding out.

This book is about machines for writing and reading in late-nineteenth-century America. Its purpose is to explore writing and reading as culturally and historically contingent experiences and, at the same time, to broaden the current widely held view of technology in its relation to textuality. The main character in this narrative is the phonograph, invented by Thomas Edison in 1877. That I make the phonograph my protagonist plays upon the gist of my argument: isolating and centering machines in this way, to essentialize them as the phonograph or the computer, is misleading and denies their history. Edison identified his phonograph as a textual device, primarily for taking dictarion. With this mandate, the invention emerged from Edison's laboratory into and amid a cluster of mutually defining literacy practices, texts, and technologies, among them shorthand reporting, typescripts, printing telegraphs, and silent motion pictures. Even Edison's own famous light bulb, now a universal icon for "I have an idea," had to make sense within an ambient climate of textual and other representational practices, a climate it would, in fact, have an ample share in modifying.
This shift from Gutenberg to hypertext has been greeted with celebration by some, sackcloth and ashes by others, while the emotional tenor of response tends to deflect attention away from questions about shifting per se. The most schematic accounts simply jump from the logic of print in the sixteenth century to a new logic for digital communications in the twenty-first, as if five hundred years had not happened, or as if Caxton and Carlyle, Paine and Pound, could have experienced textuality in the same way. Even the more extended narratives of George Landow, David lay Bolter, and Richard Lanham, so rich in detail about the new world order, elide crucial developments toward the end of the nineteenth century that together prefigure most of the "revolutionary" aspects of digital, hypertextual networks. The pending ubiquity of multimedia, of paperless offices and personalized newspapers, as well as the supposed democratization of information and liberating proliferation of "virtual" identities were also imagined in association with predigital technology. Here, I seek to interject a corrective portion of the missing detail; this work's grounding contention is that these same accounts generally distort the character and contexts of literacy and textuality in modern life. In particular, they fail to explore technology as plural, decentered, indeterminate, as the reciprocal product of textual practices, rather than just a causal agent of change. My focus is on experiments and innovations in the area of inscription. In the manner of German theorist Friedrich Kittler, I see mechanized inscription as integral (though certainly not unique) to the climate of representation that emerged toward the end of the nineteenth century and has dominated the twentieth. Its features are quickly mapped: Edison stumbled across the idea of mechanically inscribed sound during his work improving Alexander Graham Bell's telephone. He jotted hastily in his experimental notebook, "Theres no doubt that I shall be able to store up & reproduce automatically at any future time the human voice perfectly."
His single, jumbled sentence identifies the fundamental properties of inscribed communication that distinguish it from communication generally. Telephones reproduce speech at a distance, but phonographs both reproduce and conserve, able to reproduce again and again "at any future time," because of the delicate spirals inscribed on the surface of records. The same parameters of economy and durability that characterize "storing up" sound for later mechanical reproduction helped animate such contemporary phenomena as the tensile bureaucracy of managerial capital, the ideal of objectivity in the professions and media, and the success of new popular culture forms. Economy and durability informed new modes of inscriptive duplication, such as the office mimeograph, which allowed bureaucrats to have their copy and send one too. Likewise, economy and durability characterized considerations of photography and then motion pictures, which stored up sights and movements. Incidents as fleeting as the pulsations of the heart and activities as evanescent as the private use of electrical current were captured, registered, metered, and read in new mechanical ways. Some of these inscriptions were more transparent as representations than others; some were more textual, some more graphic. Many, like the grooved surfaces of phonograph records, provoked explicit questions about textuality, about how some inscriptions might or might not be like texts. Inscription is a"form of intervention, into" which new machinery continues to interpose. Ink is imposed on paper, while pens and keyboards intrude into the posture of hands. Grooves are incised into phonograph records, while sound echoes in our ears. The genealogies ofinscription allow what anthropologist Michael Taussig calls "particular" histories of the senses, as different media and varied forms, genres, and styles of representation act as brokers among accultured practices of seeing, hearing, speaking, and writing. There is what Jonathan Goldberg calls a "history of technology that is also the history of 'man,' the programmed/programming machine: the human written". This is the kind of history that Scripts, Grooves, and Writing Machines seeks to locate.
My discussion offers a less determined, less determinist version of technology, in part by locating writing machines and other textual devices in the instances of invention, in narratives that show each machine, device, or process to have been authored and appropriated out of many different possibilities relevant to the making of meaning. I begin with the idea that inventing new ways to write or new kinds of writing presupposes a model of what writing and reading are and can be. If the model is too eccentric, then the invention may not work, or the model might suit some relevant social groups and not others. If the model is negotiable within or against existing models, then the invention has a chance of appropriation and dissemination. In this way, shorthand alphabets, phonographs, typewriters, and other nineteenth-century innovations in the area of inscriptive practice are so many theories of language and textuality. They are not the theory of language held by all Americans at the time; they are not "our" theory of language. Instead, they are modest, local, and often competitive embodiments of the way people wrote, read, and interacted over the perceived characteristics of writing and reading. This perspective has a couple of advantages. First, it generalizes from consensus and thus from technologies that eventually proved unworkable as well as workable, since both are revealing sites of negotiation. In this sense the view has "symmetry" according to the sociology of knowledge: it hints at a more complete compass of experience, at the category of technical "workability," in the same way that noncanonical and "sub"-literary works and everyday textuality hin~ at the category of "literariness" in any epoch. Thomas Edison's "electric pen" stenciling devices, to take one example, proved far less successful than office mimeographs as a means of textual duplication, as did numerous other contrivances that never made it to market at all. Yet electric pens and the rest of the objects in the discard pile are no less worthy of study than mimeographs.
Together they permit an interrogation of textual duplication as a socioeconomic and a linguistic fact, part of emerging business practices, shifting labor cohorts, and the burgeoning potential of writing to be all over and all the same, somewhere between manuscript and print. The eventual failure of electric pens reveals something; the eventual success of mimeographs probably tells a little more; looking into the social negotiations of failure and success promises to reveal the most. The second advantage of considering machines for reading and writing as consensual, embodied theories of language is the potential the approach offers for an additionally symmetric account of cultural production and consumption. It does no good to look at theories of language foisted on a blank public by individual and frequently idiosyncratic inventors. No inventor is the beginning of a circuit, sprung whole, like Athena from the head of Zeus. No public is a blank receptor. And "foisting" is far too simple a verb for what I want to call negotiation and appropriation. Like text, new technology is not objectively consumed. As Roger Chartier observes of the former, "Experience shows that reading is not simply submission to textual machinery". Nor, as Marshall McLuhan and Jean Baudrillard are both famous for suggesting, is the experience of text simply a submission to the inscriptive medium of print or computing, the mechanical modes or electronic codes of its presentation and reproduction. The dual symmetries of success and failure and of producer and consumer appeal in theory but prove difficult in practice, because success and production form such powerful forces of historiographic orientation. Technologies that succeed exert a teleological tug: mimeographs tend to erase electric pens on the way to photocopiers and facsimile machines. The physical and commercial shape of the pens is excised from memory and so is the partially linguistic phenomenology of their use.
As moderns and as consumers, we have been conditioned to think that technologies supersede each other one by one, the present ever liberating us from the past. Added to the double problem of forgetfulness, the producers of technology always leave more traces of themselves than consumers do. The makers and purveyors of mimeographs proportionally erase the many more numerous users of their machine by dominating the historical record. Consumers of failed electric pens, by this account, sit smack in a blind spot, equally obscure to history and discomfiting to historiography. Their technology did not "win." They were "only" its users. Even when the fewer users of the pen can be identified-Charles Dodgson, a.k.a. Lewis Carroll, had one'-it is difficult to renovate their experiences from the level of anecdote or emblem to the level of evidence. I am describing a challenge, not an impasse: being careful with questions and patient at archives can unearth a variety of sources for analysis that can help cut across teleological habits. Technology, whether inscriptive or not, involves a lot of paper. Machines get some of their meaning from what is written about them in different ways and at specific junctures, in research plans, patent applications, promotional puff, and so on. Writing machines, in particular, get some of their meaning from the yvay they are used, including the writings they produce. If paperwork can reveal so much about technology, then technology, like science, has a rhetoric of its own. It relies upon rhetorical processes, the conventions of which contribute to a "thick" description of culture, revealing the way American culture sees itself and hinting at the way it identifies and legitimates "the facts." This follows from Edwin Layton's observation, unremarkable among historians of technology today, that technology constitutes a form of knowledge. Rather than an inert and hermetic materialism, technology presupposes a "spectrum," according to Layton, ranging from an idea, through a design, and finally to an artifact and its apprehension and use.
They very rarely exist without linguistic and graphic compliments, labels, descriptions, drawings, and diagrams. In this light technological innovation becomes a process of selecting, sifting, and circulating messages, from the proverbial drawing board to the marketplace and then the drawing room. Artifacts become knowable in part because they are enmeshed within the back and forth and round about of telling what they are, and because telling devolves upon discernable rhetorical conventions, like genres and specialized vocabularies, that are themselves largely the result of unconscious consensus. Economic realities tend to enforce this rhetorical character of technological knowledge by requiring the literature of patents and the literature of commercial promotion. Both the need to identify property and the desire to exchange it ensure an insistently rhetorical character almost unmatched in science, where disciplinary pressures stand in place of commercial ones. Not only does the rhetorical character of technology allow for a "softer" determinism by which machines are not simple, unitary influences on writing but also the same character permits a degree of critique that has eluded all but a few humanist (and those particularly feminist) attentions to science. If technology is a form of knowledge, then it can be conflicted with doubt and contradiction, with assumptions and anxieties, just like other forms of knowledge. The answer to Langdon Winner's provocative question, "Do artifacts have politics?" is affirmative, if only most obviously in cases like the gender politics of Dalkon Shields, or the racial politics of Pullman sleeping cars. Fountain pens and typewriters can be just as ideological, just as much superstructure as infrastructure. Culture insinuates itself within technology at the same time that technology infiltrates culture. Rhetorical analysis provides one way to glimpse the localities of both insinuation and infiltration while testing the usefulness of their directional semantics.
This underlying sense that technology is enmeshed within textuality, that machines are discursively and physically constructed, is a view garnering surprisingly little direct attention. Historians of technology have only lately begun to ponder what they call the "interpretive flexibility" of artifacts possessing "technical content" for "relevant social groups." Drawing upon earlier work in the sociology of scientific knowledge these scholars have tried to set aside the habitual opposition that both divides and defines their discipline, the one between internalist and externalist histories. Internalists practice a sort of formalism, attending more narrowly to how things work, the way one telegraph instrument adapted the form or function of another. Externalists, by contrast, locate things more amid political, economic, and cultural contexts. The newer school of social constructionists rejects both thing and context as separate or separable units of analysis.' According to this view, an invention succeeds not because "it works," but rather it is described as "working" because it succeeds amid prevailing and possibly competitive expectations. Technological function remains something to explain; it does not comprise an explanation in (or of) itself. To put this another way, artifacts are themselves astute, yet they cannot answer all of our questions about why one invention becomes accepted and another does not, any more than a novel, for instance, can answer all of our questions about how it was written and how it was read. Artifacts cannot eveI1. answer with sufficient precision why one model of a machine is "better" than another, just as an edition of poetry cannot speak completely to its own superiority or popularity over other editions. Despite my analogy, the discursiveness of technology has sometimes been hard for scholars of literature, linguistics, and communication to rehearse. Even the most committed social-constructionists seem at times to fall back upon technology as a stable ground amid the roiling, discursive sea.
Both scholars have been influenced by the work ofJiirgen Habermas, who plots the origin and structural transformation of the public sphere in Europe as the creation and then corruption of an abstract level of rational, critical discourse among bourgeoisie. This modern public sphere was created in some degree by anew subjectivity of print in the late seventeenth century-by reading in a new way and by reading novels-and corrupted in the eventual passivity of consumer culture. 6 Like Benedict Anderson and Angel Rama, Habermas grants a great deal of weight to the circulation of print as socially constitutive and transformative. This is a particularly rich background with which to explore the related matter of inscription in the late nineteenth century. The study of inscriptions shows the realm of writing and reading, of symbolic action and experience, in its proximity to objects and machines. From ancient marks on clay or carvings in stone to the printed labels affixed to commercial goods today, inscriptions insistently belie their own double character, both material and semiotic. Modern technology has made some features of this doubleness seem particularly arcane. For example, the original electric meters of the I880s were really halves of little batteries; to "read the meter" a technician had to remove a zinc electrode and weigh it in order to determine the amount of ion deposit, the amount of electrolytic action, which in turn indicated the amount of current that had passed thIough the system. Those ion deposits, like the ion deposits on phorographic plates or strips of celluloid, are the stuff of inscription. They are the double-sided boundary at which the built system both represents technology and is technologically represented. With the same doubleness, the word Representing in my title is both verb and adjective, looking toward technologies represented as well as toward representations and inscriptions generated technologically by the typewriter, the phonograph, and the like. It is a profitable doubleness and, I will argue, particularly revealing of modernity and modern subjectivity.
It is tempting to locate shorthand in the same broad context of the history of literacy as Benn Pitman did. The history of shorthand may indeed be the history of writing; ancient Mesopotamian representations of writing show scribes taking dictation.' The dubious Near Eastern "evolution" from graphical to syllabary to alphabetic writing bears a passing resemblance to the shift from spelled, stenographic shorthand to sounded, phonography, while symbol, syllable, and sound all came under new scrutiny in nineteenth-century shorthand. Moreover, frequent, determinist claims that literacy changes cognition, improves abstract reasoning, and stimulates cultural development, run parallel to claims made for shorthand as rational and scientific, encouraging mental discipline and civic progress. The alphabet is just as much a hero for anthropologist Jack Goody as it was for Benn Pitman. Goody traces the alphabet from Pre-Canaanite to Phoenician to Greek and indicates its seminal influence on economic organization and democratic government. Determinism like Goody's has been absorbed uncritically into numerous historical and literary accounts of literacy and print culture. While many authors, including Goody, have questioned the treatment of orality and literacy as stark, indivisible opposites, anthropologists, psychologists, and cultural critics persist in rating literacy according to orality.4 Different scripts are routinely considered more or less "advanced" in their progress, with the result that the interdisciplines of literacy studies seem to offer little hope of a clear context for shorthand, unmuddied by a posteriori parameters of evaluation.
Pitman even published a weekly periodical printed mostly in phonotypy, The Phonetic Journal, for practice with and appreciation of the phonetic principle. Braced with a knowledge of "common" writing, phonographic longhand, and phonotypy, the student could finally turn to the reporting style. Here different rules of contraction apply. Hooks or loops represent common prefixes, suffixes, or groups of consonants. Vowel markers are generally omitted, the vowel sounds suggested by different lengths and positions of the usual consonants when they are not left to context alone. And long lists of frequently used words are abbreviated to a single letter or a single group of consonant signs. These are the socalled "arbitraries," which some systems were more apologetic than others about using. The sign for p sometimes means "up"; the sign for t sometimes means "it." As the student wades deeper into reporting, contractions become more involved and patently less phonetic. Whole phrases are represented by contractions for words composed of contractions for groups of letters. The avowed principle at the heart of Pitmanic phonography is a one-to-one mapping of sign to sound, in contrast to the Roman alphabet, yet in practice phonographic reporting relies upon a one-tomany mapping of sign to sense. Homonyms and many short or similar sounding words or phrases end up having the same signs in the hands of a skilled reporter. Every boast of "natural" can be met with the challenge of "arbitrary." And every claim of "system" may be qualified hy "personal." Where one-to-one mapping remains pivotal, of course, is in the conversion ofshorthand repotts to full transcripts, where the unique person of the reporter and the necessary uniqueness of transcript stand in for the lacking uniqueness of phonographic signs. Though in theory any reporter can transcribe any report, practice suggested that every reporter be responsible for her or his own transcriptions.
The vociferousness with which rules were promulgated marks the desire to keep every practitioner of a particular system the same-interchangeable parts perhaps, within some larger machine for turning sound into text. Paeans to shorthand as a manner of cultivating habits of memoty and attention undercut the probability of achieving such a goal. Why boast that reporters have "wonderfully improved and mechanically strengthened" their memories if following the rules means producing uniquely sensible reports? The conscious exertions of reporters seemed balanced against their unconscious sensibility. Discipline vouched for accuracy, but exertion undercut probable objectivity. Finally, reporters use different degrees of contraction, even within the same system of phonography, and much remains to be figured out from context while transcribing. With steady use, a system of shorthand can be multiply personalized. Little improvements suggest themselves. Such was the origin of so many of the new, "improved," systems of the nineteenth century. Pitman's phonography itself passed through at least ten different editions, changing incrementally, under the supervision of a Phonetic Council comprised of prominent reporters in Britain and the United States. In 1851, for instance, the Phonetic Council agreed on new consonant strokes for w, y, and h. The resulting ninth edition lasted only five years; the subsequent tenth made changes to the representations of the vowel sounds. This tenth edition caused much dissension, particularly in the United States, and further stimulated the invention of new systems by practicing phonographers. As one prescient commentator noted, Americans launched into "go-as-youplease phonographic authorship," wherein the "exposition of an accepted system soon gave place to the exploitation of individual innovations." For instance, the reporter Elias Longley vaunted a whole new system on the basis of a single new consonant character (Brown, 289). Many authors dissembled the degree of difference that their systems bore to predecessors.
Shorthand publishers made money in five ways. They sold their manuals directly to students. They sold their manuals indirectly to students, through the shorthand schools they established. They collected tuition for study at the schools. They collected tuition for correspondence courses. And finally, they provided skilled reporters for hire, their schools acting as clearing houses and employment bureaus for graduated students." It was a text-based economy without authorship, in which copyright didn't matter, couldn't matter, and allegiances were everything. With intellectual property moot, mental exchange loomed large. Agents were granted exclusive territorial rights to sell books in exchange for signed agreements requiring them to establish schools and promote the system. The Phonographic Institute in London offered the stereotype plates of its books and pamphlets to Americans for relatively modest sums, if only responsible parties would accept exclusive territorial rights in exchange for promoting Pitman's phonography. A board-bound mallual cost less than a half dollar through the I840S; by the late I880s only the most sumptuously bound halldbook cost two dollars.u With prices like these, entrepreneurial eyes were not only on the book trade but also on the educable labor market. The aim of course was to create an ever-growing base of customers wanting to learn just that system. Shorthand publishers competed for market share the way periodical presses competed for circulation and advertising copy. Like the publishers of small-town newspapers, shorthand authors probably generated copy, handled elements of the printing process, and attended to promotion as well as other business matters. (And they took reporting gigs to make ends meet.) But the analogy is imperfect. Shorthand publishers resembled the purveyors of new, component technology as much as they resembled newsmen. They needed to amass market share, but the rewards for doing so were more than purely arithmetic.
The fortunes of the QWERTY keyboard may be even more pertinent. Paul David explains the tenacity of the modern QWERTY keyboard as a matter of economically determined error. The QWERTY arrangement of the original Remington typewriter has remained virtually universal since the I 890s, even though more efficient arrangements have been developed. The market has tipped to the wrong standard. Remington's arrangement was based on that of the inventor ChIistopher Latham Sholes, who wanted to keep the typebars from clashing when the operator typed quickly. Better-engineered machines and then electrics and then computers vitiated the need, yet QWERTY stuck (in parts of Europe, AZERTY). (Reporredly the Remington Company also liked its product name, "type-writer," to appear acrostically in the top row.) David's work has been attacked by S. J. Liebowitz and Stephen E. Margolis. Liebowitz and Margolis challenge David's account of the facts, particularly his identification of a proven-better keyboard, but they also attack his model of the market. Their market is less prone to error. They quickly sketch the early history of market competition and suggest that QWERTY succeeded because it was the best, implying that it may have been easier to learn than its rivals, since ease of tuition could be just as important as ease of use. In the end there is less difference between David and Liebowitz and Margolis than the latter make out. Both parties admit the pressure of standards, even if Liebowitz and Margolis hold that standards are more sensible. All ascribe the conservatism of standards to economics: retooling and retraining make design changes prohibitive. My sympathies are with Liebowitz and Margolis's facts and David's view of the market, which are not necessarily contradictory. The former's description of market competition is forcibly reminiscent of shorthand.
So too did shorthand publishers need to cultivate the labor market in order to disseminate their systems. But the publishers never internalized cost. Though there may have been on-the-job training for typists 'or free typing classes and job placement for typists, little was ever free in shorthand. Rapid training had rhetorical appeal more than it had cost effectiveness. Even the quickest system, if one emerged from the shorthand contests of the later century, like the quickest keyboard, might succeed or fail for any number of reasons, particularly if "quickest" was a matter of inches, as it appears to have been in several documented cases.14 Among the pertinent variables of success were geography, institutional vigor, marketing skills, and access to literate, educable labor, and a variety of target markets. It is impossible to know in retrospect whether or to what degree Gregg shorthand eventually succeeded in the United States because it was quicker or easier, more rational or practical, or because John Gregg most successfully oriented himself near centers of power, within trademark law, and amidst a vigorous and flexible contractual network of book suppliers, employment providers, and educational agents.
These rewards follow the collective accedence to any standard, the individual acquisition of any skill. But they also accrue to any able participation in literacy practices, which interpenetrate the broader power relations of a literate society. The prohibitive expense of retooling and retraining that conserves the QWERTY keyboard finds a compliment in the conservative qualities of literacy practice. Admitted normative characteristics make spelling, for instance, a matter of right and wrong. Misspelling is transgressive and signals the speller's marginal status, either preeducated, uneducated, or sloppy. So knowing and complying with a specific shorthand system, amid its rigorous structure of approved rules and its vociferous promotion of "best" systems, must have lent its users a sense of rightness, of authority, of being in step, which comprises so much of the ideology of literacy. Many disparate elements of different literacy practices are normative, notwithstanding the appreciable liberation and empowerment rightly ascribed to literacy as a whole.
Newspaper reporters in the gallery kept running too, not toward the members of Congress but away from them, in the direction of the Washington telegraph offices. Shorthand reporting thus formed a web of inscriptive action, making the work of Congress public record by doubly making it public and making it record. A similar web had existed before 1848 and 1849, but the promotion and spread of verbatim shorthand reporting made the weave closer and changed the way its patterns appeared. Prior to Pitmanic reporting the only elements of American legislative proceedings to receive consistently detailed treatment were the texts of bills and the prepared speeches delivered by representatives and senators who handed a copy over to the congressional clerks for inclusion in the Globe, or who published copies themselves for franking to constituents. Speeches that were otherwise "taken down by hand" were often shown to their speakers for correction before publication, where they were cut and spliced into the newspapers of the region. American government thus passed into history smacking of oratory, rather than debate, of issues and positions, rather than exchanges, and of arrangements, rather than events. That made democratic government different. Certain details of congressional hearings, for instance, which are today such a fundamental part of civic life, could seldom reach the public eye and inflame the public imagination. While television, not shorthand, is responsible for the immediacy of the McCarthy and Anita Hill! Clarence Thomas hearings, immediacy is not the whole story. What does not or cannot exist as record cannot be made immediate. Before the adoption of verbatim reporting there was less opportunity for hearings and debates to matter, literally, to comprise the material records of governance. They were narrated more properly than they were quoted.
The implication that electrical medicine (electric belts, "vitalizers," and tonics of various sorts) was impractical to the people who used it, or removed from a functionalist model of electrical process, warrants correction. Electrical medicine was used exactly because it was considered salutary, and because electricity and metabolism were understood to function congruently, however mistaken the specifics of this understanding later proved to be. In fact, popular interest in technology has often been functionalist in the truest sense of the word: the idea letters are about machines or devices intended to do things in particular ways, whether they later did them or not. Letter writers reveal their investment in the question of "how things work," with the same directness of Edison compiling his laboratory notebooks. Answers to the question varied greatly, of course, and historians of technology are now beginning to pursue "failures" and alternatives as a subject of inquiry.
The "idea letters" received by Edison are a nebulous lot, making a definitive, quantitative sample of them impossible, if only because their identification was-and remains-somewhat arbitrary. (In the current archival arrangement, perpetual-motion proposals count, Edison Star inquiries do not.) The extant archive does allow of some general impressions, however. Idea letters came from everywhere, but mostly from the United States, from both rural and urban areas. They came from women and children, though the majority were from men. They came from grocery clerks and housewives, medical doctors and farm hands. Writers usually specified lack of training or lack of capital (or both) as their particular claim to outsider status and their impetus from writing. Sometimes many writers would seize the same occasion to write, a news story carried on the wire, for instance, like one incorrect report of 1908 that Edison was working on aerial navigation. In such cases hundreds of correspondents seemed to feel that their ideas had been solicited by the inventor-as they really were during World War I when Edison and other members of the blue-ribbon Naval Consulting Board made a public appeal for suggestions.
Despite this sorting, the remaining idea letters are far from uniform. Typescripts on printed letterhead lie beside handwritten notes on crumpled loose-leaf; the grammatical and well appointed lie beside the unlettered and illegible. Despite their diversity, the idea letters do exhibit identifiable patterns of expression. The most surprising thing about them is that so many concern ideas, not inventions or discoveries, not machines or devices. "Are you in the market for ideas?" queried a livestock merchant from Kansas City, "I have one that I believe would prove serviceable" (Waters 1905)." Like the word "curiosity" during the previous century, the word "idea" here denotes an epistemological currency of learning, a unit of knowing that connects an individual's mind to an experience of nature. But unlike the curiosity cabinet, the idea always possesses a kind of exchange value in the sense that people have ideas, and having appears to have been far easier than inventing. Two days after the livestock merchant wrote, another man inquired, "Do you ever help invent another person's idea?" (McTillen 1905). He writes as if inventing and having an idea were entirely distinct: ideas involve possession while inventions involve action. Writers beseech Edison to "work out" their ideas; the word "out" is as operative as the term "work." "I have several good ideas in my head," writes one man, "but that is as far as they ever got" (Braymer 1915). Ideas are written out, worked out, tried out, carried out, perfected, and made practical. In their outward progress, their path away from the self, they are invented. Their trajectory carries them toward a series of imagined marketplaces, one where ideas are bought and sold, and another where inventions may prove successful. The goal of many correspondents was to become what one termed "a multi-million air" (Parliman 1905).
Such expressions reinforce the fact that writers perceived ideas as property, private and personal, with little sense of collectivity and no recognition of the inevitability avowed by so many contemporary paeans to progress. "The idea" constituted a unit of understanding associated with technological problem solving, its value assumed to be a convertible currency, able to make the progress from the psyche of an isolated owner-citizen to the public world of technological elites and institutions. A republican ideal to be sure, fraught with the ideology of the American dream: as one self-identified "poor little country raised Texas woman" wrote from Oklahoma, her mindthe valuable idea it harbored-was "as undeveloped as her native lands" (Dale r915). Technological possibility was anybody's grubstake, and letters to Edison, like letters to the editor, were part of the arsenal of the public sphere in America, ready-to-hand instruments of republican participation, gesturing at an abstract, rather than personal, level of connectedness.
What were those features? The clock is by itself among the most richly symbolic devices; Lewis Mumford calls it the "key-machine of the modern industrial age." Clock towers "almost defined urban existence". Preindustrial church towers meted out parameters of social identity and psychological composure: in the Old World, London Cockneys defined themselves as a group born within earshot of the bells at Bow Church. Marcel Proust made the receding steeples of Martinville one cynosure amid his A /a recherche du temps perdu. Inside the home, clocks introduced an urban current. This held true even in rural areas, where they were sold by urban-identified salesmen or purchased from urban-based mail-order houses. During the nineteenth centuty, the clock maker was something of a "type" in the New World imagination, embodied by Thomas Chandler Haliburton's humorous character, Sam Slick, an itinerant clock seller true to his name. For all, clocks regulated private lives and connected their regulation to the patterning of social activity elsewhere. They mediated between the private and public, the individual and collective. More pointed forms of the same mediation transformed the workplace in the nineteenth and the early twentieth centuries. Factory time clocks, time-motion studies, and assembly-line time each added new weight to the symbolic burdens of the clock.
Edison proposed the phonograph-clock, assuring his readers "The phonograph clock will tell you the hour of the day; call you to lunch; send your lover home at ten, etc." (534). In Bellamy's short story, a gentleman traveler is awakened abruptly in his hotel bed by a young woman's voice saying, "My Dear Sir, you may possibly be interested in knowing that it now wants just a quarter of three." The effect of these words is so "thrilling and lifelike" that Bellamy's modest protagonist gets up and gets dressed before he lights the lamp to investigate. He spends the rest of the night lying awake, "enjoying the society of [his] bodiless companion and the delicious shock of her quarter-hourly remarks". In both Bellamy's fiction and Edison's wry prognostication, the phonographclock speaks with the voice of decorum, yet for both authors that decorum exits in opposition to titillating circumstance. For many others the phonograph-clock must have inhabited a similar symbolic terrain, interrogating the familiar clock, its regulation of private desires and patterning of social activity. And maybe it was the discomfort of this interrogation that kept the phonograph-clock and its "brazen tablets" from "taking hold," as so many letter writers wrongly predicted it would. On a Simpler level, the recurrent idea of the phonograph-clock continued to mediate berween the largely discarded utilitarian purpose of the phonograph and a sense of having time for leisure. The musical phonograph helped define leisure time and space: in 1907 Edison's National Phonograph Company netted more than a million dollars on phonographs and prerecorded.musical records for home amusement, even after skimming another million off to pay Edison for use of his patents.
While the functional diversity of this shape is some indication of the varied textuality Edison and his staff experienced in experimental telegraphs, printers, and the like, its comparable public apprehension remained more limited to phonograph mandrels and typewriter platens. This chapter shifts the axis of inscription, from the spirals winding around a cylindrical phonograph record to the alphabetic characters, spaces, and nonalphabetic signs that move across the typed page. Like shorthand alphabets and phonographs, typewriters were appropriated within textual practices, which they also had a share in transforming. Phonography and phonographs converted aural experience into inscribed evidence, the former by representing the sounds of speech on paper, the latter by reproducing the same sounds in the grooves of a record. Typewriters intervened more directly into the experiences of writing itself in ways that further interrogated categories of orality, aurality, and textuality. By staying the course as a textual device, typewriters formed an important compliment to phonographs, whIch ultimately diverged dramatically to amusement purposes. The amusement phonograph retained some of the power of text, enrolling authors as producers, writing as hieroglyphics, and machines as readers. Typewriters, however, seemed more certain instruments of textual production, making up in the clarity of their typeface presentation what they lacked as a means of wholly objective representation or reproduction.


In the short space of a current college student’s lifetime, the internet has gone from a specialized, futuristic system to the network that most significantly structures how we engage daily with the world at large. It is now obvious to anyone who uses a computer that intellectual exercises as basic as reading the newspaper or doing research have become fundamentally different activities largely because of the internet. So too have our views of communication in general; the very notion of globalization, so consuming in today’s world, is predicated on the possibilities engendered by a technology barely twenty years old. Such is the nature of “new media.” Computers, and the digital systems and products for which they are currently a shorthand, are what most of us think of when we hear the words new media. And why not? The world of computer hardware, software, email, and ebusiness is for most of us the latest communication and information frontier. Part of our experience of digital media is the experience of their novelty. Yet if we were asked to think of other “new media,” we might have a harder time coming up with obvious examples. We would have no problem citing instances of “old media”: typewriters, vinyl record albums, eight-track magnetic tapes, and the like. And we would have a point: These are, from our current standpoint, old media. But they were not always old, and studying them in terms that allow us to understand what it meant for them to be new is a timely and culturally important task, an exercise that in this volume we hope profitably to apply to media much older than we are. As our title suggests, this collection of essays challenges the notion that to study “new media” is to study today’s new media. All media were once “new media,” and our purpose in these essays is to consider such emergent media within their historical contexts—to seek out the past on its own passed terms.
There is a moment, before the material means and the conceptual modes of new media have become fixed, when such media are not yet accepted as natural, when their own meanings are in flux. At such a moment, we might say that new media briefly acknowledge and question the mythic character and the ritualized conventions of existing media, while they are themselves defined within a perceptual and semiotic economy that they then help to transform. This collection of essays explores such moments in order to enrich our contemporary perspective on what media are, and on when and how they are meaningfully “new.” New Media, 1740–1915 focuses on the two centuries before commercial broadcasting because its purpose is, in part, to recuperate different (and past) senses of media in transition and thus to deepen our historical understanding of, and sharpen our critical dexterity toward, the experience of modern communication. Indeed, we have marked the years between 1740 and 1915 as boundaries for our project because this period is crucial to understanding how electronic and digital media have come to mean what and how they do. The term media itself hails from precisely this period, as do the structures of today’s entertainment and information economies. Thus, the media forms and practices studied in this collection are “new” in a double sense: First, they newly receive the scholarly attention they deserve; and second, they are considered within their original historical contexts, their novelty years. In this, these essays provide a new perspective on the meaning of “newness” that attends to all emerging media, while they also tell us something about what all media have in common. Yet our intention is not only to acknowledge the initial novelty of diverse media, but also to understand better how such media acquire particular meanings, powers, and characteristics. Drawing from Rick Altman’s idea of “crisis historiography,” we might say that new media, when they first emerge, pass through a phase of identity crisis, a crisis precipitated at least by the uncertain status of the given medium in relation to established, known media and their functions.1 In other words, when new media emerge in a society, their place is at first ill defined, and their ultimate meanings or functions are shaped over time by that society’s existing habits of media use (which, of course, derive from experience with other, established media), by shared desires for new uses, and by the slow process of adaptation between the two.
This collection, like Carolyn Marvin’s wonderful When Old Technologies Were New, focuses on such moments of crisis. While it begins with the zograscope and ends in the heyday of silent cinema, the volume does not aspire to cover all forms of media that emerged during the years named in its title. Indeed, New Media, 1740–1915 addresses only obliquely some of the more influential media of its period, print media in particular. Most of the following essays (unlike Carolyn Marvin’s work) focus on media—zograscopes, optical telegraphs, the physiognotrace—that failed to survive for very long. They are, in Bruce Sterling’s words, today’s “dead media.” Yet because their “deaths,” like those of all “dead” media, occurred in relation to those that “lived,” even the most bizarre and the most short lived are profoundly intertextual, tangling during their existence with the dominant, discursive practices of representation that characterized the total cultural economy of their day. Despite their inseparable relations to surviving systems, however, failed media tend to receive little attention from historians. “Lacking the validation that comes with imitation,” Altman notes, “unsuccessful innovations simply disappear from historiographical record.” His suggested corrective for this excessive focus on, for example, “cinema-as-itis,” is an attention to “cinema-as-it-could-have-been” or “cinema-as-it-once-was-for-ashort-time-but-ceased-to-be.” New Media aims to apply some of this “could-have-been” and “was-for-a-short-time” kind of thinking to past new media. Because our understanding of what media are and why they matter derives largely from our understanding and use of the media that survived—those devices, social practices, and forms of representation with which we interact every day—the importance of this kind of analysis is easy to overlook. By getting inside the “identity crises,” by exploring the “failures” (in some cases) of older new media, the essays in this collection will help to counter what Paul Duguid has warned are two reductive “futurological tropes” characteristic of the experience of modern media. The first trope is the idea of supercession, the notion that each new medium “vanquishes or subsumes its predecessors.” From this idea follows the current belief that in the digital age the book is doomed, or, according to the peculiar auguries of earlier times, the conviction that typewriters would replace pens or that radios would replace phonographs. The second futurological trope is the idea of increasing transparency, the assumption that each new medium actually mediates less, that it successfully “frees” information from the constraints of previously inadequate or “unnatural” media forms that represented reality less perfectly.
This assumption creates an interesting paradox. The best media, it would seem, are the ones that mediate least. They are not, as we think of them, media at all. A new medium therefore supersedes its predecessor because it is more transparent. Few would disagree, for example, that a conversation with a friend on the telephone allows for a greater exchange of personal, idiosyncratic information than a dialogue conducted via telegraph. And to a large degree, this thinking is persuasive. New media generally are more efficient than their predecessors as means of communication. Yet there is more to understanding what happens when people communicate through a given medium than merely ascertaining what level of accuracy and amount of data the exchange involves. This observation—that there is more than accuracy and amount to any exchange—comprises a founding rationale for the field of media studies, whether characterized aphoristically by Marshall McLuhan (“the medium is the message”) or more recently expressed (and complicated) in Derridian terms, that the supplement—the “specific characteristics of material media”—can never be “mere” supplement; it is “a necessary constituent of [any] representation.”5 To put it simply, looking for content apart from context just won’t work. Owing in part to the linear progress unthinkingly ascribed to modern technology, media (so often referred to portentously as the media) tend to erase their own historical contexts. Whether shadows in a darkened cave or pixelated images on a luminous monitor, the media before us tend, anachronistically, to mediate our understanding of their past. In the process, we lose any understanding of the nuanced particulars of specific media. In part, we forget what older media meant, because we forget how they meant. Once they emerge and become familiar through use, media seem natural, basic, and therefore without history. Of course we say “Hello?” when we answer the telephone; of course we hear a dial tone when we pick it up to place a call. Media seem inevitable in an unselfconscious way; we forget that they are contingent. Alexander Graham Bell apparently wanted people to say “Ahoy!” when they picked up the phone, but English speakers settled on “Hello?” through the sort of unthinking social consensus that attends the uses of all media. In a similar fashion, the dial tones, 12-volt lines, and modular jacks we use today all were shaped historically by a complex of forces—technological, to be sure, but also social, economic, and representational.
When we forget or ignore the histories of each of these new media we lose a kind of understanding more substantive than either the commercially interested definitions spun by today’s media corporations or the causal plots of technological innovation offered by some historians. For example, it is undoubtedly important to be able to note, as many scholars have, how the invention of the cinema is linked to past practices of, say, lecturing with slides, as well as how it predicts certain elements of future practices. But what we often overlook are the kinds of things that only a deep analysis of specific media cases can offer—how interpretive communities are built or destroyed, how normative epistemologies emerge. No medium new or old exists as a static form. Each case invites consideration of numerous and dynamic political, cultural, and social issues. We might say that, inasmuch as “media” are media of communication, the emergence of a new medium is always the occasion for the shaping of a new community or set of communities, a new equilibrium. As we have suggested, when a new medium is introduced its meaning—its potential, its limitations, the publicly agreed upon sense of what it does, and for whom—has not yet been pinned down. And part of the lure of a new medium for any community is surely this uncertain status. Not yet fully defined, a new medium offers possibilities both positive (one of our authors argues that zograscopes helped construct polite society) and negative (another traces the threat telephones posed to Amish communities). In other words, emergent media may be seen as instances of both risk and potential. Today, for example, the internet offers unprecedented possibilities for global villages to coalesce, even while it threatens national or ethnic cultural traditions and provokes anguished discussions of privacy in a “connected” age. The same sorts of issues and anxieties surrounded the emergence of other media. Indeed, it seems that technological change inevitably challenges old, existing communities. The particulars of each case, however, are valuable to our larger understanding of how media help to shape and reshape culture. Essays in this collection therefore examine media as socially realized structures of communication, where communication is culture—as James Carey explains it—a cultural process that involves not only the actual transmission of information, but also the ritualized collocation of senders and recipients. Habits of communication mediate among people, pragmatically and conceptually. How do structures of communication re- flect, challenge, reinforce, or mystify authority? How do they help imagine community? How do they help construct the aesthetic, or the mimetic? How do they orient the production and experience of meaning?
Ultimately, then, this is a book about framing: about how particular habits and media of communication frame our collective sense of time, place, and space; how they define our understanding of the public and the private; how they inform our apprehension of “the real”; and how they orient us in relation to competing forms of representation. We have selected the cases of new media that follow because they support these inquiries, casting such habits and media into relief, affording a vantage point from which better to see how cultural meanings are negotiated. But this collection is also about how we frame our own discussions of new media, for if this interrogation of emergent media is genuinely to illuminate our understanding of cultural definition and of cultural change, then we must be responsible about our own language. We must, in other words, acknowledge the key terms that are in play in our own discussions and attempt to define and deploy them as precisely as possible, not only for us now, but as they were used in earlier—and different—contexts.
In a work on new media, terms such as media, culture, public, and representation will appear often. But insofar as this collection seeks to understand how the very idea of “media” evolves over time, we wish to employ such critical terms with care and to bring questions about their use and meaning squarely into the discussion itself as it proceeds. Our use of the word technology is a good example. This term denotes, as Leo Marx suggests, a necessary but “hazardous concept”; in this book the term helps organize our thinking about the material, instrumental conditions of modern life, yet for many readers it will also come larded with less considered shades of meaning, assumptions about “Progress” with a capital “P,” or about technology as a preeminent cause in history.8 Thus although we rely on this term as an organizing device in this collection (the essays proceed from technology to technology as a form of convenience), we also wish to urge particular awareness of its hazards. Likewise with other key critical terms. We know that we cannot exhaustively define “media,” for instance, any more than we can completely pin down “culture” (a notion that is, as Naomi Mezey observes, “everywhere invoked and virtually nowhere explained”). Indeed, the cases we offer are about culture as struggle and media as means in that struggle—a fabric continually rewoven according to the interests of a given time and place. Rather than fixing such terms and pinning them to moving targets, however, we can frame our discussions of such pervasive concepts in self-conscious ways that make our attempts to understand them more useful.
In this volume we offer cases that foreground the relationship between material and idea, between what people think or believe or wish and what they feel with their hands or see with their eyes or hear with their ears. Each of the essays in the collection thus reveals, in some fashion, the strong relationship between the contexts for some material, technological development, and shifts in self-imagining and public understanding. Erin Blake, Wendy Bellion, and Laura Schiavo, for example, consider the cultural meanings of perspective and representation in the eighteenth and nineteenth centuries by focusing on the emergence of particular visual media (zograscopes, the physiognotrace, and stereoscopes, respectively) and discussing how such media influenced notions of individual identity. Patricia Crain, Katherine Stubbs, and Diane Umble, by contrast, consider the cultural meanings of communication by focusing on the arrival and adaptation of particular networked media (optical telegraphs, electric telegraphs, and telephones, respectively) that helped shape notions of identity in relation to larger communities. All of the authors engage new media as evolving, contingent, discursive frames, sites where the unspoken rules by which Westerners know and enjoy their world are fashioned. Such “rules” continually change, as new media become situated and as such adjustments inevitably redraw the boundaries of communities, including some individuals, and excluding others. Each new medium in effect helps to produce a distinct public. Erin Blake’s work on zograscopes, for example, elaborates the idea that media assist in the construction of the modern, Western public sphere, with its corresponding liberal subject (today known as “the consumer”). Although she draws upon the work of Jürgen Habermas, Blake ignores the often-mentioned circulation of print media as the basis of the public sphere, instead looking to shared social practices to understand how space is visualized. Her public is literally a sphere; in her essay the bourgeois circles of eighteenthcentury London pop into 3-D as they enter the rational and impersonal arena of public space via engravings glimpsed through new optical devices. This new medium, according to Blake, helps the public to map itself. Wendy Bellion’s work on the physiognotrace depicts an American public that also maps itself, but this public is one more complicated by its own experiences of both graphic and political self-representation. By analyzing the American reception of this profile or silhouette-tracing device, Bellion introduces her readers to the cartography of the public sphere, showing the ways in which new media are adapted within the very discursive conditions, the very rules that they help to transform.
Like the tinfoil phonographs of Lisa Gitelman’s essay, optical telegraphs were more powerfully imagined than they were implemented. Very few were ever built or used, yet the idea of them circulated widely within the mentality, the public imagination, of their age. Joseph Lancaster’s classroom telegraphs literally disciplined students, while even broader disciplinary measures may be read in their controlling institutional contexts, as well as glimpsed in the titles of early American newspapers like the American Telegraph [Conn.], Hillsboro[N.H.] Telegraph, and Lincoln [Me.] Telegraph. (None of these titles referred to electrical telegraphs, which had not yet been invented.) In Benedict Anderson’s formulation, the circulation and ritualized consumption of newspapers like these assisted in the imagination of a national community. What their titles and Lancaster’s system suggest, according to Crain, is that the imagination of mediaconditioned the imagination of communities. Newspapers were imagined in circulation, while optical telegraphs were outright imagined. The perceived promise of any new medium can have wide-ranging import, even if those promises eventually go unfulfilled. To many observers, the tinfoil phonographs of 1878 promised a new, more modern and immediate type of text, as recordings might indelibly “capture” speech, without the intercession of literate humans wielding pencils and paper. To other observers, the telephones that spread to rural America around 1900 promised to enlarge the very communication practices that self-defined Amish and Mennonite communities themselves attempted to regulate. The wide popular reception of the first promise, Lisa Gitelman speculates, challenged and helped to transform vernacular experiences of writing and print, while raising questions about the instruments and the subjects of public memory. The Old Order perception of the second promise, Diane Umble shows, helped divide the aggregate Amish and Mennonite population, for this perception coincided with the ongoing regulation of intra- and inter-group communication and excommunication. Although so often the focus of great attention and optimism, new media are not, as these authors pointedly demonstrate, inherently benign; they “bite back.”10 They thrive amid unforeseen consequences, often despite the best, most vigorous intentions of their inventors, their promoters, their initial consumers, or of the customary arbiters of public intelligence.
To scientists, the stereoscope could be used objectively to demonstrate that vision is subjective, that the body can produce its own experiences of depth when presented with the right cues. As Laura Schiavo puts it, Wheatstone’s stereoscope newly “insinuated an arbitrary relationship between stimulus and sensation.” Yet within the context of commercially exploited and popularly apprehended photography, stereoscopes were ultimately recast as mimetic amusements that tendered to consumers an instructive and positivist model of how their eyesactually worked to see the world as it really is. Vernacular discourse, in other words, completely inverted the meaning of what the stereoscope “proved.” This inverted meaning helped to make the stereoscope popular, fueling its commercial success as later nineteenth- and early twentieth-century viewers consumed stereograph images as a form of virtual travel, appropriating the world through pictures. At stake was far more than the prestige of Wheatstone or the anti-intellectualism of the marketplace. The rules by which the West knew the world had again come into play. The popularity of stereoscopes helped redraw the very category of the “real,” the consensual practices of “accurate” representation. Assumptions about what count as “rules,” about what is “real” or “accurate” or “normal,” are no less at issue when new media are less popular than stereoscopes were or less patently involved in describing normal human perception. Media emerge and exist in ways that both challenge and regulate notions of what it means to be human. Gregory Radick’s essay provides a clear-cut case. An amateur ethologist using the new medium of recorded sound set out to learn the “language” of monkeys and stumbled into one of the hottest debates in Victorian evolutionary biology and linguistics: How is language uniquely human? In the course of his research, Richard Garner’s recording phonograph became an instrument of knowledge deployed in various philosophic and scientific controversies—in the tension between amateur and professional science, for example, or in the dispute over whether abstraction or instinct founds thought and language, or in discussions about the fundamental differences between humans and animals. Garner worked on monkeys, but not without meddling with the category of the human in two ways. First, he raised anew the definition of “Man” as “the talking animal”; second, he wielded his phonograph as if it were a necessary—and better—third ear.
Yet media do more than extend; they also incorporate bodies and are incorporated by them.11 Media are designed to fit the human, the way telephone handsets or headsets literally fit from ear to mouth, but also the way telephone circuits, satellites, and antennas fit among their potential consumers, as integral parts of communication/information networks that literally shape what communication entails for individuals in the modern age. And if media fit humans, humans adjust themselves in various waysto fit media, knowingly and not. Hands physically adjust themselves to different keyboards, different keypads, and different pointing devices, while users subtly adjust their sense of who they are. Some of these complexities may be glimpsed in Katherine Stubbs’s essay, which reads the history of electrical telegraphy in the United States against and within the fiction that appeared in telegraph trade journals. Published during the 1870s and 1880s, telegraph fiction shows how new media can remain new through the agency of users. Amid ongoing conflicts between labor and capital arising in part from the feminization of the workforce, telegrapher-authors both used and represented the telegraph as a means to explore identity in its relation to the body. In remaking themselves, by negotiating gender-at-a-distance-and-by-telegraph, for instance, telegraphers kept the character of their medium unsettled. In other words, the “newness” of new media is more than diachronic, more than just a chunk of history, a passing phase; it is relative to the “oldness” of old media in a number of different ways. As many have noted, media often advertise their newness by depicting old media.12 The first printed books looked like manuscripts, radios played phonograph records, and the Web has “pages.” Ellen Gruber Garvey and Paul Young each explore less familiar instances in which the new represents the old in order to understand more fully the purchase that “newness” has on the process of representation. As Garvey’s account of scrapbooks explains, scrapbook-makers took old media—literally the old books and periodicals they had lying around—and made them into new media in the form of scrapbooks. “Newness” in this case resonated as much with personal and domestic experiences as it did with public and collective apprehensions of novelty, posterity, or periodicity. Scrapbook-makers tampered with the meanings of the scraps they collected by collecting them, a practice Garvey refers to as “gleaning” and connects to the composition and use of the Web today. Young, on the other hand, presents a “telegraphic history of early American cinema,” reading filmic representations of telegraphs as only the most obvious link between these two media, which seem, in retrospect, so different.
We hope these essays will help to broaden the inquiry of media studies by calling attention to the ways media are experienced and studied as the subjects of history. No ten essays can do more than open the question, but opening the question is crucial, we think, particularly as today’s new media are peddled and saluted as the ultimate, the end of media history. “Newness” deserves a closer look. To that end, we include a brief section of documents for discussion. These documents are not illustrations of our text as much as they are artifacts that themselves point toward the rich and diverse record available to media historians. We hope that they will suggest specific historical and cultural meanings for media and promote a broader discussion of media history. Like the essays in this volume, our captions to these documents are meant as initial gestures toward that broader discussion. We include them to remind readers that the history of media is an ongoing, highly self-reflexive conversation about what we mean and—literally—how we mean it.

First we had media art. In the early days of electronic and digital culture media art was an important way of considering relationships between society and technology, suggesting new practices and cultural techniques. It served as an outlet for the critique of the dark side of computer culture's roots in the military-industrial complex; and it suggested numerous utopian and beautiful ways of engagement with technology, new types of interactivity, sensuous interfaces, participative media practices, for instance. However, the more critical, egalitarian and participative branches of media art tended to be overshadowed by the advocacy of a high-tech and high-art version of it. This high-media art conceptually merged postmodern media theories with the techno-imaginary from computersciences and new wave cybernetics. Uncritical towards capitalisms embrace of technology as provider of economic growth and a weirdly paradoxical notion of progress, high-media art was successful in institutionalizing itself and finding the support of the elites but drew a lot of criticism from other quarters of society. It stuck to the notion of the artist as a solitary genius who creates works of art which exist in an economy of scarcity and for which intellectual ownership rights are declared.

In the course of the 1990ies media art was superseded by what I call The Next Layer or, for help of better words, Open Source Culture. I am not claiming that the hackers who are the key protagonists of Open Source Culture are the new media artists. Such a claim would be rubbish as their work, their ways of working and how it is referenced is distinct from media art. I simply say that media art has become much less relevant through the emergence of The Next Layer. In the Next Layer many more protagonists come together than in the more narrowly defined field of media art. It is much less elitist and it is not based on exclusivity but on inclusion and collaboration. Instead of relying on ownership of ideas and control of intellectual property Open Source Culture is testing the limits if a new egalitarian and collaborative culture.

In the following paragraphs I would like to map out some of the key components of Open Source Culture. It has been made possible by the rise of Free, Libre and Open Source Software. Yet Open Source Culture is about much more than just writing software. Like any real culture it is based on shared values and a community of people.

Open Source Culture is about creating new things, be they software, artefacts or social platforms. It therefore embraces the values inherent to any craft and it cherishes the understanding and mastery of the materials and the production processes involved. Going beyond craftmanship and being 'open source', it advocates free access to the means of production (instead of just "ownership" of them). Creativity is not just about work but about playfulness, experimentation and the joy of sharing. In Open Source Culture everybody has the chance to create immaterial and material things, express themselves, learn, teach, hear and be heard.

Open Source Culture is not a tired version of enforced collectivism and old fashioned speculations about the 'death of authorship'. It is not a culture where the individual vanishes but where the individual remains visible and is credited as a contributor to a production process which can encompass one, a few or literally thousands of contributors.

Fundamental to Open Source Culture's value system is the belief that knowledge should be in the public domain. What is generally known by humans should be available to all humans so that society as a whole can prosper. For most parts and whereever possible, this culture is based on a gift economy. Each one gets richer by donating their work to a growing pool of publicly available things. This is not a misguided form of altruism but more like a beneficial selfishness. Engaged in a sort of friendly competition everyone is pushing the whole thing forward a bit by trying to do something that is better, faster, more beuatiful or imaginative. Open Source Culture is a culture of conversation and as such based on multiple dialogues on different layers of language, code and artefacts. But the key point is that the organisation of labour is based on the self-motivated activity of many individuals and not on managerial hierarchies and 'shareholder value'.

Open Source Culture got a big push forward with the emergence of Linux and the Internet but we shouldn't forget that it has much deeper roots. History didn't start with Richard Stallmans problems with a printer driver. The historic roots could be seen as going back to the free and independent minded revolutionary artists and artisans in 19th century. More recently, it is based on post-World-War-II grassroots anti-imperialist liberation movements, on bottom-up self-organised culture of the new political movements of the 1960ies and 1970ies such as the African American civil rights movements, feminisim, lesbian, gay, queer and transgender movements, on the first and second wave of hacker culture, punk and the DIY culture, squatter movements, and the left-wing of critical art and media art practices.

In terms of the political economy, Open Source Culture could mark an important point of departure, by liberating the development of new technologies from being dictated by capital. The decision of what should be developed for which social goals is taken by the developers themselves. Technological development is not driven by greed but by deep intrinsic motivations to create things and to be recognized for ones contribution. Despite that, Open Source Culture is not an anti-capitalist ideology per se but has the potential to change capitalism from within and is already doing so.

Open Source Culture needs to be constantly aware of capitalisms propensity to adapt, adopt, co-opt and subjugate progressive movements and ideas to its own goals. The 'digital revolution' was already stolen once by the right-wing libertarians from Wired and their republican allies such as Newt Gingrich and the posse of American cyber-gurus from George Gilder to Nicholas Negroponte. More recently adept Open Source Capitalists have used terms such as Web 2.0 and social software to disguise the fact that what those terms are said to describe has emerged from open source culture and the net culture of the 1990ies and the early 2000s. Once more the creativity of the digital masses is exploited by alliances between new and old tycoons. The Next Layer emerges at a time when capitalism is stronger than ever before and it emerges at the very heart of it. This is the beauty of it. It cannot be described in a language of mainstream and underground. Open Source Culture is the new mainstream which is what capitalist media are doing their best to hide, scared by the spectre of communism as well as commonism. We don't need to ressort to the language of the Cold War and its dichotomies, howver.

The Next Layer contains not only a promise but also a threat. It emerges at a time when the means of suppression and control have been increased by rightwing leaders who try to scare us into believing we were engaged in an endless 'war on terror'. With their tactics they have managed to speed up the creation of a technological infrastructure for a society of control. The general thrust of technological development is coming from inside a paranoiac mindset. 25 years of neo-liberalism in the American lead empire have degraded civil liberties and human values. The education system has been turned into a sausage factory where engineers are turned out who construct their own digital panopticons. Scary new nano- and bio-technologies are created in secret laboratories by Big Science. And the bourgeioise intelligentsia meanwhile has stood still and does not recognize the world any more but still controls theatres, publishing and universities. In this situation it is better if Open Source Culture is not recognized as a political movement. The Next Layer will find ways of growing and expanding stealthily by filling the niches, nooks and crannies of a structurally militant and imperialist repressive regime from which, given time, it will emerge like a clear spring at the bottom of a murky glacier.



Technological determinism is the belief that science and technology are autonomous and the main force for change in society. It is neither new nor particularly original but has become an immensely powerful and largely orthodox view of the nature of social change in highly industrialised societies. In this paper I analyse the presence of technological determinism in general discourses about the relationship between social change and science and technology. I show that techno-determinist assumptions underlie developments in what is called technoscience, a term describing new practices in science and technology with particular relevancy for the related fields of genetic engineering and computer science. Those areas create a specific set of narratives, images and myths, which is called the techno-imaginary. The thesis of my paper is that the discourse on media art uncritically relies on many elements of the techno-imaginary. A specific type of media art, which is identifiable with people, institutions and a period in time, is particularly engaged with the tropes of the techno-imaginary. This strand, which I call high media art, successfully engaged in institution building by using techno-determinist language. It achieved its goals but was short lived, because it was built on false theoretical premises. It made wrong predictions about the future of a 'telematic society' and a 'telematic consciousness'; and it missed the chance to build the foundations of a theory of media art because it was and is contaminated by the false assumptions behind technological determinism.
Science and technology are widely understood to be the major, if not the only forces which cause social change. This opinion is called technological determinism. According to this view science and technology are autonomous, which means that they develop according to their own internal logic only. Once new technologies have been invented and are released into the world they have an irresistible impact on the social world. This implies that history is largely a result of the impact of new technologies. By denying the importance of other social forces such as politics and the economy human agency is effectively cancelled as a factor in the shaping of history. In the field of art a new domain has been developed which is variously called media art, digital art or just new media. This field has deeper historical roots but has gained major significance only over the past 25 years. Within this area, which is very diverse and comprises a variety of practices and approaches, a particular discourse has become dominant. I call it 'high media art'. Its ascendancy started in the 1980s and peaked by the mid 1990s. Its proponents used specific narrative strategies which were highly successful in drawing attention to the field and building institutions devoted exclusively to high media art. That discourse on high media art claimed a radical break with the past and a transgression of all other art forms. It took the material basis of its practice, the use of new media technologies and in particular the computer, as major justification for its claims. It presented itself as an avant-garde, not unlike the classical avant-garde of the 1920s, which employed high-technology to create a new aesthetics. This new aesthetics was tied into postmodern theories as well as the idea of a three-dimensional cyberspace, and it borrowed freely from the myths of computer science. Artists produced works which uncritically repeated the narrative strategies of artifical intelligence and artificial life. The techno-imaginary of the 'closed world' (Edwards 1996), developed at a time when America fought ideoligical battles with its nuclear enemy, the Soviet Union, still provides the principles of our own imaginary futures (Barbrook 2005). The media theory of McLuhan, hardened into an ideology, McLuhanism, provides the intellectual framework for high media art in the mid-1990s combined with the fashionable thesises of postmodernism about the immateriality of the world. The discourse of high media art was successful in institution building but compromised by technological determinism. I will show that technological determinism in high media art isn't just a question of interpretation or opinion but foundational for the field, as a major influence on the creation of works and the theories which came with it. Instead of taking a critical position high media art only illustrates science and technology and glorifies the aesthetics and ideology of technoscience.
The thesis, which I present in this paper, is based on a literature review which includes relevant theoretical areas, histories of media art, catalogues, articles, web sites and discussions on mailing lists. It is also based on my own experience of 20 years of working in the field as an artist, curator, critique and theorist. My own close involvement in the field over a long period of time is my main motivation for this work with which I hope to explore and analyse some major theoretic deficiencies. As a practitioner I have acquired knowledge of the practice, of the actual making and doing, which is rarely reflected in theoretic texts which are only based on the analysis of other texts. I hope to be able to bring the theory and the practice more closely together. Although the focus of this paper is primarily a critique my aim is to open up, through this critique, possibilities for further work. My analysis of the field is influenced methodologically by The Field of Cultural Production by Pierre Bourdieu (1993). He presents his approach as an alternative to two positions which were dominant at the time of writing, structuralism and post structuralism on one hand, and Marxist inspired critical theory on the other. According to Bourdieu structuralism’s and critical theory’s ways of reading works mutually exclude each other (Bourdieu 1993, 177). (Post)Structuralism favours an internal reading of works, critical theory an external reading. Bourdieu describes structuralism as "more powerful" (ibid., 178), yet criticises it for stripping the reading of the work off any "references to the social or economic conditions of its production (ibid., 178)." External analysis, in contrast, "directly links these works to the social characteristics (the social origins) of their authors or of the groups for whom they were really or potentially destined and whose expectations they are intended to meet (ibid., 180)." The weakness of this approach is, according to Bourdieu, that "understanding the work means understanding the world view of the social group that is supposed to have expressed itself through the artist acting as a sort of medium." In other words, the author is seen as a ventriloquist for his own social background and supposed 'class interest'. But this approach fails to provide means of understanding the structure of the work, its subtleties and poetic motions which are, "the prerequisite for the fulfilling of its function? (ibid., 181)" Bourdieu claims he can overcome the deficiencies of both poststructuralism and critical theory by applying the theory of the field, a "relational or structural mode of thought to the social space of the producers (ibid., 181)." Different fields are characterised by positions and position taking, by writings and writers, art works and artists who are involved in a struggle to carve out their own niche within a specific area.

A key concept in Bourdieu’s theory is contained in the term 'symbolic capital'. Paradoxically, in avant-garde movements of literature or art, those who show the least interest in outward signs of success such as awards, titles and money, accumulate the highest amount of 'symbolic capital'. They receive strong support from a closely-knit group of followers, often other artists or professional insiders (curators, critics). This results in the 'non-economy' of autonomous art. The economic and the symbolic hierarchies cannot be directly mapped onto each other. The poorest, most obscure artists are the most famous ones. If they get successful too quickly, they run danger of loosing their reputation as being relevant, cutting-edge, fresh, and innovative. Bourdieu loosely groups artists according to this perception. There are successful artists who cater to the tastes of the dominant social group. They have money, wealth, but no symbolic capital. There is the consecrated avant-garde, an avant-garde which is already partly absorbed by the system, which has its critics, its recognised names. They are in danger of being seen as selling out. New artists will come and attack their perceived dominance. Only this latest group, by being seen as staying outside heteronomic power structures, is attributed the highest symbolic capital. It acts in a field of 'restricted' cultural production which has hardly any audience and very little quantifiably measurable impact, yet this group is seen as the true avant-garde.1 Bourdieu's description of the 'game' of cultural production clearly has some limitations insofar as it may perfectly describe the French literary and artistic avant-garde of the 19th and 20th century but might not be universally applicable. For instance, the notion of popular culture with its own subcultures and avant-garde is not reflected properly in Bourdieu's theory. Bourdieu's approach is useful but cannot be adopted blindly. Therefore I use other theoretical frameworks in addition to Bourdieu, in particular science studies and critical theory.
Technological determinism is hardly ever formulated as a clearly stated theoretical position but has nevertheless become "an immensely powerful and now largely orthodox view of the nature of social change (Williams 1974, 13)." According to this opinion science and technology are autonomous, their development follows an inherent logic and is independent of influences from society. Science and technology are the main forces that shape social change, therefore history is determined by technological development. Paul N. Edwards calls it the "billiard ball theory of scientifically induced change" (Edwards 1996). According to this metaphor technology impacts on society like a billiard ball and whirls everything around. Social change is conceptualized in a very particular way, namely as a causal relationship between technology, as the origin of the force for change, and society, as its target. Society is the passive receiver of an 'impact' and has no agency in the process. The 'impact of science' is presented as something completely unavoidable, like a force of nature. In this model, science and society are completely separated. Scientists, locked away in citadels of knowledge, conduct research entirely uninfluenced by society. Scientific research is a disinterested pursuit of truth which follows its own internal dynamics only. Scientific progress is based on the strict application of the scientific method alone. New technologies are applications of scientific knowledge - applied science - put to work in the world. The effects of technology are seen as the primary mechanism that shapes history. These are the core believes behind what is called the strong version of technological determinism. It is the content of statements such as that the computer created the information society; or that the steam engine brought about industrial society. In social struggles about new technologies often the opponents also adhere to the belief of techno-determinism, when they vent their anger at a particular technology because they think it is intrinsically bad. There are a number of weaker versions of technological determinism. In those versions, technologies are seen as symptoms of society, as effects rather than as causes. The development of technologies is still seen as largely autonomous, but the impact is less deterministic. Technologies are perceived as being 'instruments' only, they are neither intrinsically good nor bad, they are only neutral tools. Any ethical questions would arise depending on the way of use or abuse of those instruments. As we cannot know which use society will make of a particular technology, unintended consequences might occur, and we cannot predict in which way exactly technology will shape society.
Technological determinism is behind assumptions such as that technological progress is the key to greater prosperity, wealth and security. Technology will solve a wide range of human and social problems. For instance, government administrations believe that the implementation of CCTV surveillance systems will help to prevent crime and contribute to the upkeep of public order. In TV advertisements the ability of gadgets such as the mobile phones is praised to win new friends or find a lover. Yoghurts are advertised as containing a 'scientifically proven formula' to make you slimmer, healthier and more attractive. This emphasis on technology as the harbinger of hope to solve all kinds of social problems is reflected in the way governments have created technology impact assessment centres since the 1970s. The direction of this type of research ignores the possibility that the assumptions behind the basic formula, technology as cause and effect, might be wrong. The real nature of the relationship between technology and society poses some of the most difficult and most unresolved historical and philosophical questions.
The concept of determinism in science has different meanings. It does not relate to the question if science determines society but to another complex of questions. Is matter organised in such a way that deterministic processes can be observed? And can science formulate descriptions or models of those processes which form objective laws of nature? In this sense, science must believe in determinism to a degree, otherwise it could not conduct its activity. "Determinism came down from the skies to earth", wrote Gaston Bachelard (Bachelard 1934/1988, 101). As a psychologist, he reflected on how the scientific spirit formed, and came to the conclusion that the observation of planets and stars was essential in the historic shaping of a scientific mindset. Whereas life on earth is messy and unpredictable, the observation of regular bodies moving in predictable ways enables to shape the expectation that objective laws of nature exist which can be understood and formulated with the help of mathematics and geometry (Bachelard 1934/1988). In the long run, this enabled the development of an exact science by Descartes and Newton. Until recently histories of science presented the development of modern science from there on as an unbroken continuity to more clarity, preciseness and abstraction. But it can also be argued that regarding the being or ontology of the world and the epistemology, the theory of knowledge that we have about it, at the beginning of the scientific project some crucial design decisions have been made. The gap between subject and object, which the ancient Greeks had already thought about, started to be conceptualized in a much more polarized way than ever before. Descartes distinguished between res extensa and res cogitans.
Philosophical interest turns to the subject, to consciousness, to the possibility of cognition and human rationality (Weber 2003, 27). Nature is turned into an object of cognition, in other words, science 'invents' nature as its object. It is incredibly successful in doing so and science gains ever more knowledge about it. But at the same time the divide between human cognition and the world gets bigger. The more we know about it, nature gets ever stranger to us. Nature is the non-self, the outside, the 'other'. Nature becomes conceptualized as lifeless, dead and abstract matter (ibid., 31). As science uses ever more abstract tools and methods it becomes 'constructivist'. This particular way of conceptualizing nature in science which arguably started with Renaissance opened the door to all kinds of ways of intrumentalising and operationalising it. In a movement which should become more fully understood only recently, science emancipates itself from nature and starts inventing or constructing it. But this process of the emancipation of science is slow and takes hundreds of years. The philosophical debates surrounding this process culminate in logical positivism. In the 1920s and 1930s members of the Vienna Circle tried to achieve two main things. They wanted to purge theories of knowledge from meta-physics and make philosophy a scientific way of speaking about the world. This in turn should help to guide science to become more rigidly defined and therefore more objective. Those theoretical goals led to an increased focus on formalized theories of language, logic and mathematic. Philosophical questioning of logical operators should help to find the universal logic of the world. Logical positivism had many important results and is a complex philosophical school but appearantly makes one major false inference. The logic of the operation of the human mind is projected on nature (Bachelard 1934/1988, Weber 2003). This false inference, also called the 'cultural fallacy', continues as mainstream model of understanding to-day and is where the scientific meaning and the social meaning of determinism meet. By saying that science is the only source of objective knowledge it becomes transcendent to society. This is not religious transcendentalism but means that scientific forms of knowledge transcend the historicity of creating knowledge and theories about knowledge. What is once objectively true must always - and everywhere - remain so. There is a philosophical tension between the objectivity of scientific discovery and personal and political freedom within human societies. Early 'natural philosophy', as science was called in Renaissance, freed humans from the dogmatic truth of the church. But this freedom would hundreds of years later found to be threatened by science becoming a dogma itself, a repressive ideology. According to critical theory and science studies the invention of a new concept of nature by science opens the door for its instrumentalisation. The scientific project of gaining knowledge about the 'laws of nature' means to put nature at our disposal, to operationalize and functionalize it. And rational mastery of the forces of nature implies social mastery, the dominance of one social group over another one (Marcuse 1964/1994). The absolute character of scientific knowledge weighs down from sky on the life of people on earth.
Logical positivism gained a defining influence on the philosophy of science in Britain and the USA after WWII. Moreover, the positivists deep engagement with logic and formal thinking contributed to the newly emerging disciplines of computer science and cybernetics. At the same time science had to open up to the possibility of increased indeterminacy, after Heisenberg's discovery of an objective indeterminacy on the level of matter. Ideas of a mechanistic universe have been put aside with quantum theory. Since then, the main questions in the epistemology of science concern relationships between determinacy and indeterminacy.
Critical Theory is inspired by the analytical method which Karl Marx developed when writing Das Kapital, but went further than Marx and could even turn against him (cf. Cox et all 2004, 8). Marx has shown that technologies are embodiments of social relationships (Marx 1957). In capitalist societies technologies, far from being neutral, are developed with specific social relationships in mind. Critical theory, inspired by Marx, sees the technology that we have as a specific type of technology developed under a capitalist economy (Marcuse 1964/1994). According to Herbert Marcuse an ideology of dominance was intrinsic to the development of the scientific worldview from the beginning. Each techno-social system introduced over the last 150 years, the railway, electricity, cars and highways, created "ideology embodied in the production process (Marcuse 1964/1994, 114)." It reorganised the strata of society according to the original vision contained in the design. Marcuse believed that political represssion is not so much a function of ideology but a function of an apparatus which uses people without them being able to see behind the machinery and overcome its heteronomic tendency. Heteronomy, as opposed to autonomy, means that people's lives are determined by outside factors beyond their control. In capitalism, technological progress is specifically set against the negotiating powers of workers. New inventions are designed to rationalize production and to increase worker's productivity in order to maximize profit. By investing into better machines, workers are submitted to a dialectical process of deskilling and reskilling. Marx analysed this tendency correctly even though he observed industrial capitalism in its very early phase. Since the days of Marx, the rationalization of labour continued, culminating in the Fordist factory, and ultimately in fully automated factories. Rationalization is not only carried out by investment into better machines but also by scientific management, also known as Taylorism. In the late 20th century with the help of the computer also other areas of human labour, not just physical work, can be replaced by machines. Ever more sophisticated forms of technological and organisational dominance are developed.
A second insight by Marx, which was also made productive by critical theory, concerns the fetishisation of commodities. By basing the exchange value solely on money the human labour that goes into the production of goods is hidden. The labour, equals human life-times, is not visible in the product anymore. Hiding the origins of commodities enables them being fetishized. The world appears as a world of shiny things, of decontextualized consumer products which nowadays appear all dancing and singing in TV adverts. Marx's insights about commodity fetishism and technologies as embodiments of social relationships has been of defining influence on both critical theory and a branch of science studies called the social shaping of technologies (SST). The social shaping of technology is a line of inquiry that asks why and how nature is made operational in specific ways serving particular interests. SST forces us to rethink what we mean when we speak about technology. Technology is never just technical but combines what is possible in terms of the engineering techniques of a time and what is desireable in a certain socio-historic context. Technologies do not just exist as technical artefacts but imply certain forms of social organisation which they help to create and maintain and on which they also depend. Therefore we should better think of technologies as techno-social artefacts. Those artefacts are not merely things - dead objects - but results of and constituitive for social relationships. The development of technology and capitalism in a mutually dependend interplay has gone on over a considerably long period of time. Techno-social artefacts have been created layer upon layer. Because we have become accustomed to live with and inside techno-social systems created by capitalism, we tend to forget that they are man-made and contingent. Because they have shaped our habitat for such a long time, we see them as a sort of second nature; it is SST's task to unentangle the social content of technologies (MacKenzkie and Wajcman 1985). Because in capitalism the work of people is hidden behind fetishised commodities this task has become so hard. The ideology of technological determinism masks the social content of technology and naturalises both technological progress and the capitalist economy.
This might be due to the influence of Marshal McLuhan who is generally credited as being one of the most, if not the most influential thinker on the influence of (new) media on society in the 20th century. The theory about media and social change which he developed, influenced by Harold Innis, is epitomised by the slogan "the medium is the message". According to McLuhan the way we think is determined by the proportionate relationship of the senses - the sense ratio. He believed that all technologies were extensions of us. As tools such as the knife or the axe were extensions of our body and limbs, media technologies were extensions of our senses and central nervous system.
The introduction of a new medium which favours a particular sense was of profound influence on the patterns of perception and the way we thought. Each new medium signifies a break boundary in human history and history can be presented as a sequence of a few large chapters - from oral culture to script, to print, to electronic culture. When we moved from an oral culture to a culture based on script we exchanged an 'ear for an eye'. With writing we left behind magic and the tribal world. But only with the printing press literacy could fully develop. Modern western society is a direct result of the influence of the printing press which favours the visual sense: "Civilisation is based on literacy because literacy is a uniform processing of a culture by a visual sense extended in space and time by the alphabet (McLuhan 1964/1965, 86)." Literacy is made responsible for the homogenisation of western societies; it created the preconditions for getting people used to the clock; and it automatically led to the violent birth of nation states competing for military and industrial hegemony. From the printing press it was a logical step (a logic inherent to the technology itself) to the Fordist factory, the defining technology of modern society. But then in McLuhan’s history of civilisation, at first unnoticed, with the advent of the telegraph and electric light, then more visibly with the invention of wireless telegraphy, radio and television, the 'electric age' began. The sense ratio once more changed. We exchanged an 'eye for an ear' because electronic media foster an oral culture, and accordingly we moved forward into the past of a tribal society living in a global village
Williams criticises that in McLuhan’s theory "all media operations are in effect de-socialised; they are simply physical events in an abstract sensorium (ibid., 127)." The apparent sophistication in McLuhan's approach is that he pays tribute to the specificity of media and their characteristics. But he does so on the basis of excluding all other factors such as social, cultural, political or ethical decisions made by people who by their very nature would be open to scrutiny and questioning. Whereas the initial formulation that the medium is the message is a simple formalism "the subsequent formulation - 'the medium is the massage' - is a direct and functioning ideology (Williams 1974, 127)." Williams, in 1974, said that McLuhan's particular rhetoric was unlikely to last long. But because this particular ideological representation of technology was coming from the most powerful nation state of the world, it would have its successors (ibid. 128). Richard Barbrook (2005) set out to explore that path. In Imaginary Futures (2005) he shows how McLuhan inspired a discourse which has still a lasting influence. According to Barbrook, in 1964 the 'Commission on the year 2000', also known as Bell commission, tried to formulate a plausible alternative to 'cybernetic communism'. America was still reeling from the Sputnik shock, when the Soviet Union was first capable of sending a communication satellite into orbit. As the Cold War logic locked the nuclear enemies into an arms race, any hot war was not winnable. Therefore the only way of winning the war was by showing that America had the better ideology, that it 'owned' the future. The Bell commission took McLuhan's ideas and re-rendered them in a more rationally sounding way. It created an ideology of McLuhanism which was purged from the more eccentric aspects of its originator.
At the time when the Bell Commission formulated its thesis, The US military was pouring huge resources into artificial intelligence (AI) research. J.C.R Licklider initiated a concerted effort of academic research into computer science funded, largely, by the military. One of the many research programmes funded by Licklider led to the invention of the internet. Other research areas included interaction with a computer via a graphical user interface using first a light-pen, then a mouse, video conferencing and early forms of virtual reality. As Barbrook argues, the military origins of the net and many advancements in computer science are well known, but usually brushed aside as insignificant, thereby obscuring the fact that the imaginary future of the 1960s was still the imaginary future of today. McLuhanism, a theory which was fetishised because it had de-linked itself from its origins, promised the glorious future of a post-industrial information society.
Contemporary technosciences have abandoned the 'correspondence theory' of science which demands a truthful representation of nature. Technoscienes instead construct their objects of study, they produce artefacts and hybrids in the laboratory (or on the computer) and then examine them. This method of constructivism is constituitive for the methodology of technoscience as well as for its understanding of nature (Weber 2003, 132). Over the course of the 20th century technoscience develops a radically new understanding of nature, of mind and of what is life. Natural systems of order and architectures which had been seen as unchangeable become historicised and open for modification. The dynamisation and historisation of the concept of nature implies that nature is becoming dynamic and self-organising. As cognition is increasingly recognized as being constructed, nature itself is also understood as an organising and constructing entity (Weber 2003). This shift is not marked by a break with the modernistic past, but by a radicalisation of some of its tendencies (Weber 2003, 136). It keeps some of modernism's epistemological foundations, which were used by science but not made explicit (tinkering, purposeful manipulation), but it takes them to the extreme and makes them more visible. Technoscience continues the logic of modern science by keeping a distanced relationship to nature, which is founded on a deep distrust of the possibility of gaining direct knowledge of nature and world. As Jutta Weber (2003) puts it, science can only explain how things work, not why and what for. Science does not answer ontological questions, because it is based on a deep ontological split in its very foundation (the cognitive subject vs. lifeless matter). This would not be a problem if the experimental and constructivist character of science was generally acknowledged, as an activity of humans under given socio-historic circumstances. Under such premises science cannot be expected to give answers which are eternally and universally true. This should be seen as a liberation of science from political demands, not as a weakening of its epistemological foundations (Latour 1999). But unfortunately science carries the historic baggage of objectivity and therefore technoscience turns into a battle ground over social power. There is a strong tension between the changed ontological foundation of technoscience and its continued naturalising rhetoric about nature. According to the representational strategy of technoscience nature has become a generalised formal system for processing algorithms and information. Nature has turned into a giant universal computer which transforms information, which is immaterial and free of context (Weber 2003, 220).
Instead of making it clear that this marks a fundamental ontological shift, technoscience sticks to naive concepts of realism and hides behind veils of mystification. Even though nature is no longer its object, strictly speaking, just its material, it nevertheless still uses nature as important legitimising and ideological entity. To the outside world technoscience presents itself as the science of old, involved in a disinterested pursuit of truth. Technoscience does only what nature always has done, the apologists of technoscience say. Technoscience, by creating new disciplines such as artificial intelligence (AI) and artificial life (AL) does nothing else but applying the 'principles of life' in artefacts. This is possible because 'in principle' organisms function like that flexible chameleon computer. The organising, saving, modifying and re-disseminating of information are being declared to be life's indispensable characteristics, characteristics which are fortunately shared by computers. In a tricky mimetic movement the specific qualities of the universal calculating machine and its software applications become essential characteristics of life (Weber 2003, 176). So, a reversal of principles happens, nature gets naturalised, reified, nailed down by technoscience. Value free and objectively science has to say what is the nature of nature, the nature of man, the nature of woman, and by doing so, our place in the world gets objectively determined. The narrative strategies of naturalism, biologism and positivism can be seen as 'manuals' how to declare nature to become the only foundation for norms and values (Weber 2003, p.40) - but of course this is nature as analysed, segmented, augmented, sliced and stitched together by reductionist and male dominated science. As many feminist studies of science have shown, biologism has served to legitimise the hierarchical structuring of gender relations (cf. Haraway 1985, 1996, Weber 2003). Naturalising strategies turn social relationships into matters of objective truth.
The re-interpretation of the concepts of nature, mind and life was made possible by the development and convergence of the paradigms of cybernetics, information theory and computer science. Alan Turing formulated the theory of the universal symbolic processing machine - the theoretic principles behind the computer. Of special significance is the separation of hardware and software. One and the same apparatus (mechanical and electronical) can be used to process any kind of algorithm. This introduces the new category of trans-classic machines. Earlier, machines could essentially only perform tasks they were specifically made for. The trans-classic machine can perform any operation that can be formulated as an algorithm. Claude Shannon, aided by Warren Weaver, formulated a mathematical theory of information which separated the content of communication from its carrier medium. Shannon was explicitely only concerned with the optimisation of the transmission of data via electronic networks, independently of the content of the data. Nevertheless Shannon's model was extended into a general model of communication.
Shannon's model contained an element of feedback which allowed for error correction. Aspects of two-way communication involving feedback mechanisms within machines, animals and humans also were of central concern to Norbert Wiener's cybernetic theory. At around the same time biology turned its focus to the molecule, following the reductionist strategy of science, yet also recognising properties of living things as systems understood according to the cybernetic paradigm (Weber 2003). The cross-fertilisation of those theories led to a new understanding of life as patterns of information (code) independent of the carrier medium (matter, hardware, the body). Life was no longer thought to be a property of matter but one of structure, a pattern of information, represented in the genetic code. During the second half of the 20th century technoscience rewrote body as text, used the metaphor of the immune-system and re-invented the self as (genetic) code (Weber 2003, 196-202). A key concept in the construction of this new paradigm are 'cellular automata', an idea of John von Neumann, inspired by an earlier text by Turing. Those are 'finite state machines' based on an on-off logic. Von Neumann took also inspiration from work by Warren McCulloch and Walter Pitts (Weber 2003, 160-196). They tried to develop a mathematical model for nerve functions and interpreted neurons in the brain according to an on-off logic. In the 1940s von Neumann tried to develop a computer model (on paper) which could simulate a biological neural network. Decades later, with progess in computing power and new programming techniques his concept could finally be realized. In the 1980s von Neumann's cellular automata, advances in neuroscience and computing (parallel processing) inspired 'connectionism' - a brain-computer analogy based on an assumed analogy between the network of neurons in the brain and the interaction of cellular automata in parallel processing computers - so called neural networks (Turkle 1995).
Another key development was the attempt to create a computer based artificial intelligence (AI). Since the late 1950s AI tried to construct machines which were intelligent, whereby a limited notion of intelligence was applied which prioritized symbolic operations and logical thought (Edwards 1996). Major funding for this project came from the US military. Following a 'closed worlds' logic of containment during the climactic years of the Cold War, the military tried to eliminate the slow and error prone human factor from decision making in fully automated and closely corresponding weapon systems involving early detection radar systems (SAGE) and nuclear retaliation capabilities by intercontinental ballistic missiles (Edwards 1996). The project of AI had the not so insignificant side effect of channeling massive financial resources into nascent computer sciences and build ever faster supercomputers (ibid.).
The project of AI was of significant influence on areas it came in touch with. Psychology, which until the 1950s had been dominated by behaviourism, now dared to turn its attention to internal states of mind. A new type of psychology was invented, which described inner states in terms of rules and logic - cognitive science. According to Sherry Turkle computer science was its 'sustaining myth' (Turkle 1996, 128). A sustaining myth is not an explicit part of a theory but an unacknowledged assumption which is called upon in representative strategies. To serve the specific needs of AI, the discipline of linguistics was reshaped as computer linguistics (Edwards 1996). As the objectives of scientific disciplines were redefined and new sciences were created the understanding of the human mind was fundamentally reshaped. Thinking became an act of information processing. The act of creating an 'intelligent' computer implied that intelligence was a function of computation. Joseph Weizenbaum, a professor in the Department of Engineering and Computer Science at the MIT, became the first prominent critic of the mechanistic approach to concepts such as mind, intelligence and consciousness. As a rather lone voice in the 1960s within the computer science community his criticism of the trivialisation of life was of no big effect.
AL and the closely related field of emergent artificial intelligence (AI) were developed in the 1980s through combined theoretical and practical efforts in computer science, cybernetics and biology rebranded as life science. The cybernetic paradigm had made possible the parallelisation of nonorganic and organic systems as open and changeable systems. Both, organic and non-organic systems can be conceptualized as consisting of variable components whose properties can be formulated according to communication- and information-theoretical models. "This makes not only possible the technologisation of the living but also the making seem alife of technology [trans.A.M.] (Weber 2003, 139)." When life basically can be described as a pattern of information - the genetic 'code' -, then information can also be seen as alife (Yoxen 1986, Weber 2003). Under this basic premise the new disciplines of emergent AI and artificial life (AL) were developed. Using new programming techniques such as genetic algorithms,8 life-like phenomena were simulated inside computers. Some scientists such as Richard A Langton and Tom Ray stand for a 'strong' approach in AL (Turkle 1995, Reichle 2005). They do not interprete replicating pieces of code as reasonable simulations of life but as living beings in the literal sense. A similar trend is observable in so called bottom-up robotics (Steels 1999, Brooks 2002) whereby robots are programmed to develop forms of emergent behaviour. Emergence is a key concept in AL and the related area of emergent AI. It means that systems are capable of arriving at a higher level of organisation spontaneously. That qualitative leap can not be programmed into systems from the top-down but can only emerge from the interaction of individual pieces of code known as 'agents' in a bottom-up way. Those software agents - strings of code that represent relatively simple actions and behaviours - are called autonomous agents. They have been designed by a programmer initually but equipped with ways of 'learning' their future interactions are not predictable. Emergent behaviour can be simulated in digital systems and also in robot systems conceptualized as embodied AL.
Technoscience is more than the activity of researchers doing their work. It is also projecting an image of itself to key audiences and the public at large. This discourse, inspired by science, but going beyond it, uses narrative strategies aimed at persuading the world that its actions are not only justified but necessary. It uses scientific findings, popular science, visual means (computer graphics, animations) and sensational announcements to shape the image of technoscience. All elements of this discourse together, and the sort of images and intellectual representations it creates, are referred to as the techno-imaginary.
Technoscience claims to be doing nothing but its job, but is actually massively involved in representational politics, not only with its practices and interventions but also its promises. According to Haraway, the promises of technoscience make its main social importance. "It does not matter if they ever get realized, what matters is that those ideas always remain alive in the timezone of unbelievable promises (Weber 2003, 144)." Actually, it is better if those promises never get realized, so that the expectation can be kept alive. The prophecies of technoscience about the future already shape the present. And, as Richard Barbrook points out, current activies in technoscience and related discourses in academic writing and the press are shaped by the technoimaginary of the past (Barbrook 2005). Some of the more extreme threats and promises of technoscience's need to be seen in the light of this strategy of the unfulfilled prophecy. The techno-imaginary projects futures in a grey zone between science and science fiction. Biologists in search of the 'secret of life' promise to slow down the ageing process so that the life-span could extend to hundreds of years and potentially, immortality. The robotics scientist Hans Moravec has predicted that robot intelligence would soon overtake human intelligence and render human life meaningless unless we decide to become robots, or cyborgs, too (Moravec 1998). The same author also wishes to upload himself to a main frame computer and continue a life freed from the fetters of bodily existence, not unsimilar to his collegue Marvin Minsky, who, like other cyber-Platonists, suggests that the body is only a burdon without which we would do better. Tom Ray's Tierra project was already mentioned. Here, small bits of code forming an 'information ecology' competing for resources inside a computer's RAM (rapid access memory), are considered to be new forms of life. Those proposals are easily dismissible as fantasmagories, yet they serve an important function within the discourse of the techno-imaginary by diverting the attention from the mainstream discourse of technoscience, which, on closer looks, tries to appear more rational yet is based on similar fundamental shifts in the understanding of the being of nature and humans.
When I reviewed the literature on media art, it became apparent that there is a problem with finding systems of classification, of categorisation and even a clear definition of the art form. Despite a 25 year history of media art, and some would say it's much longer, this work is only just beginning. "The terminology for technological art forms has always been extremely fluid" says Christiane Paul (2003). According to her, 'digital art' has first been called computer art, then multimedia art and is now subsumed under the umbrella term 'new media art' (Paul 2003, 7). Other words which have been used to refer to the field as a whole or to sub-genres of it are: electronic art, art & technology, video art, software art, net art, generative art, information art, virtual reality art, game art, tele/robotics art, hypermedia, hypertext, interactive installation. Potentially this list could be much longer. The choice of different terms for more or less the same thing often betrays a preference for a certain flavour: someone is speaking historically situated and from a specific theoretic or artistic perspective - Bourdieu's position taking. For instance, while some artists are happy being labelled as net artists, others prefer to talk about telematic art, whereby the latter appears to give the field more gravity.
It is not a diffuse 'essence' of media art which justifies it to speak of it as a separate field but the existence of a system of institutions which are more or less exclusively concerned with it. Institutionally media art is characterised by the existence of two types of institutions. On one hand there are large festivals, such as Ars Electronica, since 1979 held in Linz, Austria, and large brickand-mortar institutions such as the ZKM in Karlsruhe, which attract major funding, organise big exhibitions and produce heavy catalogues. On the other hand there are many small institutions, sometimes called 'self-institutions'11 - so called media labs or hack labs - which have been thriving over the last 10 years, forming an alternative or 'unstable' field (Druckrey 2005) with increasingly world-wide connections and a more decentralised and networked approach. Whereas the large institutions face typical pressures for legitimisation such as demands to be instrumental in regional development, the world-wide network of small institutions often lives on shoe-string budgets mostly provided by state funding agencies. Some activities are not funded at all or are rather selffunded - made possible by the energy and work of participants. According to Bourdieu this area could be called a field of restricted production. Economically it is insignificant but discursively it is important. I am not trying to construct a binary opposition between two types of institutions and acknowledge the existence of many medium sized institutions and a lively transfer between the fields. However, it is important to state that there is an institutionalised field and that it is not homogenous but heterogeneous.
Histories of media art are put into a trajectory of the genealogy of media technologies rather than art history. In The Automation of Sight: From Photography to Computer Vision, Lev Manovich (1996), draws a direct line from the invention of perspective to computer generated images. He also places this trajectory within a history of automation. "By automating perspectival imaging, digital computers completed the process which began in the Renaissance (Manovich 1996, 231)." But, as Manovich points out, the inventor of the algorithm which makes perspectival rendering on computers possible, Lawrence G.Roberts, had a 'more daring goal' in mind than creating a tool for art. The computer should not only be able to render but also to 'understand' 3-D images (through pattern recognition). Thus, the project of 3-D computer generated images was a part of the project of AI in the context of the Cold War. Yet Manovich portrays this in an euphemistic language, presenting computer vision as "the culmination of at least two histories, each a century long" (ibid., 233), the history of mechanical devices designed to aid human perception, and the history of automata. Manovich does mention that the history of automation is situated in the context of rationalisation in the industrial process and that the Czech word Robot means forced labour, yet he does not spell out what this means. Instead he celebrates 3-D imaging as technology's inevitable progress. Siegfried Zielinski comments on this frequently encountered narrative strategy.
Zielinski demands that we should not continue to find the old confirmed in the new (Zielinski 2002, 11). In those readings, history turns into a promise of continuity, a celebration of progression. He thinks that this is boring as well as paralysing for the work of the mediaarchaeologist. He demands instead to find the new in the old, to let ourselves be surprised and not just look for confirmation of what we already know. As a counter-strategy Zielinski proposes the concept of a 'deep time of media' in the form of a un-archaeology (ibid., 13) which opens up spaces for the imagination. Too quickly we tend to orient ourselves toward a new 'master medium' after which all symbolic systems have to be re-arranged, until the next master medium arrives (ibid., 17).
Christiane Paul presents a slightly different trajectory. She claims that "the notion of interactivity and 'virtuality' in art were explored early on by artists such as Marcel Duchamp and László Moholy-Nagy in relation to objects and their optical effects (Paul 2003, 13)." According to Paul, Duchamp's work was "extremely influential in the realm of digital art" because of the "shift from object to concept" (ibid., 13). Paul formulates a genealogy of digital art slightly different from Weibel's or Manovich's, emphasising the influence of Duchamp via OULIPO, a French literary group, to Fluxus and conceptual art. The conceptual 'link' here is that Dadaists, the OULIPO writers and Fluxus artists frequently created art works which were based on the execution of a set of instructions and/or rules, which can be compared to computer algorithms, which are, conceptually speaking, nothing else but sequences of instructions carried out in loops (ibid., 13). This view is supported by Peter Suchin who argues that the art of the 1960s, "institutionalised under the collective heading of 'Conceptual Art' and its legacies [...] is a key determinant of today's new media art practices." (Suchin 2004, 67) Other conceptual links between contemporary media art and art movements in the past focus on the exhibition Cybernetic Serendipity, at the ICA, London 1968, (Paul 2003, 18), as well as on the exhibition 'Software Art', curated by Jack Burnham in 1970. Younger artists who are now using the term 'software art' for their own work are claiming Burnham's show as a conceptual predecessor (Goriunova and Shulgin 2003). However, there is no continuity between the surge in cybernetic art in the late 1960s and the reappearance of the 'cyber' paradigm in the 1980s. There is even less continuity between Burnham's Software Art show which remains an early and isolated example from which there can be traced no continuous line toward software art in 2005. Thus, when such long jumps are being made, it is reasonable to assume that a desire for historic legitimisation is at work. Such moves can also be seen as following the logic that Bourdieu describes in Principles for a Sociology of Cultural Works.
Despite that there are obviously problems with history writing, I want to present a short overview of themes and positions which have been taken in a 'deep' history of media art. The most obvious theme is the notion of techno-utopianism. Hereby I differentiate between a totalitarian technoutopianism and a more participatory or democratic form of utopianism. Futurism, Suprematism and Constructivism formulated a programme of techno-utopianism which demanded that artists should use science and technology to help create the utopian society populated by the new man. Humanity reinvents itself based on the powers of science and technology. Art is carrying the banner of an utopianism which is totalitarian. The poetic writing of Khlebnikov about radio which becomes 'the heart and brain of the future society' (Khlebnikov 1921/2005) is characterised by a one-size-fits-all solution. Similar sentiments and totalitarian leanings are contained in the radio manifesto by Marinetti and Masnata (1933/1992). The high point of modern art was also the high point of modernity. Connecting new communications media with a utopian vision of society was not exclusive to art but prevalent in societies both sides of the Atlantic. Between 1900 and 1939 techno-utopianism was a common strand independently of the political ideology. While the Russians supported communism and Marinetti supported Mussolini, other techno-utopians such as Marconi and Edison built monopolistic business empires. In Not Just Another Wireless Utopia I have compared the different utopian visions competing at around 1900 and how they relate to the utopianism surrounding the internet and mobile telephony (Medosch 2004). Linking media with utopian social ideas is not unique to the 20th century. Richard Barbrook traces the political roots of media totalitarianism in the name of media freedom back to the French Revolution. The Jacobean's idea of media freedom was that the revolutionary elite needs to tightly control all media (in those days the press) because unfortunately the masses are not yet able to act in their own best interest.
In the 1980s a particular type of media art started to gather momentum. The media art scene, for which the Ars Electronica Festival in Austria provided a platform, became increasingly international, with contributions from Japan, Brazil, Australia, Canada, the USA and Europe. Changes in the thematic orientation of the festival allow charting the rise of this new type of media art. In the early 1980s the festival presented itself as an odd mix with some pioneers of cybernetic art such as Herbert W. Franke and Otto Piene attending, but also spectacles aimed at winning over large audiences, musical concerts and even operatic productions in public space involving workers from a local steel factory. As Peter Weibel recalls, in those early years Ars Electronica had been almost everything it possibly could have been, "an Ars Metallica, an Ars Pneumatica, an Ars Pyrotechnica, only not an Ars Electronica (cf. Weibel 1999, 72)." This changed, according to Weibel, when he and Gottfried Hattinger gained more influence on the festival's direction from 1986 onwards. In 1987 Ars Electronica for the first time had a theme, The Free Sound. In 1989 networks became the festivals theme for the first time with In the Network of Systems. In 1992 Peter Weibel took sole responsibility for the festival's artistic direction and presented the scientific disciplines endophysics and nanotechnology as the festival's themes. In the following year the theme was Artificial Life - Genetic Art (cf. Weibel 1999, 72-74). By that time a certain type of media art, which I call high media art, was established as the leading paradigm. In the decade between 1985 and 1995, high media art developed its forms, its milestone works and its narrative strategies, which altogether were successfully deployed in institution building. I call this form high media art for two main reasons, because it resonates with high-tech as well as with high-brow or high-cultural values. For the realisation of those works expensive and complex technology was used - which implies issues of accessibility, inclusiveness and structural dependencies. The works themselves usually presented themselves as clean, large scale productions in a sterile technological atmosphere. The dirt of the streets, so present in Gibson's Neuromancer (Gibson 1984), was rarely to be felt at high media art exhibitions. The digital aesthetics of high media art was compatible with the black cube inside the white cube of the museum: as viewers enter a darkened room with multiple projection screens they are made aware that they are entering the holy inner sanctum of a shrine to the digital; the same aesthetics is also compatible with corporate lobbies or boardrooms, with steel, glass and transparency. As the yet to be built institutions will cost a lot of money, the institutional projects have to be pitched at the highest level in industry and politics, and those circles need to get assured that they are getting value for money - high-tech, high-end art.
Shown at Ars Electronic in 1993, which had the theme Artificial Life - Genetic Art, Sommerer and Mignonneau's Interactive Plant Growing was considered one of the most advaned works of art in this genre. They went on to create Trans Plant (1995-1996) and Life Spacies (1997), works which are variations on the theme of AL. Trans Plant was realized at the ATR - Advanced Telecommunications Research Laboratory - in Kyoto, where the artists hold a research residency, and was commissioned for the permanent collection of NTTICC, Tokyo (Reichle 2005). In this work the artists used a 3-D video-key technology which they had developed themselves and which they patented. As soon as the user enters the exhibition space an image of her within a threedimensional jungle of virtual plants is created. Movements of the user influence the plant growth. The works of Sommerer and Mignonneau are shown around the world and considered to be realising many demands of the paradigm of high media art. They do not create static objects but a framework for a processual exchange between viewer and art work. In most of their works, if there is no viewer, nothing happens at all. The result of the interaction appears to be indeterministic to a certain degree. AL algorithms formulate local rules of interaction so that the exact result cannot be predicted. However, whereas the growth of plants appears to be spontaneous, the extent to which the viewer can actually 'interact' with those virtual worlds is very limited. The framework has been entirely defined by the artists and the viewer/user is left with a few simple forms of physical interaction such as moving hands over a plant or moving the body in front of a screen. The 'message' of the work appears to be very similar to the message of the discipline of AL as a whole: the principles of life can be replicated in digital code. The major difference then is that the artists have developed more beautiful visuals than the scientists. Instead of challenging the naturalistic assumptions behind AL, artists lend their aesthetic skills to the illustration of science and thereby help to advertise the achievements of technoscientific progress.
The claim that 'the world itself' was 'digitally organised' belongs to the class of strong ontological statements. Weibel echoes Flusser's theory of 'digital apparition' (Flusser 1996) where he writes that there was no ontological difference between reality and technological images such as those created by virtual reality techniques. If the latter seem 'less real' then this was only a function of their lower resolution. Turning the attention to digital apparition has the benefit of making us recognise that reality is an apparition too. "What remains is that everything is digital, i.e. that everything has to be looked at as a more or less dense distribution of point elements, of bits (ibid., 244)." Therefore, what we call real, "are those areas, those curvatures and convexities, in which the particles are distributed more densely and in which potentialities realise themselves (ibid., 244)." As Flusser suggests, this parallelism of digital and real is "the digital world picture as it is being suggested to us by science (ibid., 244)." High media art falls in line with an explanation of the ontological status of the world provided by technoscience.
Noticable is the difference between the Ascott of 1984 (Grundmann ed. 1984) and of 1996 who is increasingly working with esoteric concepts. There are many religious motives in high tech spiritualism, not all Christian. A deeper study of religious motives in the discourses of the technoimaginary is not the focus of this paper. However, ideas of a ghost in the machine or the idolatry of machinery are too many to be ignored. High-tech new age spiritualism is pervasive in discourses of technoscience such as AI and AL and is replicated in the works and words of some high media artists such as Roy Ascott.
The discourse on high media art ignores the conditions of its own creations. Nobody seems to be uncomfortable with the degree to which the artists are dependent on the computer industry and technology corporations such as Deutsche Telekom, NTT or Canon. Artists producing work at the highest technical and aesthetic level need to get supported by institutions. They need to accept high levels of separation of labour and bureaucratic management. To a degree, which would need to be established specifically for each work, the works are determined by the black box character of proprietary hard- and software, by the products of commercial software applications and by the skills of the actors involved. In a very direct way the art work is determined by the technologies used. Since in media art works form cannot be clearly separated from function, this is of major significance. It would be naive to assume that the institutionalised context does not have a bearing on the form and content of the art work (Mitchell 2003). The discourses on high media art and postmodernism share a strong focus on information and immateriality. This is something they have in common with technosciene which conducts its own strategy of dematerialisation. High media art provides high class illustration to both post-modern ideas and concepts of computer science, and unconditionally accepts the premises on which those are built. It fails to take a critical position against ideologies of dominance embedded in those theories. As Jutta Weber (2003) argues, although most post-modern theories are civilization critical, they are unable to challenge the ideology of technological determinism inherent to technoscience. As Barbrook (2005) points out, information becomes fetishised in those theories. The two grand narratives of modernism, Adam Smith's liberalism and Marx's analysis of the capitalist political economy agree at least insofar as they see humans at the driving seat of history. Post-modern theories, by dismissing those grand narratives, are actually saying that history was a process without a human subject (Barbrook 2005). High media art takes a similar position by insisting on the ontological status of the work of art as being digital. The seemingly progressive digital aesthetic is socially and politically conservative. The discourse of high media art endorses a version of McLuhanism which has technological determinism at its core.
The narrative strategies of Weibel, Ascott et al succeeded in institution building. But ironically, after high-media art climaxed at around 1995, it soon lost its discoursive relevance. A new paradigm unfolded with the mass popularisation of the internet. The 'really existent' internet showed to be rather different from what the gurus of cybernetic art had imagined it to be. People learned to do their email, download a video or a song, but failed to encounter the ghost in the machine or telematic consciousness. The ordinaryness of life on the net took over. Actually, nobody ever really lived 'on' the net. We learned to step back and understand that our bodies are still real. The ZKM and Ars Electronica remain powerful institutions, but the new aesthetical strategies are now developed elsewhere. Socially and politically aware artists shape the discoursive agenda outside the institutional context provided by high-media art. Weibel, just like the software giant Microsoft, had misinterpreted the relevancy of the internet. Instead of glorification of the products of multinational corporations net artists high-light the participatory culture of the internet. Microsoft quickly developed its own browser software Internet Explorer. Weibel got the guest curator Benjamin Weill to curate the large scale exhibition net_condition in 1998 (Greene 2004). But the discoursive bandwaggon had already left the station. Digital artists joined new alliances with hackers developing free and open source software and a new discourse on art, activism and free or copyleft culture florished. High media art, by winning institutional power, lost its symbolic capital. The discourse of high media art, which had all chances to do so, had not generated foundations on which it was safe to build. As I hope to have shown, the ideology of technological determinism has prevented it from doing so. High media art with its high-tech visions has won a phyrric victory. At the same time the technologisation of society continues and a strong critical art movement dealing with issues surrounding technology and society is as urgently needed as ever.
‘‘In the bubble’’ is a phrase used by air traffic controllers to describe their state of mind, among their glowing screens and flows of information, when they are in the flow and in control. Lucky them. Most of us feel far from in control. We’re filling up the world with amazing devices and systems—on top of the natural and human ones that were already here— only to discover that these complex systems seem to be out of control: too complex to understand, let alone to shape, or redirect. Things may seem out of control—but they are not out of our hands. Many of the troubling situations in our world are the result of design decisions. Too many of them were bad design decisions, it is true—but we are not the victims of blind chance. The parlous condition of the planet, our only home, is a good example. Eighty percent of the environmental impact of the products, services, and infrastructures around us is determined at the design stage. Design decisions shape the processes behind the products we use, the materials and energy required to make them, the ways we operate them on a daily basis, and what happens to them when we no longer need them. We may not have meant to do so, and we may regret the way things have turned out, but we designed our way into the situations that face us today. The premise of this book is simply stated: If we can design our way into difficulty, we can design our way out. ‘‘Everyone designs,’’ wrote scientist Herb Simon, ‘‘who devises courses of action aimed at changing existing situations, into preferred ones.’’ For Victor Papanek, too, ‘‘design is basic to all human activities—the placing and patterning of any act towards a desired goal constitutes a design process.’’ Designing is what human beings do.
Two questions follow this understanding of design. First, where do we want to be? What exactly are the ‘‘preferred situations’’ or ‘‘desired goals’’ that Simon and Papanek talk about? Second, how do we get there? What courses of action will take us from here to there? Although this book addresses those two questions, it is not about the future, and it is not really about the new. I have organized the chapters that follow around ten themes that deal with daily life as it is lived now—not around fantastical science fiction futures. And I will tell you about aspects of daily life in which radical innovation is already emerging: Nothing you read here is a promise or a fantasy that may, one day, come true. One of the things that drove me to write this book was boredom with the schlock of the new. Many of the ‘‘preferred situations’’ that Simon talked about already exist—but in a different and often unexpected context. One of the things you can do next Monday morning, after reading this book, is walk out of your door and take a look around. I am confident you will be surprised by the variety of social innovation taking place in your environment. I have been. That said, addressing the question ‘‘Where do we want to be?’’ brings us up against an innovation dilemma. We’ve built a technology-focused society that is remarkable on means, but hazy about ends. It’s no longer clear to which question all this stuff—tech—is an answer, or what value it adds to our lives. Too many people I meet assume that being innovative means ‘‘adding technology to it.’’ Technology has become a powerful, self-replicating system that is accustomed to respect and receives the lion’s share of research funding. In NASDAQ, tech even has its own stock exchange.
I do not suggest that we have fallen out of love with technology, more that we are regaining appreciation and respect for what people can do that tech can’t. Throughout the modern age we have subordinated the interests of people to those of technology, an approach that has led to the unthinking destruction of traditional cultures and the undermining of forms of life that we judged, once, to be backward. The victims of this approach to modernization have not just been hapless people in rain forests. ‘‘Getting people to adapt’’ to new technology has affected us all. We believed that the assembly line and standardization would make the world a better place, yet along with efficiency came a dehumanization of work. We act no less as slaves to the machine today when we lambaste teachers as ‘‘obstacles to progress’’ when they do not embrace the latest technological fix for education.6 The introduction of a new mass technology—telegraph, railway, electrifi- cation, radio, telephone, television, automobiles, air travel—has always been accompanied by a spectacular package of promises. A certain naı¨vete´ is excusable for the inventors of those early technologies: They had no way of knowing about the unforeseen consequences of their innovations. Today, we don’t have that alibi. We know that new technologies have unexpected consequences.7 The worst kind of tech push combines irresponsibility with wishful thinking. One of the worst current offenders is biotech. When Eugene Thacker (no relation) studied the biotech industry for a book he was writing, he encountered ‘‘blatant disparity between hyper-optimism and an overall lack of concrete results.’’ The future promises of biotech are many and far reaching, but Thacker could not help noticing the comparative absence of any concrete, widespread, sustainable results of the application of biotech in medicine and health care. We are victims, says Thacker, of ‘‘biotech imagineering’’ by vested interests that participate in the assemblage of enticing future visions.
Being skeptical about technology does not mean rejecting it. There’s a lot of technology in this book. For one thing, we don’t have an either/or choice: Terra firma, and terabits, are both here to stay. Broadband, smart materials, wearables, pervasive computing, connected appliances, and other stuff we don’t know about yet will continue to transform the ways we live. The question is, how? Means and ends have lived apart too long in discussions of innovation. Understanding why things change—and reflecting on how they should change—are not separate issues. In the pages that follow, I try to reframe issues of technology and innovation in ways that make it easier for nonspecialists to engage in meaningful dialogue—as things happen. Theodor Zeldin calls this the transition from an age of specifications to one of deliberation.10 We cannot stop tech, and there’s no reason why we should. It’s useful. But we need to change the innovation agenda in such a way that people come before tech. It will be an ongoing struggle, of course. From nineteenth-century mill owners to twentieth-century dot-commers, businesspeople have looked for ways to remove people from production, using technology and automation to do so. A lot of organizations will continue on this path, but they’re behind the times. This book is about a world in which well-being is based on less stuff and more people. It describes an approach to innovation in which people are designed back into situations. In these situations, we will no longer be persuaded that to be better off, we must consume more trashy products and devices. The following pages describe the transition, which is already under way, from innovation driven by science fiction to innovation inspired by social fiction. I’ve collected the best examples I could find of designed services and situations in which people carry out familiar, daily-life activities in new ways: moving around, learning, caring for each other, playing, working. Some of these services involve the use of products, or equipment, to carry them out. This equipment ranges from body implants to wide-bodied jets. But objects, as a rule, play a supporting role. New principles—above all, lightness—inform the ways they are designed, made, used, and looked after. The design focus is overwhelmingly on services and systems, not on things.
One issue we need time to reflect on concerns the sheer number of people we have in the world. The planet’s population has doubled in my generation’s lifetime—something that never happened to a generation before. You and I are the first human beings who have had to adjust to such an explosion of numbers. And yet we persist in the pursuit of ‘‘laborsaving’’ devices and services—using tech as the means. It’s not that we’re dumb. On the contrary, many millions of people have exerted great intelligence and creativity in building the modern world. It’s more that we’re being swept into unknown and dangerous waters by accelerating economic growth. On just one single day of the days I have spent writing this book, as much world trade was carried out as in the whole of 1949; as much scientific research was published as in the whole of 1960; as many telephone calls were made as in all of 1983; as many e-mails were sent as in 1990. Our natural, human, and industrial systems, which evolve slowly, are struggling to adapt. Laws and institutions that we might expect to regulate these flows have not been able to keep up. A good example is what is inaccurately described as mindless sprawl in our physical environment. We deplore the relentless spread of low-density suburbs over millions of acres of formerly virgin land. We worry about its environmental impact, about the obesity in people that it fosters, and about the other social problems that come in its wake. But nobody seems to have designed urban sprawl, it just happens—or so it appears. On closer inspection, however, urban sprawl is not mindless at all. There is nothing inevitable about its development. Sprawl is the result of zoning laws designed by legislators, low-density buildings designed by developers, marketing strategies designed by ad agencies, tax breaks designed by economists, credit lines designed by banks, geomatics designed by retailers, data-mining software designed by hamburger chains, and automobiles designed by car designers. The interactions between all these systems and human behavior are complicated and hard to understand—but the policies themselves are not the result of chance. ‘‘Out of control’’ is an ideology, not a fact.
To do things differently, we need to perceive things differently. In discussing where we want to be, breakthrough ideas often come when people look at the world through a fresh lens. One of the most important design challenges I pose in this book is to make the processes and systems that surround us intelligible and knowable. We need to design macroscopes, as well as microscopes, to help us understand where things come from and why: the life story of a hamburger, or time pressure, or urban sprawl. Equipped with a fresh understanding of why our present situations are as they are, we can better describe where we want to be. With alternative situations evocatively in mind, we can design our way from here to there. Macroscopes can help us understand complex systems, but our own eyes, unaided, are just as important. All over the world, alternative models of organizing daily life are being tried and tested right now. We just need to look for them. When Ezio Manzini ran design workshops in Brazil, China, and India to develop new design ideas for an exhibition about daily life, he encountered dozens of examples of new services for daily life he had never thought of before—and also new attitudes. In many different cultures, he discovered, ‘‘an obsession with things is being replaced by a fascination with events.’’ Both young and old people are designing activities and environments in which energy and material consumption is modest and more people are used, not fewer, in the ways we take care of people, work, study, move around, find food, eat, and share equipment. In a less-stuff-more-people world, we still need systems, platforms, and services that enable people to interact more effectively and enjoyably. These platforms and infrastructures will require some technology and a lot of design. Some services will help us share the load of everyday activities: washing clothes on the roof of apartment blocks, looking after children, communal kitchens and gardens, communal workshops for maintenance activities, tool and equipment sharing, networks and clubs for health care and prevention. The most important potential impact of wireless communications, for example, will be on the resource ecologies of cities. Connecting people, resources, and places to each other in new combinations, on a real-time basis, delivers demand-responsive services that, when combined with location awareness and dynamic resource allocation, have the potential to reduce drastically the amount of hardware—from gadgets to buildings—that we need to function effectively.
There are many things wrong with design in our world, but designers, as a group of people, are not the problem. Thirty years ago, in Design for the Real World, Victor Papanek observed that ‘‘there are professions more harmful than industrial design—but only a few.’’ This kind of blaming and shaming is counterproductive and unjustified. The world contains its share of selfish and incurious designers, of course. But no designer that I ever met set out to wreck the planet, force us to eat fast food, or make life miserable. Our dilemma is that small design actions can have big effects—often unexpectedly—and designers have only recently been told, with the rest of us, how incredibly sensitive we need to be to the possible consequences of any design steps we take. Another reason not to blame designers for our ills is that many of them are working hard, right now, to fix them. They are designing new services and systems that are radically less environmentally damaging, and more socially responsible, than the ones we have now. This book contains many examples of their often-inspiring work. But the challenges and opportunities that face us will not be solved by designers acting on our behalf. On the contrary: As we suffuse the world with complex technical systems—on top of the natural and social systems already here—old-style top-down, outside-in design simply won’t work. The days of the celebrity solo designer are over. Complex systems are shaped by all the people who use them, and in this new era of collaborative innovation, designers are having to evolve from being the individual authors of objects, or buildings, to being the facilitators of change among large groups of people. Sensitivity to context, to relationships, and to consequence.
The chicken breast packets in my supermarket in Amsterdam bear a photograph of the Swedish farmer who rears the birds. He is leaning on the wooden fence of an attractive-looking farm. Behind him are blue sky and green trees. The label recounts a little story about the town where the farmer lives. Before you ask: No, they don’t show a picture of the exchicken itself—but I’m nonetheless intrigued. What’s going on here— why am I being provided with this background information? It’s a packet of chicken, not a package holiday. My questions contain their own answer. The farmer’s locality has become as much a product as the chicken’s leg. The legs of dead chickens look and taste pretty much the same, and it’s a challenge to make each one look attractive and different. Human beings and places, on the other hand, are different from one another. Associate your product with nice people, and a nice place, and it should do well. My chicken-in-a-context is an example of how the focus of both business and social innovation is shifting from locomotion—sourcing things in poor places and shipping them to rich ones—to locality. Authenticity, local context, and local production are increasingly desirable attributes in the things we buy and the services we use. Local sells, and for that reason is a powerful antidote to mobility expansion. But design to enhance locality is easier said than done. Localities contain a lot of nature, for example, and nature is the result of millions of years of iterative, trial-and-error design. Biologists describe as choronomic the influence on a process of its specific context. Choronomy adds value, but often in ways we do not yet understand. Janine Benyus counsels humility in the face of how little we know about even small natural locations.
Social contexts, too, are more complicated the closer you look. The kind of design that focuses on the shape of buildings or that draws thick lines across maps with a felt-tip pen, reconfiguring whole neighborhoods at a stroke, is not well-suited for local situations. The lesson is that design for locality is not about a return to simplicity; it involves dealing with more complexity, not less. Locality matters not just as a place to sell things, but as a medium of innovation. Social contexts, for example, determine the ease with which new ideas, trends, and social behavior spread through populations. ‘‘Once you understand that context matters,’’ writes Malcolm Gladwell, ‘‘you realize that specific and relatively small elements in the environment can serve as Tipping Points.’’ Disregard for context is one of the main reasons, for me, why the new economy failed. Dot-commers promoted ‘‘anytime, anywhere’’ over and above the here and now—and we didn’t buy it. As I explained in chapter , globalization brought with it numerous assertions that economic power is less and less rooted in a place. Distance is dead, geography is obsolete, the pundits declared. They argued that sophisticated distribution and logistics systems, computer-integrated manufacturing and design, and direct marketing have changed what it means to design, produce, distribute, or sell a product or service. Investor pressure to reduce costs, more or less regardless of the consequences, increased pressure on companies to move production around constantly in search of low-cost materials and cheap labor. As the distance between the producers of products or services and their users grew as a result, activities that used to be centralized downtown were steadily dispersed. Two geographers, Stephen Graham and Simon Marvin, described this phenomenon as ‘‘splintering urbanism.’’ Many cities, persuaded that they were now in competition with one another, embraced the concept of marketing. Some started to think of themselves as brands. At first, many were persuaded that snappy communications were the key to success; these places spent lavishly on logos, slogans, and corporate identities. Many of these campaigns were banal— ‘‘Glasgow’s Miles Better,’’ ‘‘EuroLille,’’ and the like—but advertising and design consultants did good business peddling these surface treatments, which persist to this day.
The trouble is that place marketers are not alone in missing this point. Cultural producers, too, are stuck in a point-to-mass mindset. I attended a meeting in Amsterdam on the subject of ‘‘hosting.’’ The invitation posed an interesting question: ‘‘What is the relationship between art biennales and their host cities?’’ Many international art power brokers turned up for this meeting, which was hosted by an organization called Manifesta. At the meeting, the curators and critics and producers seemed to be most interested in ‘‘viewers’’ and ‘‘audiences’’ and ‘‘publics.’’ It dawned on me, as I listened to the art world’s heavy hitters in action, that art has become most attractive to the interests it once ridiculed. The tourism industry loves art because its events and museums are ‘‘attractions.’’ Property developers love art because a bijou gallery lends allure to egregious projects. For city marketers, an art biennale bestows an aura of intelligence on a city. Planners are bewitched by the idea that if they can only lure the ‘‘creative class’’ to their city, their place will become more glamorous. ‘‘Our events are not summer camps,’’ pleaded Franco Bonami, director of the Venice Biennale. Bonami invited more than five hundred artists to that year’s event. But he did not mention one single word about what, if anything, these five hundred people had to say—or why the rest of us should care. After two hours I had to leave. ‘‘Hosting’’ felt like a sales meeting for Saga Holidays. So then I went to Japan where Prada, which at the time was said to be 1.5 billion euros in debt, had lavished 87 million dollars on a new Herzog and de Meuron–designed store in Tokyo. ‘‘Shopping,’’ a public relations person gushed in the press, ‘‘is the fundamental purpose of cities today.’’ In a busy Tokyo street the new store’s Plexiglas exterior, which is like bubble wrap, certainly stood out—and so it should, for that much more. A creative consultant named Christopher Everard told The Economist that ‘‘by using iconic architects, the label is building brand equity.’’ (Everard’s firm is called InterLife Consultancy. I e-mailed him the suggestion that he change its name to ‘‘Get A Life Consultancy’’—but he has not replied.) For me the Prada project smelled like the last days of Rome.
This is not to deny that the economic case for the creative industries is strong. After all, designing spectacles is big business, and tourism is a huge one. One trade fair and exhibition, called Exp, announced itself as ‘‘The Event That Defines the Experience Industry.’’ For bewildered first-time visitors, Exp conveniently divided its global industry into four domains: corporate visitor centers, retail, casinos, and museums. Exp promised to show visitors ‘‘how to gain a greater share of your guest’s discretionary time and disposable income’’; how to ‘‘destroy the myth that great experience need [sic] huge budgets’’; and ‘‘how to surf the generational shift.’’ The website for the exhibition did not mention a session on how to speak English, but so-called experience designers (in Europe, they tend to be called ‘‘interaction designers’’), undeterred, flocked to Exp. In some places, sport is replacing culture as an attractor in urban regeneration. Paris, in its bid for the 2012 Olympics, says the role that investment in sports infrastructures plays in the Games of the twenty-first century will be ‘‘comparable to that played by industrialisation at the end of the 19th century.’’ Claude Be´be´ar, chairman of the Paris Olympic Committee, does not think of sport as kicking a ball around a field. He thinks about twentymillion-dollar sponsorships and about the well-being of those who provide the spectacle. His plans for a sporty Paris, celebrated in a lavish and immense book, feature a boulevard dedicated to sport, bordered with hotels to lodge journalists, an international media center, a superdome, and the Olympic pool. Private road lanes, of the kind Stalin pioneered in Moscow, and a travel time of twelve minutes from bed to track are promised for athletes and officials. If the bed-to-track journey proves too taxing, an electronic games and Internet center will be provided to ‘‘help athletes relax and get in touch with the outside world.
It is not a question of good building, and bad. A beautiful place may never bring about an explosion of life, while a haphazard hall may be a tremendous meeting place. This is the mystery of the theatre, but in the understanding of this mystery lies the one science. It is not a matter of saying analytically, what are the requirements, how best they could be organized—this will usually bring into existence a tame, conventional, often cold hall. The science of theatre building must come from studying what it is that brings about the more vivid relationships between people.15 Many people in theater question whether new buildings are needed at all. Big theaters, in particular, tend to sap energy out of productions and money out of producers. Some producers have taken literally to the streets in so-called promenade and site-specific theater. In Chaucer-like journeys, players and audience move together around cities, through forests, up mountains, or into resonant but abandoned spaces. In the age of the rave, street-level events are everywhere: festivals, concerts, corporate events, church pageants, and fashion shows vie with each other to occupy the streets. In Europe, where theater people are leading the way to a sane policy for space planning, the term ‘‘territorial capital’’ is now being used to describe the ‘‘hard’’ and ‘‘soft’’ assets of a region. Hard assets include natural beauty and features; shopping facilities; cultural attractions; and buildings, museums, monuments, and the like. Soft assets are all about people and culture: skills, traditions, festivals, events and occasions, situations, settings, social ties, civic loyalty, memories, and the capacity to facilitate learning of various kinds. Turning the notion of territorial capital into a policy or a design program is a challenging task. EU countries are committed formally to the worthy ambition to enable a ‘‘European knowledge economy’’ by the year 2010. The problem is that these countries understand what the knowledge economy means in different ways.
The answer lies in webs, chains, and networks of cities and regions. By aggregating their hard and soft assets, collective cities—multicentered cities—can match the array of functions and resources of the metropolitan centers while still (in theory) delivering superior social quality. The ability of small cities to offer a context that supports intimacy and encounter— what the French call la vie associative—is where small-city webs will win out over the big centers. Multicity networks are not a new idea. They date back to the thirteenth century when, in the Hansa League, more than seventy merchant cities collaborated for their common good in order to control exports and imports over a wide swath of Europe. A powerful network of trading partners, with its own accounting system and shared vocabulary, the Hansa League became one of the major economic forces of the Middle Ages. At one stage it controlled much of Scandinavia, the Baltic states, northern Germany, and Poland—and outposts can be found even today as far away as Scotland and the Basque Country.18 Hanze Expo, the League’s modern incarnation, links the Baltic Rim—St. Petersburg, Tallinn, Riga, Rostock—to northern Germany and Holland. One part of this link, Estonia, has pronounced itself to be the Hong Kong of the Baltic.
For networked, multicentered localities to succeed, different kinds of territorial and social capital need to be linked by a combination of physical and informational networks. This integration of hard and soft factors is complex. For one thing, planners and policymakers have been joined by a variety of new players in a game they used to play on their own. Privatized network industries, such as railway companies, airports, electricity suppliers, and telecommunications operators, all want a say in planning discussions. So, too, do citizens. With growing confidence and sophistication, citizen groups are demanding that social agendas—such as social inclusion or environmental sustainability—be factored into planning processes. A nonprofit technology organization called The Open Planning Project (TOPP) argues that information about public places is as important a public good as the physical places themselves. TOPP advocates a free, distributed, and open geographic information infrastructure and is developing new ways to enhance the ability of all citizens to engage in meaningful dialogue about their environment. One of TOPP’s projects, a collaborative weblog called DigitalEarth.org, is conceived as a shared public online space for talking about the environmental information infrastructure. The site includes technology and tools to help citizens deploy geographic data, environmental models, and visualizations.21 The spread of open planning is a profound challenge to planning and design professionals. They are torn between the increasing complexity of the systems they have to deal with and the demand that people be put first.
The Regionmaker as one way to help designers of cities and regions cope with the new demands. ‘‘We keep getting asked to make ‘visions’ for cities and regions,’’ Winy Maas, a principal of the firm, told me, ‘‘but we want to base these on real data, not just our imagination.’’ The design challenge, for Maas, is to represent complex data about regions and cities visually, in order to provide a space in which the different actors now involved can explore options together. The Regionmaker, developed by MVRDV initially for a project called RhineRuhrCity, orchestrates a variety of existing information sources and flows—demographic data or outputs from geographical information systems (GIS) (or geomatics, as they are also called). The Regionmaker supports maps, study charts, and access to databases; imports and exports images and video feeds from helicopters or satellites; connects to the Internet; and uses computer-aided design (CAD) drawings. Maas and his colleagues plan to add to the system information on the movement of people, goods, and information. A housing subroutine will develop scenarios for optimal housing designs. A calculator will optimize natural light in built spaces. A function mixer will propose optimal mixtures of activities according to economic, social, or cultural criteria. The long-term aim is for the system to become a decision support environment in a more proactive and critical sense. ‘‘We could add an Evaluator, or an Evolver that can suggest criticism of the input we make,’’ speculates Maas. Deciding who gets to use these new tools is itself a design action. The principle of open planning is that nonspecialized actors and stakeholders are involved in the creation process, not simply as yes-no responders to precooked proposals. MVRDV’s system has the potential to enable municipalities, citizen groups, and planners to ‘‘compose’’ an optimized mixed neighborhood—but they have to be invited to do so and shown how. All of this takes commitment and time.
