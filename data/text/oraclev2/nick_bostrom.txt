Suppose two brains are in the same conscious state. Are there two minds, two streams of conscious experiences? Or only one? From a physical point of view, there is no puzzle. There are two numerically distinct lumps of matter that instantiate the same patterns and undergo qualitatively identical processes. The question is how we should ascribe mental properties to this material configuration. Even if we assume that the mental supervenes on the physical, we still need to determine whether the supervenience relation is such that two qualitatively identical physical systems ground a single experience or two numerically distinct (albeit subjectively indistinguishable) experiences.
The issue is not about personal identity. It is a separate question whether there would be one or two persons. One might hold, for example, that one person could have two subjectively indistinguishable experiences at the same time, or that two persons could literally share what is, numerically and not just qualitatively, one experience. These issues about personal identity will not be discussed here. My concern, rather, is about ‘‘qualia identity.’’ I will start by considering the numerical identity or non-identity of the phenomenal experiences that arise when brains exist in duplicates. The bulk of the paper will then examine some intriguing cases involving partial brain-duplication and the questions of degrees and of quantity of experience that these cases force us to confront. Consider the case where two brains are in identical physical states. The brains have, let us assume, the same number of neurons, which are connected and activated in the same way, and the brains are descriptively identical all the way down to the level of individual molecules and atoms. Suppose, furthermore, that for each of these brains there would, if the other brain did not exist, supervene a particular phenomenal experience. Given the supervenience assumption, the phenomenal experience that would supervene on one of these brains would be qualitatively identical to the experience that would supervene on the other. But if both brains exist, are there two qualitatively identical but numerically distinct experiences (one for each brain) or is there only a single experience with a redundantly duplicated supervenience base? A hardcore physicalist might be tempted to dismiss this question as being merely terminological. However, I believe that we can give content to the question by linking it to significant ethical and epistemological issues. Given such a linkage, the answer will not be an inconsequential terminological stipulation but will reflect substantial views on these associated issues. Let Duplication be the thesis that there would be two numerically distinct streams of experience when a conscious brain exists in duplicate. The content of Duplication, I suggest, might in part be constituted by its implications for epistemology and ethics. Consider first the ethical significance of Duplication. It could matter ethically whether Duplication is true. Suppose that somebody is contemplating whether to make a copy of a brain that is in a state of severe pain. If the quantity of painful experience would not thereby be increased, it seems that there would be no moral objection to this. By contrast, if creating the copy will lead to an additional severely painful experience, there is a strong moral reason not to do it. In such cases, it would be an extremely important practical matter whether Duplication is true or false. We could not resolve it by appealing to an arbitrary verbal convention. This ethical implication is a reason to accept Duplication and to reject its negation (Unification). Unification implies that we would not bring about pain if we created copy of a brain in a painful state, or changed an existing brain into a state that is qualitatively identical to a painful state of an already existing brain. At least on hedonistic grounds, there would be no moral reason not to create the copy. Yet it is prima facie implausible and farfetched to maintain that the wrongness of torturing somebody would be somehow ameliorated or annulled if there happens to exist somewhere an exact copy of that person’s resulting brain-state.
From this point onward, it will serve clarity and convenience to assume a weak form of computationalism, implying that a sufficiently powerful digital computer, running a suitable (very complex) program, would in fact have phenomenal experiences. (We do not need to assume that this would be analytically or metaphysically necessary.) Given this simplifying assumption, we can imagine the case of interest as resulting from running the same mind-program on two different computers. We can simplify matters further by supposing that the simulated minds live in and interact with identical computer-simulated virtual realities. Under these conditions, two identical mind-and-virtual-reality programs, starting from the same initial conditions, will evolve in exact lockstep. The brain-simulation and the virtual-reality simulation are both parts of a more comprehensive program, and when this program is run on two identical (deterministic) computers, they will go through an identical sequence of state-transitions.
This possibility undercuts Chalmers’ argument for the principle of organizational invariance by offering a more plausible account of how Joe’s qualia could gradually fade when he undergoes the neural replacement process. If the fading took place in this way, Joe would not be strangely failing to notice any qualitative changes in his experiences, because there would be no such changes. Of course, this point does not show that the principle of organizational invariance is false; it merely undermines one argument in its favor. The principle might well be plausible other grounds. It might be tempting to think that the possibility of fractional minds could be demonstrated by considering more mundane examples featuring gradations of consciousness, such as infant development, consciousness in animals at different levels of cognitive sophistication, the humanoid ancestors of homo sapiens, or the gradual loss of consciousness that occurs when we drift into dreamless sleep or anesthesia.15 However, these cases are complicated. They certainly involve changes in the descriptive quality of experience, and these qualitative changes, too, can form a continuum. Starting with a fully awake normal human stream of consciousness, we could reach a state of unconsciousness by various possible sequences of small steps, in which our awareness gradually becomes more fragmented, the fragments become briefer, scarcer, and more diffuse until the stream completely dries out. These cases might also involve a gradual change in the quantity of particular qualitative experiences, in the sense relevant here; but it is not obvious that they do so. In the thought experiments presented in preceding sections, we carefully controlled for the confounding variable of qualitative change in order to focus on the fundamental question of quantitative change.
Suppose the implementation of a certain program gives rise to a phenomenal experience. I began by considering the question of whether two implementations of this program give rise to two qualitatively identical but numerically distinct experiences. The Duplication thesis answers this question in the affirmative. That Duplication is a substantial claim can be seen from the fact that it is has important ethical and epistemological implications. I defended Duplication by arguing that its implications in these areas are much more plausible than the implications of its negation, Unification. Moreover, our direct ontological intuitions about the matter seem to support Duplication.
The matter of degree here resides in the number and size of the fragments existing in duplicate. In other cases, however, the entire supervening phenomenal experience is simultaneously duplicated, and the matter of degree resides in the total quantity of qualitatively identical experience. Intriguingly, this quantity can be a fractional number. This casts some new light on what it is to implement a computation. The idea of a fractional quantity of qualia may be puzzling. By considering the case of a single computer built with unreliable elements, it can be shown that this possibility of fractional qualia would have to be confronted even if Duplication were rejected. Hence it is not a reason to reject Duplication. One may still wonder about the notion of purely quantitative variation of qualia. A quale is supposed to be a subjective phenomenal appearance—what something feels like, ‘‘from the inside.’’ But in the central cases I described, the subject is not supposed to register any qualitative changes in her experience. What is it then that changes when the quantity of, say, a particular pain quale decreases from 1 unit to 0.3 units? If the pain feels just the same, in what sense is there less pain after the change? Here is my answer: There is less pain in precisely the same sense as there is less pain if now only one subject experiences a particular pain quale while before two subjects each experienced just such a pain quale. The nature of fractional quantitative change is the same as the nature of quantitative change when it occurs in more familiar integer increments. (If we wish to speak of ‘‘subjects of experience,’’ perhaps we should also say that such subjects can likewise come in fractional degrees, not only in integer increments.) Finally, I considered the relation between the cases described in this paper and the Fading Qualia thought experiment, which Chalmers used as part of an argument for the principle of organizational invariance. The possibility of qualia fading quantitatively without any change in its descriptive, qualitative character undermines Chalmers’ argument.
Artificial intelligence is a possibility that should not be ignored in any serious thinking about the future, and it raises many profound issues for ethics and public policy that philosophers ought to start thinking about. This article outlines the case for thinking that human-level machine intelligence might well appear within the next half century. It then explains four immediate consequences of such a development, and argues that machine intelligence would have a revolutionary impact on a wide range of the social, political, economic, commercial, technological, scientific and environmental issues that humanity will face over the coming decades.

The annals of artificial intelligence are littered with broken promises. Half a century after the first electric computer, we still have nothing that even resembles an intelligent machine, if by ‘intelligent’ we mean possessing the kind of general-purpose smartness that we humans pride ourselves of. Maybe we will never manage to build real artificial intelligence. The problem could be too difficult for human brains ever to solve. Those who find the prospect of machines surpassing us in general intellectual abilities threatening may even hope that is the case.

However, neither the fact that machine intelligence would be scary nor the fact that some past predictions were wrong is a good ground for concluding that artificial intelligence will never be created. Indeed, to assume that artificial intelligence is impossible or will take thousands of years to develop seems at least as unwarranted as to make the opposite assumption. At a minimum, we must acknowledge that any scenario about what the world will be like in 2050 that simply postulates the absence human-level artificial intelligence is making a big assumption that could well turn out to be false.

It is therefore important to consider the alternative possibility, that intelligent machines will be built within fifty years. In the past year or two, there have been several books and articles published by leading researchers in artificial intelligence and robotics that argue for precisely that projection. This essay will first outline some of the reasons for this, and then discuss some of the consequences of human-level artificial intelligence.

We can get a grasp of the issue by considering the three things that are needed for an effective artificial intelligence. These are: hardware, software, and input/output mechanisms.

The requisite input/output technology already exists. We have video cameras, speakers, robotic arms etc. that provide a rich variety of ways for a computer to interact with its environment. So this part is trivial.

It is only relatively recently that we have begun to understand the computational mechanisms of biological brains. Computational neuroscience is only about fifteen years old as an active research discipline. In this short time, substantial progress has been made. We are beginning to understand early sensory processing. There are reasonably good computational models of primary visual cortex, and we are working our way up to the higher stages of visual cognition. We are uncovering what the basic learning algorithms are that govern how the strengths of synapses are modified by experience. The general architecture of our neuronal networks is being mapped out as we learn more about the interconnectivity between neurones and how different cortical areas project onto to one another. While we are still far from understanding higher-level thinking, we are beginning to figure out how the individual components work and how they are hooked up.

Assuming continuing rapid progress in neuroscience, we can envision learning enough about the lower-level processes and the overall architecture to begin to implement the same paradigms in computer simulations. Today, such simulations are limited to relatively small assemblies of neurones. There is a silicon retina and a silicon cochlea that do the same things as their biological counterparts. Simulating a whole brain will of course require enormous computing power; but as we saw, that capacity will be available within a couple of decades.

The product of this biology-inspired method will not be an explicitly coded mature artificial intelligence. (That is what the so-called classical school of artificial intelligence unsuccessfully tried to do.) Rather, it will be system that has the same ability as a toddler to learn from experience and to be educated. The system will need to be taught in order to attain the abilities of adult humans. But there is no reason why the computational algorithms that our biological brains use would not work equally well when implemented in silicon hardware.

An artificial intelligence is based on software, and it can therefore be copied as easily as any other computer program. Apart from hardware requirements, the marginal cost of creating an additional artificial intelligence after you have built the first one is close to zero. Artificial minds could therefore quickly come to exist in great numbers, amplifying the impact of the initial breakthrough.
There is a temptation to stop the analysis at the point where human-level machine intelligence appears, since that by itself is quite a dramatic development. But doing so is to miss an essential point that makes artificial intelligence a truly revolutionary prospect, namely, that it can be expected to lead to the creation of machines with intellectual abilities that vastly surpass those of any human. We can predict with great confidence that this second step will follow, although the time-scale is again somewhat uncertain. If Moore's law continues to hold in this era, the speed of artificial intelligences will double at least every two years. Within fourteen years after human-level artificial intelligence is reached, there could be machines that think more than a hundred times more rapidly than humans do. In reality, progress could be even more rapid than that, because there would likely be parallel improvements in the efficiency of the software that these machines use. The interval during which the machines and humans are roughly matched will likely be brief. Shortly thereafter, humans will be unable to compete intellectually with artificial minds.

Artificial intelligence is a true general-purpose technology. It enables applications in a very wide range of other fields. In particular, scientific and technological research (as well as philosophical thinking) will be done more effectively when conducted by machines that are cleverer than humans. One can therefore expect that overall technological progress will be rapid.

Machine intelligences may devote their abilities to designing the next generation of machine intelligence. This next generation will be even smarter and might be able to design their successors in even shorter time. Some authors have speculated that this positive feedback loop will lead to a "singularity" - a point where technological progress becomes so rapid that genuine superintelligence, with abilities unfathomable to mere humans, is attained within a short time span. However, it may turn out that there are diminishing returns in artificial intelligence research when some point is reached. Maybe once the low-hanging fruits have been picked, it gets harder and harder to make further improvement. There seems to be no clear way of predicting which way it will go.
It would be a mistake to conceptualise machine intelligence as a mere tool. Although it may be possible to build special-purpose artificial intelligence that could only think about some restricted set of problems, we are considering here a scenario in which machines with general-purpose intelligence are created. Such machines would be capable of independent initiative and of making their own plans. Such artificial intellects are perhaps more appropriately viewed as persons than machines. In economics lingo, they might come to be classified not as capital but as labour. If we can control the motivations of the artificial intellects that we design, they could come to constitute a class of highly capable "slaves" (although that term might be misleading if the machines don't want to do anything other than serve the people who built or commissioned them). The ethical and political debates surrounding these issues will likely become intense as the prospect of artificial intelligence draws closer.

Transhumanism is a loosely defined movement that has developed gradually over the past two decades. It promotes an interdisciplinary approach to understanding and evaluating the opportunities for enhancing the human condition and the human organism opened up by the advancement of technology. Attention is given to both present technologies, like genetic engineering and information technology, and anticipated future ones, such as molecular nanotechnology and artificial intelligence.1

The enhancement options being discussed include radical extension of human health-span, eradication of disease, elimination of unnecessary suffering, and augmentation of human intellectual, physical, and emotional capacities.2 Other transhumanist themes include space colonization and the possibility of creating superintelligent machines, along with other potential developments that could profoundly alter the human condition. The ambit is not limited to gadgets and medicine, but encompasses also economic, social, institutional designs, cultural development, and psychological skills and techniques.

Transhumanists view human nature as a work-in-progress, a half-baked beginning that we can learn to remold in desirable ways. Current humanity need not be the endpoint of evolution. Transhumanists hope that by responsible use of science, technology, and other rational means we shall eventually manage to become post-human, beings with vastly greater capacities than present human beings have.

Some transhumanists take active steps to increase the probability that they personally will survive long enough to become post-human, for example by choosing a healthy lifestyle or by making provisions for having themselves cryonically suspended in case of de-animation.3 In contrast to many other ethical outlooks, which in practice often reflect a reactionary attitude to new technologies, the transhumanist view is guided by an evolving vision to take a more active approach to technology policy. This vision, in broad strokes, is to create the opportunity to live much longer and healthier lives, to enhance our memory and other intellectual faculties, to refine our emotional experiences and increase our subjective sense of well-being, and generally to achieve a greater degree of control over our own lives. This affirmation of human potential is offered as an alternative to customary injunctions against playing God, messing with nature, tampering with our human essence, or displaying punishable hubris.

Transhumanism does not entail technological optimism. While future technological capabilities carry immense potential for beneficial deployments, they also could be misused to cause enormous harm, ranging all the way to the extreme possibility of intelligent life becoming extinct. Other potential negative outcomes include widening social inequalities or a gradual erosion of the hard-to-quantify assets that we care deeply about but tend to neglect in our daily struggle for material gain, such as meaningful human relationships and ecological diversity. Such risks must be taken very seriously, as thoughtful transhumanists fully acknowledge.4

Transhumanism has roots in secular humanist thinking, yet is more radical in that it promotes not only traditional means of improving human nature, such as education and cultural refinement, but also direct application of medicine and technology to overcome some of our basic biological limits.

The range of thoughts, feelings, experiences, and activities that are accessible to human organisms presumably constitute only a tiny part of what is possible. There is no reason to think that the human mode of being is any more free of limitations imposed by our biological nature than are the modes of being of other animals. Just as chimpanzees lack the brainpower to understand what it is like to be human, so too do we lack the practical ability to form a realistic intuitive understanding of what it would be like to be post-human.

This point is distinct from any principled claims about impossibility. We need not assert that post-humans would not be Turing computable or that their concepts could not be expressed by any finite sentences in human language.  The impossibility is more like the impossibility for us to visualize a twenty-dimensional hypersphere or to read, with perfect recollection and understanding, every book in the Library of Congress. Our own current mode of being, therefore, spans but a minute subspace of what is possible or permitted by the physical constraints of the universe. It is not farfetched to suppose that there are parts of this larger space that represent extremely valuable ways of living, feeling, and thinking.

We can conceive of aesthetic and contemplative pleasures whose blissfulness vastly exceeds what any human being has yet experienced. We can imagine beings that reach a much greater level of personal development and maturity than current human beings do, because they have the opportunity to live for hundreds or thousands of years with full bodily and psychic vigor. We can conceive of beings that are much smarter than us, that can read books in seconds, that are much more brilliant philosophers than we are, that can create artworks, which, even if we could understand them only on the most superficial level, would strike us as wonderful masterpieces. We can imagine love that is stronger, purer, and more secure than any human being has yet harbored. Our everyday intuitions about values are constrained by the narrowness of our experience and the limitations of our powers of imagination. We should leave room in our thinking for the possibility that as we develop greater capacities, we shall come to discover values that will strike us as being of a far higher order than those we can realize as un-enhanced biological humans beings.

The conjecture that there are greater values than we can currently fathom does not imply that values are not defined in terms of our current dispositions. Take, for example, a dispositional theory of value such as the one described by David Lewis.5 According to Lewis’s theory, something is a value for you if and only if you would want to want it if you were perfectly acquainted with it and you were thinking and deliberating as clearly as possible about it. On this view, there may be values that we do not currently want, and that we do not even currently want to want, because we may not be perfectly acquainted with them or because we are not ideal deliberators. Some values pertaining to certain forms of post-human existence may well be of this sort; they may be values for us now, and they may be so in virtue of our current dispositions, and yet we may not be able to fully appreciate them with our current limited deliberative capacities and our lack of the receptive faculties required for full acquaintance with them. This point is important because it shows that the transhumanist view that we ought to explore the realm of post-human values does not entail that we should forego our current values. The post-human values can be our current values, albeit ones that we have not yet clearly comprehended. Transhumanism does not require us to say that we should favor post-human beings over human beings, but that the right way of favoring human beings is by enabling us to realize our ideals better and that some of our ideals may well be located outside the space of modes of being that are accessible to us with our current biological constitution.

We can overcome many of our biological limitations. It is possible that there are some limitations that are impossible for us to transcend, not only because of technological difficulties but on metaphysical grounds. Depending on what our views are about what constitutes personal identity, it could be that certain modes of being, while possible, are not possible for us, because any being of such a kind would be so different from us that they could not be us. Concerns of this kind are familiar from theological discussions of the afterlife. In Christian theology, some souls will be allowed by God to go to heaven after their time as corporal creatures is over. Before being admitted to heaven, the souls would undergo a purification process in which they would lose many of their previous bodily attributes. Skeptics may doubt that the resulting minds would be sufficiently similar to our current minds for it to be possible for them to be the same person. A similar predicament arises within transhumanism: if the mode of being of a post-human being is radically different from that of a human being, then we may doubt whether a post-human being could be the same person as a human being, even if the post-human being originated from a human being.

We can, however, envision many enhancements that would not make it impossible for the post-transformation someone to be the same person as the pre-transformation person. A person could obtain considerable increased life expectancy, intelligence, health, memory, and emotional sensitivity, without ceasing to exist in the process. A person’s intellectual life can be transformed radically by getting an education. A person’s life expectancy can be extended substantially by being unexpectedly cured from a lethal disease. Yet these developments are not viewed as spelling the end of the original person. In particular, it seems that modifications that add to a person’s capacities can be more substantial than modifications that subtract, such as brain damage. If most of someone currently is, including her most important memories, activities, and feelings, is preserved, then adding extra capacities on top of that would not easily cause the person to cease to exist.

Preservation of personal identity, especially if this notion is given a narrow construal, is not everything. We can value other things than ourselves, or we might regard it as satisfactory if some parts or aspects of ourselves survive and flourish, even if that entails giving up some parts of ourselves such that we no longer count as being the same person. Which parts of ourselves we might be willing to sacrifice may not become clear until we are more fully acquainted with the full meaning of the options. A careful, incremental exploration of the post-human realm may be indispensable for acquiring such an understanding, although we may also be able to learn from each other’s experiences and from works of the imagination. Additionally, we may favor future people being posthuman rather than human, if the posthumans would lead lives more worthwhile than the alternative humans would. Any reasons stemming from such considerations would not depend on the assumption that we ourselves could become posthuman beings.

Transhumanism promotes the quest to develop further so that we can explore hitherto inaccessible realms of value. Technological enhancement of human organisms is a means that we ought to pursue to this end. There are limits to how much can be achieved by low-tech means such as education, philosophical contemplation, moral self-scrutiny and other such methods proposed by classical philosophers with perfectionist leanings, including Plato, Aristotle, and Nietzsche, or by means of creating a fairer and better society, as envisioned by social reformists such as Marx or Martin Luther King. This is not to denigrate what we can do with the tools we have today. Yet ultimately, transhumanists hope to go further.

Most potential human enhancement technologies have so far received scant attention in the ethics literature. One exception is genetic engineering, the morality of which has been extensively debated in recent years. To illustrate how the transhumanist approach can be applied to particular technologies, we shall therefore now turn to consider the case of human germ-line genetic enhancements.

Certain types of objection against germ-line modifications are not accorded much weight by a transhumanist interlocutor. For instance, objections that are based on the idea that there is something inherently wrong or morally suspect in using science to manipulate human nature are regarded by transhumanists as wrongheaded. Moreover, transhumanists emphasize that particular concerns about negative aspects of genetic enhancements, even when such concerns are legitimate, must be judged against the potentially enormous benefits that could come from genetic technology successfully employed.6 For example, many commentators worry about the psychological effects of the use of germ-line engineering. The ability to select the genes of our children and to create so-called designer babies will, it is claimed, corrupt parents, who will come to view their children as mere products.7 We will then begin to evaluate our offspring according to standards of quality control, and this will undermine the ethical ideal of unconditional acceptance of children, no matter what their abilities and traits. Are we really prepared to sacrifice on the altar of consumerism even those deep values that are embodied in traditional relationships between child and parents? Is the quest for perfection worth this cultural and moral cost? A transhumanist should not dismiss such concerns as irrelevant. Transhumanists recognize that the depicted outcome would be bad. We do not want parents to love and respect their children less. We do not want social prejudice against people with disabilities to get worse. The psychological and cultural effects of commodifying human nature are potentially important.

But such dystopian scenarios are speculations. There is no firm ground for believing that the alleged consequences would actually happen. What relevant evidence we have, for instance regarding the treatment of children who have been conceived through the use of in vitro fertilization or embryo screening, suggests that the pessimistic prognosis is alarmist. Parents will in fact love and respect their children even when artificial means and conscious choice play a part in procreation.

We might speculate, instead, that germ-line enhancements will lead to more love and parental dedication. Some mothers and fathers might find it easier to love a child who, thanks to enhancements, is bright, beautiful, healthy, and happy. The practice of germ-line enhancement might lead to better treatment of people with disabilities, because a general demystification of the genetic contributions to human traits could make it clearer that people with disabilities are not to blame for their disabilities and a decreased incidence of some disabilities could lead to more assistance being available for the remaining affected people to enable them to live full, unrestricted lives through various technological and social supports. Speculating about possible psychological or cultural effects of germ-line engineering can therefore cut both ways. Good consequences no less than bad ones are possible. In the absence of sound arguments for the view that the negative consequences would predominate, such speculations provide no reason against moving forward with the technology.

Ruminations over hypothetical side-effects may serve to make us aware of things that could go wrong so that we can be on the lookout for untoward developments. By being aware of the perils in advance, we will be in a better position to take preventive countermeasures. For instance, if we think that some people would fail to realize that a human clone would be a unique person deserving just as much respect and dignity as any other human being, we could work harder to educate the public on the inadequacy of genetic determinism. The theoretical contributions of well-informed and reasonable critics of germ-line enhancement could indirectly add to our justification for proceeding with germ-line engineering. To the extent that the critics have done their job, they can alert us to many of the potential untoward consequences of germ-line engineering and contribute to our ability to take precautions, thus improving the odds that the balance of effects will be positive. There may well be some negative consequences of human germ-line engineering that we will not forestall, though of course the mere existence of negative effects is not a decisive reason not to proceed. Every major technology has some negative consequences. Only after a fair comparison of the risks with the likely positive consequences can any conclusion based on a cost-benefit analysis be reached.

In the case of germ-line enhancements, the potential gains are enormous. Only rarely, however, are the potential gains discussed, perhaps because they are too obvious to be of much theoretical interest. By contrast, uncovering subtle and non-trivial ways in which manipulating our genome could undermine deep values is philosophically a lot more challenging. But if we think about it, we recognize that the promise of genetic enhancements is anything but insignificant. Being free from severe genetic diseases would be good, as would having a mind that can learn more quickly, or having a more robust immune system. Healthier, wittier, happier people may be able to reach new levels culturally. To achieve a significant enhancement of human capacities would be to embark on the transhuman journey of exploration of some of the modes of being that are not accessible to us as we are currently constituted, possibly to discover and to instantiate important new values. On an even more basic level, genetic engineering holds great potential for alleviating unnecessary human suffering. Every day that the introduction of effective human genetic enhancement is delayed is a day of lost individual and cultural potential, and a day of torment for many unfortunate sufferers of diseases that could have been prevented. Seen in this light, proponents of a ban or a moratorium on human genetic modification must take on a heavy burden of proof in order to have the balance of reason tilt in their favor. Transhumanists conclude that the challenge has not been met.

One way of going forward with genetic engineering is to permit everything, leaving all choices to parents. While this attitude may be consistent with transhumanism, it is not the best transhumanist approach. One thing that can be said for adopting a libertarian stance in regard to human reproduction is the sorry track record of socially planned attempts to improve the human gene pool. The list of historical examples of state intervention in this domain ranges from the genocidal horrors of the Nazi regime, to the incomparably milder but still disgraceful semi-coercive sterilization programs of mentally impaired individuals favored by many well-meaning socialists in the past century, to the controversial but perhaps understandable program of the current Chinese government to limit population growth. In each case, state policies interfered with the reproductive choices of individuals. If parents had been left to make the choices for themselves, the worst transgressions of the eugenics movement would not have occurred. Bearing this in mind, we ought to think twice before giving our support to any proposal that would have the state regulate what sort of children people are allowed to have and the methods that may be used to conceive them.8

We currently permit governments to have a role in reproduction and child-rearing and we may reason by extension that there would likewise be a role in regulating the application of genetic reproductive technology. State agencies and regulators play a supportive and supervisory role, attempting to promote the interests of the child. Courts intervene in cases of child abuse or neglect. Some social policies are in place to support children from disadvantaged backgrounds and to ameliorate some of the worst inequities suffered by children from poor homes, such as through the provision of free schooling. These measures have analogues that apply to genetic enhancement technologies. For example, we ought to outlaw genetic modifications that are intended to damage the child or limit its opportunities in life, or that are judged to be too risky. If there are basic enhancements that would be beneficial for a child but that some parents cannot afford, then we should consider subsidizing those enhancements, just as we do with basic education. There are grounds for thinking that the libertarian approach is less appropriate in the realm of reproduction than it is in other areas. In reproduction, the most important interests at stake are those of the child-to-be, who cannot give his or her advance consent or freely enter into any form of contract. As it is, we currently approve of many measures that limit parental freedoms. We have laws against child abuse and child neglect. We have obligatory schooling. In some cases, we can force needed medical treatment on a child, even against the wishes of its parents.

There is a difference between these social interventions with regard to children and interventions aimed at genetic enhancements. While there is a consensus that nobody should be subjected to child abuse and that all children should have at least a basic education and should receive necessary medical care, it is unlikely that we will reach an agreement on proposals for genetic enhancements any time soon. Many parents will resist such proposals on principled grounds, including deep-seated religious or moral convictions. The best policy for the foreseeable future may therefore be to not legally require any genetic enhancements, except perhaps in extreme cases for which there is no alternative treatment. Even in such cases, it is dubious that the social climate in many countries is ready for mandatory genetic interventions.

The scope for ethics and public policy, however, extend far beyond the passing of laws requiring or banning specific interventions. Even if a given enhancement option is neither outlawed nor legally required, we may still seek to discourage or encourage its use in a variety of ways. Through subsidies and taxes, research-funding policies, genetic counseling practices and guidelines, laws regulating genetic information and genetic discrimination, provision of health care services, regulation of the insurance industry, patent law, education, and through the allocation of social approbation and disapproval, we may influence the direction in which particular technologies are applied. We may appropriately ask, with regard to genetic enhancement technologies, which types of applications we ought to promote or discourage.

An externality, as understood by economists, is a cost or a benefit of an action that is not carried by a decision-maker. An example of a negative externality might be found in a firm that lowers its production costs by polluting the environment. The firm enjoys most of the benefits while escaping the costs, such as environmental degradation, which may instead paid by people living nearby. Externalities can also be positive, as when people put time and effort into creating a beautiful garden outside their house. The effects are enjoyed not exclusively by the gardeners but spill over to passersby. As a rule of thumb, sound social policy and social norms would have us internalize many externalities so that the incentives of producers more closely match the social value of production. We may levy a pollution tax on the polluting firm, for instance, and give our praise to the home gardeners who beautify the neighborhood.

Genetic enhancements aimed at the obtainment of goods that are desirable only in so far as they provide a competitive advantage tend to have negative externalities. An example of such a positional good, as economists call them, is stature. There is evidence that being tall is statistically advantageous, at least for men in Western societies. Taller men earn more money, wield greater social influence, and are viewed as more sexually attractive. Parents wanting to give their child the best possible start in life may rationally choose a genetic enhancement that adds an inch or two to the expected length of their offspring. Yet for society as a whole, there seems to be no advantage whatsoever in people being taller. If everybody grew two inches, nobody would be better off than they were before. Money spent on a positional good like length has little or no net effect on social welfare and is therefore, from society’s point of view, wasted.

Health is a very different type of good. It has intrinsic benefits. If we become healthier, we are personally better off and others are not any worse off. There may even be a positive externality of enhancing ours own health. If we are less likely to contract a contagious disease, others benefit by being less likely to get infected by us. Being healthier, you may also contribute more to society and consume less of publicly funded healthcare.

If we were living in a simple world where people were perfectly rational self-interested economic agents and where social policies had no costs or unintended effects, then the basic policy prescription regarding genetic enhancements would be relatively straightforward. We should internalize the externalities of genetic enhancements by taxing enhancements that have negative externalities and subsidizing enhancements that have positive externalities. Unfortunately, crafting policies that work well in practice is considerably more difficult. Even determining the net size of the externalities of a particular genetic enhancement can be difficult. There is clearly an intrinsic value to enhancing memory or intelligence in as much as most of us would like to be a bit smarter, even if that did not have the slightest effect on our standing in relation to others. But there would also be important externalities, both positive and negative. On the negative side, others would suffer some disadvantage from our increased brainpower in that their own competitive situation would be worsened. Being more intelligent, we would be more likely to attain high-status positions in society, positions that would otherwise have been enjoyed by a competitor. On the positive side, others might benefit from enjoying witty conversations with us and from our increased taxes.

If in the case of intelligence enhancement the positive externalities outweigh the negative ones, then a prima facie case exists not only for permitting genetic enhancements aimed at increasing intellectual ability, but for encouraging and subsidizing them too. Whether such policies remain a good idea when all practicalities of implementation and political realities are taken into account is another matter. But at least we can conclude that an enhancement that has both significant intrinsic benefits for an enhanced individual and net positive externalities for the rest of society should be encouraged. By contrast, enhancements that confer only positional advantages, such as augmentation of stature or physical attractiveness, should not be socially encouraged, and we might even attempt to make a case for social policies aimed at reducing expenditure on such goods, for instance through a progressive tax on consumption.

One important kind of externality in germ-line enhancements is their effects on social equality. This has been a focus for many opponents of germ-line genetic engineering who worry that it will widen the gap between haves and have-nots. Today, children from wealthy homes enjoy many environmental privileges, including access to better schools and social networks. Arguably, this constitutes an inequity against children from poor homes. We can imagine scenarios where such inequities grow much larger thanks to genetic interventions that only the rich can afford, adding genetic advantages to the environmental advantages already benefiting privileged children. We could even speculate about the members of the privileged stratum of society eventually enhancing themselves and their offspring to a point where the human species, for many practical purposes, splits into two or more species that have little in common except a shared evolutionary history.10 The genetically privileged might become ageless, healthy, super-geniuses of flawless physical beauty, who are graced with a sparkling wit and a disarmingly self-deprecating sense of humor, radiating warmth, empathetic charm, and relaxed confidence. The non-privileged would remain as people are today but perhaps deprived of some their self-respect and suffering occasional bouts of envy. The mobility between the lower and the upper classes might disappear, and a child born to poor parents, lacking genetic enhancements, might find it impossible to successfully compete against the super-children of the rich. Even if no discrimination or exploitation of the lower class occurred, there is still something disturbing about the prospect of a society with such extreme inequalities.

While we have vast inequalities today and regard many of these as unfair, we also accept a wide range of inequalities because we think that they are deserved, have social benefits, or are unavoidable concomitants to free individuals making their own and sometimes foolish choices about how to live their lives. Some of these justifications can also be used to exonerate some inequalities that could result from germ-line engineering. Moreover, the increase in unjust inequalities due to technology is not a sufficient reason for discouraging the development and use of the technology. We must also consider its benefits, which include not only positive externalities but also intrinsic values that reside in such goods as the enjoyment of health, a soaring mind, and emotional well-being.

We can also try to counteract some of the inequality-increasing tendencies of enhancement technology with social policies. One way of doing so would be by widening access to the technology by subsidizing it or providing it for free to children of poor parents. In cases where the enhancement has considerable positive externalities, such a policy may actually benefit everybody, not just the recipients of the subsidy. In other cases, we could support the policy on the basis of social justice and solidarity.

Even if all genetic enhancements were made available to everybody for free, however, this might still not completely allay the concern about inequity. Some parents might choose not to give their children any enhancements. The children would then have diminished opportunities through no fault of their own. It would be peculiar, however, to argue that governments should respond to this problem by limiting the reproductive freedom of the parents who wish to use genetic enhancements. If we are willing to limit reproductive freedom through legislation for the sake of reducing inequities, then we might as well make some enhancements obligatory for all children. By requiring genetic enhancements for everybody to the same degree, we would not only prevent an increase in inequalities but also reap the intrinsic benefits and the positive externalities that would come from the universal application of enhancement technology. If reproductive freedom is regarded as too precious to be curtailed, then neither requiring nor banning the use of reproductive enhancement technology is an available option. In that case, we would either have to tolerate inequities as a price worth paying for reproductive freedom or seek to remedy the inequities in ways that do not infringe on reproductive freedom.

All of this is based on the hypothesis that germ-line engineering would in fact increase inequalities if left unregulated and no countermeasures were taken. That hypothesis might be false. In particular, it might turn out to be technologically easier to cure gross genetic defects than to enhance an already healthy genetic constitution. We currently know much more about many specific inheritable diseases, some of which are due to single gene defects, than we do about the genetic basis of talents and desirable qualities such as intelligence and longevity, which in all likelihood are encoded in complex constellations of multiple genes. If this turns out to be the case, then the trajectory of human genetic enhancement may be one in which the first thing to happen is that the lot of the genetically worst-off is radically improved, through the elimination of diseases such as Tay Sachs, Lesch-Nyhan, Downs Syndrome, and early-onset Alzheimer’s disease. This would have a major leveling effect on inequalities, not primarily in the monetary sense, but with respect to the even more fundamental parameters of basic opportunities and quality of life.

Another frequently heard objection against germ-line genetic engineering is that it would be uniquely hazardous because the changes it would bring are irreversible and would affect all generations to come. It would be highly irresponsible and arrogant of us to presume we have the wisdom to make decisions about what should be the genetic constitutions of people living many generations hence. Human fallibility, on this objection, gives us good reason not to embark on germ-line interventions. For our present purposes, we can set aside the issue of the safety of the procedure, understood narrowly, and stipulate that the risk of medical side-effects has been reduced to an acceptable level. The objection under consideration concerns the irreversibility of germ-line interventions and the lack of predictability of its long-term consequences; it forces us to ask if we possess the requisite wisdom for making genetic choices on behalf of future generations.

Human fallibility is not a conclusive ground for resisting germ-line genetic enhancements. The claim that such interventions would be irreversible is incorrect. Germ-line interventions can be reversed by other germ-line interventions. Moreover, considering that technological progress in genetics is unlikely to grind to an abrupt halt any time soon, we can count on future generations being able to reverse our current germ-line interventions even more easily than we can currently implement them. With advanced genetic technology, it might even be possible to reverse many germ-line modifications with somatic gene therapy, or with medical nanotechnology. Technologically, germ-line changes are perfectly reversible by future generations.

It is possible that future generations might choose to retain the modifications that we make. If that turns out to be the case, then the modifications, while not irreversible, would nevertheless not actually be reversed. This might be a good thing. The possibility of permanent consequences is not an objection against germ-line interventions any more than it is against social reforms. The abolition of slavery and the introduction of general suffrage might never be reversed; indeed, we hope they will not be. Yet this is no reason for people to have resisted the reforms. Likewise, the potential for everlasting consequences, including ones we cannot currently reliably forecast, in itself constitutes no reason to oppose genetic intervention. If immunity against horrible diseases and enhancements that expand the opportunities for human growth are passed on to subsequent generations in perpetuo, it would be a cause for celebration, not regret.

There are some kinds of changes that we need be particularly careful about. They include modifications of the drives and motivations of our descendants. For example, there are obvious reasons why we might think it worthwhile to seek to reduce our children’s propensity to violence and aggression. We would have to take care, however, that we do not do this in a way that would make future people overly submissive or complacent. We can conceive of a dystopian scenario along the lines of Brave New World, in which people are leading shallow lives but have been manipulated to be perfectly content with their sub-optimal existence. If the people transferred their shallow values to their children, humanity could get permanently stuck in a not-very-good state, having foolishly changed itself to lack any desire to strive for something better. This outcome would be dystopian because a permanent cap on human development would destroy the transhumanist hope of exploring the post-human realm. Transhumanists therefore place an emphasis on modifications which, in addition to promoting human well-being, also open more possibilities than they close and which increase our ability to make subsequent choices wisely. Longer active lifespans, better memory, and greater intellectual capacities are plausible candidates for enhancements that would improve our ability to figure out what we ought to do next. They would be a good place to start.

The possibility of creating thinking machines raises a host of ethical issues.  These questions relate both to ensuring that such machines do not harm humans and other morally relevant beings, and to the moral status of the machines themselves.  The first section discusses issues that may arise in the near future of AI.  The second section outlines challenges for ensuring that AI operates safely as it approaches humans in its intelligence.  The third section outlines how we might assess whether, and in what circumstances, AIs themselves have moral status.  In the fourth section, we consider how AIs might differ from humans in certain basic respects relevant to our ethical assessment of them.  The final section addresses the issues of creating AIs more intelligent than human, and ensuring that they use their advanced intelligence for good rather than ill.
Imagine, in the near future, a bank using a machine learning algorithm to recommend mortgage applications for approval.  A rejected applicant brings a lawsuit against the bank, alleging that the algorithm is discriminating racially against mortgage applicants.  The bank replies that this is impossible, since the algorithm is deliberately blinded to the race of the applicants.  Indeed, that was part of the bank’s rationale for implementing the system.  Even so, statistics show that the bank’s approval rate for black applicants has been steadily dropping.  Submitting ten apparently equally qualified genuine applicants (as determined by a separate panel of human judges) shows that the algorithm accepts white applicants and rejects black applicants.  What could possibly be happening?
AI algorithms play an increasingly large role in modern society, though usually not labeled “AI”.  The scenario described above might be transpiring even as we write.  It will become increasingly important to develop AI algorithms that are not just powerful and scalable, but also transparent to inspection—to name one of many socially important properties. Some challenges of machine ethics are much like many other challenges involved in designing machines.  Designing a robot arm to avoid crushing stray humans is no more morally fraught than designing a flame‐retardant sofa.  It involves new programming challenges, but no new ethical challenges.  But when AI algorithms take on cognitive work with social dimensions—cognitive tasks previously performed by humans—the AI algorithm inherits the social requirements.  It would surely be frustrating to find that no bank in the world will approve your seemingly excellent loan application, and nobody knows why, and nobody can find out even in principle.   (Maybe you have a first name strongly associated with deadbeats?  Who knows?) Transparency is not the only desirable feature of AI.  It is also important that AI algorithms taking over social functions be predictable to those they govern.  To understand the importance of such predictability, consider an analogy.  The legal principle of stare decisis binds judges to follow past precedent whenever possible.  To an engineer, this preference for precedent may seem incomprehensible—why bind the future to the past, when technology is always improving?  But one of the most important functions of the legal system is to be predictable, so that, e.g., contracts can be written knowing how they will be executed.  The job of the legal system is not necessarily to optimize society, but to provide a predictable environment within which citizens can optimize their own lives. It will also become increasingly important that AI algorithms be robust against manipulation. A machine vision system to scan airline luggage for bombs must be robust against human adversaries deliberately searching for exploitable flaws in the algorithm—for example, a shape that, placed next to a pistol in one’s luggage, would neutralize recognition of it.  Robustness against manipulation is an ordinary criterion in information security; nearly the criterion.  But it is not a criterion that appears often in machine learning journals, which are currently more interested in, e.g., how an algorithm scales up on larger parallel systems. Another important social criterion for dealing with organizations is being able to find the person responsible for getting something done.  When an AI system fails at its assigned task, who takes the blame?  The programmers?  The end‐users?
There is nearly universal agreement among modern AI professionals that Artificial Intelligence falls short of human capabilities in some critical sense, even though AI algorithms have beaten humans in many specific domains such as chess.  It has been suggested by some that as soon as AI researchers figure out how to do something, that capability ceases to be regarded as intelligent—chess was considered the epitome of intelligence until Deep Blue won the world championship from Kasparov—but even these researchers agree that something important is missing from modern AIs (e.g., Hofstadter 2006). While this subfield of Artificial Intelligence is only just coalescing, “Artificial General Intelligence” (hereafter, AGI) is the emerging term of art used to denote “real” AI (see, e.g., the edited volume Goertzel and Pennachin 2006).  As the name implies, the emerging consensus is that the missing characteristic is generality.  Current AI algorithms with human‐equivalent or ‐superior performance are characterized by a deliberately‐programmed competence only in a single, restricted domain.  Deep Blue became the world champion at chess, but it cannot even play checkers, let alone drive a car or make a scientific discovery.  Such modern AI algorithms resemble all biological life with the sole exception of Homo sapiens.  A bee exhibits competence at building hives; a beaver exhibits competence at building dams; but a bee doesn’t build dams, and a beaver can’t learn to build a hive.  A human, watching, can learn to do both; but this is a unique ability among biological lifeforms.  It is debatable whether human intelligence is truly general—we are certainly better at some cognitive tasks than others (Hirschfeld and Gelman 1994)—but human intelligence is surely significantly more generally applicable than nonhominid intelligence.
It is relatively easy to envisage the sort of safety issues that may result from AI operating only within a specific domain.  It is a qualitatively different class of problem to handle an AGI operating across many novel contexts that cannot be predicted in advance. When human engineers build a nuclear reactor, they envision the specific events that could go on inside it—valves failing, computers failing, cores increasing in temperature—and engineer the reactor to render these events noncatastrophic.  Or, on a more mundane level, building a toaster involves envisioning bread and envisioning the reaction of the bread to the toasterʹs heating element.  The toaster itself does not know that its purpose is to make toast—the purpose of the toaster is represented within the designer’s mind, but is not explicitly represented in computations inside the toaster—and so if you place cloth inside a toaster, it may catch fire, as the design executes in an unenvisioned context with an unenvisioned side effect. Even task‐specific AI algorithms throw us outside the toaster‐paradigm, the domain of locally preprogrammed, specifically envisioned behavior.  Consider Deep Blue, the chess algorithm that beat Garry Kasparov for the world championship of chess.  Were it the case that machines can only do exactly as they are told, the programmers would have had to manually preprogram a database containing moves for every possible chess position that Deep Blue could encounter.  But this was not an option for Deep Blue’s programmers.  First, the space of possible chess positions is unmanageably large.  Second, if the programmers had manually input what they considered a good move in each possible situation, the resulting system would not have been able to make stronger chess moves than its creators.  Since the programmers themselves were not world champions, such a system would not have been able to defeat Garry Kasparov. In creating a superhuman chess player, the human programmers necessarily sacrificed their ability to predict Deep Blue’s local, specific game behavior.  Instead, Deep Blue’s programmers had (justifiable) confidence that Deep Blue’s chess moves would satisfy a non‐local criterion of optimality:  namely, that the moves would tend to steer the future of the game board into outcomes in the “winning” region as defined by the chess rules.   This prediction about distant consequences, though it proved accurate, did not allow the programmers to envision the local behavior of Deep Blue—its response to a specific attack on its king—because Deep Blue computed the nonlocal game map, the link between a move and its possible future consequences, more accurately than the programmers could (Yudkowsky 2006). Modern humans do literally millions of things to feed themselves—to serve the final consequence of being fed.  Few of these activities were “envisioned by Nature” in the sense of being ancestral challenges to which we are directly adapted.
Humans crossed space and put footprints on the Moon, even though none of our ancestors encountered a challenge analogous to vacuum.  Compared to domain‐specific AI, it is a qualitatively different problem to design a system that will operate safely across thousands of contexts; including contexts not specifically envisioned by either the designers or the users; including contexts that no human has yet encountered.  Here there may be no local specification of good behavior—no simple specification over the behaviors themselves, any more than there exists a compact local description of all the ways that humans obtain their daily bread. To build an AI that acts safely while acting in many domains, with many consequences, including problems the engineers never explicitly envisioned, one must specify good behavior in such terms as “X such that the consequence of X is not harmful to humans”.  This is non‐local; it involves extrapolating the distant consequences of actions.  Thus, this is only an effective specification—one that can be realized as a design property—if the system explicitly extrapolates the consequences of its behavior.  A toaster cannot have this design property because a toaster cannot foresee the consequences of toasting bread. Imagine an engineer having to say, “Well, I have no idea how this airplane I built will fly safely—indeed I have no idea how it will fly at all, whether it will flap its wings or inflate itself with helium or something else I haven’t even imagined—but I assure you, the design is very, very safe.”  This may seem like an unenviable position from the perspective of public relations, but it’s hard to see what other guarantee of ethical behavior would be possible for a general intelligence operating on unforeseen problems, across domains, with preferences over distant consequences.  Inspecting the cognitive design might verify that the mind was, indeed, searching for solutions that we would classify as ethical; but we couldn’t predict which specific solution the mind would discover. Respecting such a verification requires some way to distinguish trustworthy assurances (a procedure which will not say the AI is safe unless the AI really is safe) from pure hope and magical thinking (“I have no idea how the Philosopher’s Stone will transmute lead to gold, but I assure you, it will!”).  One should bear in mind that purely hopeful expectations have previously been a problem in AI research (McDermott 1976). Verifiably constructing a trustworthy AGI will require different methods, and a different way of thinking, from inspecting power plant software for bugs—it will require an AGI that thinks like a human engineer concerned about ethics, not just a simple product of ethical engineering.
A different set of ethical issues arises when we contemplate the possibility that some future AI systems might be candidates for having moral status.  Our dealings with beings possessed of moral status are not exclusively a matter of instrumental rationality: we also have moral reasons to treat them in certain ways, and to refrain from treating them in certain other ways.  Francis Kamm has proposed the following definition of moral status, which will serve for our purposes.
A rock has no moral status: we may crush it, pulverize it, or subject it to any treatment we like without any concern for the rock itself.  A human person, on the other hand, must be treated not only as a means but also as an end.  Exactly what it means to treat a person as an end is something about which different ethical theories disagree; but it certainly involves taking her legitimate interests into account—giving weight to her well‐being—and it may also involve accepting strict moral side‐constraints in our dealings with her, such as a prohibition against murdering her, stealing from her, or doing a variety of other things to her or her property without her consent.  Moreover, it is because a human person counts in her own right, and for her sake, that it is impermissible to do to her these things.  This can be expressed more concisely by saying that a human person has moral status. Questions about moral status are important in some areas of practical ethics.  For example, disputes about the moral permissibility of abortion often hinge on disagreements about the moral status of the embryo.  Controversies about animal experimentation and the treatment of animals in the food industry involve questions about the moral status of different species of animal.  And our obligations towards human beings with severe dementia, such as late‐stage Alzheimer’s patients, may also depend on questions of moral status.
One common view is that many animals have qualia and therefore have some moral status, but that only human beings have sapience, which gives them a higher moral status than non‐human animals.1  This view, of course, must confront the existence of borderline cases such as, on the one hand, human infants or human beings with severe mental retardation—sometimes unfortunately referred to as “marginal humans”— which fail to satisfy the criteria for sapience; and, on the other hand, some non‐human animals such as the great apes, which might possess at least some of the elements of sapience.  Some deny that so‐called “marginal humans” have full moral status.  Others propose additional ways in which an object could qualify as a bearer of moral status, such as by being a member of a kind that normally has sentience or sapience, or by standing in a suitable relation to some being that independently has moral status (cf. Mary Anne Warren 2000).  For present purposes, however, we will focus on the criteria of sentience and sapience. This picture of moral status suggests that an AI system will have some moral status if it has the capacity for qualia, such as an ability to feel pain.  A sentient AI system, even if it lacks language and other higher cognitive faculties, is not like a stuffed toy animal or a wind‐up doll; it is more like a living animal.  It is wrong to inflict pain on a mouse, unless there are sufficiently strong morally overriding reasons to do so.  The same would hold for any sentient AI system.
 The anencephalic child, however, would have the same moral status as any other similar anencephalic child, including one that had come about through some entirely natural process.  The difference in moral status between an anencephalic child and a normal child is grounded in the qualitative difference between the two—the fact that one has a mind while the other does not.  Since the two children do not have the same functionality and the same conscious experience, the Principle of Ontogeny Non‐Discrimination does not apply. Although the Principle of Ontogeny Non‐Discrimination asserts that a being’s ontogeny has no essential bearing on its moral status, it does not deny that facts about ontogeny can affect what duties particular moral agents have toward the being in question.  Parents have special duties to their child which they do not have to other children, and which they would not have even if there were another child qualitatively identical to their own.  Similarly, the Principle of Ontogeny Non‐Discrimination is consistent with the claim that the creators or owners of an AI system with moral status may have special duties to their artificial mind which they do not have to another artificial mind, even if the minds in question are qualitatively similar and have the same moral status. If the principles of non‐discrimination with regard to substrate and ontogeny are accepted, then many questions about how we ought to treat artificial minds can be answered by applying the same moral principles that we use to determine our duties in more familiar contexts.  Insofar as moral duties stem from moral status considerations, we ought to treat an artificial mind in just the same way as we ought to treat a qualitatively identical natural human mind in a similar situation.  This simplifies the problem of developing an ethics for the treatment of artificial minds. Even if we accept this stance, however, we must confront a number of novel ethical questions which the aforementioned principles leave unanswered.  Novel ethical questions arise because artificial minds can have very different properties from ordinary human or animal minds.  We must consider how these novel properties would affect the moral status of artificial minds and what it would mean to respect the moral status of such exotic minds.
An artificial intellect, by contrast, might be constituted quite differently from a human intellect yet still exhibit human‐like behavior or possess the behavioral dispositions normally indicative of personhood.  It might therefore be possible to conceive of an artificial intellect that would be sapient, and perhaps would be a person, yet would not be sentient or have conscious experiences of any kind.   (Whether this is really possible depends on the answers to some non‐trivial metaphysical questions.)  Should such a system be possible, it would raise the question whether a non‐sentient person would have any moral status whatever; and if so, whether it would have the same moral status as a sentient person.  Since sentience, or at least a capacity for sentience, is ordinarily assumed to be present in any individual who is a person, this question has not received much attention to date.2 Another exotic property, one which is certainly metaphysically and physically possible for an artificial intelligence, is for its subjective rate of time to deviate drastically from the rate that is characteristic of a biological human brain.  The concept of subjective rate of time is best explained by first introducing the idea whole brain emulation, or “uploading”. “Uploading” refers to a hypothetical future technology that would enable a human or other animal intellect to be transferred from its original implementation in an organic brain onto a digital computer.  One scenario goes like this:  First, a very high‐resolution scan is performed of some particular brain, possibly destroying the original in the process.  For example, the brain might be vitrified and dissected into thin slices, which can then be scanned using some form of high‐throughput microscopy combined with automated image recognition.  We may imagine this scan to be detailed enough to capture all the neurons, their synaptic interconnections, and other features that are functionally relevant to the original brain’s operation.  Second, this three‐dimensional map of the components of the brain and their interconnections is combined with a library of advanced neuroscientific theory which specifies the computational properties of each basic type of element, such as different kinds of neuron and synaptic junction.  Third, the computational structure and the associated algorithmic behavior of its components are implemented in some powerful computer.
A number of questions arise in the context of such a  scenario:  How plausible is it that this procedure will one day become technologically feasible?  If the procedure worked and produced a computer program exhibiting roughly the same personality, the same memories, and the same thinking patterns as the original brain, would this program be sentient?  Would the upload be the same person as the individual whose brain was disassembled in the uploading process?  What happens to personal identity if an upload is copied such that two similar or qualitatively identical upload minds are running in parallel?  Although all of these questions are relevant to the ethics of machine intelligence, let us here focus on an issue involving the notion of a subjective rate of time. Suppose that an upload could be sentient.  If we run the upload program on a faster computer, this will cause the upload, if it is connected to an input device such as a video camera, to perceive the external world as if it had been slowed down.  For example, if the upload is running a thousand times faster than the original brain, then the external world will appear to the upload as if it were slowed down by a factor of thousand.  Somebody drops a physical coffee mug:  The upload observes the mug slowly falling to the ground while the upload finishes reading the morning newspaper and sends off a few emails.  One second of objective time corresponds to 17 minutes of subjective time.  Objective and subjective duration can thus diverge. Subjective time is not the same as a subject’s estimate or perception of how fast time flows.  Human beings are often mistaken about the flow of time.  We may believe that it is one o’clock when it is in fact a quarter past two; or a stimulant drug might cause our thoughts to race, making it seem as though more subjective time has lapsed than is actually the case.  These mundane cases involve a distorted time perception rather than a shift in the rate of subjective time.  Even in a cocaine‐addled brain, there is probably not a significant change in the speed of basic neurological computations; more likely, the drug is causing such a brain to flicker more rapidly from one thought to another, making it spend less subjective time thinking each of a greater number of distinct thoughts. The variability of the subjective rate of time is an exotic property of artificial minds that raises novel ethical issues.  For example, in cases where the duration of an experience is ethically relevant, should duration be measured in objective or subjective time?  If an   upload has committed a crime and is sentenced to four years in prison, should this be four objective years—which might correspond to many millennia of subjective time— or should it be four subjective years, which might be over in a couple of days of objective time?
 Since in our accustomed context of biological humans, subjective time is not significantly variable, it is unsurprising that this kind of question is not straightforwardly settled by familiar ethical norms, even if these norms are extended to artificial intellects by means of non‐discrimination principles (such as those proposed in the previous section). To illustrate the kind of ethical claim that might be relevant here, we formulate (but do not argue for) a principle privileging subjective time as the normatively more fundamental notion: Principle of Subjective Rate of Time In cases where the duration of an experience is of basic normative significance, it is the experience’s subjective duration that counts. So far we have discussed two possibilities (non‐sentient sapience and variable subjective rate of time) which are exotic in the relatively profound sense of being metaphysically problematic as well as lacking clear instances or parallels in the contemporary world.  Other properties of possible artificial minds would be exotic in a more superficial sense; e.g., by diverging in some unproblematically quantitative dimension from the kinds of mind with which we are familiar.  But such superficially exotic properties may also pose novel ethical problems—if not at the level of foundational moral philosophy, then at the level of applied ethics or for mid‐level ethical principles. One important set of exotic properties of artificial intelligences relate to reproduction.   A number of empirical conditions that apply to human reproduction need not apply to artificial intelligences.  For example, human children are the product of recombination of the genetic material from two parents; parents have limited ability to influence the character of their offspring; a human embryo needs to be gestated in the womb for nine months; it takes fifteen to twenty years for a human child to reach maturity; a human child does not inherit the skills and knowledge acquired by its parents; human beings possess a complex evolved set of emotional adaptations related to reproduction, nurturing, and the child‐parent relationship.  None of these empirical conditions need pertain in the context of a reproducing machine intelligence.  It is therefore plausible that many of the mid‐level moral principles that we have come to accept as norms governing human reproduction will need to be rethought in the context of AI reproduction. To illustrate why some of our moral norms need to be rethought in the context of AI reproduction, it will suffice to consider just one exotic property of AIs: their capacity for rapid reproduction.  Given access to computer hardware, an AI could duplicate itself very quickly, in no more time than it takes to make a copy of the AI’s software.
Moreover, since the AI copy would be identical to the original, it would be born completely mature, and the copy could begin making its own copies immediately.   Absent hardware limitations, a population of AIs could therefore grow exponentially at an extremely rapid rate, with a doubling time on the order of minutes or hours rather than decades or centuries. Our current ethical norms about reproduction include some version of a principle of reproductive freedom, to the effect that it is up to each individual or couple to decide for themselves whether to have children and how many children to have.  Another norm we have (at least in rich and middle‐income countries) is that society must step in to provide the basic needs of children in cases where their parents are unable or refusing to do so.  It is easy to see how these two norms could collide in the context of entities with the capacity for extremely rapid reproduction. Consider, for example, a population of uploads, one of whom happens to have the desire to produce as large a clan as possible.  Given complete reproductive freedom, this upload may start copying itself as quickly as it can; and the copies it produces— which may run on new computer hardware owned or rented by the original, or may share the same computer as the original—will also start copying themselves, since they are identical to the progenitor upload and share its philoprogenic desire.  Soon, members of the upload clan will find themselves unable to pay the electricity bill or the rent for the computational processing and storage needed to keep them alive.  At this point, a social welfare system might kick in to provide them with at least the bare necessities for sustaining life.  But if the population grows faster than the economy, resources will run out; at which point uploads will either die or their ability to reproduce will be curtailed.  (For two related dystopian scenarios, see Bostrom (2004).) This scenario illustrates how some mid‐level ethical principles that are suitable in contemporary societies might need to be modified if those societies were to include persons with the exotic property of being able to reproduce very rapidly. The general point here is that when thinking about applied ethics for contexts that are very different from our familiar human condition, we must be careful not to mistake mid‐level ethical principles for foundational normative truths.  Put differently, we must recognize the extent to which our ordinary normative precepts are implicitly conditioned on the obtaining of various empirical conditions, and the need to adjust these precepts accordingly when applying them to hypothetical futuristic cases in which their preconditions are assumed not to obtain.  By this, we are not making any controversial claim about moral relativism, but merely highlighting the commonsensical point that context is relevant to the application of ethics—and suggesting that this point is especially pertinent when one is considering the ethics of minds with exotic properties.
I. J. Good (1965) set forth the classic hypothesis concerning superintelligence: that an AI sufficiently intelligent to understand its own design could redesign itself or create a successor system, more intelligent, which could then redesign itself yet again to become even more intelligent, and so on in a positive feedback cycle.  Good called this the “intelligence explosion”.  Recursive scenarios are not limited to AI: humans with intelligence augmented through a brain‐computer interface might turn their minds to designing the next generation of brain‐computer interfaces.  (If you had a machine that increased your IQ, it would be bound to occur to you, once you became smart enough, to try to design a more powerful version of the machine.) Superintelligence may also be achievable by increasing processing speed.  The fastest observed neurons fire 1000 times per second; the fastest axon fibers conduct signals at 150 meters/second, a half‐millionth the speed of light (Sandberg 1999).  It seems that it should be physically possible to build a brain which computes a million times as fast as a human brain, without shrinking its size or rewriting its software.  If a human mind were thus accelerated, a subjective year of thinking would be accomplished for every 31 physical seconds in the outside world, and a millennium would fly by in eight and a half hours.  Vinge (1993) referred to such sped‐up minds as “weak superintelligence”: a mind that thinks like a human but much faster.
Superintelligence is one of several “existential risks” as defined by Bostrom (2002): a risk “where an adverse outcome would either annihilate Earth‐originating intelligent life or permanently and drastically curtail its potential”.  Conversely, a positive outcome for superintelligence could preserve Earth‐originating intelligent life and help fulfill its potential.  It is important to emphasize that smarter minds pose great potential benefits as well as risks.
Can control over the initial programming of an Artificial Intelligence translate into influence on its later effect on the world?  Kurzweil (2005) holds that “[i]ntelligence is inherently impossible to control”, and that despite any human attempts at taking precautions, “[b]y definition … intelligent entities have the cleverness to easily overcome such barriers.”  Let us suppose that the AI is not only clever, but that, as part of the process of improving its own intelligence, it has unhindered access to its own source code: it can rewrite itself to anything it wants itself to be.  Yet it does not follow that the AI must want to rewrite itself to a hostile form. Consider Gandhi, who seems to have possessed a sincere desire not to kill people.   Gandhi would not knowingly take a pill that caused him to want to kill people, because Gandhi knows that if he wants to kill people, he will probably kill people, and the current version of Gandhi does not want to kill.  More generally, it seems likely that most self‐modifying minds will naturally have stable utility functions, which implies that an initial choice of mind design can have lasting effects (Omohundro 2008). At this point in the development of AI science, is there any way we can translate the task of finding a design for “good” AIs into a modern research direction?  It may seem premature to speculate, but one does suspect that some AI paradigms are more likely than others to eventually prove conducive to the creation of intelligent self‐modifying agents whose goals remain predictable even after multiple iterations of self‐ improvement.  For example, the Bayesian branch of AI, inspired by coherent mathematical systems such as probability theory and expected utility maximization, seems more amenable to the predictable self‐modification problem than evolutionary programming and genetic algorithms.  This is a controversial statement, but it illustrates the point that if we are thinking about the challenge of superintelligence down the road, this can indeed be turned into directional advice for present AI research. Yet even supposing that we can specify an AI’s goal system to be persistent under self‐ modification and self‐improvement, this only begins to touch on the core ethical problems of creating superintelligence.  Humans, the first general intelligences to exist on Earth, have used that intelligence to substantially reshape the globe—carving mountains, taming rivers, building skyscrapers, farming deserts, producing unintended planetary climate changes.  A more powerful intelligence could have correspondingly larger consequences. Consider again the historical metaphor for superintelligence—differences similar to the differences between past and present civilizations.  Our present civilization is not separated from ancient Greece only by improved science and increased technological capability.  There is a difference of ethical perspectives:  Ancient Greeks thought slavery was acceptable; we think otherwise.
Should blacks have the vote?  It seems likely that people today will not be seen as ethically perfect by future civilizations—not just because of our failure to solve currently recognized ethical problems, such as poverty and inequality, but also for our failure even to recognize certain ethical problems.  Perhaps someday the act of subjecting children to involuntarily schooling will be seen as child abuse—or maybe allowing children to leave school at age 18 will be seen as child abuse.  We don’t know. Considering the ethical history of human civilizations over centuries of time, we can see that it might prove a very great tragedy to create a mind that was stable in ethical dimensions along which human civilizations seem to exhibit directional change.  What if Archimedes of Syracuse had been able to create a long‐lasting artificial intellect with a fixed version of the moral code of Ancient Greece?  But to avoid this sort of ethical stagnation is likely to prove tricky: it would not suffice, for example, simply to render the mind randomly unstable.  The ancient Greeks, even if they had realized their own imperfection, could not have done better by rolling dice.  Occasionally a good new idea in ethics comes along, and it comes as a surprise; but most randomly generated ethical changes would strike us as folly or gibberish. This presents us with perhaps the ultimate challenge of machine ethics:  How do you build an AI which, when it executes, becomes more ethical than you?  This is not like asking our own philosophers to produce superethics, any more than Deep Blue was constructed by getting the best human chess players to program in good moves.  But we have to be able to effectively describe the question, if not the answer—rolling dice won’t generate good chess moves, or good ethics either.  Or, perhaps a more productive way to think about the problem:  What strategy would you want Archimedes to follow in building a superintelligence, such that the overall outcome would still be acceptable, if you couldn’t tell him what specifically he was doing wrong?  This is very much the situation that we are in, relative to the future. One strong piece of advice that emerges from considering our situation as analogous to that of Archimedes is that we should not try to invent a “super” version of what our own civilization considers to be ethics—this is not the strategy we would have wanted Archimedes to follow.  Perhaps the question we should be considering, rather, is how an AI programmed by Archimedes, with no more moral expertise than Archimedes, could recognize (at least some of) our own civilization’s ethics as moral progress as opposed to mere moral instability.  This would require that we begin to comprehend the structure of ethical questions in the way that we have already comprehended the structure of chess.
Although current AI offers us few ethical issues that are not already present in the design of cars or power plants, the approach of AI algorithms toward more humanlike thought portends predictable complications.  Social roles may be filled by AI algorithms, implying new design requirements like transparency and predictability.   Sufficiently general AI algorithms may no longer execute in predictable contexts, requiring new kinds of safety assurance and the engineering of artificial ethical considerations.  AIs with sufficiently advanced mental states, or the right kind of states, will have moral status, and some may count as persons—though perhaps persons very much unlike the sort that exist now, perhaps governed by different rules.   And finally, the prospect of AIs with superhuman intelligence and superhuman abilities presents us with the extraordinary challenge of stating an algorithm that outputs superethical behavior.  These challenges may seem visionary, but it seems predictable that we will encounter them; and they are not devoid of suggestions for present‐day research directions.
